{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11bf63b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007985,
     "end_time": "2025-06-11T05:07:10.050946",
     "exception": false,
     "start_time": "2025-06-11T05:07:10.042961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3a3357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:10.066224Z",
     "iopub.status.busy": "2025-06-11T05:07:10.065959Z",
     "iopub.status.idle": "2025-06-11T05:07:16.314071Z",
     "shell.execute_reply": "2025-06-11T05:07:16.312877Z"
    },
    "papermill": {
     "duration": 6.257582,
     "end_time": "2025-06-11T05:07:16.315761",
     "exception": false,
     "start_time": "2025-06-11T05:07:10.058179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4338fa71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:16.332415Z",
     "iopub.status.busy": "2025-06-11T05:07:16.332191Z",
     "iopub.status.idle": "2025-06-11T05:07:17.499303Z",
     "shell.execute_reply": "2025-06-11T05:07:17.498139Z"
    },
    "papermill": {
     "duration": 1.176892,
     "end_time": "2025-06-11T05:07:17.500924",
     "exception": false,
     "start_time": "2025-06-11T05:07:16.324032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/depth-save.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1e4975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:17.517178Z",
     "iopub.status.busy": "2025-06-11T05:07:17.516916Z",
     "iopub.status.idle": "2025-06-11T05:07:17.521139Z",
     "shell.execute_reply": "2025-06-11T05:07:17.520403Z"
    },
    "papermill": {
     "duration": 0.01358,
     "end_time": "2025-06-11T05:07:17.522422",
     "exception": false,
     "start_time": "2025-06-11T05:07:17.508842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4844d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:17.538096Z",
     "iopub.status.busy": "2025-06-11T05:07:17.537899Z",
     "iopub.status.idle": "2025-06-11T05:07:17.766297Z",
     "shell.execute_reply": "2025-06-11T05:07:17.765066Z"
    },
    "papermill": {
     "duration": 0.238033,
     "end_time": "2025-06-11T05:07:17.768030",
     "exception": false,
     "start_time": "2025-06-11T05:07:17.529997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/tta_debug_vis\n",
    "!rm -rf /kaggle/working/visualization_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef6ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:17.785814Z",
     "iopub.status.busy": "2025-06-11T05:07:17.785563Z",
     "iopub.status.idle": "2025-06-11T05:07:42.653118Z",
     "shell.execute_reply": "2025-06-11T05:07:42.652221Z"
    },
    "papermill": {
     "duration": 24.878129,
     "end_time": "2025-06-11T05:07:42.654777",
     "exception": false,
     "start_time": "2025-06-11T05:07:17.776648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# from lightglue import DISK\n",
    "from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\n",
    "from scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint,DISK, DoGHardNet, LightGlue, SIFT\n",
    "from fastprogress import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a5c064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.672383Z",
     "iopub.status.busy": "2025-06-11T05:07:42.671846Z",
     "iopub.status.idle": "2025-06-11T05:07:42.675418Z",
     "shell.execute_reply": "2025-06-11T05:07:42.674702Z"
    },
    "papermill": {
     "duration": 0.013525,
     "end_time": "2025-06-11T05:07:42.676712",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.663187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7399c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.693466Z",
     "iopub.status.busy": "2025-06-11T05:07:42.693246Z",
     "iopub.status.idle": "2025-06-11T05:07:42.808080Z",
     "shell.execute_reply": "2025-06-11T05:07:42.807089Z"
    },
    "papermill": {
     "duration": 0.124659,
     "end_time": "2025-06-11T05:07:42.809535",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.684876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cca51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.826554Z",
     "iopub.status.busy": "2025-06-11T05:07:42.826308Z",
     "iopub.status.idle": "2025-06-11T05:07:42.830340Z",
     "shell.execute_reply": "2025-06-11T05:07:42.829670Z"
    },
    "papermill": {
     "duration": 0.013657,
     "end_time": "2025-06-11T05:07:42.831562",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.817905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b628e4cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.848351Z",
     "iopub.status.busy": "2025-06-11T05:07:42.848082Z",
     "iopub.status.idle": "2025-06-11T05:07:42.856759Z",
     "shell.execute_reply": "2025-06-11T05:07:42.856205Z"
    },
    "papermill": {
     "duration": 0.018355,
     "end_time": "2025-06-11T05:07:42.857988",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.839633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Set seed for reproducibility across random, numpy, torch (CPU + CUDA).\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad18f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.874400Z",
     "iopub.status.busy": "2025-06-11T05:07:42.874132Z",
     "iopub.status.idle": "2025-06-11T05:07:42.877327Z",
     "shell.execute_reply": "2025-06-11T05:07:42.876517Z"
    },
    "papermill": {
     "duration": 0.01271,
     "end_time": "2025-06-11T05:07:42.878560",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.865850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f001c9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.894766Z",
     "iopub.status.busy": "2025-06-11T05:07:42.894563Z",
     "iopub.status.idle": "2025-06-11T05:07:42.902268Z",
     "shell.execute_reply": "2025-06-11T05:07:42.901644Z"
    },
    "papermill": {
     "duration": 0.016889,
     "end_time": "2025-06-11T05:07:42.903366",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.886477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = True\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 8,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = False\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.05,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "    }\n",
    "\n",
    "    params_rot_detection_aliked_lightglue={\n",
    "        \"num_features\" : 4096,\n",
    "        \"resize_to\":960,\n",
    "        \"min_matches\":60,\n",
    "        \"min_inliers\":40\n",
    "        }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.3,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6676ab1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.919494Z",
     "iopub.status.busy": "2025-06-11T05:07:42.919292Z",
     "iopub.status.idle": "2025-06-11T05:07:42.922420Z",
     "shell.execute_reply": "2025-06-11T05:07:42.921839Z"
    },
    "papermill": {
     "duration": 0.01236,
     "end_time": "2025-06-11T05:07:42.923611",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.911251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Helper function for image loading (if not already defined) ---\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6d884d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.939883Z",
     "iopub.status.busy": "2025-06-11T05:07:42.939675Z",
     "iopub.status.idle": "2025-06-11T05:07:42.943594Z",
     "shell.execute_reply": "2025-06-11T05:07:42.942990Z"
    },
    "papermill": {
     "duration": 0.013349,
     "end_time": "2025-06-11T05:07:42.944753",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.931404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_cache_from_imglist(img_fnames, n_rotations=4, n_groups=5):\n",
    "    \"\"\"\n",
    "    Initializes a nested defaultdict cache structure for each image and rotation.\n",
    "    Structure: cache[image_key][rotation][index] = dict()\n",
    "    \"\"\"\n",
    "    cache = defaultdict(lambda: [[] for _ in range(n_rotations)])\n",
    "    for fname in img_fnames:\n",
    "        key = fname.split('/')[-1]  # use basename\n",
    "        for r in range(n_rotations):\n",
    "            cache[key][r] = [dict() for _ in range(n_groups)]\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c9c083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.961042Z",
     "iopub.status.busy": "2025-06-11T05:07:42.960840Z",
     "iopub.status.idle": "2025-06-11T05:07:42.970741Z",
     "shell.execute_reply": "2025-06-11T05:07:42.970087Z"
    },
    "papermill": {
     "duration": 0.019345,
     "end_time": "2025-06-11T05:07:42.971959",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.952614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class RotationEstimator:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = torch.device(device)\n",
    "        self.extractor = ALIKED(weights=f\"/kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth\", \n",
    "                                # detection_threshold=CONFIG.params_rot_detection_aliked_lightglue[\"detection_threshold\"],\n",
    "                                num_features=CONFIG.params_rot_detection_aliked_lightglue[\"num_features\"],\n",
    "                                min_matches=CONFIG.params_rot_detection_aliked_lightglue[\"min_matches\"]\n",
    "                               ).to(self.device, dtype = torch.float32).eval()\n",
    "        lg_cfg = {\n",
    "            \"features\": \"aliked\",\n",
    "            \"depth_confidence\": -1,         # 禁用深度置信度过滤\n",
    "            \"width_confidence\": -1,         # 禁用宽度置信度过滤\n",
    "            \"filter_threshold\": 0.1,        # 设置匹配分数阈值\n",
    "            \"mp\": True                     # 禁用多进程加速\n",
    "        }\n",
    "        self.lightglue = LightGlue(**lg_cfg).eval().to(self.device)\n",
    "        self.verbose = VERBOSE\n",
    "        \n",
    "    def extract(self, img):\n",
    "        with torch.inference_mode():\n",
    "            return self.extractor.extract(img, resize=CONFIG.params_rot_detection_aliked_lightglue[\"resize_to\"])\n",
    "\n",
    "    def match_and_filter(self, desc0, desc1, kpts0, kpts1):\n",
    "        data = {\n",
    "            \"image0\": {\"keypoints\": kpts0, \"descriptors\": desc0},\n",
    "            \"image1\": {\"keypoints\": kpts1, \"descriptors\": desc1},\n",
    "        }\n",
    "        with torch.inference_mode():\n",
    "            pred = self.lightglue(data)\n",
    "        matches0 = pred[\"matches0\"][0].cpu().numpy()\n",
    "        valid = matches0 > -1\n",
    "        if np.sum(valid) == 0:\n",
    "            return 0\n",
    "        pts0 = kpts0[0][valid].cpu().numpy()\n",
    "        pts1 = kpts1[0][matches0[valid]].cpu().numpy()\n",
    "        try:\n",
    "            _, inliers = cv2.findFundamentalMat(pts0, pts1, cv2.USAC_MAGSAC, 1, 0.9999, 50000)\n",
    "            return int(np.sum(inliers)) if inliers is not None else 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def run(self,img_fnames, index_pairs):\n",
    "        rot_dict = defaultdict(dict)\n",
    "        for idx1, idx2 in tqdm(index_pairs, desc=\"Finding valid rotations\"):\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n",
    "            try:\n",
    "                img0 = load_torch_image(fname1, device = self.device)\n",
    "                img1 = load_torch_image(fname2, device = self.device)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load image: {fname1}, {fname2}, reason: {e}\")\n",
    "                continue\n",
    "\n",
    "            desc0 = self.extract(img0)\n",
    "            kpts0, desc0 = desc0[\"keypoints\"], desc0[\"descriptors\"]\n",
    "\n",
    "            for rot in range(4):  # 0, 90, 180, 270 degrees\n",
    "                rotated_img1 = torch.rot90(img1, k=rot, dims=[2, 3])\n",
    "                desc1 = self.extract(rotated_img1)\n",
    "                kpts1, desc1 = desc1[\"keypoints\"], desc1[\"descriptors\"]\n",
    "\n",
    "                inliers = self.match_and_filter(desc0, desc1, kpts0, kpts1)\n",
    "                if inliers > CONFIG.params_rot_detection_aliked_lightglue['min_inliers']:\n",
    "                    rot_dict[key1][key2] = rot\n",
    "                    # if self.verbose:\n",
    "                    #     print(f\"matching {key1}-{key2} with {inliers} inliers!\")\n",
    "                    break\n",
    "        del self.extractor\n",
    "        gc.collect()\n",
    "        del self.lightglue\n",
    "        gc.collect()    \n",
    "        return dict(rot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3bb503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:42.988072Z",
     "iopub.status.busy": "2025-06-11T05:07:42.987871Z",
     "iopub.status.idle": "2025-06-11T05:07:42.995725Z",
     "shell.execute_reply": "2025-06-11T05:07:42.994943Z"
    },
    "papermill": {
     "duration": 0.017274,
     "end_time": "2025-06-11T05:07:42.996957",
     "exception": false,
     "start_time": "2025-06-11T05:07:42.979683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_and_save_keypoints_matches(\n",
    "    unique_kpts,\n",
    "    out_match,\n",
    "    rot_dict,\n",
    "    feature_dir\n",
    "):\n",
    "    # Step 1: Merge all keypoints per image (across rotations)\n",
    "    merged_kpts = {}\n",
    "    kpt_offset = {}\n",
    "\n",
    "    for img_name, rot_kpts_dict in unique_kpts.items():\n",
    "        merged = []\n",
    "        offset = {}\n",
    "        total = 0\n",
    "        for rot in sorted(rot_kpts_dict):  # rot 0,1,2,3\n",
    "            kpts = rot_kpts_dict[rot]\n",
    "            offset[rot] = total\n",
    "            total += len(kpts)\n",
    "            merged.append(kpts)\n",
    "        if total == 0:\n",
    "            continue\n",
    "        merged_kpts[img_name] = np.concatenate(merged, axis=0)\n",
    "        kpt_offset[img_name] = offset\n",
    "\n",
    "    # Step 2: Remap match indices based on offset\n",
    "    updated_out_match = defaultdict(dict)\n",
    "\n",
    "    for k1, match_group in out_match.items():\n",
    "        for k2, match_data in match_group.items():\n",
    "            matches = match_data[\"matches\"]\n",
    "            rot = match_data[\"rot\"]\n",
    "            if len(matches) == 0:\n",
    "                continue\n",
    "\n",
    "            offset0 = kpt_offset[k1][rot]\n",
    "            offset1 = kpt_offset[k2][0]  # image2 is always rot 0\n",
    "\n",
    "            updated_matches = matches.copy()\n",
    "            updated_matches[:, 0] += offset0\n",
    "            updated_matches[:, 1] += offset1\n",
    "            updated_out_match[k1][k2] = updated_matches.astype(np.int32)\n",
    "\n",
    "    unified_kp_path = f'{feature_dir}/keypoints.h5'\n",
    "    remapped_matches_path = f'{feature_dir}/matches.h5'\n",
    "    # Step 3: Save merged keypoints\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for img_name, kpts in merged_kpts.items():\n",
    "            f_kp[img_name] = kpts.astype(np.float32)  # shape (N, 2)\n",
    "            print(f\"{img_name}, features length{len(kpts)}\")\n",
    "\n",
    "    # Step 4: Save remapped matches\n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, match_group in updated_out_match.items():\n",
    "            g = f_match.require_group(k1)\n",
    "            for k2, matches in match_group.items():\n",
    "                if len(matches) > 0:\n",
    "                    g[k2] = matches  # shape (M, 2), dtype=int32\n",
    "                    print(f\"{k1}-{k2}, matches:{len(matches)}\")\n",
    "\n",
    "    # print(\"✔ Merged keypoints and remapped matches saved.\")\n",
    "    return unified_kp_path, remapped_matches_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ba7d74b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.013268Z",
     "iopub.status.busy": "2025-06-11T05:07:43.013024Z",
     "iopub.status.idle": "2025-06-11T05:07:43.020642Z",
     "shell.execute_reply": "2025-06-11T05:07:43.019828Z"
    },
    "papermill": {
     "duration": 0.017084,
     "end_time": "2025-06-11T05:07:43.021819",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.004735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def draw_one_match_pair(\n",
    "    img0_tensor, img1_tensor,\n",
    "    group_pred,\n",
    "    i0, i1,\n",
    "    key1, key2,\n",
    "    output_dir=\"tta_matches_vis\"\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def tensor_to_image(t):\n",
    "        img = t[0].detach().cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img * 255).clip(0, 255).astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    img0 = tensor_to_image(img0_tensor)\n",
    "    img1 = tensor_to_image(img1_tensor)\n",
    "\n",
    "    kpts0 = group_pred[\"keypoints0\"][0].detach().cpu().numpy()\n",
    "    kpts1 = group_pred[\"keypoints1\"][0].detach().cpu().numpy()\n",
    "    matches0 = group_pred[\"matches0\"][0].detach().cpu().numpy()\n",
    "\n",
    "    valid = matches0 > -1\n",
    "    if np.sum(valid) == 0:\n",
    "        print(f\"No matches for TTA ({i0}, {i1}) | {key1} vs. {key2}\")\n",
    "        return\n",
    "\n",
    "    match_pairs = np.stack([np.where(valid)[0], matches0[valid]], axis=1)\n",
    "\n",
    "    # Compose canvas\n",
    "    h0, w0 = img0.shape[:2]\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    canvas = np.zeros((max(h0, h1), w0 + w1, 3), dtype=np.uint8)\n",
    "    canvas[:h0, :w0] = img0\n",
    "    canvas[:h1, w0:] = img1\n",
    "\n",
    "    for pt0_idx, pt1_idx in match_pairs:\n",
    "        pt0 = tuple(np.round(kpts0[pt0_idx]).astype(int))\n",
    "        pt1 = tuple(np.round(kpts1[pt1_idx]).astype(int) + np.array([w0, 0]))\n",
    "        cv2.line(canvas, pt0, pt1, color=(0, 255, 0), thickness=1)\n",
    "\n",
    "    # Sanitize filenames to avoid slashes etc.\n",
    "    key1_safe = key1.replace(\"/\", \"_\")\n",
    "    key2_safe = key2.replace(\"/\", \"_\")\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"{key1_safe}_vs_{key2_safe}_TTA_{i0}_{i1}.jpg\")\n",
    "    cv2.imwrite(out_path, canvas)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1c48e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.037875Z",
     "iopub.status.busy": "2025-06-11T05:07:43.037666Z",
     "iopub.status.idle": "2025-06-11T05:07:43.040490Z",
     "shell.execute_reply": "2025-06-11T05:07:43.039881Z"
    },
    "papermill": {
     "duration": 0.011955,
     "end_time": "2025-06-11T05:07:43.041613",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.029658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path  # for creating output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39c9ec71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.057858Z",
     "iopub.status.busy": "2025-06-11T05:07:43.057649Z",
     "iopub.status.idle": "2025-06-11T05:07:43.074185Z",
     "shell.execute_reply": "2025-06-11T05:07:43.073550Z"
    },
    "papermill": {
     "duration": 0.026047,
     "end_time": "2025-06-11T05:07:43.075307",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.049260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGlueCustomMatching_sep(torch.nn.Module):\n",
    "    def __init__(self, device=None, extractor_cfg=None):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.extractor = ALIKED(weights=f\"/kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth\", \\\n",
    "                                **extractor_cfg).to(self.device, dtype = torch.float32).eval()\n",
    "        lg_cfg = {\n",
    "            \"features\": \"aliked\",\n",
    "            \"depth_confidence\": -1,         # 禁用深度置信度过滤\n",
    "            \"width_confidence\": -1,         # 禁用宽度置信度过滤\n",
    "            \"filter_threshold\": 0.0,        # 设置匹配分数阈值\n",
    "            \"mp\": False                     # 禁用多进程加速\n",
    "        }\n",
    "        self.lightglue = LightGlue(**lg_cfg).eval().to(self.device)\n",
    "        self.ttas = list(range(5))\n",
    "        self.tta2id = {k: i for i, k in enumerate(self.ttas)}\n",
    "        self.tta_combination = [[i, j] for i in range(5) for j in range(5)]\n",
    "\n",
    "    def forward_flat(self, data, cache_args):\n",
    "        # print(\"in forward\")\n",
    "        pred = {}        \n",
    "        cache, key1, key2, quad = cache_args # quad: Rotation times of image1. 0:No rotation, 1:90deg, 2:180deg, 3:270deg\n",
    "        img_key_list = [key1,key2]\n",
    "        data[\"image0\"] = {\"image\": data[\"image0\"]}\n",
    "        data[\"image1\"] = {\"image\": data[\"image1\"]}\n",
    "        keypoints_dict, descriptors_dict = {}, {}\n",
    "        for i, img_key in enumerate(['image0', 'image1']):\n",
    "            keypoints_list, descriptors_list = [], []\n",
    "            if i == 0:\n",
    "                _quad = 0\n",
    "            else:\n",
    "                _quad = quad\n",
    "            if \"pred\" not in cache[img_key_list[i]][_quad][0]:\n",
    "                no_cache = True\n",
    "            else:\n",
    "                no_cache = False\n",
    "            # Get ALIKED descriptors\n",
    "            for j, img in enumerate(data[img_key][\"image\"]):\n",
    "                if no_cache:\n",
    "                    img = img.unsqueeze(0)\n",
    "                    cache[img_key_list[i]][_quad][j][\"pred\"] = self.extractor.extract(img, resize=None)\n",
    "                    # print(f\"aliked extraction done {j}\")\n",
    "                pred = cache[img_key_list[i]][_quad][j][\"pred\"]\n",
    "                keypoints_list.append(pred['keypoints'])\n",
    "                descriptors_list.append(pred['descriptors'])\n",
    "            keypoints_dict[img_key] = keypoints_list\n",
    "            descriptors_dict[img_key] = descriptors_list\n",
    "        # print(\"get keypoints and descirptions\")\n",
    "        # Prepare data for LightGlue and run matching one by one\n",
    "        group_pred_list = []\n",
    "        for tta_group in self.tta_combination:\n",
    "            group_idx = self.tta2id[tta_group[0]], self.tta2id[tta_group[1]]\n",
    "            i0, i1 = group_idx[0], group_idx[1]\n",
    "            data[\"image0\"][\"keypoints\"], data[\"image0\"][\"descriptors\"]= keypoints_dict['image0'][i0], descriptors_dict['image0'][i0]\n",
    "            data[\"image1\"][\"keypoints\"], data[\"image1\"][\"descriptors\"]= keypoints_dict['image1'][i1], descriptors_dict['image1'][i1]\n",
    "            group_pred = self.lightglue(data)\n",
    "            group_pred.update({\"keypoints0\":data['image0'][\"keypoints\"],\n",
    "                                \"keypoints1\":data['image1'][\"keypoints\"]})\n",
    "            group_pred_list.append(group_pred)\n",
    "        # print(\"out forward\")\n",
    "        return group_pred_list, cache\n",
    "\n",
    "    def forward_flat_draw(self, data, cache_args):\n",
    "        output_dir = \"tta_debug_vis\"\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        group_pred_list = []\n",
    "        cache, key1, key2, quad = cache_args\n",
    "        img_key_list = [key1, key2]\n",
    "        data[\"image0\"] = {\"image\": data[\"image0\"]}\n",
    "        data[\"image1\"] = {\"image\": data[\"image1\"]}\n",
    "    \n",
    "        keypoints_dict, descriptors_dict = {}, {}\n",
    "        for i, img_key in enumerate([\"image0\", \"image1\"]):\n",
    "            keypoints_list, descriptors_list = [], []\n",
    "            _quad = 0 if i == 0 else quad\n",
    "            no_cache = \"pred\" not in cache[img_key_list[i]][_quad][0]\n",
    "            for j, img in enumerate(data[img_key][\"image\"]):\n",
    "                if no_cache:\n",
    "                    img = img.unsqueeze(0)\n",
    "                    cache[img_key_list[i]][_quad][j][\"pred\"] = self.extractor.extract(img, resize=None)\n",
    "                pred = cache[img_key_list[i]][_quad][j][\"pred\"]\n",
    "                keypoints_list.append(pred[\"keypoints\"])\n",
    "                descriptors_list.append(pred[\"descriptors\"])\n",
    "            keypoints_dict[img_key] = keypoints_list\n",
    "            descriptors_dict[img_key] = descriptors_list\n",
    "    \n",
    "        # Match and optionally visualize\n",
    "        for tta_group in self.tta_combination:\n",
    "            i0, i1 = self.tta2id[tta_group[0]], self.tta2id[tta_group[1]]\n",
    "            data[\"image0\"][\"keypoints\"] = keypoints_dict[\"image0\"][i0]\n",
    "            data[\"image0\"][\"descriptors\"] = descriptors_dict[\"image0\"][i0]\n",
    "            data[\"image1\"][\"keypoints\"] = keypoints_dict[\"image1\"][i1]\n",
    "            data[\"image1\"][\"descriptors\"] = descriptors_dict[\"image1\"][i1]\n",
    "    \n",
    "            group_pred = self.lightglue(data)\n",
    "            group_pred.update({\n",
    "                \"keypoints0\": data[\"image0\"][\"keypoints\"],\n",
    "                \"keypoints1\": data[\"image1\"][\"keypoints\"]\n",
    "            })\n",
    "            group_pred_list.append(group_pred)\n",
    "    \n",
    "            # Check match count and visualize if > 50\n",
    "            matches0 = group_pred[\"matches0\"][0].detach().cpu().numpy()\n",
    "            valid = matches0 > -1\n",
    "            num_matches = np.sum(valid)\n",
    "            if num_matches > 50 and key1 == 'et_et007.png' and key2 == 'et_et008.png':\n",
    "                draw_one_match_pair(\n",
    "                    img0_tensor=data[\"image0\"][\"image\"][i0].unsqueeze(0),\n",
    "                    img1_tensor=data[\"image1\"][\"image\"][i1].unsqueeze(0),\n",
    "                    group_pred=group_pred,\n",
    "                    i0=i0,\n",
    "                    i1=i1,\n",
    "                    key1=key1,\n",
    "                    key2=key2,\n",
    "                    output_dir=output_dir\n",
    "                )\n",
    "        return group_pred_list, cache\n",
    "\n",
    "\n",
    "# class LightGlueMatcherPipeline_sep:\n",
    "#     def __init__(self, device=None, conf_th=None, extractor_cfg=None, lg_cfg=None):\n",
    "#         self.device = device\n",
    "#         self.extractor_cfg = extractor_cfg\n",
    "#         self.lg_cfg = lg_cfg\n",
    "#         self._lightglue_matcher = LightGlueCustomMatching_sep(\n",
    "#             device=self.device, extractor_cfg=self.extractor_cfg)\n",
    "#         self.conf_thresh = conf_th\n",
    "#         self.tta_combination = self._lightglue_matcher.tta_combination\n",
    "\n",
    "#     def prep_img(self, img, long_side=None):\n",
    "#         \"\"\"Resize the tensor image to a specified long side.\"\"\"\n",
    "#         img = img.clone()\n",
    "#         if long_side is not None:\n",
    "#             scale = long_side / max(img.shape[2], img.shape[3])\n",
    "#             w = int(img.shape[3] * scale)\n",
    "#             h = int(img.shape[2] * scale)\n",
    "#             img = torch.nn.functional.interpolate(img, size=(h, w), mode='bilinear', align_corners=False)\n",
    "#         else:\n",
    "#             scale = 1.0\n",
    "#         return img, scale\n",
    "\n",
    "#     def split_image(self, image):\n",
    "#         \"\"\"Split the image into 4 quadrants and return them along with a resized version.\"\"\"\n",
    "#         h, w = image.shape[2], image.shape[3]\n",
    "#         if h % 2 != 0:\n",
    "#             h = h - 1\n",
    "#         if w % 2 != 0:\n",
    "#             w = w - 1\n",
    "#         image = image[:, :, :h, :w]\n",
    "#         return [image[:, :, :h//2, :w//2], \n",
    "#             image[:, :, :h//2, w//2:], \n",
    "#             image[:, :, h//2:, :w//2], \n",
    "#             image[:, :, h//2:, w//2:],\n",
    "#             transforms.functional.resize(image, size=(h//2,w//2))]\n",
    "\n",
    "#     def reconstruct_coords(self, coords, quadrant, w, h):\n",
    "#         \"\"\"Reconstruct coordinates based on the separation quadrant.\"\"\"\n",
    "#         if quadrant == 1:\n",
    "#             coords[:, 0] += w//2\n",
    "#         elif quadrant == 2:\n",
    "#             coords[:, 1] += h//2\n",
    "#         elif quadrant == 3:\n",
    "#             coords[:, 0] += w//2\n",
    "#             coords[:, 1] += h//2\n",
    "#         elif quadrant == 4:\n",
    "#             coords = [[y*2, x*2] for y, x in coords]            \n",
    "#         return coords\n",
    "\n",
    "#     def __call__(self, img_ts0, img_ts1, cache_args, input_longside=None):\n",
    "#         with torch.no_grad():\n",
    "#             img_ts0, scale0 = self.prep_img(img_ts0, input_longside)\n",
    "#             img_ts1, scale1 = self.prep_img(img_ts1, input_longside)\n",
    "#             img_parts0 = self.split_image(img_ts0) \n",
    "#             img_parts1 = self.split_image(img_ts1)\n",
    "#             cat_mkpts0, cat_mkpts1 = [], []\n",
    "#             pred, cache = self._lightglue_matcher.forward_flat(\n",
    "#                 data={\n",
    "#                     \"image0\": torch.cat(img_parts0),\n",
    "#                     \"image1\": torch.cat(img_parts1),\n",
    "#                 },\n",
    "#             cache_args=cache_args)\n",
    "#             # print(\"self._lightglue_matcher.forward_flat done\")\n",
    "#         kpts0_all, kpts1_all = [], []  # Reserve original keypoints\n",
    "#         cat_kpts0_all, cat_kpts1_all = [], []\n",
    "#         matched_kpts0, matched_kpts1 = [], []\n",
    "#         matched_ids = []\n",
    "#         for idx, [i0, i1] in enumerate(self.tta_combination):\n",
    "#             group_pred = pred[idx]\n",
    "#             pred_aug = {}\n",
    "#             use_keys = [\"keypoints0\", \"keypoints1\", \"matches0\", \"matching_scores0\"]\n",
    "#             for k in use_keys:\n",
    "#                 v = group_pred[k]\n",
    "#                 if isinstance(v, torch.Tensor):\n",
    "#                     pred_aug[k] = v[0].detach().cpu().numpy().squeeze()\n",
    "#                 else:\n",
    "#                     pred_aug[k] = v\n",
    "        \n",
    "#             kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n",
    "#             matches = pred_aug[\"matches0\"]\n",
    "#             valid = matches > -1\n",
    "        \n",
    "#             if np.sum(valid) == 0:\n",
    "#                 continue\n",
    "        \n",
    "#             kpts0_all.append(self.reconstruct_coords(kpts0, i0, img_ts0.shape[3], img_ts0.shape[2]))\n",
    "#             kpts1_all.append(self.reconstruct_coords(kpts1, i1, img_ts1.shape[3], img_ts1.shape[2]))\n",
    "\n",
    "#             # print(f\"cur_len1:{cur_len1}, cur_len2:{cur_len2}\")\n",
    "\n",
    "#             # Keep matched coords\n",
    "#             matched_kpts0.append(self.reconstruct_coords(kpts0[valid], i0, img_ts0.shape[3], img_ts0.shape[2]))\n",
    "#             matched_kpts1.append(self.reconstruct_coords(kpts1[matches[valid]], i1, img_ts1.shape[3], img_ts1.shape[2]))\n",
    "        \n",
    "#             # Keep index pairs\n",
    "#             matched_ids.append(np.stack([np.where(valid)[0], matches[valid]], axis=1))\n",
    "\n",
    "            \n",
    "#         if len(matched_kpts0) == 0:\n",
    "#             _, key1, key2, _ = cache_args\n",
    "#             print(f\"No matches at {key1} vs. {key2}\")\n",
    "#             return np.empty((0, 2)), np.empty((0, 2)), np.empty((0, 2), dtype=np.int32), cache\n",
    "        \n",
    "#         cat_mkpts0 = np.concatenate(matched_kpts0)\n",
    "#         cat_mkpts1 = np.concatenate(matched_kpts1)\n",
    "#         cat_kpts0_all = np.concatenate(kpts0_all) \n",
    "#         cat_kpts1_all = np.concatenate(kpts1_all) \n",
    "        \n",
    "#         matched_ids = np.concatenate(matched_ids).astype(np.int32)\n",
    "#         # print(\"before ransac\")\n",
    "#         # Apply RANSAC\n",
    "#         try:\n",
    "#             _, inliers = cv2.findFundamentalMat(cat_mkpts0, cat_mkpts1, cv2.USAC_MAGSAC, ransacReprojThreshold=5, confidence=0.9999, maxIters=50000)\n",
    "#             inliers = inliers.ravel() > 0\n",
    "#             cat_mkpts0 = cat_mkpts0[inliers]\n",
    "#             cat_mkpts1 = cat_mkpts1[inliers]\n",
    "#             matched_ids = matched_ids[inliers]\n",
    "#         except Exception:\n",
    "#             _, key1, key2, _ = cache_args\n",
    "#             print(f\"Error in findFundamentalMat: {key1}-{key2}\")\n",
    "#             return np.empty((0, 2)), np.empty((0, 2)), np.empty((0, 2), dtype=np.int32), cache\n",
    "        \n",
    "#         # Bounds check\n",
    "#         mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < img_ts0.shape[3]) & \\\n",
    "#                 (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < img_ts0.shape[2])\n",
    "#         mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < img_ts1.shape[3]) & \\\n",
    "#                 (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < img_ts1.shape[2])\n",
    "#         mask = mask0 & mask1\n",
    "        \n",
    "#         return cat_kpts0_all/scale0, \\\n",
    "#                cat_kpts1_all/scale1, \\\n",
    "#                matched_ids[mask], \\\n",
    "#                cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645f71a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.091498Z",
     "iopub.status.busy": "2025-06-11T05:07:43.091290Z",
     "iopub.status.idle": "2025-06-11T05:07:43.108083Z",
     "shell.execute_reply": "2025-06-11T05:07:43.107478Z"
    },
    "papermill": {
     "duration": 0.026187,
     "end_time": "2025-06-11T05:07:43.109183",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.082996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGlueMatcherPipeline_sep:\n",
    "    def __init__(self, device=None, conf_th=None, extractor_cfg=None, lg_cfg=None):\n",
    "        self.device = device\n",
    "        self.extractor_cfg = extractor_cfg\n",
    "        self.lg_cfg = lg_cfg\n",
    "        self._lightglue_matcher = LightGlueCustomMatching_sep(\n",
    "            device=self.device, extractor_cfg=self.extractor_cfg)\n",
    "        self.conf_thresh = conf_th\n",
    "        self.tta_combination = self._lightglue_matcher.tta_combination\n",
    "\n",
    "    def prep_img(self, img, long_side=None):\n",
    "        img = img.clone()\n",
    "        if long_side is not None:\n",
    "            scale = long_side / max(img.shape[2], img.shape[3])\n",
    "            h, w = int(img.shape[2] * scale), int(img.shape[3] * scale)\n",
    "            img = torch.nn.functional.interpolate(img, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        return img, scale\n",
    "\n",
    "    def split_image(self, image):\n",
    "        h, w = image.shape[2], image.shape[3]\n",
    "        h, w = h - h % 2, w - w % 2\n",
    "        image = image[:, :, :h, :w]\n",
    "        return [\n",
    "            image[:, :, :h//2, :w//2], \n",
    "            image[:, :, :h//2, w//2:], \n",
    "            image[:, :, h//2:, :w//2], \n",
    "            image[:, :, h//2:, w//2:],\n",
    "            transforms.functional.resize(image, size=(h//2, w//2))\n",
    "        ]\n",
    "\n",
    "    def reconstruct_coords(self, coords, quadrant, w, h):\n",
    "        coords = np.array(coords).copy()\n",
    "        if quadrant == 1:\n",
    "            coords[:, 0] += w // 2\n",
    "        elif quadrant == 2:\n",
    "            coords[:, 1] += h // 2\n",
    "        elif quadrant == 3:\n",
    "            coords[:, 0] += w // 2\n",
    "            coords[:, 1] += h // 2\n",
    "        elif quadrant == 4:\n",
    "            coords *= 2  # correct scale, no swap\n",
    "        return coords\n",
    "\n",
    "    def __call__(self, img_ts0, img_ts1, cache_args, input_longside=None):\n",
    "        with torch.no_grad():\n",
    "            img_ts0, scale0 = self.prep_img(img_ts0, input_longside)\n",
    "            img_ts1, scale1 = self.prep_img(img_ts1, input_longside)\n",
    "            img_parts0 = self.split_image(img_ts0) \n",
    "            img_parts1 = self.split_image(img_ts1)\n",
    "            pred, cache = self._lightglue_matcher.forward_flat(\n",
    "                data={\n",
    "                    \"image0\": torch.cat(img_parts0),\n",
    "                    \"image1\": torch.cat(img_parts1),\n",
    "                },\n",
    "                cache_args=cache_args\n",
    "            )\n",
    "\n",
    "        kpts0_all, kpts1_all = [], []\n",
    "        matched_kpts0, matched_kpts1 = [], []\n",
    "        matched_ids = []\n",
    "\n",
    "        offset0, offset1 = 0, 0\n",
    "\n",
    "        for idx, (i0, i1) in enumerate(self.tta_combination):\n",
    "            group_pred = pred[idx]\n",
    "            pred_aug = {}\n",
    "            for k in [\"keypoints0\", \"keypoints1\", \"matches0\", \"matching_scores0\"]:\n",
    "                v = group_pred[k]\n",
    "                pred_aug[k] = v[0].detach().cpu().numpy().squeeze() if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "            kpts0 = pred_aug[\"keypoints0\"]\n",
    "            kpts1 = pred_aug[\"keypoints1\"]\n",
    "            matches = pred_aug[\"matches0\"]\n",
    "            valid = matches > -1\n",
    "\n",
    "            if np.sum(valid) == 0:\n",
    "                continue\n",
    "\n",
    "            # Bound check to avoid out-of-index\n",
    "            target_idx = matches[valid]\n",
    "            valid_mask = target_idx < len(kpts1)\n",
    "            src_pts = kpts0[valid][valid_mask]\n",
    "            tgt_pts = kpts1[target_idx[valid_mask]]\n",
    "\n",
    "            matched_kpts0.append(self.reconstruct_coords(src_pts, i0, img_ts0.shape[3], img_ts0.shape[2]))\n",
    "            matched_kpts1.append(self.reconstruct_coords(tgt_pts, i1, img_ts1.shape[3], img_ts1.shape[2]))\n",
    "\n",
    "            kpts0_all.append(self.reconstruct_coords(kpts0, i0, img_ts0.shape[3], img_ts0.shape[2]))\n",
    "            kpts1_all.append(self.reconstruct_coords(kpts1, i1, img_ts1.shape[3], img_ts1.shape[2]))\n",
    "\n",
    "            matched_ids.append(np.stack([\n",
    "                np.where(valid)[0][valid_mask] + offset0,\n",
    "                matches[valid][valid_mask] + offset1\n",
    "            ], axis=1))\n",
    "\n",
    "            offset0 += len(kpts0)\n",
    "            offset1 += len(kpts1)\n",
    "\n",
    "        if not matched_kpts0:\n",
    "            _, key1, key2, _ = cache_args\n",
    "            print(f\"No matches at {key1} vs. {key2}\")\n",
    "            return np.empty((0, 2)), np.empty((0, 2)), np.empty((0, 2), dtype=np.int32), cache\n",
    "\n",
    "        cat_mkpts0 = np.concatenate(matched_kpts0)\n",
    "        cat_mkpts1 = np.concatenate(matched_kpts1)\n",
    "        cat_kpts0_all = np.concatenate(kpts0_all)\n",
    "        cat_kpts1_all = np.concatenate(kpts1_all)\n",
    "        match_counts = [len(match) for match in matched_ids]\n",
    "        matched_ids = np.concatenate(matched_ids).astype(np.int32)\n",
    "        _, key1, key2, _ = cache_args\n",
    "        # print(f\"before ransac the original number of key1 {key1}, key2{key2}: {len(cat_kpts0_all)} and {len(cat_kpts1_all)},\\\n",
    "        #       matches len is {len(matched_ids)}: {match_counts}\" )\n",
    "        \n",
    "        # RANSAC\n",
    "        try:\n",
    "            _, inliers = cv2.findFundamentalMat(\n",
    "                cat_mkpts0, cat_mkpts1,\n",
    "                cv2.USAC_MAGSAC, ransacReprojThreshold=5,\n",
    "                confidence=0.9999, maxIters=50000\n",
    "            )\n",
    "            inliers = (inliers.ravel() > 0) if inliers is not None else np.zeros(len(cat_mkpts0), dtype=bool)\n",
    "        except Exception:\n",
    "            _, key1, key2, _ = cache_args\n",
    "            print(f\"Error in findFundamentalMat: {key1}-{key2}\")\n",
    "            return np.empty((0, 2)), np.empty((0, 2)), np.empty((0, 2), dtype=np.int32), cache\n",
    "\n",
    "        # Bounds check after RANSAC\n",
    "        cat_mkpts0 = cat_mkpts0[inliers]\n",
    "        cat_mkpts1 = cat_mkpts1[inliers]\n",
    "        matched_ids = matched_ids[inliers]\n",
    "\n",
    "        h0, w0 = img_ts0.shape[2:]\n",
    "        h1, w1 = img_ts1.shape[2:]\n",
    "        mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < w0) & \\\n",
    "                (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < h0)\n",
    "        mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < w1) & \\\n",
    "                (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < h1)\n",
    "        mask = mask0 & mask1\n",
    "\n",
    "        # print(f\"after ransac the matches length is {len(matched_ids[mask])}\")\n",
    "\n",
    "        return cat_kpts0_all / scale0, cat_kpts1_all / scale1, matched_ids[mask], cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfd45b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.125101Z",
     "iopub.status.busy": "2025-06-11T05:07:43.124902Z",
     "iopub.status.idle": "2025-06-11T05:07:43.131897Z",
     "shell.execute_reply": "2025-06-11T05:07:43.131297Z"
    },
    "papermill": {
     "duration": 0.016211,
     "end_time": "2025-06-11T05:07:43.133057",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.116846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_keypoints_and_matches_split(\n",
    "    img_fnames, rot_dict, matcher_pip, feature_dir,\n",
    "    device = \"cuda\",\n",
    "    verbose=VERBOSE\n",
    "):\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    cache = init_cache_from_imglist(img_fnames)\n",
    "    # print(\"cache initializetion done\")\n",
    "    unique_kpts = {}\n",
    "    out_match = {}\n",
    "    for key1 in tqdm(rot_dict, desc=\"Matching and storing\"):\n",
    "        fname1 = next((f for f in img_fnames if os.path.basename(f) == key1), None)\n",
    "        if fname1 is None:\n",
    "            continue\n",
    "        img0 = load_torch_image(fname1, device = device)\n",
    "\n",
    "        for key2 in rot_dict[key1]:\n",
    "            fname2 = next((f for f in img_fnames if os.path.basename(f) == key2), None)\n",
    "            if fname2 is None:\n",
    "                continue\n",
    "            rot = rot_dict[key1][key2]\n",
    "            # print(f\"{key1}-{key2}-rot{rot}\")\n",
    "            img1 = load_torch_image(fname2, device = device)\n",
    "            with torch.inference_mode():\n",
    "                pts0, pts1, matches, cache = matcher_pip(\n",
    "                    img0, img1,\n",
    "                    cache_args=[cache, key1, key2, rot],\n",
    "                    input_longside = 1216\n",
    "                )\n",
    "            print(f\"{key1}_{rot}-{key2}:{len(matches)}\")\n",
    "\n",
    "            # Save keypoints once per image per rotation\n",
    "            if key1 not in unique_kpts and len(pts0) > 0:\n",
    "                unique_kpts[key1] = {}\n",
    "            if rot not in unique_kpts[key1] and len(pts0) > 0:\n",
    "                unique_kpts[key1][rot] = pts0.astype(np.float32)\n",
    "            \n",
    "            if key2 not in unique_kpts and len(pts1) > 0:\n",
    "                unique_kpts[key2] = {}\n",
    "            if 0 not in unique_kpts[key2] and len(pts1) > 0:\n",
    "                unique_kpts[key2][0] = pts1.astype(np.float32)\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                if key1 not in out_match:\n",
    "                    out_match[key1] = {}\n",
    "                out_match[key1][key2] = {\n",
    "                    \"rot\": rot,\n",
    "                    \"matches\": matches}\n",
    "    print(\"before merge_and_save_keypoints_matches\")\n",
    "    unified_kp_path, remapped_matches_path = merge_and_save_keypoints_matches(\n",
    "        unique_kpts,\n",
    "        out_match,\n",
    "        rot_dict,\n",
    "        feature_dir)\n",
    "    print(\"after merge_and_save_keypoints_matches\")\n",
    "    return unified_kp_path, remapped_matches_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0df72c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.149360Z",
     "iopub.status.busy": "2025-06-11T05:07:43.149140Z",
     "iopub.status.idle": "2025-06-11T05:07:43.163068Z",
     "shell.execute_reply": "2025-06-11T05:07:43.162446Z"
    },
    "papermill": {
     "duration": 0.0232,
     "end_time": "2025-06-11T05:07:43.164109",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.140909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume these are available from your environment or previous code\n",
    "# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n",
    "# from kornia.feature import ALIKED # Already in your detect_aliked\n",
    "# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n",
    "# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n",
    "# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n",
    "\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=VERBOSE\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    # extractor = extractor_class(max_num_keypoints=num_features, detection_threshold=detection_threshold, \n",
    "    #                             resize=resize_to).eval().to(device, dtype)\n",
    "    if model_name == 'disk':\n",
    "        # print(\"dissskkkkkk\")\n",
    "        # extractor = DISK(\n",
    "        #     max_num_keypoints=num_features,\n",
    "        #     detection_threshold=detection_threshold,\n",
    "        #     resize=resize_to\n",
    "        # ).to(device).eval()\n",
    "        # checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        # extractor.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "        extractor = DISK(\n",
    "            max_num_keypoints=num_features,\n",
    "            detection_threshold=detection_threshold,\n",
    "            resize=resize_to\n",
    "        ).eval().to(device)\n",
    "        \n",
    "        print(\"get extractor\")\n",
    "        \n",
    "    else:\n",
    "        extractor_class = dict_model[model_name]\n",
    "        extractor = extractor_class(\n",
    "            max_num_keypoints=num_features,\n",
    "            detection_threshold=detection_threshold,\n",
    "            resize=resize_to\n",
    "        ).to(device, dtype).eval()\n",
    "\n",
    "    \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    # print(\"KF glue matcher\")\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09b48da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.180329Z",
     "iopub.status.busy": "2025-06-11T05:07:43.180105Z",
     "iopub.status.idle": "2025-06-11T05:07:43.206875Z",
     "shell.execute_reply": "2025-06-11T05:07:43.206334Z"
    },
    "papermill": {
     "duration": 0.03609,
     "end_time": "2025-06-11T05:07:43.208003",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.171913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    verbose = VERBOSE\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                if verbose:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    if verbose:\n",
    "                        print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6fed92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.224166Z",
     "iopub.status.busy": "2025-06-11T05:07:43.223909Z",
     "iopub.status.idle": "2025-06-11T05:07:43.227633Z",
     "shell.execute_reply": "2025-06-11T05:07:43.226833Z"
    },
    "papermill": {
     "duration": 0.012952,
     "end_time": "2025-06-11T05:07:43.228830",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.215878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98508388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.244775Z",
     "iopub.status.busy": "2025-06-11T05:07:43.244549Z",
     "iopub.status.idle": "2025-06-11T05:07:43.250121Z",
     "shell.execute_reply": "2025-06-11T05:07:43.249335Z"
    },
    "papermill": {
     "duration": 0.01494,
     "end_time": "2025-06-11T05:07:43.251380",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.236440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37633148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.267744Z",
     "iopub.status.busy": "2025-06-11T05:07:43.267520Z",
     "iopub.status.idle": "2025-06-11T05:07:43.273015Z",
     "shell.execute_reply": "2025-06-11T05:07:43.272127Z"
    },
    "papermill": {
     "duration": 0.015198,
     "end_time": "2025-06-11T05:07:43.274379",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.259181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_pairs_shortlist_org(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 60,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    \n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 10000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c21f7e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.291213Z",
     "iopub.status.busy": "2025-06-11T05:07:43.290956Z",
     "iopub.status.idle": "2025-06-11T05:07:43.310460Z",
     "shell.execute_reply": "2025-06-11T05:07:43.309673Z"
    },
    "papermill": {
     "duration": 0.029303,
     "end_time": "2025-06-11T05:07:43.311636",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.282333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc4754e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.327982Z",
     "iopub.status.busy": "2025-06-11T05:07:43.327747Z",
     "iopub.status.idle": "2025-06-11T05:07:43.333718Z",
     "shell.execute_reply": "2025-06-11T05:07:43.333091Z"
    },
    "papermill": {
     "duration": 0.015367,
     "end_time": "2025-06-11T05:07:43.334845",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.319478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(feature_dir, img_dir):\n",
    "    result = {}\n",
    "    local_timings = {'RANSAC': [], 'Reconstruction': []}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(\"colmap database\")\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # options = pycolmap.SiftMatchingOptions()\n",
    "    # options.confidence = 0.9999\n",
    "    # options.max_num_trials = 20000\n",
    "    # pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    # print(\"matching done!!!!\")\n",
    "    local_timings['RANSAC'].append(time() - t)\n",
    "    print(f'RANSAC in {local_timings[\"RANSAC\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # mapper_options.mapper.filter_max_reproj_error\t = 10.0\n",
    "    mapper_options.min_model_size = 8\n",
    "    mapper_options.max_num_models = 25\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, \n",
    "                                        output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    for map_index, rec in maps.items():\n",
    "        result[map_index] = {}\n",
    "        for img_id, image in rec.images.items():\n",
    "            result[map_index][image.name] = {\n",
    "                'R': image.cam_from_world.rotation.matrix().tolist(),\n",
    "                't': image.cam_from_world.translation.tolist()\n",
    "            }\n",
    "    # clear_output(wait=False)\n",
    "    local_timings['Reconstruction'].append(time() - t)\n",
    "    print(f'Reconstruction done in {local_timings[\"Reconstruction\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    return result, local_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a747fd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.351030Z",
     "iopub.status.busy": "2025-06-11T05:07:43.350822Z",
     "iopub.status.idle": "2025-06-11T05:07:43.498761Z",
     "shell.execute_reply": "2025-06-11T05:07:43.497773Z"
    },
    "papermill": {
     "duration": 0.157423,
     "end_time": "2025-06-11T05:07:43.500073",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.342650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e82d91e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.516771Z",
     "iopub.status.busy": "2025-06-11T05:07:43.516555Z",
     "iopub.status.idle": "2025-06-11T05:07:43.532612Z",
     "shell.execute_reply": "2025-06-11T05:07:43.531985Z"
    },
    "papermill": {
     "duration": 0.025632,
     "end_time": "2025-06-11T05:07:43.533863",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.508231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_keypoints_and_matches(images_input, unified_kp_path, remapped_matches_path, feature_dir='visualization_output'):\n",
    "    output_dir = os.path.join(feature_dir, 'visualization_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load images and determine image_keys for HDF5 lookup\n",
    "    if isinstance(images_input[0], str):\n",
    "        loaded_images = [cv2.imread(img_path) for img_path in images_input]\n",
    "        image_keys = [os.path.basename(img_path) for img_path in images_input]\n",
    "    else:\n",
    "        loaded_images = images_input\n",
    "        # If images_input are already arrays, you need to provide the corresponding keys\n",
    "        # This part is crucial: image_keys MUST align with the HDF5 keys\n",
    "        image_keys = image_keys_in_h5 # Use the predefined list for the dummy case\n",
    "\n",
    "    # Load unified keypoints\n",
    "    keypoints_data = {}\n",
    "    with h5py.File(unified_kp_path, 'r') as f_kp:\n",
    "        for img_name_raw in f_kp.keys():\n",
    "            img_name = img_name_raw.decode('utf-8') if isinstance(img_name_raw, bytes) else img_name_raw\n",
    "            keypoints_data[img_name] = f_kp[img_name_raw][()] # Access with raw key if bytes\n",
    "\n",
    "    # Load remapped matches - CORRECTED LOGIC\n",
    "    # Store (img1_key, img2_key) directly with matches for robust iteration\n",
    "    matches_data_pairs = [] # Will store (img1_key, img2_key, matches_array)\n",
    "    with h5py.File(remapped_matches_path, 'r') as f_matches:\n",
    "        print(\"\\n--- Loading remapped matches from HDF5 ---\")\n",
    "        for img1_group_key_candidate in tqdm(f_matches.keys(), desc=\"Loading matches\"):\n",
    "            img1_key = img1_group_key_candidate.decode('utf-8') if isinstance(img1_group_key_candidate, bytes) else img1_group_key_candidate\n",
    "\n",
    "            img1_group = f_matches[img1_group_key_candidate] # Access with raw key\n",
    "\n",
    "            if isinstance(img1_group, h5py.Group):\n",
    "                for img2_dataset_key_candidate in img1_group.keys():\n",
    "                    img2_key = img2_dataset_key_candidate.decode('utf-8') if isinstance(img2_dataset_key_candidate, bytes) else img2_dataset_key_candidate\n",
    "\n",
    "                    try:\n",
    "                        matches_array = img1_group[img2_dataset_key_candidate][()]\n",
    "                        matches_data_pairs.append((img1_key, img2_key, matches_array))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading matches for pair ({img1_key}, {img2_key}): {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Expected '{img1_key}' to be a group, but found {type(img1_group)}. Skipping its contents.\")\n",
    "\n",
    "\n",
    "    # --- Drawing Keypoints ---\n",
    "    print(\"\\n--- Drawing Keypoints ---\")\n",
    "    for i, img_key in enumerate(image_keys):\n",
    "        if img_key in keypoints_data:\n",
    "            img = loaded_images[i].copy()\n",
    "            kpts = keypoints_data[img_key]\n",
    "\n",
    "            for kp in kpts:\n",
    "                x, y = int(kp[0]), int(kp[1])\n",
    "                cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Green circle for keypoint\n",
    "\n",
    "            output_kp_path = os.path.join(output_dir, f\"keypoints_{img_key}\")\n",
    "            if len(img.shape) == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(output_kp_path, img)\n",
    "            print(f\"Keypoints drawn on {img_key}, saved to {output_kp_path}\")\n",
    "        else:\n",
    "            print(f\"No keypoints found for {img_key} in unified keypoints file.\")\n",
    "\n",
    "    # --- Drawing Matches ---\n",
    "    print(\"\\n--- Drawing Matches ---\")\n",
    "    # Iterate through the (img1_key, img2_key, matches) tuples directly\n",
    "    for img_name1, img_name2, matches in matches_data_pairs:\n",
    "        # We no longer need to split img_pair_key, as we have img_name1 and img_name2 directly\n",
    "\n",
    "        # Find the actual image objects and their keypoints using image_keys list\n",
    "        try:\n",
    "            img1_idx = image_keys.index(img_name1)\n",
    "            img2_idx = image_keys.index(img_name2)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: One or both image names not found in the provided 'images' list/keys.\")\n",
    "            continue\n",
    "\n",
    "        img1 = loaded_images[img1_idx].copy()\n",
    "        img2 = loaded_images[img2_idx].copy()\n",
    "\n",
    "        kpts1 = keypoints_data.get(img_name1)\n",
    "        kpts2 = keypoints_data.get(img_name2)\n",
    "\n",
    "        if kpts1 is None or kpts2 is None:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: keypoints not found for one or both images in unified keypoints.\")\n",
    "            continue\n",
    "        if len(matches) == 0:\n",
    "            print(f\"No matches to draw for {img_name1}-{img_name2}.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure images are 3 channels for drawing lines\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "        if len(img2.shape) == 2:\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Create a concatenated image for drawing matches\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        max_h = max(h1, h2)\n",
    "        matched_img = np.zeros((max_h, w1 + w2, 3), dtype=np.uint8)\n",
    "        matched_img[0:h1, 0:w1] = img1\n",
    "        matched_img[0:h2, w1:w1+w2] = img2\n",
    "\n",
    "        num_matches_to_draw = min(len(matches), 600)\n",
    "        indices = np.linspace(0, len(matches) - 1, num_matches_to_draw, dtype=int)\n",
    "        \n",
    "        for i in indices:\n",
    "            match = matches[i]\n",
    "            kp1_idx, kp2_idx = int(match[0]), int(match[1])\n",
    "\n",
    "            # Bounds check for keypoint indices\n",
    "            if kp1_idx >= len(kpts1) or kp2_idx >= len(kpts2):\n",
    "                # print(f\"Warning: Match index out of bounds for {img_name1}-{img_name2}. Skipping match {kp1_idx}-{kp2_idx}.\")\n",
    "                continue\n",
    "\n",
    "            pt1 = tuple(map(int, kpts1[kp1_idx][:2]))\n",
    "            pt2 = tuple(map(int, kpts2[kp2_idx][:2]))\n",
    "\n",
    "            # Draw circles on the concatenated image\n",
    "            cv2.circle(matched_img, pt1, 5, (0, 0, 255), 2) # Red circle on img1 side\n",
    "            cv2.circle(matched_img, (pt2[0] + w1, pt2[1]), 5, (255, 0, 0), 2) # Blue circle on img2 side\n",
    "\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            cv2.line(matched_img, pt1, (pt2[0] + w1, pt2[1]), color, 1)\n",
    "\n",
    "        output_match_path = os.path.join(output_dir, f\"matches_{img_name1}_{img_name2}.png\")\n",
    "        cv2.imwrite(output_match_path, matched_img)\n",
    "        print(f\"Matches drawn between {img_name1} and {img_name2}, saved to {output_match_path}\")\n",
    "\n",
    "\n",
    "# Example call (replace with your actual 'images' list)\n",
    "# If your 'images' are file paths:\n",
    "# images_file_paths = ['path/to/your/image1.jpg', 'path/to/your/image2.jpg', ...]\n",
    "# draw_keypoints_and_matches(images_file_paths, unified_kp_path, remapped_matches_path)\n",
    "\n",
    "# If your 'images' are loaded numpy arrays (as in the dummy example above):\n",
    "# draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18a5424d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:07:43.550296Z",
     "iopub.status.busy": "2025-06-11T05:07:43.550056Z",
     "iopub.status.idle": "2025-06-11T05:10:19.035419Z",
     "shell.execute_reply": "2025-06-11T05:10:19.034319Z"
    },
    "papermill": {
     "duration": 155.495049,
     "end_time": "2025-06-11T05:10:19.036882",
     "exception": false,
     "start_time": "2025-06-11T05:07:43.541833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model for patch feature extraction...\n",
      "DINOv2 model loaded.\n",
      "CONFIG.ROTATION_CORRECTION: True\n",
      "Skipping \"imc2023_haiper\"\n",
      "Skipping \"imc2023_heritage\"\n",
      "Skipping \"imc2023_theather_imc2024_church\"\n",
      "Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Skipping \"imc2024_lizard_pond\"\n",
      "Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Skipping \"pt_stpeters_stpauls\"\n",
      "Skipping \"amy_gardens\"\n",
      "Skipping \"fbk_vineyard\"\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisting. Number of pairs to match: 231. Done in 2.9082 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/aliked_lightglue.pth\" to /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv.pth\n",
      "100%|██████████| 45.4M/45.4M [00:01<00:00, 41.3MB/s]\n",
      "Finding valid rotations: 100%|██████████| 231/231 [00:54<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotation_detection for 22 images : 56.7305 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et007.png_3-et_et003.png:186\n",
      "et_et007.png_0-et_et006.png:3581\n",
      "et_et007.png_0-et_et001.png:599\n",
      "et_et007.png_0-et_et004.png:722\n",
      "et_et007.png_0-et_et002.png:962\n",
      "et_et007.png_0-et_et008.png:2584\n",
      "et_et007.png_0-et_et005.png:3356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:   6%|▋         | 1/16 [00:06<01:33,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et007.png_0-et_et000.png:303\n",
      "et_et003.png_0-et_et006.png:198\n",
      "et_et003.png_0-et_et001.png:3159\n",
      "et_et003.png_0-et_et004.png:2552\n",
      "et_et003.png_0-et_et002.png:2224\n",
      "et_et003.png_1-et_et008.png:155\n",
      "et_et003.png_0-et_et005.png:258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  12%|█▎        | 2/16 [00:11<01:16,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et003.png_0-et_et000.png:4625\n",
      "et_et006.png_0-et_et001.png:603\n",
      "et_et006.png_0-et_et004.png:396\n",
      "et_et006.png_0-et_et002.png:1111\n",
      "et_et006.png_0-et_et008.png:2058\n",
      "et_et006.png_0-et_et005.png:3305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  19%|█▉        | 3/16 [00:15<01:02,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et006.png_0-et_et000.png:386\n",
      "et_et001.png_0-et_et004.png:3295\n",
      "et_et001.png_0-et_et002.png:3739\n",
      "et_et001.png_0-et_et008.png:593\n",
      "et_et001.png_0-et_et005.png:667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  25%|██▌       | 4/16 [00:18<00:51,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et001.png_0-et_et000.png:3721\n",
      "et_et004.png_0-et_et002.png:2400\n",
      "et_et004.png_0-et_et008.png:541\n",
      "et_et004.png_0-et_et005.png:574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  31%|███▏      | 5/16 [00:21<00:41,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et004.png_0-et_et000.png:2841\n",
      "et_et002.png_0-et_et008.png:617\n",
      "et_et002.png_0-et_et005.png:988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  38%|███▊      | 6/16 [00:23<00:32,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et002.png_0-et_et000.png:2686\n",
      "et_et008.png_0-et_et005.png:2674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  44%|████▍     | 7/16 [00:24<00:23,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et008.png_0-et_et000.png:374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  50%|█████     | 8/16 [00:25<00:16,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et_et005.png_0-et_et000.png:413\n",
      "another_et_another_et006.png_0-another_et_another_et002.png:1182\n",
      "another_et_another_et006.png_0-another_et_another_et004.png:951\n",
      "another_et_another_et006.png_0-another_et_another_et007.png:1152\n",
      "another_et_another_et006.png_0-another_et_another_et008.png:742\n",
      "another_et_another_et006.png_0-another_et_another_et003.png:802\n",
      "another_et_another_et006.png_0-another_et_another_et005.png:1167\n",
      "another_et_another_et006.png_0-another_et_another_et001.png:1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  56%|█████▋    | 9/16 [00:31<00:21,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et006.png_0-another_et_another_et009.png:204\n",
      "another_et_another_et002.png_0-another_et_another_et004.png:1943\n",
      "another_et_another_et002.png_0-another_et_another_et007.png:819\n",
      "another_et_another_et002.png_0-another_et_another_et008.png:226\n",
      "another_et_another_et002.png_0-another_et_another_et003.png:1316\n",
      "another_et_another_et002.png_0-another_et_another_et005.png:1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  62%|██████▎   | 10/16 [00:34<00:19,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et002.png_0-another_et_another_et001.png:2602\n",
      "another_et_another_et010.png_0-another_et_another_et007.png:287\n",
      "another_et_another_et010.png_0-another_et_another_et008.png:675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  69%|██████▉   | 11/16 [00:36<00:14,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et010.png_0-another_et_another_et009.png:743\n",
      "another_et_another_et004.png_0-another_et_another_et007.png:591\n",
      "another_et_another_et004.png_0-another_et_another_et008.png:313\n",
      "another_et_another_et004.png_0-another_et_another_et003.png:1391\n",
      "another_et_another_et004.png_0-another_et_another_et005.png:1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  75%|███████▌  | 12/16 [00:39<00:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et004.png_0-another_et_another_et001.png:1948\n",
      "another_et_another_et007.png_0-another_et_another_et008.png:1100\n",
      "another_et_another_et007.png_0-another_et_another_et003.png:539\n",
      "another_et_another_et007.png_0-another_et_another_et005.png:494\n",
      "another_et_another_et007.png_0-another_et_another_et001.png:702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  81%|████████▏ | 13/16 [00:42<00:08,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et007.png_0-another_et_another_et009.png:495\n",
      "another_et_another_et008.png_0-another_et_another_et001.png:223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  88%|████████▊ | 14/16 [00:44<00:04,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et008.png_0-another_et_another_et009.png:1063\n",
      "another_et_another_et003.png_0-another_et_another_et005.png:1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing:  94%|█████████▍| 15/16 [00:45<00:02,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et003.png_0-another_et_another_et001.png:1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching and storing: 100%|██████████| 16/16 [00:46<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et005.png_0-another_et_another_et001.png:2123\n",
      "before merge_and_save_keypoints_matches\n",
      "et_et007.png, features length33070\n",
      "et_et003.png, features length38290\n",
      "et_et006.png, features length15415\n",
      "et_et001.png, features length21110\n",
      "et_et004.png, features length22205\n",
      "et_et002.png, features length18860\n",
      "et_et008.png, features length19300\n",
      "et_et005.png, features length17125\n",
      "et_et000.png, features length22070\n",
      "another_et_another_et006.png, features length9675\n",
      "another_et_another_et002.png, features length10885\n",
      "another_et_another_et004.png, features length10125\n",
      "another_et_another_et007.png, features length10310\n",
      "another_et_another_et008.png, features length10640\n",
      "another_et_another_et003.png, features length9965\n",
      "another_et_another_et005.png, features length10320\n",
      "another_et_another_et001.png, features length12395\n",
      "another_et_another_et009.png, features length8680\n",
      "another_et_another_et010.png, features length7850\n",
      "et_et007.png-et_et003.png, matches:186\n",
      "et_et007.png-et_et006.png, matches:3581\n",
      "et_et007.png-et_et001.png, matches:599\n",
      "et_et007.png-et_et004.png, matches:722\n",
      "et_et007.png-et_et002.png, matches:962\n",
      "et_et007.png-et_et008.png, matches:2584\n",
      "et_et007.png-et_et005.png, matches:3356\n",
      "et_et007.png-et_et000.png, matches:303\n",
      "et_et003.png-et_et006.png, matches:198\n",
      "et_et003.png-et_et001.png, matches:3159\n",
      "et_et003.png-et_et004.png, matches:2552\n",
      "et_et003.png-et_et002.png, matches:2224\n",
      "et_et003.png-et_et008.png, matches:155\n",
      "et_et003.png-et_et005.png, matches:258\n",
      "et_et003.png-et_et000.png, matches:4625\n",
      "et_et006.png-et_et001.png, matches:603\n",
      "et_et006.png-et_et004.png, matches:396\n",
      "et_et006.png-et_et002.png, matches:1111\n",
      "et_et006.png-et_et008.png, matches:2058\n",
      "et_et006.png-et_et005.png, matches:3305\n",
      "et_et006.png-et_et000.png, matches:386\n",
      "et_et001.png-et_et004.png, matches:3295\n",
      "et_et001.png-et_et002.png, matches:3739\n",
      "et_et001.png-et_et008.png, matches:593\n",
      "et_et001.png-et_et005.png, matches:667\n",
      "et_et001.png-et_et000.png, matches:3721\n",
      "et_et004.png-et_et002.png, matches:2400\n",
      "et_et004.png-et_et008.png, matches:541\n",
      "et_et004.png-et_et005.png, matches:574\n",
      "et_et004.png-et_et000.png, matches:2841\n",
      "et_et002.png-et_et008.png, matches:617\n",
      "et_et002.png-et_et005.png, matches:988\n",
      "et_et002.png-et_et000.png, matches:2686\n",
      "et_et008.png-et_et005.png, matches:2674\n",
      "et_et008.png-et_et000.png, matches:374\n",
      "et_et005.png-et_et000.png, matches:413\n",
      "another_et_another_et006.png-another_et_another_et002.png, matches:1182\n",
      "another_et_another_et006.png-another_et_another_et004.png, matches:951\n",
      "another_et_another_et006.png-another_et_another_et007.png, matches:1152\n",
      "another_et_another_et006.png-another_et_another_et008.png, matches:742\n",
      "another_et_another_et006.png-another_et_another_et003.png, matches:802\n",
      "another_et_another_et006.png-another_et_another_et005.png, matches:1167\n",
      "another_et_another_et006.png-another_et_another_et001.png, matches:1127\n",
      "another_et_another_et006.png-another_et_another_et009.png, matches:204\n",
      "another_et_another_et002.png-another_et_another_et004.png, matches:1943\n",
      "another_et_another_et002.png-another_et_another_et007.png, matches:819\n",
      "another_et_another_et002.png-another_et_another_et008.png, matches:226\n",
      "another_et_another_et002.png-another_et_another_et003.png, matches:1316\n",
      "another_et_another_et002.png-another_et_another_et005.png, matches:1854\n",
      "another_et_another_et002.png-another_et_another_et001.png, matches:2602\n",
      "another_et_another_et010.png-another_et_another_et007.png, matches:287\n",
      "another_et_another_et010.png-another_et_another_et008.png, matches:675\n",
      "another_et_another_et010.png-another_et_another_et009.png, matches:743\n",
      "another_et_another_et004.png-another_et_another_et007.png, matches:591\n",
      "another_et_another_et004.png-another_et_another_et008.png, matches:313\n",
      "another_et_another_et004.png-another_et_another_et003.png, matches:1391\n",
      "another_et_another_et004.png-another_et_another_et005.png, matches:1904\n",
      "another_et_another_et004.png-another_et_another_et001.png, matches:1948\n",
      "another_et_another_et007.png-another_et_another_et008.png, matches:1100\n",
      "another_et_another_et007.png-another_et_another_et003.png, matches:539\n",
      "another_et_another_et007.png-another_et_another_et005.png, matches:494\n",
      "another_et_another_et007.png-another_et_another_et001.png, matches:702\n",
      "another_et_another_et007.png-another_et_another_et009.png, matches:495\n",
      "another_et_another_et008.png-another_et_another_et001.png, matches:223\n",
      "another_et_another_et008.png-another_et_another_et009.png, matches:1063\n",
      "another_et_another_et003.png-another_et_another_et005.png, matches:1134\n",
      "another_et_another_et003.png-another_et_another_et001.png, matches:1309\n",
      "another_et_another_et005.png-another_et_another_et001.png, matches:2123\n",
      "after merge_and_save_keypoints_matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local feature extracting and matching. Done in 46.5854 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 67.03it/s]\n",
      " 57%|█████▋    | 68/120 [00:00<00:00, 3308.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "RANSAC in 26.4009 sec\n",
      "{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=5147, num_observations=17897), 1: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=3095, num_observations=11323)}\n",
      "Reconstruction done in 14.9995 sec\n",
      "{0: {'et_et000.png': {'R': [[0.47514995139000193, 0.38393469583763706, -0.7917238615995731], [-0.37989390325371464, 0.901112686456712, 0.20898935039236963], [0.7936706785340731, 0.201469788402203, 0.5740180993631084]], 't': [3.7848348671391556, -1.5454003491313557, 1.2491744484553182]}, 'et_et001.png': {'R': [[0.6503766620225196, 0.4413334233035666, -0.6182515725589383], [-0.3682422480577865, 0.8950533782829568, 0.2515493923061906], [0.6643853130840512, 0.06406449484247162, 0.7446394404399931]], 't': [1.8468486160710371, -1.7749852063213418, 1.5578070010702918]}, 'et_et002.png': {'R': [[0.8128944378427339, 0.30106731249395746, -0.4985590298771165], [-0.24992264408249906, 0.9535245887164195, 0.16831378638719835], [0.5260620732377105, -0.012220149788309029, 0.8503583732991598]], 't': [0.8266218686691496, -1.3915337756190993, 1.579302768533998]}, 'et_et003.png': {'R': [[0.425158121876294, 0.32662000656217816, -0.8441326570604558], [-0.32189944020785255, 0.9262109711983196, 0.19624980821834398], [0.8459440417642846, 0.18828862988823386, 0.498924914248946]], 't': [4.145282336808255, -1.5918111281294949, 1.6435709025106715]}, 'et_et004.png': {'R': [[0.7433791715586678, 0.20281283999829455, -0.637380858847028], [-0.40165822369936854, 0.8973342739358781, -0.1829258651876181], [0.5348439759662782, 0.3919925417042993, 0.7485210542267909]], 't': [1.9416618446739755, 1.5993406944341033, 0.5923137065603237]}, 'et_et005.png': {'R': [[0.9826056743324993, -0.03031989703324977, 0.18321242483376357], [0.062449657935470755, 0.9830732661401936, -0.17224109156700856], [-0.17488890471758642, 0.1806866271873629, 0.9678668367923027]], 't': [-3.9590537926051974, -0.2796502484354824, 1.248741755509124]}, 'et_et006.png': {'R': [[0.988674653023556, 0.05376054550276918, 0.14011507490629263], [-0.060040500365887345, 0.9973543032011397, 0.04098209611504804], [-0.1375411530581491, -0.04893053886290074, 0.9892867297108665]], 't': [-3.764177943674625, -1.3375325026383649, 1.5898573290547593]}, 'et_et007.png': {'R': [[0.9811955452958546, 0.00317881386497314, 0.19299014750494975], [-0.001097862536013419, 0.9999401140089188, -0.010888668131708126], [-0.1930132031479257, 0.010472036012271561, 0.9811402753288315]], 't': [-4.544631524100022, -0.8247709631234641, 2.362184661718816]}, 'et_et008.png': {'R': [[0.884309989774455, -0.24714154425115042, 0.3961273773551382], [0.301893030821332, 0.9498666518901929, -0.08132614320392484], [-0.35616911703095994, 0.19150561540610678, 0.9145868790559565]], 't': [-5.9056684296620565, -2.162648238235896, 2.85687570855085]}}, 1: {'another_et_another_et001.png': {'R': [[0.9152924424629177, -0.22088808076801694, 0.33682072463684376], [0.18550376781739406, 0.9734256616429834, 0.13427893870771546], [-0.35753055379340215, -0.060422984281993844, 0.931944722649708]], 't': [-2.7544539449595655, -1.8460370227075285, 2.9152724711178344]}, 'another_et_another_et002.png': {'R': [[0.9209444578090785, -0.21839786746002912, 0.3227439807644039], [0.19297989960269243, 0.9751091923587566, 0.10918251383251895], [-0.3385558506067753, -0.03826792999843342, 0.9401678049973621]], 't': [-2.6153953594533914, -1.131255932255873, 1.4694645702610118]}, 'another_et_another_et003.png': {'R': [[0.8939633892267849, -0.25974452255214037, 0.365187953972089], [0.25445887995349786, 0.9649989660378564, 0.06346395795000107], [-0.36889041344953827, 0.03619086279751816, 0.9287680465622189]], 't': [-2.783899917643011, 0.24870767308863187, -0.1912223420238928]}, 'another_et_another_et004.png': {'R': [[0.9059588032274509, -0.22878325468991945, 0.35622586827485214], [0.15032137625307684, 0.9604182124267107, 0.23452151517603348], [-0.3957804071654707, -0.15891846844394475, 0.9044903480365974]], 't': [-2.7073086441704115, -1.334383571293629, 0.23878092806183146]}, 'another_et_another_et005.png': {'R': [[0.8841791115819306, -0.21620522272140558, 0.41410457653855415], [0.15813945978534866, 0.9726433458153526, 0.1701670740769627], [-0.43956707099256875, -0.08497189834941794, 0.8941815065130291]], 't': [-3.1784805973151973, -1.9913289825444163, 1.5893156679532368]}, 'another_et_another_et006.png': {'R': [[0.999991376977682, 0.002119992117104495, -0.003570938770535701], [-0.0021210375192398157, 0.9999977088393602, -0.0002889911281777147], [0.0035703179300276924, 0.000296562731311981, 0.9999935824196198]], 't': [-0.7932588023133424, -0.6995179363666353, 1.2559373308082045]}, 'another_et_another_et007.png': {'R': [[0.9602508768965661, 0.10975463205902719, -0.25665575029787163], [-0.10861016786001759, 0.9939089833201304, 0.018675232608015808], [0.25714214912533234, 0.00994251563399607, 0.9663224417998751]], 't': [0.7346587430345539, -0.39633498240433845, 0.7621983521417905]}, 'another_et_another_et008.png': {'R': [[0.8384148841333019, 0.2082940491120211, -0.5036606706585903], [-0.2125488021575661, 0.9758822234800361, 0.04976838953637994], [0.5018799545360683, 0.06532591369571474, 0.8624668319620727]], 't': [2.2556902767632265, -0.6485354252741558, 1.382746625401903]}, 'another_et_another_et009.png': {'R': [[0.6559548786926845, 0.29796684421917624, -0.6934976257099449], [-0.3359790922135754, 0.9380052067533455, 0.0852307555930088], [0.6759003230614758, 0.17709317279239917, 0.7153997214395]], 't': [3.765064237383452, -0.8960584368851825, 1.9322543284000695]}, 'another_et_another_et010.png': {'R': [[0.20036927424353523, 0.42392328918221067, -0.8832560211105583], [-0.5114131555957864, 0.8142210073946966, 0.27477397147605787], [0.8356486930658596, 0.3966524877094575, 0.37994481937673485]], 't': [4.949246727675052, -1.790801957880266, 3.81314881168277]}}}\n",
      "Dataset  ETs -> Registered 19 / 22 images with 2 clusters\n",
      "Skipping \"stairs\"\n",
      "\n",
      "Results\n",
      "Dataset  ETs -> Registered 19 / 22 images with 2 clusters\n",
      "\n",
      "Timings\n",
      "rotation_detection -> total=56.73 sec.\n",
      "shortlisting -> total=2.91 sec.\n",
      "feature_matching -> total=46.28 sec.\n",
      "RANSAC -> total=26.40 sec.\n",
      "Reconstruction -> total=15.00 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t# 'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    'rotation_detection':[],\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "# Load DINOv2 model (for feature extraction, not global descriptor here)\n",
    "print(\"Loading DINOv2 model for patch feature extraction...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = dino_model.eval().to(device)\n",
    "print(\"DINOv2 model loaded.\")\n",
    "\n",
    "print(f\"CONFIG.ROTATION_CORRECTION: {CONFIG.ROTATION_CORRECTION}\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    # print (f\"Extracting on device {device}\")\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "        # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "        try:\n",
    "            # --- Pipeline Execution ---\n",
    "            \n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            # 1. Detect ALIKED features and combine with DINO patch features\n",
    "            t = time()\n",
    "            index_pairs = get_image_pairs_shortlist_org(\n",
    "                images,\n",
    "                sim_th = 0.2, # should be strict\n",
    "                min_pairs = 60, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "                exhaustive_if_less = 20,\n",
    "                device=device\n",
    "            )\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "\n",
    "            rotation_estimator = RotationEstimator(device=device)\n",
    "            rot_dict = rotation_estimator.run(images, index_pairs)\n",
    "            # print(rot_dict)\n",
    "\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print(f'rotation_detection for {len(images)} images : {t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################    \n",
    "            t=time()\n",
    "\n",
    "            lightglue_25groups_matcher_pipeline = LightGlueMatcherPipeline_sep(\n",
    "                extractor_cfg = {\"max_num_keypoints\": 4096, \"detection_threshold\":0.1}, \n",
    "                device= device)\n",
    "            # print(\"lightglue_25groups_matcher_pipeline initialization done\")            \n",
    "            unified_kp_path, remapped_matches_path = save_keypoints_and_matches_split(\n",
    "                img_fnames = images,\n",
    "                rot_dict = rot_dict,\n",
    "                matcher_pip = lightglue_25groups_matcher_pipeline,\n",
    "                feature_dir = feature_dir,\n",
    "                device = device)\n",
    "            \n",
    "            timings['feature_matching'].append(time() - t)\n",
    "            gc.collect()\n",
    "            print (f'Local feature extracting and matching. Done in {time() - t:.4f} sec')\n",
    "            # draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            future = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                feature_dir, images_dir)\n",
    "            maps, local_timings = future.result()\n",
    "            # 合并 timings（主进程里）\n",
    "            for k in local_timings:\n",
    "                timings[k].extend(local_timings[k])\n",
    "            # clear_output(wait=False)\n",
    "            print(maps)\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():  # cur_map: image_name → {'R': list, 't': list}\n",
    "                for image_name, pose in cur_map.items():\n",
    "                    idx = filename_to_index[image_name]\n",
    "                    pred = predictions[idx]\n",
    "                    pred.cluster_index = map_index\n",
    "                    pred.rotation = np.array(pose['R'])  # convert back to np.ndarray\n",
    "                    pred.translation = np.array(pose['t'])\n",
    "                    registered += 1\n",
    "            mapping_result_str = f\"Dataset  {dataset} -> Registered {registered} / {len(images)} images with {len(maps)} clusters\"\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # raise e\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bda260b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:10:19.084960Z",
     "iopub.status.busy": "2025-06-11T05:10:19.084655Z",
     "iopub.status.idle": "2025-06-11T05:10:19.251741Z",
     "shell.execute_reply": "2025-06-11T05:10:19.250643Z"
    },
    "papermill": {
     "duration": 0.192493,
     "end_time": "2025-06-11T05:10:19.253446",
     "exception": false,
     "start_time": "2025-06-11T05:10:19.060953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                # ✅ `rotation` is a list of lists, flatten it\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()  # flatten 3x3 list -> 9 elems\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                # ✅ `translation` is a flat list\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "\n",
    "# Preview the output\n",
    "!head {submission_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "059a30b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T05:10:19.301374Z",
     "iopub.status.busy": "2025-06-11T05:10:19.301049Z",
     "iopub.status.idle": "2025-06-11T05:10:19.607188Z",
     "shell.execute_reply": "2025-06-11T05:10:19.606233Z"
    },
    "papermill": {
     "duration": 0.331431,
     "end_time": "2025-06-11T05:10:19.608742",
     "exception": false,
     "start_time": "2025-06-11T05:10:19.277311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "amy_gardens: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "fbk_vineyard: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "ETs: score=51.43% (mAA=34.62%, clusterness=100.00%)\n",
      "stairs: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "Average over all datasets: score=3.96% (mAA=2.66%, clusterness=100.00%)\n",
      "Computed metric in: 0.30 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402b4a3",
   "metadata": {
    "papermill": {
     "duration": 0.022697,
     "end_time": "2025-06-11T05:10:19.656423",
     "exception": false,
     "start_time": "2025-06-11T05:10:19.633726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fe40e",
   "metadata": {
    "papermill": {
     "duration": 0.022464,
     "end_time": "2025-06-11T05:10:19.702179",
     "exception": false,
     "start_time": "2025-06-11T05:10:19.679715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7505602,
     "sourceId": 11938492,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 195.977114,
   "end_time": "2025-06-11T05:10:23.194985",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-11T05:07:07.217871",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
