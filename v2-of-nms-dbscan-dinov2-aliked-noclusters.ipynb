{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce5d7a6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006183,
     "end_time": "2025-05-19T21:31:17.706244",
     "exception": false,
     "start_time": "2025-05-19T21:31:17.700061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a759f09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:17.718020Z",
     "iopub.status.busy": "2025-05-19T21:31:17.717684Z",
     "iopub.status.idle": "2025-05-19T21:31:23.243447Z",
     "shell.execute_reply": "2025-05-19T21:31:23.242398Z"
    },
    "papermill": {
     "duration": 5.533644,
     "end_time": "2025-05-19T21:31:23.245274",
     "exception": false,
     "start_time": "2025-05-19T21:31:17.711630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b942ffe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:23.258542Z",
     "iopub.status.busy": "2025-05-19T21:31:23.258295Z",
     "iopub.status.idle": "2025-05-19T21:31:48.756356Z",
     "shell.execute_reply": "2025-05-19T21:31:48.755375Z"
    },
    "papermill": {
     "duration": 25.506543,
     "end_time": "2025-05-19T21:31:48.758028",
     "exception": false,
     "start_time": "2025-05-19T21:31:23.251485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image, ImageDraw # Import ImageDraw for drawing keypoints\n",
    "\n",
    "# ... other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abbfa2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:48.771269Z",
     "iopub.status.busy": "2025-05-19T21:31:48.770768Z",
     "iopub.status.idle": "2025-05-19T21:31:48.853023Z",
     "shell.execute_reply": "2025-05-19T21:31:48.852232Z"
    },
    "papermill": {
     "duration": 0.090072,
     "end_time": "2025-05-19T21:31:48.854304",
     "exception": false,
     "start_time": "2025-05-19T21:31:48.764232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e9187e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:48.867213Z",
     "iopub.status.busy": "2025-05-19T21:31:48.866975Z",
     "iopub.status.idle": "2025-05-19T21:31:49.007215Z",
     "shell.execute_reply": "2025-05-19T21:31:49.006093Z"
    },
    "papermill": {
     "duration": 0.148209,
     "end_time": "2025-05-19T21:31:49.008725",
     "exception": false,
     "start_time": "2025-05-19T21:31:48.860516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43790275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.022115Z",
     "iopub.status.busy": "2025-05-19T21:31:49.021875Z",
     "iopub.status.idle": "2025-05-19T21:31:49.026444Z",
     "shell.execute_reply": "2025-05-19T21:31:49.025719Z"
    },
    "papermill": {
     "duration": 0.012365,
     "end_time": "2025-05-19T21:31:49.027478",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.015113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c521165d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.040059Z",
     "iopub.status.busy": "2025-05-19T21:31:49.039840Z",
     "iopub.status.idle": "2025-05-19T21:31:49.045099Z",
     "shell.execute_reply": "2025-05-19T21:31:49.044416Z"
    },
    "papermill": {
     "duration": 0.012777,
     "end_time": "2025-05-19T21:31:49.046232",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.033455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pil_image(fname):\n",
    "    \"\"\"Loads an image using PIL.\"\"\"\n",
    "    return Image.open(fname).convert('RGB')\n",
    "\n",
    "def get_image_size(fname):\n",
    "    \"\"\"Gets image size (width, height) using PIL.\"\"\"\n",
    "    with Image.open(fname) as img:\n",
    "        return img.size # (width, height)\n",
    "\n",
    "def get_original_coords(kp_coords, img_orig_size, variation_info):\n",
    "    \"\"\"\n",
    "    Transforms keypoint coordinates from variation space back to original image space.\n",
    "\n",
    "    Args:\n",
    "        kp_coords (np.ndarray): Keypoint coordinates [N, 2] in the variation space.\n",
    "        img_orig_size (tuple): Original image size (width, height).\n",
    "        variation_info (dict): Dictionary containing 'type' ('orig' or 'crop'),\n",
    "                               'scale_factor' (scale used for resize),\n",
    "                               'crop_box' ([x, y, w, h] in original coords, None if type is 'orig').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n",
    "    \"\"\"\n",
    "    if len(kp_coords) == 0:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    coords = kp_coords.copy() # Work on a copy\n",
    "\n",
    "    # 1. Reverse scaling\n",
    "    scale_factor = variation_info['scale_factor']\n",
    "    coords /= scale_factor # Now coords are in the space of the original/cropped image (before resize)\n",
    "\n",
    "    # 2. Reverse cropping offset\n",
    "    if variation_info['type'] == 'crop' and variation_info['crop_box'] is not None:\n",
    "        x_crop, y_crop, _, _ = variation_info['crop_box']\n",
    "        coords[:, 0] += x_crop\n",
    "        coords[:, 1] += y_crop\n",
    "\n",
    "    # Ensure points are within original image bounds (optional, but good practice)\n",
    "    # coords[:, 0] = np.clip(coords[:, 0], 0, img_orig_size[0] - 1)\n",
    "    # coords[:, 1] = np.clip(coords[:, 1], 0, img_orig_size[1] - 1)\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f80eee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.058672Z",
     "iopub.status.busy": "2025-05-19T21:31:49.058467Z",
     "iopub.status.idle": "2025-05-19T21:31:49.063920Z",
     "shell.execute_reply": "2025-05-19T21:31:49.063262Z"
    },
    "papermill": {
     "duration": 0.01318,
     "end_time": "2025-05-19T21:31:49.065275",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.052095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "GLOBAL_DESC_MODEL = '/kaggle/input/dinov2/pytorch/base/1' # Path to your DINOv2 model\n",
    "DATA_DIR = '.' # Base directory for outputs\n",
    "# FEATURE_DIR = os.path.join(DATA_DIR, 'features_combined')\n",
    "# MATCH_DIR = os.path.join(DATA_DIR, 'matches_global')\n",
    "\n",
    "# Initial detection parameters for cropping data collection\n",
    "INITIAL_DETECTION_RESIZE = 1280\n",
    "INITIAL_DETECTION_NUM_FEATURES = 2048\n",
    "\n",
    "# Parameters for TTA detection and combination\n",
    "TTA_SCALES = [1280, 2048]\n",
    "TTA_NUM_FEATURES = 2048\n",
    "USE_CROPPED_IMAGES = True\n",
    "\n",
    "# Parameters for New Cropping Method\n",
    "MIN_PAIRS_FOR_CROPPING = 10\n",
    "CROP_PADDING = 50\n",
    "DBSCAN_EPS = 20 # Fixed EPS fallback or base value\n",
    "DBSCAN_MIN_SAMPLES = 5 # DBSCAN min_samples parameter\n",
    "# New config for adaptive EPS based on resolution\n",
    "DBSCAN_EPS_RESOLUTION_RATIO = 0.02 # Ratio of max image dimension for EPS (e.g., 0.01 -> 1% of longer side)\n",
    "\n",
    "\n",
    "# Coordinate precision for deduplication (rounding float coordinates)\n",
    "COORD_PRECISION = 1 # Number of decimal places to round coordinates for uniqueness check\n",
    "\n",
    "# Matching parameters\n",
    "MIN_MATCHES_PER_VARIATION = 5 # Lowered this threshold slightly, as combining might filter some\n",
    "MIN_TOTAL_MATCHES_PER_PAIR = 20 # Minimum unique matches for a pair to be saved in global list\n",
    "\n",
    "# Output file names (within FEATURE_DIR and MATCH_DIR)\n",
    "KEYPOINTS_SUBDIR = 'keypoints'\n",
    "DESCRIPTORS_H5 = 'descriptors.h5'\n",
    "MATCHES_PT = 'matches.pt'\n",
    "CROP_DATA = 'crop_data.h5'\n",
    "CROP_INFO = 'crop_info.h5'\n",
    "\n",
    "# Parameters for graph building and clustering thresholds\n",
    "# These are the internal names used in the function; map external arguments to these if needed\n",
    "MIN_MATCHES_FOR_GRAPH_EDGE = 20  # Min matches for adding edge to graph\n",
    "MIN_MATCHES_FOR_FILTERED_GRAPH = 75 # Min matches for filtering graph (your aliked_dis_min)\n",
    "MIN_IMAGES_PER_CLUSTER = 5 # Min images in a final cluster\n",
    "\n",
    "\n",
    "# Config for NMS\n",
    "NMS_SIZE_PIXELS = 3 # Radius in pixels for NMS suppression\n",
    "NMS_SIZE_PIXELS_ratio = 0.003\n",
    "\n",
    "# --- NEW: Constant for Keypoint Visualization ---\n",
    "VISUALIZE_KEYPOINTS = False # Set to True to enable saving visualized keypoints\n",
    "KEYPOINT_VIS_SUBDIR = 'keypoint_visualizations' # Subdirectory for saving visualized images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef8ae29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.078786Z",
     "iopub.status.busy": "2025-05-19T21:31:49.078555Z",
     "iopub.status.idle": "2025-05-19T21:31:49.091280Z",
     "shell.execute_reply": "2025-05-19T21:31:49.090668Z"
    },
    "papermill": {
     "duration": 0.020152,
     "end_time": "2025-05-19T21:31:49.092380",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.072228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "\n",
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 30,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n",
    "    # removing half\n",
    "    # thr = min(np.percentile(dm_flat, 50), sim_th)\n",
    "    mask = dm <= np.percentile(dm_flat, 50)\n",
    "    # print(\"thr :\", thr)\n",
    "    # mask = dm<=threshold\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afeb2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.105184Z",
     "iopub.status.busy": "2025-05-19T21:31:49.104956Z",
     "iopub.status.idle": "2025-05-19T21:31:49.109755Z",
     "shell.execute_reply": "2025-05-19T21:31:49.108987Z"
    },
    "papermill": {
     "duration": 0.012378,
     "end_time": "2025-05-19T21:31:49.110948",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.098570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 自定义 NMS 函数 ---\n",
    "def custom_nms_2d_keypoints(keypoints_np, scores_np, nms_radius):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression on 2D keypoints based on scores.\n",
    "    This is a custom implementation to replace kornia.feature.non_maximum_suppression2d\n",
    "    due to potential API differences across Kornia versions.\n",
    "\n",
    "    Args:\n",
    "        keypoints_np (np.ndarray): Keypoint coordinates (N, 2) in pixel space.\n",
    "        scores_np (np.ndarray): Scores for each keypoint (N,).\n",
    "        nms_radius (float): Radius for suppression in pixel units.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Indices of the keypoints that survive NMS (original indices).\n",
    "    \"\"\"\n",
    "    if len(keypoints_np) == 0:\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "    # Get indices sorted by score in descending order\n",
    "    order = scores_np.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    # suppressed array to track which keypoints have been suppressed\n",
    "    suppressed = np.zeros(len(keypoints_np), dtype=bool)\n",
    "\n",
    "    for i_sorted in order: # Iterate through keypoints from highest score to lowest\n",
    "        if suppressed[i_sorted]:\n",
    "            continue # This keypoint has already been suppressed by a higher-scoring one\n",
    "\n",
    "        keep.append(i_sorted) # Keep this keypoint\n",
    "\n",
    "        current_kp = keypoints_np[i_sorted]\n",
    "\n",
    "        # Calculate squared distances from the current keypoint to all other keypoints\n",
    "        # Using squared distance avoids sqrt for efficiency if only comparing to radius^2\n",
    "        distances_sq = np.sum((keypoints_np - current_kp)**2, axis=1)\n",
    "\n",
    "        # Identify keypoints within the suppression radius\n",
    "        points_within_radius_mask = distances_sq < nms_radius**2\n",
    "\n",
    "        # Mark these keypoints as suppressed\n",
    "        suppressed[points_within_radius_mask] = True\n",
    "\n",
    "    return np.array(keep, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6866a58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.123312Z",
     "iopub.status.busy": "2025-05-19T21:31:49.123083Z",
     "iopub.status.idle": "2025-05-19T21:31:49.131840Z",
     "shell.execute_reply": "2025-05-19T21:31:49.131048Z"
    },
    "papermill": {
     "duration": 0.016317,
     "end_time": "2025-05-19T21:31:49.133041",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.116724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Assuming these constants are defined elsewhere in your setup\n",
    "# from .config import CROP_DATA, CROP_BOXES_FILE, CROP_PADDING_FACTOR, MIN_CROP_DIMENSION\n",
    "\n",
    "# Placeholder constants for demonstration if not imported\n",
    "CROP_DATA = 'crop_data.h5'\n",
    "CROP_BOXES_FILE = 'crop_boxes.h5' # File to save the final crop boxes\n",
    "CROP_PADDING_FACTOR = 0.1 # e.g., 10% padding\n",
    "MIN_CROP_DIMENSION = 512 # Minimum width/height for a crop box\n",
    "\n",
    "def calculate_crop_boxes(data_dir: str):\n",
    "    \"\"\"\n",
    "    Calculates the crop boxes for each image based on the frequent keypoints\n",
    "    stored in the crop_data_file.\n",
    "    Assumes all input data (HDF5 files, datasets, attributes) are valid and exist.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where crop_data.h5 is located and\n",
    "                        where crop_boxes.h5 will be saved.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the generated crop_boxes.h5 file.\n",
    "    \"\"\"\n",
    "    crop_data_file = os.path.join(data_dir, CROP_DATA)\n",
    "    crop_boxes_output_file = os.path.join(data_dir, CROP_BOXES_FILE)\n",
    "\n",
    "    print(f\"Calculating crop boxes from {crop_data_file}...\")\n",
    "\n",
    "    image_crop_boxes = {}\n",
    "\n",
    "    with h5py.File(crop_data_file, 'r') as f_crop_data:\n",
    "        for img_key in f_crop_data.keys():\n",
    "            # Retrieve keypoints and original image size, assuming they are present and valid\n",
    "            kps = f_crop_data[img_key]['keypoints'][...] # Now simply 'keypoints'\n",
    "            original_pil_size = f_crop_data[img_key].attrs['original_pil_size'] # (W, H)\n",
    "            original_width, original_height = original_pil_size\n",
    "\n",
    "            if len(kps) == 0:\n",
    "                print(f\"Warning: No frequent keypoints for {img_key}. Skipping crop box calculation.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate bounding box of keypoints\n",
    "            min_x, min_y = np.min(kps, axis=0)\n",
    "            max_x, max_y = np.max(kps, axis=0)\n",
    "\n",
    "            # Apply padding\n",
    "            padding_w = (max_x - min_x) * CROP_PADDING_FACTOR\n",
    "            padding_h = (max_y - min_y) * CROP_PADDING_FACTOR\n",
    "\n",
    "            crop_min_x = max(0, min_x - padding_w)\n",
    "            crop_min_y = max(0, min_y - padding_h)\n",
    "            crop_max_x = min(original_width, max_x + padding_w)\n",
    "            crop_max_y = min(original_height, max_y + padding_h)\n",
    "\n",
    "            # Ensure minimum crop dimensions\n",
    "            current_crop_width = crop_max_x - crop_min_x\n",
    "            current_crop_height = crop_max_y - crop_min_y\n",
    "\n",
    "            if current_crop_width < MIN_CROP_DIMENSION:\n",
    "                center_x = (crop_min_x + crop_max_x) / 2\n",
    "                crop_min_x = max(0, center_x - MIN_CROP_DIMENSION / 2)\n",
    "                crop_max_x = min(original_width, center_x + MIN_CROP_DIMENSION / 2)\n",
    "                # Adjust if clipping occurred\n",
    "                if crop_max_x - crop_min_x < MIN_CROP_DIMENSION:\n",
    "                    if crop_min_x == 0:\n",
    "                        crop_max_x = min(original_width, MIN_CROP_DIMENSION)\n",
    "                    elif crop_max_x == original_width:\n",
    "                        crop_min_x = max(0, original_width - MIN_CROP_DIMENSION)\n",
    "\n",
    "            if current_crop_height < MIN_CROP_DIMENSION:\n",
    "                center_y = (crop_min_y + crop_max_y) / 2\n",
    "                crop_min_y = max(0, center_y - MIN_CROP_DIMENSION / 2)\n",
    "                crop_max_y = min(original_height, center_y + MIN_CROP_DIMENSION / 2)\n",
    "                # Adjust if clipping occurred\n",
    "                if crop_max_y - crop_min_y < MIN_CROP_DIMENSION:\n",
    "                    if crop_min_y == 0:\n",
    "                        crop_max_y = min(original_height, MIN_CROP_DIMENSION)\n",
    "                    elif crop_max_y == original_height:\n",
    "                        crop_min_y = max(0, original_height - MIN_CROP_DIMENSION)\n",
    "\n",
    "            # Ensure integer coordinates for crop box (x_min, y_min, x_max, y_max)\n",
    "            crop_box = np.array([\n",
    "                int(round(crop_min_x)),\n",
    "                int(round(crop_min_y)),\n",
    "                int(round(crop_max_x)),\n",
    "                int(round(crop_max_y))\n",
    "            ], dtype=np.int32)\n",
    "\n",
    "            image_crop_boxes[img_key] = crop_box\n",
    "\n",
    "    # Save the calculated crop boxes to a new HDF5 file\n",
    "    with h5py.File(crop_boxes_output_file, 'w') as f_crop_boxes:\n",
    "        for img_key, crop_box in image_crop_boxes.items():\n",
    "            f_crop_boxes.create_dataset(img_key, data=crop_box)\n",
    "\n",
    "    print(f\"Crop boxes calculated and saved to {crop_boxes_output_file}\")\n",
    "    return crop_boxes_output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c701e33a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.145666Z",
     "iopub.status.busy": "2025-05-19T21:31:49.145447Z",
     "iopub.status.idle": "2025-05-19T21:31:49.161827Z",
     "shell.execute_reply": "2025-05-19T21:31:49.161029Z"
    },
    "papermill": {
     "duration": 0.024175,
     "end_time": "2025-05-19T21:31:49.163179",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.139004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... (imports, configs, utility functions like load_torch_image, load_pil_image, get_image_size)\n",
    "\n",
    "def perform_initial_detection_and_matching(img_fnames, index_pairs, data_dir, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Performs detection (ALIKED) and matching (LightGlue) on original images\n",
    "    at a base resolution (e.g., 1024) to collect data for the cropping step.\n",
    "    Stores keypoints (in original image coordinates) and matches in temporary HDF5.\n",
    "    Analyzes matches to create crop data file.\n",
    "    Assumes all image inputs are valid and exist.\n",
    "    \"\"\"\n",
    "    temp_feature_dir = os.path.join(data_dir, '.temp_crop_features')\n",
    "    os.makedirs(temp_feature_dir, exist_ok=True)\n",
    "\n",
    "    initial_feature_file = os.path.join(temp_feature_dir, f'initial_features_{INITIAL_DETECTION_RESIZE}.h5')\n",
    "    initial_match_file = os.path.join(temp_feature_dir, f'initial_matches_{INITIAL_DETECTION_RESIZE}.h5')\n",
    "    crop_data_file = os.path.join(data_dir, CROP_DATA)\n",
    "\n",
    "    if os.path.exists(crop_data_file):\n",
    "        print(f\"Initial detection and matching data for cropping exists: {crop_data_file}. deleting.\")\n",
    "        os.remove(crop_data_file)\n",
    "\n",
    "    print(f\"Performing initial ALIKED detection ({INITIAL_DETECTION_RESIZE}) and LightGlue matching for cropping data...\")\n",
    "\n",
    "    # 1. Initial Detection\n",
    "    print(\"Running initial ALIKED detection...\")\n",
    "    extractor = ALIKED(max_num_keypoints=INITIAL_DETECTION_NUM_FEATURES, detection_threshold=0.2).eval().to(device, dtype=torch.float32)\n",
    "    extractor.preprocess_conf[\"resize\"] = INITIAL_DETECTION_RESIZE\n",
    "\n",
    "    with h5py.File(initial_feature_file, mode='w') as f_kp_desc:\n",
    "        for img_path in tqdm(img_fnames, desc=\"Initial ALIKED Detection\"):\n",
    "            img_key = os.path.basename(img_path)\n",
    "\n",
    "            img_orig_pil = load_pil_image(img_path)\n",
    "            original_pil_size = img_orig_pil.size # (W, H)\n",
    "\n",
    "            timg = K.image_to_tensor(np.array(img_orig_pil), keepdim=True).to(device, torch.float32) / 255.0 # Normalize\n",
    "            if timg.ndim == 3: timg = timg[None, ...] # Ensure BxCxHxW\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                feats = extractor.extract(timg)\n",
    "\n",
    "                # Keypoints (kps) are already in ORIGINAL image coordinates from ALIKED\n",
    "                kps = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                desc = feats['descriptors'].reshape(len(kps), -1).detach().cpu().numpy()\n",
    "\n",
    "            # Save keypoints and descriptors\n",
    "            img_group = f_kp_desc.create_group(img_key)\n",
    "            img_group.create_dataset('keypoints', data=kps.astype(np.float32)) # Stored simply as 'keypoints'\n",
    "            img_group.create_dataset('descriptors', data=desc.astype(np.float32))\n",
    "            img_group.attrs['original_pil_size'] = original_pil_size # (W, H) tuple\n",
    "            img_group.attrs['original_path'] = img_path # Store original path\n",
    "\n",
    "    # 2. Initial Matching\n",
    "    print(\"Running initial LightGlue matching...\")\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                 \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    if device == torch.device('cpu'):\n",
    "        lg_matcher.to('cpu')\n",
    "\n",
    "    # Ensure initial_match_file is created even if empty\n",
    "    with h5py.File(initial_match_file, mode='w') as f:\n",
    "        pass # Create an empty file\n",
    "\n",
    "    # Read keys from the initial_feature_file HDF5\n",
    "    extracted_image_keys = []\n",
    "    with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read:\n",
    "        extracted_image_keys = list(f_kp_desc_read.keys())\n",
    "\n",
    "    # Filter index_pairs to include only pairs where both images had features extracted\n",
    "    filtered_index_pairs = []\n",
    "    for idx1, idx2 in index_pairs:\n",
    "        key1, key2 = os.path.basename(img_fnames[idx1]), os.path.basename(img_fnames[idx2])\n",
    "        if key1 in extracted_image_keys and key2 in extracted_image_keys:\n",
    "            filtered_index_pairs.append((idx1, idx2))\n",
    "\n",
    "    if not filtered_index_pairs:\n",
    "        print(\"No image pairs with extracted features to perform initial matching.\")\n",
    "    else:\n",
    "        with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read, \\\n",
    "             h5py.File(initial_match_file, mode='a') as f_match:\n",
    "\n",
    "            for idx1, idx2 in tqdm(filtered_index_pairs, desc=\"Initial LightGlue Matching\"):\n",
    "                fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "                key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n",
    "\n",
    "                # Load keypoints and descriptors from the initial detection file (these are at original scale)\n",
    "                kp1 = torch.from_numpy(f_kp_desc_read[key1]['keypoints'][...]).to(device)\n",
    "                kp2 = torch.from_numpy(f_kp_desc_read[key2]['keypoints'][...]).to(device)\n",
    "                desc1 = torch.from_numpy(f_kp_desc_read[key1]['descriptors'][...]).to(device)\n",
    "                desc2 = torch.from_numpy(f_kp_desc_read[key2]['descriptors'][...]).to(device)\n",
    "\n",
    "                if len(kp1) == 0 or len(kp2) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create LAFs based on original scale keypoints\n",
    "                laf1 = KF.laf_from_center_scale_ori(kp1[None])\n",
    "                laf2 = KF.laf_from_center_scale_ori(kp2[None])\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    dists, idxs = lg_matcher(desc1, desc2, laf1, laf2)\n",
    "                if len(idxs) > 0:\n",
    "                    group = f_match.require_group(key1)\n",
    "                    group.create_dataset(key2, data=idxs.detach().cpu().numpy().astype(np.int32))\n",
    "\n",
    "    # --- Analyze Initial Matches for Cropping Data ---\n",
    "    print(\"Analyzing initial matches for cropping data...\")\n",
    "\n",
    "    kp_match_pairs = {}\n",
    "\n",
    "    with h5py.File(initial_match_file, mode='r') as f_match:\n",
    "        for img_key1 in f_match.keys():\n",
    "            for img_key2 in f_match[img_key1].keys():\n",
    "                matches = f_match[img_key1][img_key2][...] # Indices (kp1_idx, kp2_idx)\n",
    "\n",
    "                for kp1_idx, kp2_idx in matches:\n",
    "                    if img_key1 not in kp_match_pairs: kp_match_pairs[img_key1] = {}\n",
    "                    if kp1_idx not in kp_match_pairs[img_key1]: kp_match_pairs[img_key1][kp1_idx] = set()\n",
    "                    kp_match_pairs[img_key1][kp1_idx].add(img_key2)\n",
    "\n",
    "                    if img_key2 not in kp_match_pairs: kp_match_pairs[img_key2] = {}\n",
    "                    if kp2_idx not in kp_match_pairs[img_key2]: kp_match_pairs[img_key2][kp2_idx] = set()\n",
    "                    kp_match_pairs[img_key2][kp2_idx].add(img_key1)\n",
    "\n",
    "\n",
    "    frequent_kp_data = {}\n",
    "\n",
    "    with h5py.File(initial_feature_file, mode='r') as f_kp_desc:\n",
    "        for img_key in kp_match_pairs.keys():\n",
    "            # Retrieve keypoints directly; they are already at original scale\n",
    "            original_scale_kps = f_kp_desc[img_key]['keypoints'][...]\n",
    "            original_pil_size = f_kp_desc[img_key].attrs['original_pil_size'] # (W, H)\n",
    "\n",
    "            kp_data_dict = kp_match_pairs[img_key]\n",
    "\n",
    "            frequent_indices = [kp_idx for kp_idx, matched_pairs in kp_data_dict.items()\n",
    "                                if len(matched_pairs) >= MIN_PAIRS_FOR_CROPPING and kp_idx < len(original_scale_kps)]\n",
    "\n",
    "\n",
    "            if frequent_indices:\n",
    "                # Get coordinates directly from the stored keypoints (already at original scale)\n",
    "                frequent_kps = original_scale_kps[frequent_indices]\n",
    "\n",
    "                # Check for valid coordinates (non-negative)\n",
    "                valid_frequent_kps = frequent_kps[~np.any(frequent_kps < 0, axis=1)]\n",
    "\n",
    "\n",
    "                if len(valid_frequent_kps) > 0:\n",
    "                    frequent_kp_data[img_key] = {\n",
    "                        'keypoints': valid_frequent_kps,\n",
    "                        'original_pil_size': original_pil_size # ADDED: Store original_pil_size here\n",
    "                    }\n",
    "\n",
    "\n",
    "    # Save frequent_kp_data to crop_data_file\n",
    "    with h5py.File(crop_data_file, mode='w') as f_crop_data:\n",
    "        if frequent_kp_data:\n",
    "            for img_key, data in frequent_kp_data.items():\n",
    "                group = f_crop_data.create_group(img_key)\n",
    "                group.create_dataset('keypoints', data=data['keypoints'])\n",
    "                group.attrs['original_pil_size'] = data['original_pil_size'] # ADDED: Save as attribute\n",
    "\n",
    "\n",
    "    print(f\"Initial detection and matching complete. Cropping data saved to {crop_data_file}\")\n",
    "\n",
    "    return crop_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d24964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.175603Z",
     "iopub.status.busy": "2025-05-19T21:31:49.175400Z",
     "iopub.status.idle": "2025-05-19T21:31:49.179091Z",
     "shell.execute_reply": "2025-05-19T21:31:49.178467Z"
    },
    "papermill": {
     "duration": 0.01112,
     "end_time": "2025-05-19T21:31:49.180197",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.169077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume this is defined globally or imported\n",
    "def transform_points_from_processed(points, crop_box=None):\n",
    "    \"\"\"\n",
    "    Transforms keypoints from a cropped image's coordinate system\n",
    "    back to the original full image's coordinate system by adding the crop offset.\n",
    "    This function *does not perform any scaling*, as ALIKED handles that internally.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Nx2 numpy array of (x,y) keypoint coordinates\n",
    "                             relative to the cropped image.\n",
    "        crop_box (list or None): [x, y, w, h] of the crop region in original image coordinates.\n",
    "                                 If None, no offset is applied (e.g., for full original images).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Nx2 numpy array of keypoint coordinates in the original\n",
    "                    full image's coordinate system.\n",
    "    \"\"\"\n",
    "    transformed_points = points.copy() # Start with a copy to avoid modifying original array\n",
    "\n",
    "    if crop_box is not None:\n",
    "        # If these points came from a cropped image, add the crop's top-left offset\n",
    "        # crop_box is [x, y, w, h]\n",
    "        offset_x, offset_y = crop_box[0], crop_box[1]\n",
    "        transformed_points[:, 0] += offset_x\n",
    "        transformed_points[:, 1] += offset_y\n",
    "    \n",
    "    return transformed_points\n",
    "\n",
    "# calculate_kornia_resize_scale will no longer be used for keypoint handling.\n",
    "# You can remove its definition if it's not used anywhere else in your project.\n",
    "# If it's used for other non-keypoint-related resizing calculations, keep it.\n",
    "# For the scope of this request, it's not needed for feature extraction or transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9450fc7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.192677Z",
     "iopub.status.busy": "2025-05-19T21:31:49.192475Z",
     "iopub.status.idle": "2025-05-19T21:31:49.196149Z",
     "shell.execute_reply": "2025-05-19T21:31:49.195366Z"
    },
    "papermill": {
     "duration": 0.011216,
     "end_time": "2025-05-19T21:31:49.197360",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.186144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image_names_from_json(cluster_path):\n",
    "    with open(os.path.join(cluster_path, 'images.json'), 'r') as f:\n",
    "        full_paths = json.load(f)  # 可能是 ['/path/to/images/img001.jpg', ...]\n",
    "        image_names = [os.path.basename(p) for p in full_paths]  # 提取 'img001.jpg'\n",
    "    return image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8913fe14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.209813Z",
     "iopub.status.busy": "2025-05-19T21:31:49.209573Z",
     "iopub.status.idle": "2025-05-19T21:31:49.226404Z",
     "shell.execute_reply": "2025-05-19T21:31:49.225803Z"
    },
    "papermill": {
     "duration": 0.024426,
     "end_time": "2025-05-19T21:31:49.227560",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.203134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... (imports like kornia.feature as KF, torch, numpy, os, h5py, tqdm)\n",
    "\n",
    "# Placeholder constants for NMS if not globally defined. Adjust these values as needed.\n",
    "NMS_SIZE_PIXELS_ratio = 0.005 # Example: 0.5% of the max original dimension\n",
    "NMS_SIZE_PIXELS = 8 # Example: absolute max NMS radius in pixels\n",
    "\n",
    "# Assume transform_points_from_processed and load_pil_image are defined as previously discussed.\n",
    "\n",
    "\n",
    "def detect_and_combine_features(img_fnames, crop_info_file, feature_dir, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Detects ALIKED features for multiple scales and original/cropped images,\n",
    "    combines unique features per image (deduplicating based on original coords),\n",
    "    applies NMS, and saves combined features to .pt and .h5 files per image.\n",
    "    All error handling via try-except blocks has been removed, assuming valid inputs.\n",
    "    ALIKED's `resize` parameter handles internal scaling; keypoints are returned\n",
    "    in the coordinate system of the *input image* provided to `extractor.extract`.\n",
    "    `transform_points_from_processed` is used only for applying crop offsets.\n",
    "    \"\"\"\n",
    "    print(\"Running multi-variation ALIKED detection and combining features (with NMS)...\")\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    keypoints_subdir_path = os.path.join(feature_dir, KEYPOINTS_SUBDIR)\n",
    "    os.makedirs(keypoints_subdir_path, exist_ok=True)\n",
    "    descriptors_h5_path = os.path.join(feature_dir, DESCRIPTORS_H5)\n",
    "    # NEW: Create directory for keypoint visualizations if enabled\n",
    "    if VISUALIZE_KEYPOINTS:\n",
    "        keypoint_vis_dir = os.path.join(feature_dir, KEYPOINT_VIS_SUBDIR)\n",
    "        os.makedirs(keypoint_vis_dir, exist_ok=True)\n",
    "    with h5py.File(descriptors_h5_path, mode='w') as f_descriptors, \\\n",
    "         h5py.File(crop_info_file, mode='r') as f_crop_info:\n",
    "\n",
    "        extractor = ALIKED(max_num_keypoints=TTA_NUM_FEATURES, detection_threshold=0.1).eval().to(DEVICE, dtype=torch.float32)\n",
    "        if DEVICE == torch.device('cpu'):\n",
    "            extractor.to('cpu', torch.float32)\n",
    "\n",
    "        for img_path in tqdm(img_fnames, desc=\"Detecting & Combining Features\"):\n",
    "            img_key = os.path.basename(img_path)\n",
    "            kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt')\n",
    "\n",
    "            # Skip if combined features already exist for this image\n",
    "            if os.path.exists(kp_pt_path) and img_key in f_descriptors:\n",
    "                continue\n",
    "\n",
    "            img_orig_pil = load_pil_image(img_path)\n",
    "            img_orig_w, img_orig_h = img_orig_pil.size # Get original image dimensions for NMS radius calculation\n",
    "\n",
    "            # Get crop info\n",
    "            has_crop = False\n",
    "            crop_box = None\n",
    "            if img_key in f_crop_info:\n",
    "                img_crop_group = f_crop_info[img_key]\n",
    "                has_crop_attr = img_crop_group.attrs.get('has_crop', False)\n",
    "                if has_crop_attr:\n",
    "                    temp_crop_box = img_crop_group.attrs.get('crop_box', [0, 0, 0, 0]).tolist()\n",
    "                    if temp_crop_box[2] > 0 and temp_crop_box[3] > 0:\n",
    "                        has_crop = True\n",
    "                        crop_box = temp_crop_box\n",
    "            \n",
    "            variations_to_process = []\n",
    "            for scale in TTA_SCALES:\n",
    "                variations_to_process.append({'type': 'orig', 'scale_target': scale, 'pil_img': img_orig_pil, 'input_crop_box_offset': None})\n",
    "                \n",
    "                if USE_CROPPED_IMAGES and has_crop:\n",
    "                    x, y, w, h = crop_box\n",
    "                    img_cropped_pil = img_orig_pil.crop((x, y, x + w, y + h))\n",
    "                    variations_to_process.append({'type': 'crop', 'scale_target': scale, 'pil_img': img_cropped_pil, 'input_crop_box_offset': crop_box})\n",
    "\n",
    "            all_kps = []\n",
    "            all_descriptors = []\n",
    "            all_scores = []\n",
    "\n",
    "            for var_info in variations_to_process:\n",
    "                var_type = var_info['type']\n",
    "                var_scale_target = var_info['scale_target']\n",
    "                var_pil_img = var_info['pil_img']\n",
    "                input_crop_box_offset = var_info['input_crop_box_offset']\n",
    "\n",
    "                timg = K.image_to_tensor(np.array(var_pil_img), keepdim=True).to(device, torch.float32) / 255.0\n",
    "                if timg.ndim == 3: timg = timg[None, ...]\n",
    "                \n",
    "                if timg.shape[2] == 0 or timg.shape[3] == 0:\n",
    "                    all_kps.append(np.empty((0, 2), dtype=np.float32))\n",
    "                    all_descriptors.append(np.empty((0, all_descriptors[0].shape[1] if all_descriptors and len(all_descriptors[0]) > 0 else 0), dtype=np.float32))\n",
    "                    all_scores.append(np.empty(0, dtype=np.float32))\n",
    "                    continue\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    feats = extractor.extract(timg, resize=var_scale_target, return_processed_size=False, return_scores=True)\n",
    "\n",
    "                kp_variation = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                desc_variation = feats['descriptors'].reshape(len(kp_variation), -1).detach().cpu().numpy()\n",
    "                score_variation = feats['keypoint_scores'].reshape(-1).detach().cpu().numpy()\n",
    "\n",
    "                kps_in_original_img_coords = transform_points_from_processed(\n",
    "                    kp_variation, input_crop_box_offset\n",
    "                )\n",
    "\n",
    "                all_kps.append(kps_in_original_img_coords)\n",
    "                all_descriptors.append(desc_variation)\n",
    "                all_scores.append(score_variation)\n",
    "\n",
    "            # Combine all detected points and descriptors/scores\n",
    "            valid_kps = [k for k in all_kps if len(k) > 0]\n",
    "            valid_descriptors = [d for d in all_descriptors if len(d) > 0]\n",
    "            valid_scores = [s for s in all_scores if len(s) > 0]\n",
    "\n",
    "            if not valid_kps:\n",
    "                torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n",
    "                if img_key not in f_descriptors:\n",
    "                    f_descriptors.create_group(img_key)\n",
    "                continue\n",
    "\n",
    "            combined_kps = np.concatenate(valid_kps, axis=0)\n",
    "            combined_descriptors = np.concatenate(valid_descriptors, axis=0)\n",
    "            combined_scores = np.concatenate(valid_scores, axis=0) if valid_scores else np.empty(0, dtype=np.float32)\n",
    "\n",
    "            # --- NMS (Non-Maximum Suppression) ---\n",
    "            combined_kps_for_dedup = combined_kps\n",
    "            combined_descs_for_dedup = combined_descriptors\n",
    "            \n",
    "            # Apply NMS if scores are available and keypoints exist\n",
    "            if len(combined_kps) > 0 and len(combined_scores) > 0 and len(combined_kps) == len(combined_scores):\n",
    "                # Calculate adaptive NMS radius based on original image dimensions\n",
    "                max_orig_dim = max(img_orig_w, img_orig_h)\n",
    "                adaptive_nms_size = min(max_orig_dim * NMS_SIZE_PIXELS_ratio, NMS_SIZE_PIXELS)\n",
    "\n",
    "                indices_after_nms_np = custom_nms_2d_keypoints(\n",
    "                    combined_kps,\n",
    "                    combined_scores,\n",
    "                    adaptive_nms_size\n",
    "                )\n",
    "                \n",
    "                # Filter the combined arrays using NMS results\n",
    "                combined_kps_for_dedup = combined_kps[indices_after_nms_np]\n",
    "                combined_descs_for_dedup = combined_descriptors[indices_after_nms_np]\n",
    "                # Note: Scores are not saved to the final output, so no need to filter combined_scores.\n",
    "\n",
    "                # print(f\"Image {img_key}: Features before NMS = {len(combined_kps)}, Features after NMS = {len(combined_kps_for_dedup)}\")\n",
    "\n",
    "            # --- Perform Coordinate-based Deduplication ---\n",
    "            seen_coords = {}\n",
    "            unique_kps = []\n",
    "            unique_descriptors = []\n",
    "\n",
    "            for i, (kp_coord, descriptor) in enumerate(zip(combined_kps_for_dedup, combined_descs_for_dedup)):\n",
    "                rounded_coord = tuple(np.round(kp_coord + 1e-6, COORD_PRECISION).astype(float))\n",
    "\n",
    "                if rounded_coord not in seen_coords:\n",
    "                    seen_coords[rounded_coord] = len(unique_kps)\n",
    "                    unique_kps.append(kp_coord)\n",
    "                    unique_descriptors.append(descriptor)\n",
    "\n",
    "            unique_kps_np = np.array(unique_kps, dtype=np.float32)\n",
    "            unique_descriptors_np = np.array(unique_descriptors, dtype=np.float32)\n",
    "\n",
    "            # Save unique keypoints to .pt\n",
    "            torch.save(torch.from_numpy(unique_kps_np), kp_pt_path)\n",
    "\n",
    "            # Save unique descriptors to descriptors.h5\n",
    "            img_desc_group = f_descriptors.require_group(img_key)\n",
    "            img_desc_group.create_dataset('data', data=unique_descriptors_np, compression=\"gzip\")\n",
    "\n",
    "    \n",
    "            # --- NEW: Visualize and Save Keypoints ---\n",
    "            if VISUALIZE_KEYPOINTS:\n",
    "                if len(unique_kps_np) > 0:\n",
    "                    vis_img = img_orig_pil.copy()\n",
    "                    draw = ImageDraw.Draw(vis_img)\n",
    "                    radius = 3 # Radius for drawing keypoints\n",
    "\n",
    "                    for kp_x, kp_y in unique_kps_np:\n",
    "                        # Draw a small circle at each keypoint\n",
    "                        draw.ellipse((kp_x - radius, kp_y - radius, kp_x + radius, kp_y + radius),\n",
    "                                     fill='red', outline='red')\n",
    "                    \n",
    "                    vis_output_path = os.path.join(keypoint_vis_dir, f'{img_key}_kps.jpg')\n",
    "                    vis_img.save(vis_output_path)\n",
    "                    # print(f\"Saved keypoint visualization for {img_key} to {vis_output_path}\") # Optional verbose\n",
    "                # else:\n",
    "                    # print(f\"No keypoints to visualize for {img_key}.\") # Optional verbose\n",
    "\n",
    "\n",
    "    print(\"Multi-variation detection, combination, and deduplication complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfcc513b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.240463Z",
     "iopub.status.busy": "2025-05-19T21:31:49.240261Z",
     "iopub.status.idle": "2025-05-19T21:31:49.244563Z",
     "shell.execute_reply": "2025-05-19T21:31:49.243970Z"
    },
    "papermill": {
     "duration": 0.011721,
     "end_time": "2025-05-19T21:31:49.245655",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.233934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_into_colmap_cluster(\n",
    "    img_dir,\n",
    "    cluster_path='.featureout/cluster_0',\n",
    "    database_path = '.featureout/cluster_0/colmap.db',\n",
    "    image_names = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Import keypoints and matches into COLMAP database using helper functions.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Directory containing image files\n",
    "        cluster_path (str): Path with matches.h5\n",
    "        database_path (str): Output database location\n",
    "        image_names (list[str]): Optional subset of image names to include\n",
    "    \"\"\"\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    # Add keypoints and images\n",
    "    fname_to_id = add_keypoints(\n",
    "        db=db,\n",
    "        h5_path=cluster_path,\n",
    "        image_path=img_dir,\n",
    "        img_ext='',\n",
    "        camera_model='simple-pinhole',\n",
    "        single_camera=single_camera\n",
    "    )\n",
    "    # Filter fname_to_id to only use the selected subset (if provided)\n",
    "    if image_names is not None:\n",
    "        fname_to_id = {k: v for k, v in fname_to_id.items() if k in image_names}\n",
    "\n",
    "    # Add matches between selected image pairs\n",
    "    add_matches(\n",
    "        db=db,\n",
    "        h5_path=cluster_path,\n",
    "        fname_to_id=fname_to_id\n",
    "    )\n",
    "    db.commit()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e3ccdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.258081Z",
     "iopub.status.busy": "2025-05-19T21:31:49.257873Z",
     "iopub.status.idle": "2025-05-19T21:31:49.269804Z",
     "shell.execute_reply": "2025-05-19T21:31:49.269171Z"
    },
    "papermill": {
     "duration": 0.019535,
     "end_time": "2025-05-19T21:31:49.271034",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.251499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... (imports)\n",
    "import kornia.feature as KF\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import json # Ensure json is imported\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def match_images_global(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    data_dir='.',\n",
    "    device=torch.device('cpu'),\n",
    "    min_matches=MIN_MATCHES_FOR_GRAPH_EDGE,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs LightGlue matching on combined features for all image pairs\n",
    "    in index_pairs, builds a match graph (implicitly), and saves global files\n",
    "    (images.json, keypoints.h5, matches.h5) for the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        img_fnames (list): List of full paths to image files.\n",
    "        index_pairs (list): List of (idx1, idx2) tuples for image pairs to match.\n",
    "        data_dir (str): Base directory where 'features_combined' is located and\n",
    "                        where the global output will be created.\n",
    "        device (torch.device): Device to use for matching.\n",
    "        min_matches (int): Min matches for considering a pair and saving its matches.\n",
    "        verbose (bool): Whether to print detailed match info.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing a single list with the global indices of all images.\n",
    "              This is to maintain a similar structure to the clustering output,\n",
    "              indicating a single group.\n",
    "    \"\"\"\n",
    "    # Define paths based on data_dir and configuration\n",
    "    feature_dir_combined = os.path.join(data_dir, 'features_combined')\n",
    "    keypoints_subdir_path = os.path.join(feature_dir_combined, KEYPOINTS_SUBDIR) # Directory holding combined per-image .pt files\n",
    "    descriptors_h5_path = os.path.join(feature_dir_combined, DESCRIPTORS_H5) # HDF5 holding combined per-image descriptors\n",
    "\n",
    "    # Directory where the global output files will be created\n",
    "    global_output_dir = os.path.join(feature_dir_combined, 'global')\n",
    "    os.makedirs(global_output_dir, exist_ok=True) # Ensure global output dir exists\n",
    "\n",
    "    lg_matcher = KF.LightGlueMatcher(\n",
    "        \"aliked\", {\n",
    "            \"width_confidence\": -1,\n",
    "            \"depth_confidence\": -1,\n",
    "            \"mp\": 'cuda' in str(device)\n",
    "        }\n",
    "    ).eval().to(device)\n",
    "\n",
    "    # Store match indices (relative to combined per-image features)\n",
    "    all_matches = {}\n",
    "\n",
    "    # Open combined descriptors file once\n",
    "    f_descriptors = h5py.File(descriptors_h5_path, mode='r')\n",
    "\n",
    "    print(\"Performing LightGlue matching on combined features...\")\n",
    "\n",
    "    # Iterate through shortlisted pairs\n",
    "    for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        key1 = os.path.basename(fname1)\n",
    "        key2 = os.path.basename(fname2)\n",
    "\n",
    "        kp1_pt_path = os.path.join(keypoints_subdir_path, f'{key1}.pt')\n",
    "        kp2_pt_path = os.path.join(keypoints_subdir_path, f'{key2}.pt')\n",
    "\n",
    "        # Load combined keypoints (original coordinates)\n",
    "        kp1_combined_orig = torch.load(kp1_pt_path, weights_only=False).to(device)\n",
    "        kp2_combined_orig = torch.load(kp2_pt_path, weights_only=False).to(device)\n",
    "\n",
    "        # Load combined descriptors\n",
    "        desc1_combined = torch.from_numpy(f_descriptors[key1]['data'][...]).to(device)\n",
    "        desc2_combined = torch.from_numpy(f_descriptors[key2]['data'][...]).to(device)\n",
    "\n",
    "        # Skip if zero features are found\n",
    "        if len(kp1_combined_orig) == 0 or len(kp2_combined_orig) == 0 or \\\n",
    "           len(desc1_combined) == 0 or len(desc2_combined) == 0:\n",
    "            if verbose:\n",
    "                tqdm.write(f\"Skipping {key1}-{key2}: Zero features found.\")\n",
    "            continue\n",
    "\n",
    "        # Create dummy LAFs centered at keypoints (using original coordinates)\n",
    "        kp1_tensor = kp1_combined_orig.float()[None] # Add batch dim\n",
    "        kp2_tensor = kp2_combined_orig.float()[None] # Add batch dim\n",
    "        laf1 = KF.laf_from_center_scale_ori(kp1_tensor) # Use batch size 1, scale 1.0\n",
    "        laf2 = KF.laf_from_center_scale_ori(kp2_tensor)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            scores, matches = lg_matcher(desc1_combined, desc2_combined, laf1, laf2)\n",
    "\n",
    "        n_matches = len(matches) # Number of matches\n",
    "\n",
    "        if verbose:\n",
    "            tqdm.write(f'{key1}-{key2}: {n_matches} matches')\n",
    "\n",
    "        # Store matches if enough are found\n",
    "        if n_matches >= min_matches:\n",
    "            matches_indices = matches.cpu().detach().numpy().astype('int16')\n",
    "            all_matches.setdefault(key1, {})[key2] = matches_indices\n",
    "\n",
    "    # Close the descriptors file\n",
    "    f_descriptors.close()\n",
    "\n",
    "    print(\"Matching complete. Saving global files...\")\n",
    "\n",
    "    # --- Save Global Files ---\n",
    "\n",
    "    # 1. Save images.json (list of full filenames for all images)\n",
    "    images_json_path = os.path.join(global_output_dir, 'images.json')\n",
    "    with open(images_json_path, 'w') as f_json:\n",
    "        json.dump(img_fnames, f_json, indent=2)\n",
    "    print(f\"Saved global images list to {images_json_path}\")\n",
    "\n",
    "    # 2. Save keypoints.h5 for all images (using combined keypoints per image)\n",
    "    keypoints_h5_path = os.path.join(global_output_dir, 'keypoints.h5')\n",
    "    with h5py.File(keypoints_h5_path, 'w') as f_out_kp:\n",
    "        for img_idx, img_fname in enumerate(img_fnames):\n",
    "            img_key = os.path.basename(img_fname)\n",
    "            kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt') # Path to combined KPs for this image\n",
    "            \n",
    "            # Load combined KPs for this image (original coords)\n",
    "            kp_combined_np = torch.load(kp_pt_path, weights_only=False).cpu().numpy() # Load and convert to numpy\n",
    "            f_out_kp.create_dataset(img_key, data=kp_combined_np.astype(np.float32))\n",
    "            if verbose:\n",
    "                print(f\"Saved combined KPs for {img_key} to global H5\")\n",
    "\n",
    "    print(f\"Saved global keypoints to {keypoints_h5_path}\")\n",
    "\n",
    "    # 3. Save matches.h5 for all valid pairs\n",
    "    matches_h5_path = os.path.join(global_output_dir, 'matches.h5')\n",
    "    if all_matches: # Only create file if there are matches to save\n",
    "        with h5py.File(matches_h5_path, 'w') as f_match:\n",
    "            for key1, matches_dict in all_matches.items():\n",
    "                if matches_dict:\n",
    "                    group = f_match.create_group(key1)\n",
    "                    for key2, match_data in matches_dict.items():\n",
    "                        group.create_dataset(key2, data=match_data, dtype='int16')\n",
    "        print(f\"Saved global matches to {matches_h5_path}\")\n",
    "    else:\n",
    "        print(f\"No matches found above threshold {min_matches} to save to {matches_h5_path}\")\n",
    "\n",
    "    # Return a list containing a single list of all image indices\n",
    "    all_image_indices = list(range(len(img_fnames)))\n",
    "    return [all_image_indices]\n",
    "\n",
    "# Note: This function assumes that detect_and_combine_features\n",
    "# has already been run and created the combined features in\n",
    "# data_dir/features_combined/keypoints/ and data_dir/features_combined/descriptors.h5.\n",
    "# It saves the global feature and match files into data_dir/features_combined/global/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "414e4a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.283407Z",
     "iopub.status.busy": "2025-05-19T21:31:49.283206Z",
     "iopub.status.idle": "2025-05-19T21:31:49.290117Z",
     "shell.execute_reply": "2025-05-19T21:31:49.289512Z"
    },
    "papermill": {
     "duration": 0.014414,
     "end_time": "2025-05-19T21:31:49.291262",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.276848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from time import time, sleep\n",
    "import pycolmap\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Assume import_into_colmap_cluster is defined elsewhere and can handle\n",
    "# importing keypoints and matches from the .h5 files into a COLMAP database.\n",
    "# It will need access to keypoints.h5 and matches.h5 paths.\n",
    "# from your_colmap_utils import import_into_colmap_cluster\n",
    "\n",
    "\n",
    "def run_colmap_global(\n",
    "    feature_dir,\n",
    "    images_dir,\n",
    "    timings\n",
    "):\n",
    "    \"\"\"\n",
    "    Run COLMAP reconstruction for the entire dataset using global feature and match files.\n",
    "    All error handling via try-except blocks has been removed, assuming valid inputs\n",
    "    and successful COLMAP operations.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_dir: Base directory containing 'features_combined/global' folder.\n",
    "    - images_dir: Path to raw image files.\n",
    "    - timings: dict to record durations.\n",
    "    \"\"\"\n",
    "    # Define the path to the global files\n",
    "    global_path = os.path.join(feature_dir, 'features_combined', 'global')\n",
    "\n",
    "    database_path = os.path.join(global_path, 'colmap.db')\n",
    "    image_list_path = os.path.join(global_path, 'images.json')\n",
    "    keypoints_h5_path = os.path.join(global_path, 'keypoints.h5')\n",
    "    matches_h5_path = os.path.join(global_path, 'matches.h5')\n",
    "\n",
    "    # Load image names\n",
    "    with open(image_list_path, 'r') as f:\n",
    "        # images.json in the global folder contains full paths, extract basenames\n",
    "        image_names = [os.path.basename(x) for x in json.load(f)]\n",
    "\n",
    "    # Remove existing database if it exists\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "        print(f\"[Global Reconstruction] Removed existing database at {database_path}\")\n",
    "\n",
    "    gc.collect()\n",
    "    sleep(1)\n",
    "\n",
    "    # Step 1: Import keypoints, matches into COLMAP db\n",
    "    # Your import_into_colmap_cluster function is assumed to read from\n",
    "    # keypoints_h5_path and matches_h5_path and populate the database.\n",
    "    import_into_colmap_cluster(\n",
    "        img_dir=images_dir,\n",
    "        cluster_path=global_path, # Pass the global path where H5 files are\n",
    "        database_path=database_path,\n",
    "        image_names=image_names # Pass the list of image basenames\n",
    "    )\n",
    "\n",
    "    # Step 2: RANSAC (Geometric verification)\n",
    "    t = time()\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    t_ransac = time() - t\n",
    "    timings['RANSAC_Global'] = t_ransac # Use a specific key for global timings\n",
    "    print(f'[Global Reconstruction] Ran RANSAC in {t_ransac:.4f} sec')\n",
    "\n",
    "    # Step 3: Incremental mapping for the global dataset\n",
    "    output_path = os.path.join(global_path, 'colmap_rec_aliked')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # Adjust mapper options as needed for a large global reconstruction\n",
    "    mapper_options.min_model_size = 4 # Minimum number of registered images\n",
    "    mapper_options.max_num_models = 25 # We expect only one main model\n",
    "    mapper_options.mapper.filter_max_reproj_error = 10.0 # Example, adjust as needed\n",
    "\n",
    "    t = time()\n",
    "    # pycolmap.incremental_mapping expects image_path to be the directory\n",
    "    # containing the actual image files.\n",
    "    maps = pycolmap.incremental_mapping(\n",
    "        database_path=database_path,\n",
    "        image_path=images_dir, # Path to the actual image files\n",
    "        output_path=output_path,\n",
    "        options=mapper_options\n",
    "    )\n",
    "    \n",
    "    # maps is already a dictionary of valid reconstructions, so its length gives the number of maps\n",
    "    num_successful_maps = len(maps)\n",
    "    print(f'[Global Reconstruction] Found {num_successful_maps} successful reconstruction(s).')\n",
    "\n",
    "    t_rec = time() - t\n",
    "    timings['Reconstruction_Global'] = t_rec # Use a specific key for global timings\n",
    "    print(f'[Global Reconstruction] Reconstruction done in {t_rec:.4f} sec')\n",
    "    all_maps = [maps] # Return as a list containing the single global map\n",
    "    all_maps_len = [num_successful_maps]\n",
    "    return timings, all_maps, all_maps_len\n",
    "\n",
    "# Note: This function now expects the output of match_images_global to be in\n",
    "# feature_dir/features_combined/global/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "319a8772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.303441Z",
     "iopub.status.busy": "2025-05-19T21:31:49.303243Z",
     "iopub.status.idle": "2025-05-19T21:31:49.462491Z",
     "shell.execute_reply": "2025-05-19T21:31:49.461696Z"
    },
    "papermill": {
     "duration": 0.166688,
     "end_time": "2025-05-19T21:31:49.463691",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.297003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fa8d3e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.476498Z",
     "iopub.status.busy": "2025-05-19T21:31:49.476288Z",
     "iopub.status.idle": "2025-05-19T21:31:49.480065Z",
     "shell.execute_reply": "2025-05-19T21:31:49.479415Z"
    },
    "papermill": {
     "duration": 0.011397,
     "end_time": "2025-05-19T21:31:49.481321",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.469924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_cluster_folders(base_dir):\n",
    "    for name in os.listdir(base_dir):\n",
    "        path = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(path) and name.startswith(\"cluster\"):\n",
    "            print(f\"Deleting: {path}\")\n",
    "            shutil.rmtree(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5178493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:31:49.494303Z",
     "iopub.status.busy": "2025-05-19T21:31:49.494092Z",
     "iopub.status.idle": "2025-05-19T21:35:44.647273Z",
     "shell.execute_reply": "2025-05-19T21:35:44.646361Z"
    },
    "papermill": {
     "duration": 235.205843,
     "end_time": "2025-05-19T21:35:44.693182",
     "exception": false,
     "start_time": "2025-05-19T21:31:49.487339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting on device cuda\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "25%:  0.2547\n",
      "USED 50%:  0.2804\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 142. Done in 6.5966 sec\n",
      "Performing initial ALIKED detection (1280) and LightGlue matching for cropping data...\n",
      "Running initial ALIKED detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial ALIKED Detection: 100%|██████████| 22/22 [00:01<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running initial LightGlue matching...\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial LightGlue Matching: 100%|██████████| 142/142 [00:08<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing initial matches for cropping data...\n",
      "Initial detection and matching complete. Cropping data saved to /kaggle/working/result/featureout/ETs/crop_data.h5\n",
      "Initial detection for cropping done in 10.3399 sec\n",
      "Calculating crop boxes from /kaggle/working/result/featureout/ETs/crop_data.h5...\n",
      "Crop boxes calculated and saved to /kaggle/working/result/featureout/ETs/crop_boxes.h5\n",
      "Crop boxes calculated in 0.0080 sec\n",
      "Running multi-variation ALIKED detection and combining features (with NMS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting & Combining Features: 100%|██████████| 22/22 [00:07<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-variation detection, combination, and deduplication complete.\n",
      "Features combined in 7.9923 sec\n",
      "Loaded LightGlue model\n",
      "Performing LightGlue matching on combined features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGlue Matching: 100%|██████████| 142/142 [00:10<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching complete. Saving global files...\n",
      "Saved global images list to /kaggle/working/result/featureout/ETs/features_combined/global/images.json\n",
      "Saved global keypoints to /kaggle/working/result/featureout/ETs/features_combined/global/keypoints.h5\n",
      "Saved global matches to /kaggle/working/result/featureout/ETs/features_combined/global/matches.h5\n",
      "Features matched globally in 10.6912 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 89.84it/s]\n",
      " 74%|███████▍  | 127/171 [00:00<00:00, 5135.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Global Reconstruction] Ran RANSAC in 4.5090 sec\n",
      "[Global Reconstruction] Found 1 successful reconstruction(s).\n",
      "[Global Reconstruction] Reconstruction done in 10.5718 sec\n",
      "Global reconstruction done in 53.8531 sec\n",
      "Dataset \"ETs\" -> Registered 21 / 22 images across 1 clusters\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/amy_gardens/peach_0000.png\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/fbk_vineyard/vineyard_split_1_frame_0900.png\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_haiper/bike_image_004.png\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_heritage\": 209 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/209 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_heritage/cyprus_dsc_6480.png\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_theather_imc2024_church\": 76 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_theather_imc2024_church/church_00004.png\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_dioscuri_baalshamin\": 138 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_dioscuri_baalshamin/baalshamin_182z.png\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_lizard_pond\": 214 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/214 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_lizard_pond/lizard_00003.png\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_brandenburg_british_buckingham\": 225 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_brandenburg_british_buckingham/brandenburg_gate_01069771_8567470929.png\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_piazzasanmarco_grandplace\": 168 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/168 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_piazzasanmarco_grandplace/grand_place_brussels_00460368_4162644685.png\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_sacrecoeur_trevi_tajmahal\": 225 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_sacrecoeur_trevi_tajmahal/sacre_coeur_02928139_3448003521.png\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_stpeters_stpauls\": 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_stpeters_stpauls/st_pauls_cathedral_00162897_2573777698.png\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "USED 50%:  0.2767\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 650. Done in 9.5356 sec\n",
      "Performing initial ALIKED detection (1280) and LightGlue matching for cropping data...\n",
      "Running initial ALIKED detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial ALIKED Detection: 100%|██████████| 51/51 [00:05<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running initial LightGlue matching...\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial LightGlue Matching: 100%|██████████| 650/650 [00:34<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing initial matches for cropping data...\n",
      "Initial detection and matching complete. Cropping data saved to /kaggle/working/result/featureout/stairs/crop_data.h5\n",
      "Initial detection for cropping done in 39.7550 sec\n",
      "Calculating crop boxes from /kaggle/working/result/featureout/stairs/crop_data.h5...\n",
      "Crop boxes calculated and saved to /kaggle/working/result/featureout/stairs/crop_boxes.h5\n",
      "Crop boxes calculated in 0.0190 sec\n",
      "Running multi-variation ALIKED detection and combining features (with NMS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting & Combining Features: 100%|██████████| 51/51 [00:21<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-variation detection, combination, and deduplication complete.\n",
      "Features combined in 21.5801 sec\n",
      "Loaded LightGlue model\n",
      "Performing LightGlue matching on combined features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGlue Matching: 100%|██████████| 650/650 [00:46<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching complete. Saving global files...\n",
      "Saved global images list to /kaggle/working/result/featureout/stairs/features_combined/global/images.json\n",
      "Saved global keypoints to /kaggle/working/result/featureout/stairs/features_combined/global/keypoints.h5\n",
      "Saved global matches to /kaggle/working/result/featureout/stairs/features_combined/global/matches.h5\n",
      "Features matched globally in 46.5853 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 22.33it/s]\n",
      " 33%|███▎      | 375/1128 [00:00<00:00, 5492.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Global Reconstruction] Ran RANSAC in 13.3617 sec\n",
      "[Global Reconstruction] Found 3 successful reconstruction(s).\n",
      "[Global Reconstruction] Reconstruction done in 41.5079 sec\n",
      "Global reconstruction done in 177.6079 sec\n",
      "Dataset \"stairs\" -> Registered 47 / 51 images across 3 clusters\n",
      "\n",
      "Results\n",
      "Dataset \"ETs\" -> Registered 21 / 22 images across 1 clusters\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "Dataset \"stairs\" -> Registered 47 / 51 images across 3 clusters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from time import time, sleep\n",
    "import pycolmap\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy # For deepcopy(image.cam_from_world...)\n",
    "# Assuming `clear_output` is from IPython.display\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Placeholder definitions for constants and functions\n",
    "# Ensure these are correctly imported or defined in your actual script\n",
    "# For example:\n",
    "# from .config import (\n",
    "#     MIN_MATCHES_FOR_GRAPH_EDGE, KEYPOINTS_SUBDIR, DESCRIPTORS_H5,\n",
    "#     CROP_DATA, CROP_BOXES_FILE, CROP_PADDING_FACTOR, MIN_CROP_DIMENSION\n",
    "# )\n",
    "# from .feature_extraction_utils import (\n",
    "#     load_pil_image, transform_points_from_processed,\n",
    "#     perform_initial_detection_and_matching, detect_and_combine_features\n",
    "# )\n",
    "# from .matching_utils import get_image_pairs_shortlist, match_images_global\n",
    "# from .colmap_utils import run_colmap_global, import_into_colmap_cluster\n",
    "\n",
    "# If not imported, define placeholders here to avoid NameErrors:\n",
    "# (These values should match your actual configurations)\n",
    "# MIN_MATCHES_FOR_GRAPH_EDGE = 100\n",
    "# KEYPOINTS_SUBDIR = 'keypoints'\n",
    "# DESCRIPTORS_H5 = 'descriptors.h5'\n",
    "# CROP_DATA = 'crop_data.h5'\n",
    "# CROP_BOXES_FILE = 'crop_boxes.h5'\n",
    "# CROP_PADDING_FACTOR = 0.1\n",
    "# MIN_CROP_DIMENSION = 512\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "# is_train and samples are assumed to be defined externally, e.g., from your notebook setup\n",
    "# For demonstration:\n",
    "# is_train = True\n",
    "# class Prediction:\n",
    "#     def __init__(self, filename, cluster_index=None, rotation=None, translation=None):\n",
    "#         self.filename = filename\n",
    "#         self.cluster_index = cluster_index\n",
    "#         self.rotation = rotation\n",
    "#         self.translation = translation\n",
    "# samples = {\n",
    "#     'stairs': [Prediction('image001.jpg'), Prediction('image002.jpg'), Prediction('image003.jpg'), Prediction('image004.jpg'), Prediction('image005.jpg')]\n",
    "# }\n",
    "\n",
    "\n",
    "if is_train:\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "        # New data.\n",
    "        'amy_gardens',\n",
    "        'ETs',\n",
    "        'fbk_vineyard',\n",
    "        'stairs',\n",
    "        # Data from IMC 2023 and 2024.\n",
    "        # 'imc2024_dioscuri_baalshamin',\n",
    "        # 'imc2023_theather_imc2024_church',\n",
    "        # 'imc2023_heritage',\n",
    "        # 'imc2023_haiper',\n",
    "        # 'imc2024_lizard_pond',\n",
    "        # Crowdsourced PhotoTourism data.\n",
    "        # 'pt_stpeters_stpauls',\n",
    "        # 'pt_brandenburg_british_buckingham',\n",
    "        # 'pt_piazzasanmarco_grandplace',\n",
    "        # 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_augmentation\":[],\n",
    "    \"feature_merge\":[],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "print (f\"Extracting on device {DEVICE}\") # Changed to DEVICE for consistency\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # All try-except blocks around main pipeline steps are removed.\n",
    "    # We assume successful execution for each step.\n",
    "    try:\n",
    "\n",
    "        t_start_pipeline = time() # Start timer for the whole pipeline for this dataset\n",
    "    \n",
    "        # 1. Image Pair Shortlisting\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.5,\n",
    "            min_pairs = 10,\n",
    "            exhaustive_if_less = 20,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Perform initial detection and matching for cropping data\n",
    "        # (This step will overwrite crop_data.h5 if it exists)\n",
    "        t = time()\n",
    "        # pass `feature_dir` as the `data_dir` for initial_detection_and_matching\n",
    "        # so it creates its temp files within feature_dir\n",
    "        crop_data_file = perform_initial_detection_and_matching(images, index_pairs, data_dir=feature_dir, device=DEVICE)\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Initial detection for cropping done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "        \n",
    "        # 3. Calculate crop boxes based on initial match analysis\n",
    "        t = time()\n",
    "        # `calculate_crop_boxes` only needs `data_dir` which is `feature_dir` in this context\n",
    "        # It will find crop_data.h5 within feature_dir and save crop_boxes.h5 there too.\n",
    "        # The return value `crop_info_file` is effectively the path to `crop_boxes.h5`\n",
    "        crop_info_file = calculate_crop_boxes(data_dir=feature_dir)\n",
    "        timings['feature_augmentation'].append(time() - t)\n",
    "        print(f'Crop boxes calculated in {time() - t:.4f} sec') # Renamed print output\n",
    "        gc.collect()    \n",
    "        \n",
    "        # 4. Perform multi-variation ALIKED detection, combine features per image, save to .pt/.h5\n",
    "        t = time()\n",
    "        # `detect_and_combine_features` expects `feature_dir` as the base for its outputs\n",
    "        # and `crop_info_file` (which is `crop_boxes.h5`) for crop information.\n",
    "        detect_and_combine_features(images, crop_info_file, os.path.join(feature_dir, 'features_combined'), device=DEVICE)\n",
    "        timings['feature_merge'].append(time() - t)\n",
    "        print(f'Features combined in {time() - t:.4f} sec') # Renamed print output\n",
    "        gc.collect()   \n",
    "    \n",
    "        # 5. Load combined features and perform LightGlue matching, save global matches to .h5\n",
    "        t = time()\n",
    "        # `match_images_global` expects `data_dir` (where `features_combined` is a subdir)\n",
    "        # which is `feature_dir` in this context.\n",
    "        match_images_global(images, index_pairs, data_dir=feature_dir, device=DEVICE)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched globally in {time() - t:.4f} sec') # Renamed print output\n",
    "        gc.collect()\n",
    "        \n",
    "        # 6. Run COLMAP global reconstruction\n",
    "        # timings dict is passed by reference, so updates inside run_colmap_global are reflected\n",
    "        timings, all_maps, all_maps_len = run_colmap_global(feature_dir, images_dir, timings)\n",
    "        timings['Reconstruction'].append(time() - t_start_pipeline) # This timing is for the whole reconstruction part\n",
    "        print(f'Global reconstruction done in {time() - t_start_pipeline:.4f} sec') # This measures the whole pipeline\n",
    "        # clear_output(wait=False) # Uncomment if using in Jupyter/Colab\n",
    "    \n",
    "        registered = 0\n",
    "        global_num = 0\n",
    "        # Iterate through all maps and registered images to update predictions\n",
    "        for index, maps_dict in enumerate(all_maps): # `all_maps` is a list containing potentially one dictionary of maps\n",
    "            for map_idx, cur_map in maps_dict.items(): # Each `maps_dict` is Dict[int, Reconstruction]\n",
    "                for _, image in cur_map.images.items():\n",
    "                    prediction_index = filename_to_index[image.name]\n",
    "                    predictions[prediction_index].cluster_index = global_num + map_idx\n",
    "                    predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                    predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                    registered += 1\n",
    "            global_num += all_maps_len[index]                \n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images across {global_num} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "    \n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "# print('\\nTimings')\n",
    "# for k, v in timings.items():\n",
    "#      print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c69775b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:35:44.790828Z",
     "iopub.status.busy": "2025-05-19T21:35:44.790511Z",
     "iopub.status.idle": "2025-05-19T21:35:44.973140Z",
     "shell.execute_reply": "2025-05-19T21:35:44.972228Z"
    },
    "papermill": {
     "duration": 0.233952,
     "end_time": "2025-05-19T21:35:44.974511",
     "exception": false,
     "start_time": "2025-05-19T21:35:44.740559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster0,another_et_another_et001.png,0.895209201;-0.223757540;0.385399857;0.330351604;0.913646261;-0.236893072;-0.299112527;0.339386319;0.891823201,-0.891446232;-0.987406864;0.949917937\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster0,another_et_another_et002.png,0.896448752;-0.228359804;0.379778139;0.335733847;0.909349889;-0.245694042;-0.289244565;0.347756493;0.891853689,-0.796626808;-0.427795747;0.041047928\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster0,another_et_another_et003.png,0.861906479;-0.250342449;0.440960179;0.384948663;0.889085242;-0.247673090;-0.330048100;0.383218073;0.862677321,-1.029921046;0.679308230;-1.238214999\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster0,another_et_another_et004.png,0.881905660;-0.223805741;0.414913722;0.298563969;0.946276600;-0.124177910;-0.364831417;0.233391489;0.901347020,-0.908841033;-0.693847602;-0.908375595\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster0,another_et_another_et005.png,0.872339504;-0.191372456;0.449889290;0.311097535;0.927150007;-0.208832921;-0.377149989;0.322132656;0.868325076,-1.286589536;-1.135245899;0.106500232\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster0,another_et_another_et006.png,0.989375604;-0.140945649;0.035640409;0.144230355;0.920791860;-0.362408547;0.018262510;0.363698604;0.931337644,0.823880907;-0.334120918;0.378158338\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster0,another_et_another_et007.png,0.967239458;-0.115615552;-0.226010785;0.031884646;0.938552245;-0.343661246;0.251855515;0.325196444;0.911491126,2.079452785;-0.259708389;0.378560976\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster0,another_et_another_et008.png,0.867754631;-0.086443068;-0.489417507;-0.080609043;0.947238460;-0.310228112;0.490412155;0.308653358;0.815002468,3.240039384;-0.666744956;1.279276902\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster0,another_et_another_et009.png,0.715007902;-0.035819665;-0.698198146;-0.213338600;0.939873055;-0.266693236;0.665770487;0.339640386;0.664374944,4.294597976;-1.092523536;2.022534174\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5787cf10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:35:45.072202Z",
     "iopub.status.busy": "2025-05-19T21:35:45.071919Z",
     "iopub.status.idle": "2025-05-19T21:35:45.076591Z",
     "shell.execute_reply": "2025-05-19T21:35:45.075762Z"
    },
    "papermill": {
     "duration": 0.054437,
     "end_time": "2025-05-19T21:35:45.077815",
     "exception": false,
     "start_time": "2025-05-19T21:35:45.023378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e71aa",
   "metadata": {
    "papermill": {
     "duration": 0.047056,
     "end_time": "2025-05-19T21:35:45.172468",
     "exception": false,
     "start_time": "2025-05-19T21:35:45.125412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11217117,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 273.980101,
   "end_time": "2025-05-19T21:35:48.939293",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-19T21:31:14.959192",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
