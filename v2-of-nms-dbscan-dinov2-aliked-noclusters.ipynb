{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11217117,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":4535,"sourceType":"modelInstanceVersion","modelInstanceId":3327,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Example submission\n\nImage Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n\nThis notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n\nRemember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# IMPORTANT \n#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:09:23.534350Z","iopub.execute_input":"2025-05-22T22:09:23.534871Z","iopub.status.idle":"2025-05-22T22:09:29.246125Z","shell.execute_reply.started":"2025-05-22T22:09:23.534826Z","shell.execute_reply":"2025-05-22T22:09:29.244967Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nInstalling collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\n  Attempting uninstall: kornia-rs\n    Found existing installation: kornia_rs 0.1.8\n    Uninstalling kornia_rs-0.1.8:\n      Successfully uninstalled kornia_rs-0.1.8\n  Attempting uninstall: kornia\n    Found existing installation: kornia 0.8.0\n    Uninstalling kornia-0.8.0:\n      Successfully uninstalled kornia-0.8.0\nSuccessfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\n\n# IMPORTANT Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric\nfrom sklearn.cluster import DBSCAN\nfrom PIL import Image, ImageDraw # Import ImageDraw for drawing keypoints\n\n# ... other imports","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:09:29.247410Z","iopub.execute_input":"2025-05-22T22:09:29.247714Z","iopub.status.idle":"2025-05-22T22:09:50.162974Z","shell.execute_reply.started":"2025-05-22T22:09:29.247689Z","shell.execute_reply":"2025-05-22T22:09:50.162255Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nprint(\"PyTorch version:\", torch.__version__)\nimport sys\nprint(\"Python version:\", sys.version)\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"Device count:\", torch.cuda.device_count())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.496069Z","iopub.execute_input":"2025-05-22T22:13:36.496402Z","iopub.status.idle":"2025-05-22T22:13:36.505551Z","shell.execute_reply.started":"2025-05-22T22:13:36.496379Z","shell.execute_reply":"2025-05-22T22:13:36.504635Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu121\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nCUDA available: True\nCUDA version: 12.1\nDevice count: 1\nCurrent device: 0\nDevice name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!rm -rf /kaggle/working/result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.506709Z","iopub.execute_input":"2025-05-22T22:13:36.506996Z","iopub.status.idle":"2025-05-22T22:13:36.678215Z","shell.execute_reply.started":"2025-05-22T22:13:36.506959Z","shell.execute_reply":"2025-05-22T22:13:36.676951Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Do not forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.680281Z","iopub.execute_input":"2025-05-22T22:13:36.680537Z","iopub.status.idle":"2025-05-22T22:13:36.685362Z","shell.execute_reply.started":"2025-05-22T22:13:36.680513Z","shell.execute_reply":"2025-05-22T22:13:36.684545Z"}},"outputs":[{"name":"stdout","text":"device=device(type='cuda', index=0)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def load_pil_image(fname):\n    \"\"\"Loads an image using PIL.\"\"\"\n    return Image.open(fname).convert('RGB')\n\ndef get_image_size(fname):\n    \"\"\"Gets image size (width, height) using PIL.\"\"\"\n    with Image.open(fname) as img:\n        return img.size # (width, height)\n\ndef get_original_coords(kp_coords, img_orig_size, variation_info):\n    \"\"\"\n    Transforms keypoint coordinates from variation space back to original image space.\n\n    Args:\n        kp_coords (np.ndarray): Keypoint coordinates [N, 2] in the variation space.\n        img_orig_size (tuple): Original image size (width, height).\n        variation_info (dict): Dictionary containing 'type' ('orig' or 'crop'),\n                               'scale_factor' (scale used for resize),\n                               'crop_box' ([x, y, w, h] in original coords, None if type is 'orig').\n\n    Returns:\n        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n    \"\"\"\n    if len(kp_coords) == 0:\n        return np.empty((0, 2))\n\n    coords = kp_coords.copy() # Work on a copy\n\n    # 1. Reverse scaling\n    scale_factor = variation_info['scale_factor']\n    coords /= scale_factor # Now coords are in the space of the original/cropped image (before resize)\n\n    # 2. Reverse cropping offset\n    if variation_info['type'] == 'crop' and variation_info['crop_box'] is not None:\n        x_crop, y_crop, _, _ = variation_info['crop_box']\n        coords[:, 0] += x_crop\n        coords[:, 1] += y_crop\n\n    # Ensure points are within original image bounds (optional, but good practice)\n    # coords[:, 0] = np.clip(coords[:, 0], 0, img_orig_size[0] - 1)\n    # coords[:, 1] = np.clip(coords[:, 1], 0, img_orig_size[1] - 1)\n\n    return coords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.686875Z","iopub.execute_input":"2025-05-22T22:13:36.687115Z","iopub.status.idle":"2025-05-22T22:13:36.701672Z","shell.execute_reply.started":"2025-05-22T22:13:36.687094Z","shell.execute_reply":"2025-05-22T22:13:36.700932Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# --- Configuration ---\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nGLOBAL_DESC_MODEL = '/kaggle/input/dinov2/pytorch/base/1' # Path to your DINOv2 model\nDATA_DIR = '.' # Base directory for outputs\n# FEATURE_DIR = os.path.join(DATA_DIR, 'features_combined')\n# MATCH_DIR = os.path.join(DATA_DIR, 'matches_global')\n\n# Initial detection parameters for cropping data collection\nINITIAL_DETECTION_RESIZE = 1280\nINITIAL_DETECTION_NUM_FEATURES = 2048\n\n# Parameters for TTA detection and combination\nTTA_SCALES = [2048]\nTTA_NUM_FEATURES = 4096\nUSE_CROPPED_IMAGES = True\n\n# Parameters for New Cropping Method\nMIN_PAIRS_FOR_CROPPING = 10\nCROP_PADDING = 50\nDBSCAN_EPS = 20 # Fixed EPS fallback or base value\nDBSCAN_MIN_SAMPLES = 5 # DBSCAN min_samples parameter\n# New config for adaptive EPS based on resolution\nDBSCAN_EPS_RESOLUTION_RATIO = 0.02 # Ratio of max image dimension for EPS (e.g., 0.01 -> 1% of longer side)\n\n\n# Coordinate precision for deduplication (rounding float coordinates)\nCOORD_PRECISION = 1 # Number of decimal places to round coordinates for uniqueness check\n\n# Matching parameters\nMIN_MATCHES_PER_VARIATION = 5 # Lowered this threshold slightly, as combining might filter some\nMIN_TOTAL_MATCHES_PER_PAIR = 20 # Minimum unique matches for a pair to be saved in global list\n\n# Output file names (within FEATURE_DIR and MATCH_DIR)\nKEYPOINTS_SUBDIR = 'keypoints'\nDESCRIPTORS_H5 = 'descriptors.h5'\nMATCHES_PT = 'matches.pt'\nCROP_DATA = 'crop_data.h5'\nCROP_INFO = 'crop_info.h5'\n\n# Parameters for graph building and clustering thresholds\n# These are the internal names used in the function; map external arguments to these if needed\nMIN_MATCHES_FOR_GRAPH_EDGE = 20  # Min matches for adding edge to graph\nMIN_MATCHES_FOR_FILTERED_GRAPH = 75 # Min matches for filtering graph (your aliked_dis_min)\nMIN_IMAGES_PER_CLUSTER = 5 # Min images in a final cluster\n\n\n# Config for NMS\nNMS_SIZE_PIXELS = 3 # Radius in pixels for NMS suppression\nNMS_SIZE_PIXELS_ratio = 0.003\n\n# --- NEW: Constant for Keypoint Visualization ---\nVISUALIZE_KEYPOINTS = False # Set to True to enable saving visualized keypoints\nKEYPOINT_VIS_SUBDIR = 'keypoint_visualizations' # Subdirectory for saving visualized images\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.702504Z","iopub.execute_input":"2025-05-22T22:13:36.702773Z","iopub.status.idle":"2025-05-22T22:13:36.721746Z","shell.execute_reply.started":"2025-05-22T22:13:36.702739Z","shell.execute_reply":"2025-05-22T22:13:36.721102Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\n\n# Must Use efficientnet global descriptor to get matching shortlists.\ndef get_global_desc(fnames, device = torch.device('cpu')):\n    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = model.eval()\n    model = model.to(device)\n    global_descs_dinov2 = []\n    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n        timg = load_torch_image(img_fname_full)\n        with torch.inference_mode():\n            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs)\n            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n        global_descs_dinov2.append(dino_mac.detach().cpu())\n    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n    return global_descs_dinov2\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 30,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc(fnames, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n\n    # 只分析上三角（去掉对角线），避免重复\n    triu_indices = np.triu_indices_from(dm, k=1)\n    dm_flat = dm[triu_indices]\n    \n    # 打印统计信息\n    print(\"Distance Matrix Statistics:\")\n    print(f\"Min:  {dm_flat.min():.4f}\")\n    print(f\"Max:  {dm_flat.max():.4f}\")\n    print(f\"Mean: {dm_flat.mean():.4f}\")\n    print(f\"Std:  {dm_flat.std():.4f}\")\n    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n    # removing half\n    # thr = min(np.percentile(dm_flat, 50), sim_th)\n    mask = dm <= np.percentile(dm_flat, 50)\n    # print(\"thr :\", thr)\n    # mask = dm<=threshold\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < threshold:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.722553Z","iopub.execute_input":"2025-05-22T22:13:36.722834Z","iopub.status.idle":"2025-05-22T22:13:36.743730Z","shell.execute_reply.started":"2025-05-22T22:13:36.722799Z","shell.execute_reply":"2025-05-22T22:13:36.742936Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# --- 自定义 NMS 函数 ---\ndef custom_nms_2d_keypoints(keypoints_np, scores_np, nms_radius):\n    \"\"\"\n    Perform Non-Maximum Suppression on 2D keypoints based on scores.\n    This is a custom implementation to replace kornia.feature.non_maximum_suppression2d\n    due to potential API differences across Kornia versions.\n\n    Args:\n        keypoints_np (np.ndarray): Keypoint coordinates (N, 2) in pixel space.\n        scores_np (np.ndarray): Scores for each keypoint (N,).\n        nms_radius (float): Radius for suppression in pixel units.\n\n    Returns:\n        np.ndarray: Indices of the keypoints that survive NMS (original indices).\n    \"\"\"\n    if len(keypoints_np) == 0:\n        return np.array([], dtype=int)\n\n    # Get indices sorted by score in descending order\n    order = scores_np.argsort()[::-1]\n\n    keep = []\n    # suppressed array to track which keypoints have been suppressed\n    suppressed = np.zeros(len(keypoints_np), dtype=bool)\n\n    for i_sorted in order: # Iterate through keypoints from highest score to lowest\n        if suppressed[i_sorted]:\n            continue # This keypoint has already been suppressed by a higher-scoring one\n\n        keep.append(i_sorted) # Keep this keypoint\n\n        current_kp = keypoints_np[i_sorted]\n\n        # Calculate squared distances from the current keypoint to all other keypoints\n        # Using squared distance avoids sqrt for efficiency if only comparing to radius^2\n        distances_sq = np.sum((keypoints_np - current_kp)**2, axis=1)\n\n        # Identify keypoints within the suppression radius\n        points_within_radius_mask = distances_sq < nms_radius**2\n\n        # Mark these keypoints as suppressed\n        suppressed[points_within_radius_mask] = True\n\n    return np.array(keep, dtype=int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.744674Z","iopub.execute_input":"2025-05-22T22:13:36.744975Z","iopub.status.idle":"2025-05-22T22:13:36.760796Z","shell.execute_reply.started":"2025-05-22T22:13:36.744945Z","shell.execute_reply":"2025-05-22T22:13:36.760145Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import os\nimport h5py\nimport numpy as np\n\n# Assuming these constants are defined elsewhere in your setup\n# from .config import CROP_DATA, CROP_BOXES_FILE, CROP_PADDING_FACTOR, MIN_CROP_DIMENSION\n\n# Placeholder constants for demonstration if not imported\nCROP_DATA = 'crop_data.h5'\nCROP_BOXES_FILE = 'crop_boxes.h5' # File to save the final crop boxes\nCROP_PADDING_FACTOR = 0.1 # e.g., 10% padding\nMIN_CROP_DIMENSION = 512 # Minimum width/height for a crop box\n\ndef calculate_crop_boxes(data_dir: str):\n    \"\"\"\n    Calculates the crop boxes for each image based on the frequent keypoints\n    stored in the crop_data_file.\n    Assumes all input data (HDF5 files, datasets, attributes) are valid and exist.\n\n    Args:\n        data_dir (str): Directory where crop_data.h5 is located and\n                        where crop_boxes.h5 will be saved.\n\n    Returns:\n        str: Path to the generated crop_boxes.h5 file.\n    \"\"\"\n    crop_data_file = os.path.join(data_dir, CROP_DATA)\n    crop_boxes_output_file = os.path.join(data_dir, CROP_BOXES_FILE)\n\n    print(f\"Calculating crop boxes from {crop_data_file}...\")\n\n    image_crop_boxes = {}\n\n    with h5py.File(crop_data_file, 'r') as f_crop_data:\n        for img_key in f_crop_data.keys():\n            # Retrieve keypoints and original image size, assuming they are present and valid\n            kps = f_crop_data[img_key]['keypoints'][...] # Now simply 'keypoints'\n            original_pil_size = f_crop_data[img_key].attrs['original_pil_size'] # (W, H)\n            original_width, original_height = original_pil_size\n\n            if len(kps) == 0:\n                print(f\"Warning: No frequent keypoints for {img_key}. Skipping crop box calculation.\")\n                continue\n\n            # Calculate bounding box of keypoints\n            min_x, min_y = np.min(kps, axis=0)\n            max_x, max_y = np.max(kps, axis=0)\n\n            # Apply padding\n            padding_w = (max_x - min_x) * CROP_PADDING_FACTOR\n            padding_h = (max_y - min_y) * CROP_PADDING_FACTOR\n\n            crop_min_x = max(0, min_x - padding_w)\n            crop_min_y = max(0, min_y - padding_h)\n            crop_max_x = min(original_width, max_x + padding_w)\n            crop_max_y = min(original_height, max_y + padding_h)\n\n            # Ensure minimum crop dimensions\n            current_crop_width = crop_max_x - crop_min_x\n            current_crop_height = crop_max_y - crop_min_y\n\n            if current_crop_width < MIN_CROP_DIMENSION:\n                center_x = (crop_min_x + crop_max_x) / 2\n                crop_min_x = max(0, center_x - MIN_CROP_DIMENSION / 2)\n                crop_max_x = min(original_width, center_x + MIN_CROP_DIMENSION / 2)\n                # Adjust if clipping occurred\n                if crop_max_x - crop_min_x < MIN_CROP_DIMENSION:\n                    if crop_min_x == 0:\n                        crop_max_x = min(original_width, MIN_CROP_DIMENSION)\n                    elif crop_max_x == original_width:\n                        crop_min_x = max(0, original_width - MIN_CROP_DIMENSION)\n\n            if current_crop_height < MIN_CROP_DIMENSION:\n                center_y = (crop_min_y + crop_max_y) / 2\n                crop_min_y = max(0, center_y - MIN_CROP_DIMENSION / 2)\n                crop_max_y = min(original_height, center_y + MIN_CROP_DIMENSION / 2)\n                # Adjust if clipping occurred\n                if crop_max_y - crop_min_y < MIN_CROP_DIMENSION:\n                    if crop_min_y == 0:\n                        crop_max_y = min(original_height, MIN_CROP_DIMENSION)\n                    elif crop_max_y == original_height:\n                        crop_min_y = max(0, original_height - MIN_CROP_DIMENSION)\n\n            # Ensure integer coordinates for crop box (x_min, y_min, x_max, y_max)\n            crop_box = np.array([\n                int(round(crop_min_x)),\n                int(round(crop_min_y)),\n                int(round(crop_max_x)),\n                int(round(crop_max_y))\n            ], dtype=np.int32)\n\n            image_crop_boxes[img_key] = crop_box\n\n    # Save the calculated crop boxes to a new HDF5 file\n    with h5py.File(crop_boxes_output_file, 'w') as f_crop_boxes:\n        for img_key, crop_box in image_crop_boxes.items():\n            f_crop_boxes.create_dataset(img_key, data=crop_box)\n\n    print(f\"Crop boxes calculated and saved to {crop_boxes_output_file}\")\n    return crop_boxes_output_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.762979Z","iopub.execute_input":"2025-05-22T22:13:36.763194Z","iopub.status.idle":"2025-05-22T22:13:36.780561Z","shell.execute_reply.started":"2025-05-22T22:13:36.763174Z","shell.execute_reply":"2025-05-22T22:13:36.779810Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# ... (imports, configs, utility functions like load_torch_image, load_pil_image, get_image_size)\n\ndef perform_initial_detection_and_matching(img_fnames, index_pairs, data_dir, device=DEVICE):\n    \"\"\"\n    Performs detection (ALIKED) and matching (LightGlue) on original images\n    at a base resolution (e.g., 1024) to collect data for the cropping step.\n    Stores keypoints (in original image coordinates) and matches in temporary HDF5.\n    Analyzes matches to create crop data file.\n    Assumes all image inputs are valid and exist.\n    \"\"\"\n    temp_feature_dir = os.path.join(data_dir, '.temp_crop_features')\n    os.makedirs(temp_feature_dir, exist_ok=True)\n\n    initial_feature_file = os.path.join(temp_feature_dir, f'initial_features_{INITIAL_DETECTION_RESIZE}.h5')\n    initial_match_file = os.path.join(temp_feature_dir, f'initial_matches_{INITIAL_DETECTION_RESIZE}.h5')\n    crop_data_file = os.path.join(data_dir, CROP_DATA)\n\n    if os.path.exists(crop_data_file):\n        print(f\"Initial detection and matching data for cropping exists: {crop_data_file}. deleting.\")\n        os.remove(crop_data_file)\n\n    print(f\"Performing initial ALIKED detection ({INITIAL_DETECTION_RESIZE}) and LightGlue matching for cropping data...\")\n\n    # 1. Initial Detection\n    print(\"Running initial ALIKED detection...\")\n    extractor = ALIKED(max_num_keypoints=INITIAL_DETECTION_NUM_FEATURES, detection_threshold=0.2).eval().to(device, dtype=torch.float32)\n    extractor.preprocess_conf[\"resize\"] = INITIAL_DETECTION_RESIZE\n\n    with h5py.File(initial_feature_file, mode='w') as f_kp_desc:\n        for img_path in tqdm(img_fnames, desc=\"Initial ALIKED Detection\"):\n            img_key = os.path.basename(img_path)\n\n            img_orig_pil = load_pil_image(img_path)\n            original_pil_size = img_orig_pil.size # (W, H)\n\n            timg = K.image_to_tensor(np.array(img_orig_pil), keepdim=True).to(device, torch.float32) / 255.0 # Normalize\n            if timg.ndim == 3: timg = timg[None, ...] # Ensure BxCxHxW\n\n            with torch.inference_mode():\n                feats = extractor.extract(timg)\n\n                # Keypoints (kps) are already in ORIGINAL image coordinates from ALIKED\n                kps = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                desc = feats['descriptors'].reshape(len(kps), -1).detach().cpu().numpy()\n\n            # Save keypoints and descriptors\n            img_group = f_kp_desc.create_group(img_key)\n            img_group.create_dataset('keypoints', data=kps.astype(np.float32)) # Stored simply as 'keypoints'\n            img_group.create_dataset('descriptors', data=desc.astype(np.float32))\n            img_group.attrs['original_pil_size'] = original_pil_size # (W, H) tuple\n            img_group.attrs['original_path'] = img_path # Store original path\n\n    # 2. Initial Matching\n    print(\"Running initial LightGlue matching...\")\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                 \"depth_confidence\": -1,\n                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    if device == torch.device('cpu'):\n        lg_matcher.to('cpu')\n\n    # Ensure initial_match_file is created even if empty\n    with h5py.File(initial_match_file, mode='w') as f:\n        pass # Create an empty file\n\n    # Read keys from the initial_feature_file HDF5\n    extracted_image_keys = []\n    with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read:\n        extracted_image_keys = list(f_kp_desc_read.keys())\n\n    # Filter index_pairs to include only pairs where both images had features extracted\n    filtered_index_pairs = []\n    for idx1, idx2 in index_pairs:\n        key1, key2 = os.path.basename(img_fnames[idx1]), os.path.basename(img_fnames[idx2])\n        if key1 in extracted_image_keys and key2 in extracted_image_keys:\n            filtered_index_pairs.append((idx1, idx2))\n\n    if not filtered_index_pairs:\n        print(\"No image pairs with extracted features to perform initial matching.\")\n    else:\n        with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read, \\\n             h5py.File(initial_match_file, mode='a') as f_match:\n\n            for idx1, idx2 in tqdm(filtered_index_pairs, desc=\"Initial LightGlue Matching\"):\n                fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n                key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n\n                # Load keypoints and descriptors from the initial detection file (these are at original scale)\n                kp1 = torch.from_numpy(f_kp_desc_read[key1]['keypoints'][...]).to(device)\n                kp2 = torch.from_numpy(f_kp_desc_read[key2]['keypoints'][...]).to(device)\n                desc1 = torch.from_numpy(f_kp_desc_read[key1]['descriptors'][...]).to(device)\n                desc2 = torch.from_numpy(f_kp_desc_read[key2]['descriptors'][...]).to(device)\n\n                if len(kp1) == 0 or len(kp2) == 0:\n                    continue\n\n                # Create LAFs based on original scale keypoints\n                laf1 = KF.laf_from_center_scale_ori(kp1[None])\n                laf2 = KF.laf_from_center_scale_ori(kp2[None])\n\n                with torch.inference_mode():\n                    dists, idxs = lg_matcher(desc1, desc2, laf1, laf2)\n                if len(idxs) > 0:\n                    group = f_match.require_group(key1)\n                    group.create_dataset(key2, data=idxs.detach().cpu().numpy().astype(np.int32))\n\n    # --- Analyze Initial Matches for Cropping Data ---\n    print(\"Analyzing initial matches for cropping data...\")\n\n    kp_match_pairs = {}\n\n    with h5py.File(initial_match_file, mode='r') as f_match:\n        for img_key1 in f_match.keys():\n            for img_key2 in f_match[img_key1].keys():\n                matches = f_match[img_key1][img_key2][...] # Indices (kp1_idx, kp2_idx)\n\n                for kp1_idx, kp2_idx in matches:\n                    if img_key1 not in kp_match_pairs: kp_match_pairs[img_key1] = {}\n                    if kp1_idx not in kp_match_pairs[img_key1]: kp_match_pairs[img_key1][kp1_idx] = set()\n                    kp_match_pairs[img_key1][kp1_idx].add(img_key2)\n\n                    if img_key2 not in kp_match_pairs: kp_match_pairs[img_key2] = {}\n                    if kp2_idx not in kp_match_pairs[img_key2]: kp_match_pairs[img_key2][kp2_idx] = set()\n                    kp_match_pairs[img_key2][kp2_idx].add(img_key1)\n\n\n    frequent_kp_data = {}\n\n    with h5py.File(initial_feature_file, mode='r') as f_kp_desc:\n        for img_key in kp_match_pairs.keys():\n            # Retrieve keypoints directly; they are already at original scale\n            original_scale_kps = f_kp_desc[img_key]['keypoints'][...]\n            original_pil_size = f_kp_desc[img_key].attrs['original_pil_size'] # (W, H)\n\n            kp_data_dict = kp_match_pairs[img_key]\n\n            frequent_indices = [kp_idx for kp_idx, matched_pairs in kp_data_dict.items()\n                                if len(matched_pairs) >= MIN_PAIRS_FOR_CROPPING and kp_idx < len(original_scale_kps)]\n\n\n            if frequent_indices:\n                # Get coordinates directly from the stored keypoints (already at original scale)\n                frequent_kps = original_scale_kps[frequent_indices]\n\n                # Check for valid coordinates (non-negative)\n                valid_frequent_kps = frequent_kps[~np.any(frequent_kps < 0, axis=1)]\n\n\n                if len(valid_frequent_kps) > 0:\n                    frequent_kp_data[img_key] = {\n                        'keypoints': valid_frequent_kps,\n                        'original_pil_size': original_pil_size # ADDED: Store original_pil_size here\n                    }\n\n\n    # Save frequent_kp_data to crop_data_file\n    with h5py.File(crop_data_file, mode='w') as f_crop_data:\n        if frequent_kp_data:\n            for img_key, data in frequent_kp_data.items():\n                group = f_crop_data.create_group(img_key)\n                group.create_dataset('keypoints', data=data['keypoints'])\n                group.attrs['original_pil_size'] = data['original_pil_size'] # ADDED: Save as attribute\n\n\n    print(f\"Initial detection and matching complete. Cropping data saved to {crop_data_file}\")\n\n    return crop_data_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.781691Z","iopub.execute_input":"2025-05-22T22:13:36.781908Z","iopub.status.idle":"2025-05-22T22:13:36.800966Z","shell.execute_reply.started":"2025-05-22T22:13:36.781889Z","shell.execute_reply":"2025-05-22T22:13:36.800225Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Assume this is defined globally or imported\ndef transform_points_from_processed(points, crop_box=None):\n    \"\"\"\n    Transforms keypoints from a cropped image's coordinate system\n    back to the original full image's coordinate system by adding the crop offset.\n    This function *does not perform any scaling*, as ALIKED handles that internally.\n\n    Args:\n        points (np.ndarray): Nx2 numpy array of (x,y) keypoint coordinates\n                             relative to the cropped image.\n        crop_box (list or None): [x, y, w, h] of the crop region in original image coordinates.\n                                 If None, no offset is applied (e.g., for full original images).\n\n    Returns:\n        np.ndarray: Nx2 numpy array of keypoint coordinates in the original\n                    full image's coordinate system.\n    \"\"\"\n    transformed_points = points.copy() # Start with a copy to avoid modifying original array\n\n    if crop_box is not None:\n        # If these points came from a cropped image, add the crop's top-left offset\n        # crop_box is [x, y, w, h]\n        offset_x, offset_y = crop_box[0], crop_box[1]\n        transformed_points[:, 0] += offset_x\n        transformed_points[:, 1] += offset_y\n    \n    return transformed_points\n\n# calculate_kornia_resize_scale will no longer be used for keypoint handling.\n# You can remove its definition if it's not used anywhere else in your project.\n# If it's used for other non-keypoint-related resizing calculations, keep it.\n# For the scope of this request, it's not needed for feature extraction or transformation.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.801707Z","iopub.execute_input":"2025-05-22T22:13:36.801986Z","iopub.status.idle":"2025-05-22T22:13:36.820578Z","shell.execute_reply.started":"2025-05-22T22:13:36.801955Z","shell.execute_reply":"2025-05-22T22:13:36.819871Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def load_image_names_from_json(cluster_path):\n    with open(os.path.join(cluster_path, 'images.json'), 'r') as f:\n        full_paths = json.load(f)  # 可能是 ['/path/to/images/img001.jpg', ...]\n        image_names = [os.path.basename(p) for p in full_paths]  # 提取 'img001.jpg'\n    return image_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:13:36.821347Z","iopub.execute_input":"2025-05-22T22:13:36.821633Z","iopub.status.idle":"2025-05-22T22:13:36.841640Z","shell.execute_reply.started":"2025-05-22T22:13:36.821598Z","shell.execute_reply":"2025-05-22T22:13:36.840763Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# ... (imports like kornia.feature as KF, torch, numpy, os, h5py, tqdm)\n\n# Placeholder constants for NMS if not globally defined. Adjust these values as needed.\nNMS_SIZE_PIXELS_ratio = 0.005 # Example: 0.5% of the max original dimension\nNMS_SIZE_PIXELS = 8 # Example: absolute max NMS radius in pixels\n\n# Assume transform_points_from_processed and load_pil_image are defined as previously discussed.\n\n\ndef detect_and_combine_features(img_fnames, crop_info_file, feature_dir, device=DEVICE):\n    \"\"\"\n    Detects ALIKED features for multiple scales and original/cropped images,\n    combines unique features per image (deduplicating based on original coords),\n    applies NMS, and saves combined features to .pt and .h5 files per image.\n    All error handling via try-except blocks has been removed, assuming valid inputs.\n    ALIKED's `resize` parameter handles internal scaling; keypoints are returned\n    in the coordinate system of the *input image* provided to `extractor.extract`.\n    `transform_points_from_processed` is used only for applying crop offsets.\n    \"\"\"\n    print(\"Running multi-variation ALIKED detection and combining features (with NMS)...\")\n    os.makedirs(feature_dir, exist_ok=True)\n    keypoints_subdir_path = os.path.join(feature_dir, KEYPOINTS_SUBDIR)\n    os.makedirs(keypoints_subdir_path, exist_ok=True)\n    descriptors_h5_path = os.path.join(feature_dir, DESCRIPTORS_H5)\n    # NEW: Create directory for keypoint visualizations if enabled\n    if VISUALIZE_KEYPOINTS:\n        keypoint_vis_dir = os.path.join(feature_dir, KEYPOINT_VIS_SUBDIR)\n        os.makedirs(keypoint_vis_dir, exist_ok=True)\n    with h5py.File(descriptors_h5_path, mode='w') as f_descriptors, \\\n         h5py.File(crop_info_file, mode='r') as f_crop_info:\n\n        extractor = ALIKED(max_num_keypoints=TTA_NUM_FEATURES, detection_threshold=0.25).eval().to(DEVICE, dtype=torch.float32)\n        if DEVICE == torch.device('cpu'):\n            extractor.to('cpu', torch.float32)\n\n        for img_path in tqdm(img_fnames, desc=\"Detecting & Combining Features\"):\n            img_key = os.path.basename(img_path)\n            kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt')\n\n            # Skip if combined features already exist for this image\n            if os.path.exists(kp_pt_path) and img_key in f_descriptors:\n                continue\n\n            img_orig_pil = load_pil_image(img_path)\n            img_orig_w, img_orig_h = img_orig_pil.size # Get original image dimensions for NMS radius calculation\n\n            # Get crop info\n            has_crop = False\n            crop_box = None\n            if img_key in f_crop_info:\n                img_crop_group = f_crop_info[img_key]\n                has_crop_attr = img_crop_group.attrs.get('has_crop', False)\n                if has_crop_attr:\n                    temp_crop_box = img_crop_group.attrs.get('crop_box', [0, 0, 0, 0]).tolist()\n                    if temp_crop_box[2] > 0 and temp_crop_box[3] > 0:\n                        has_crop = True\n                        crop_box = temp_crop_box\n            \n            variations_to_process = []\n            for scale in TTA_SCALES:\n                variations_to_process.append({'type': 'orig', 'scale_target': scale, 'pil_img': img_orig_pil, 'input_crop_box_offset': None})\n                \n                if USE_CROPPED_IMAGES and has_crop:\n                    x, y, w, h = crop_box\n                    img_cropped_pil = img_orig_pil.crop((x, y, x + w, y + h))\n                    variations_to_process.append({'type': 'crop', 'scale_target': scale, 'pil_img': img_cropped_pil, 'input_crop_box_offset': crop_box})\n\n            all_kps = []\n            all_descriptors = []\n            all_scores = []\n\n            for var_info in variations_to_process:\n                var_type = var_info['type']\n                var_scale_target = var_info['scale_target']\n                var_pil_img = var_info['pil_img']\n                input_crop_box_offset = var_info['input_crop_box_offset']\n\n                timg = K.image_to_tensor(np.array(var_pil_img), keepdim=True).to(device, torch.float32) / 255.0\n                if timg.ndim == 3: timg = timg[None, ...]\n                \n                if timg.shape[2] == 0 or timg.shape[3] == 0:\n                    all_kps.append(np.empty((0, 2), dtype=np.float32))\n                    all_descriptors.append(np.empty((0, all_descriptors[0].shape[1] if all_descriptors and len(all_descriptors[0]) > 0 else 0), dtype=np.float32))\n                    all_scores.append(np.empty(0, dtype=np.float32))\n                    continue\n\n                with torch.inference_mode():\n                    extractor.preprocess_conf[\"resize\"] = var_scale_target\n                    feats = extractor.extract(timg, resize=var_scale_target, return_processed_size=False, return_scores=True)\n                    \n\n                kp_variation = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                desc_variation = feats['descriptors'].reshape(len(kp_variation), -1).detach().cpu().numpy()\n                score_variation = feats['keypoint_scores'].reshape(-1).detach().cpu().numpy()\n\n                kps_in_original_img_coords = transform_points_from_processed(\n                    kp_variation, input_crop_box_offset\n                )\n\n                all_kps.append(kps_in_original_img_coords)\n                all_descriptors.append(desc_variation)\n                all_scores.append(score_variation)\n\n            # Combine all detected points and descriptors/scores\n            valid_kps = [k for k in all_kps if len(k) > 0]\n            valid_descriptors = [d for d in all_descriptors if len(d) > 0]\n            valid_scores = [s for s in all_scores if len(s) > 0]\n\n            if not valid_kps:\n                torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n                if img_key not in f_descriptors:\n                    f_descriptors.create_group(img_key)\n                continue\n\n            combined_kps = np.concatenate(valid_kps, axis=0)\n            combined_descriptors = np.concatenate(valid_descriptors, axis=0)\n            combined_scores = np.concatenate(valid_scores, axis=0) if valid_scores else np.empty(0, dtype=np.float32)\n\n            # --- NMS (Non-Maximum Suppression) ---\n            combined_kps_for_dedup = combined_kps\n            combined_descs_for_dedup = combined_descriptors\n            \n            # Apply NMS if scores are available and keypoints exist\n            if len(combined_kps) > 0 and len(combined_scores) > 0 and len(combined_kps) == len(combined_scores):\n                # Calculate adaptive NMS radius based on original image dimensions\n                max_orig_dim = max(img_orig_w, img_orig_h)\n                adaptive_nms_size = min(max_orig_dim * NMS_SIZE_PIXELS_ratio, NMS_SIZE_PIXELS)\n\n                indices_after_nms_np = custom_nms_2d_keypoints(\n                    combined_kps,\n                    combined_scores,\n                    adaptive_nms_size\n                )\n                \n                # Filter the combined arrays using NMS results\n                combined_kps_for_dedup = combined_kps[indices_after_nms_np]\n                combined_descs_for_dedup = combined_descriptors[indices_after_nms_np]\n                # Note: Scores are not saved to the final output, so no need to filter combined_scores.\n\n                # print(f\"Image {img_key}: Features before NMS = {len(combined_kps)}, Features after NMS = {len(combined_kps_for_dedup)}\")\n\n            # --- Perform Coordinate-based Deduplication ---\n            seen_coords = {}\n            unique_kps = []\n            unique_descriptors = []\n\n            for i, (kp_coord, descriptor) in enumerate(zip(combined_kps_for_dedup, combined_descs_for_dedup)):\n                rounded_coord = tuple(np.round(kp_coord + 1e-6, COORD_PRECISION).astype(float))\n\n                if rounded_coord not in seen_coords:\n                    seen_coords[rounded_coord] = len(unique_kps)\n                    unique_kps.append(kp_coord)\n                    unique_descriptors.append(descriptor)\n\n            unique_kps_np = np.array(unique_kps, dtype=np.float32)\n            unique_descriptors_np = np.array(unique_descriptors, dtype=np.float32)\n\n            # Save unique keypoints to .pt\n            torch.save(torch.from_numpy(unique_kps_np), kp_pt_path)\n\n            # print(f\"KP_PT_PATH:{kp_pt_path}, number is {len(unique_kps_np)}\")\n\n            # Save unique descriptors to descriptors.h5\n            img_desc_group = f_descriptors.require_group(img_key)\n            img_desc_group.create_dataset('data', data=unique_descriptors_np, compression=\"gzip\")\n\n    \n            # --- NEW: Visualize and Save Keypoints ---\n            if VISUALIZE_KEYPOINTS:\n                if len(unique_kps_np) > 0:\n                    vis_img = img_orig_pil.copy()\n                    draw = ImageDraw.Draw(vis_img)\n                    radius = 3 # Radius for drawing keypoints\n\n                    for kp_x, kp_y in unique_kps_np:\n                        # Draw a small circle at each keypoint\n                        draw.ellipse((kp_x - radius, kp_y - radius, kp_x + radius, kp_y + radius),\n                                     fill='red', outline='red')\n                    \n                    vis_output_path = os.path.join(keypoint_vis_dir, f'{img_key}_kps.jpg')\n                    vis_img.save(vis_output_path)\n                    # print(f\"Saved keypoint visualization for {img_key} to {vis_output_path}\") # Optional verbose\n                # else:\n                    # print(f\"No keypoints to visualize for {img_key}.\") # Optional verbose\n\n\n    print(\"Multi-variation detection, combination, and deduplication complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:36:42.644963Z","iopub.execute_input":"2025-05-22T22:36:42.645313Z","iopub.status.idle":"2025-05-22T22:36:42.665128Z","shell.execute_reply.started":"2025-05-22T22:36:42.645281Z","shell.execute_reply":"2025-05-22T22:36:42.664219Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def import_into_colmap_cluster(\n    img_dir,\n    cluster_path='.featureout/cluster_0',\n    database_path = '.featureout/cluster_0/colmap.db',\n    image_names = None\n):\n    \"\"\"\n    Import keypoints and matches into COLMAP database using helper functions.\n\n    Args:\n        img_dir (str): Directory containing image files\n        cluster_path (str): Path with matches.h5\n        database_path (str): Output database location\n        image_names (list[str]): Optional subset of image names to include\n    \"\"\"\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    # Add keypoints and images\n    fname_to_id = add_keypoints(\n        db=db,\n        h5_path=cluster_path,\n        image_path=img_dir,\n        img_ext='',\n        camera_model='simple-pinhole',\n        single_camera=single_camera\n    )\n    # Filter fname_to_id to only use the selected subset (if provided)\n    if image_names is not None:\n        fname_to_id = {k: v for k, v in fname_to_id.items() if k in image_names}\n\n    # Add matches between selected image pairs\n    add_matches(\n        db=db,\n        h5_path=cluster_path,\n        fname_to_id=fname_to_id\n    )\n    db.commit()\n    db.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:36:42.666392Z","iopub.execute_input":"2025-05-22T22:36:42.666686Z","iopub.status.idle":"2025-05-22T22:36:42.688509Z","shell.execute_reply.started":"2025-05-22T22:36:42.666648Z","shell.execute_reply":"2025-05-22T22:36:42.687884Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# ... (imports)\nimport kornia.feature as KF\nimport torch\nimport numpy as np\nimport os\nimport h5py\nimport json # Ensure json is imported\nfrom tqdm import tqdm\n\n\ndef match_images_global(\n    img_fnames,\n    index_pairs,\n    data_dir='.',\n    device=torch.device('cpu'),\n    min_matches=MIN_MATCHES_FOR_GRAPH_EDGE,\n    verbose=False\n):\n    \"\"\"\n    Performs LightGlue matching on combined features for all image pairs\n    in index_pairs, builds a match graph (implicitly), and saves global files\n    (images.json, keypoints.h5, matches.h5) for the entire dataset.\n\n    Args:\n        img_fnames (list): List of full paths to image files.\n        index_pairs (list): List of (idx1, idx2) tuples for image pairs to match.\n        data_dir (str): Base directory where 'features_combined' is located and\n                        where the global output will be created.\n        device (torch.device): Device to use for matching.\n        min_matches (int): Min matches for considering a pair and saving its matches.\n        verbose (bool): Whether to print detailed match info.\n\n    Returns:\n        list: A list containing a single list with the global indices of all images.\n              This is to maintain a similar structure to the clustering output,\n              indicating a single group.\n    \"\"\"\n    # Define paths based on data_dir and configuration\n    feature_dir_combined = os.path.join(data_dir, 'features_combined')\n    keypoints_subdir_path = os.path.join(feature_dir_combined, KEYPOINTS_SUBDIR) # Directory holding combined per-image .pt files\n    descriptors_h5_path = os.path.join(feature_dir_combined, DESCRIPTORS_H5) # HDF5 holding combined per-image descriptors\n\n    # Directory where the global output files will be created\n    global_output_dir = os.path.join(feature_dir_combined, 'global')\n    os.makedirs(global_output_dir, exist_ok=True) # Ensure global output dir exists\n\n    lg_matcher = KF.LightGlueMatcher(\n        \"aliked\", {\n            \"width_confidence\": -1,\n            \"depth_confidence\": -1,\n            \"mp\": 'cuda' in str(device)\n        }\n    ).eval().to(device)\n\n    # Store match indices (relative to combined per-image features)\n    all_matches = {}\n\n    # Open combined descriptors file once\n    f_descriptors = h5py.File(descriptors_h5_path, mode='r')\n\n    print(\"Performing LightGlue matching on combined features...\")\n\n    # Iterate through shortlisted pairs\n    for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n        key1 = os.path.basename(fname1)\n        key2 = os.path.basename(fname2)\n\n        kp1_pt_path = os.path.join(keypoints_subdir_path, f'{key1}.pt')\n        kp2_pt_path = os.path.join(keypoints_subdir_path, f'{key2}.pt')\n\n        # Load combined keypoints (original coordinates)\n        kp1_combined_orig = torch.load(kp1_pt_path, weights_only=False).to(device)\n        kp2_combined_orig = torch.load(kp2_pt_path, weights_only=False).to(device)\n\n        # Load combined descriptors\n        desc1_combined = torch.from_numpy(f_descriptors[key1]['data'][...]).to(device)\n        desc2_combined = torch.from_numpy(f_descriptors[key2]['data'][...]).to(device)\n\n        # Skip if zero features are found\n        if len(kp1_combined_orig) == 0 or len(kp2_combined_orig) == 0 or \\\n           len(desc1_combined) == 0 or len(desc2_combined) == 0:\n            if verbose:\n                tqdm.write(f\"Skipping {key1}-{key2}: Zero features found.\")\n            continue\n\n        # Create dummy LAFs centered at keypoints (using original coordinates)\n        kp1_tensor = kp1_combined_orig.float()[None] # Add batch dim\n        kp2_tensor = kp2_combined_orig.float()[None] # Add batch dim\n        laf1 = KF.laf_from_center_scale_ori(kp1_tensor) # Use batch size 1, scale 1.0\n        laf2 = KF.laf_from_center_scale_ori(kp2_tensor)\n\n        with torch.inference_mode():\n            scores, matches = lg_matcher(desc1_combined, desc2_combined, laf1, laf2)\n\n        n_matches = len(matches) # Number of matches\n\n        if verbose:\n            tqdm.write(f'{key1}-{key2}: {n_matches} matches')\n\n        # Store matches if enough are found\n        if n_matches >= min_matches:\n            matches_indices = matches.cpu().detach().numpy().astype('int16')\n            all_matches.setdefault(key1, {})[key2] = matches_indices\n\n    # Close the descriptors file\n    f_descriptors.close()\n\n    print(\"Matching complete. Saving global files...\")\n\n    # --- Save Global Files ---\n\n    # 1. Save images.json (list of full filenames for all images)\n    images_json_path = os.path.join(global_output_dir, 'images.json')\n    with open(images_json_path, 'w') as f_json:\n        json.dump(img_fnames, f_json, indent=2)\n    print(f\"Saved global images list to {images_json_path}\")\n\n    # 2. Save keypoints.h5 for all images (using combined keypoints per image)\n    keypoints_h5_path = os.path.join(global_output_dir, 'keypoints.h5')\n    with h5py.File(keypoints_h5_path, 'w') as f_out_kp:\n        for img_idx, img_fname in enumerate(img_fnames):\n            img_key = os.path.basename(img_fname)\n            kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt') # Path to combined KPs for this image\n            \n            # Load combined KPs for this image (original coords)\n            kp_combined_np = torch.load(kp_pt_path, weights_only=False).cpu().numpy() # Load and convert to numpy\n            f_out_kp.create_dataset(img_key, data=kp_combined_np.astype(np.float32))\n            if verbose:\n                print(f\"Saved combined KPs for {img_key} to global H5\")\n\n    print(f\"Saved global keypoints to {keypoints_h5_path}\")\n\n    # 3. Save matches.h5 for all valid pairs\n    matches_h5_path = os.path.join(global_output_dir, 'matches.h5')\n    if all_matches: # Only create file if there are matches to save\n        with h5py.File(matches_h5_path, 'w') as f_match:\n            for key1, matches_dict in all_matches.items():\n                if matches_dict:\n                    group = f_match.create_group(key1)\n                    for key2, match_data in matches_dict.items():\n                        group.create_dataset(key2, data=match_data, dtype='int16')\n        print(f\"Saved global matches to {matches_h5_path}\")\n    else:\n        print(f\"No matches found above threshold {min_matches} to save to {matches_h5_path}\")\n\n    # Return a list containing a single list of all image indices\n    all_image_indices = list(range(len(img_fnames)))\n    return [all_image_indices]\n\n# Note: This function assumes that detect_and_combine_features\n# has already been run and created the combined features in\n# data_dir/features_combined/keypoints/ and data_dir/features_combined/descriptors.h5.\n# It saves the global feature and match files into data_dir/features_combined/global/.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:36:42.689890Z","iopub.execute_input":"2025-05-22T22:36:42.690095Z","iopub.status.idle":"2025-05-22T22:36:42.709635Z","shell.execute_reply.started":"2025-05-22T22:36:42.690075Z","shell.execute_reply":"2025-05-22T22:36:42.709022Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"import json\nimport os\nimport gc\nfrom time import time, sleep\nimport pycolmap\nimport h5py\nimport numpy as np\n\n# Assume import_into_colmap_cluster is defined elsewhere and can handle\n# importing keypoints and matches from the .h5 files into a COLMAP database.\n# It will need access to keypoints.h5 and matches.h5 paths.\n# from your_colmap_utils import import_into_colmap_cluster\n\n\ndef run_colmap_global(\n    feature_dir,\n    images_dir,\n    timings\n):\n    \"\"\"\n    Run COLMAP reconstruction for the entire dataset using global feature and match files.\n    All error handling via try-except blocks has been removed, assuming valid inputs\n    and successful COLMAP operations.\n\n    Parameters:\n    - feature_dir: Base directory containing 'features_combined/global' folder.\n    - images_dir: Path to raw image files.\n    - timings: dict to record durations.\n    \"\"\"\n    # Define the path to the global files\n    global_path = os.path.join(feature_dir, 'features_combined', 'global')\n\n    database_path = os.path.join(global_path, 'colmap.db')\n    image_list_path = os.path.join(global_path, 'images.json')\n    keypoints_h5_path = os.path.join(global_path, 'keypoints.h5')\n    matches_h5_path = os.path.join(global_path, 'matches.h5')\n\n    # Load image names\n    with open(image_list_path, 'r') as f:\n        # images.json in the global folder contains full paths, extract basenames\n        image_names = [os.path.basename(x) for x in json.load(f)]\n\n    # Remove existing database if it exists\n    if os.path.isfile(database_path):\n        os.remove(database_path)\n        print(f\"[Global Reconstruction] Removed existing database at {database_path}\")\n\n    gc.collect()\n    sleep(1)\n\n    # Step 1: Import keypoints, matches into COLMAP db\n    # Your import_into_colmap_cluster function is assumed to read from\n    # keypoints_h5_path and matches_h5_path and populate the database.\n    import_into_colmap_cluster(\n        img_dir=images_dir,\n        cluster_path=global_path, # Pass the global path where H5 files are\n        database_path=database_path,\n        image_names=image_names # Pass the list of image basenames\n    )\n\n    # Step 2: RANSAC (Geometric verification)\n    t = time()\n    pycolmap.match_exhaustive(database_path)\n    t_ransac = time() - t\n    timings['RANSAC_Global'] = t_ransac # Use a specific key for global timings\n    print(f'[Global Reconstruction] Ran RANSAC in {t_ransac:.4f} sec')\n\n    # Step 3: Incremental mapping for the global dataset\n    output_path = os.path.join(global_path, 'colmap_rec_aliked')\n    os.makedirs(output_path, exist_ok=True)\n\n    mapper_options = pycolmap.IncrementalPipelineOptions()\n    # Adjust mapper options as needed for a large global reconstruction\n    mapper_options.min_model_size = 8 # Minimum number of registered images\n    mapper_options.max_num_models = 25 # We expect only one main model\n    mapper_options.mapper.filter_max_reproj_error = 10.0 # Example, adjust as needed\n\n    t = time()\n    # pycolmap.incremental_mapping expects image_path to be the directory\n    # containing the actual image files.\n    maps = pycolmap.incremental_mapping(\n        database_path=database_path,\n        image_path=images_dir, # Path to the actual image files\n        output_path=output_path,\n        options=mapper_options\n    )\n    \n    # maps is already a dictionary of valid reconstructions, so its length gives the number of maps\n    num_successful_maps = len(maps)\n    print(f'[Global Reconstruction] Found {num_successful_maps} successful reconstruction(s).')\n\n    t_rec = time() - t\n    timings['Reconstruction_Global'] = t_rec # Use a specific key for global timings\n    print(f'[Global Reconstruction] Reconstruction done in {t_rec:.4f} sec')\n    # all_maps = [maps] # Return as a list containing the single global map\n    # all_maps_len = [num_successful_maps]\n    # print(all_maps)\n    # print(all_maps_len)\n    return timings, maps\n\n# Note: This function now expects the output of match_images_global to be in\n# feature_dir/features_combined/global/.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:36:42.710635Z","iopub.execute_input":"2025-05-22T22:36:42.710920Z","iopub.status.idle":"2025-05-22T22:36:42.730983Z","shell.execute_reply.started":"2025-05-22T22:36:42.710895Z","shell.execute_reply":"2025-05-22T22:36:42.730187Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Collect vital info from the dataset\n\n@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None\n\n# Set is_train=True to run the notebook on the training data.\n# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\nis_train = True\ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:36:42.731756Z","iopub.execute_input":"2025-05-22T22:36:42.732002Z","iopub.status.idle":"2025-05-22T22:36:42.887579Z","shell.execute_reply.started":"2025-05-22T22:36:42.731975Z","shell.execute_reply":"2025-05-22T22:36:42.886846Z"}},"outputs":[{"name":"stdout","text":"Dataset \"imc2023_haiper\" -> num_images=54\nDataset \"imc2023_heritage\" -> num_images=209\nDataset \"imc2023_theather_imc2024_church\" -> num_images=76\nDataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\nDataset \"imc2024_lizard_pond\" -> num_images=214\nDataset \"pt_brandenburg_british_buckingham\" -> num_images=225\nDataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\nDataset \"pt_stpeters_stpauls\" -> num_images=200\nDataset \"amy_gardens\" -> num_images=200\nDataset \"fbk_vineyard\" -> num_images=163\nDataset \"ETs\" -> num_images=22\nDataset \"stairs\" -> num_images=51\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"import os\nimport shutil\n\ndef delete_cluster_folders(base_dir):\n    for name in os.listdir(base_dir):\n        path = os.path.join(base_dir, name)\n        if os.path.isdir(path) and name.startswith(\"cluster\"):\n            print(f\"Deleting: {path}\")\n            shutil.rmtree(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:36:42.888380Z","iopub.execute_input":"2025-05-22T22:36:42.888705Z","iopub.status.idle":"2025-05-22T22:36:42.892978Z","shell.execute_reply.started":"2025-05-22T22:36:42.888672Z","shell.execute_reply":"2025-05-22T22:36:42.892098Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"import json\nimport os\nimport gc\nfrom time import time, sleep\nimport pycolmap\nimport h5py\nimport numpy as np\nfrom tqdm import tqdm\nfrom copy import deepcopy # For deepcopy(image.cam_from_world...)\n# Assuming `clear_output` is from IPython.display\n# from IPython.display import clear_output\n\n\n# Placeholder definitions for constants and functions\n# Ensure these are correctly imported or defined in your actual script\n# For example:\n# from .config import (\n#     MIN_MATCHES_FOR_GRAPH_EDGE, KEYPOINTS_SUBDIR, DESCRIPTORS_H5,\n#     CROP_DATA, CROP_BOXES_FILE, CROP_PADDING_FACTOR, MIN_CROP_DIMENSION\n# )\n# from .feature_extraction_utils import (\n#     load_pil_image, transform_points_from_processed,\n#     perform_initial_detection_and_matching, detect_and_combine_features\n# )\n# from .matching_utils import get_image_pairs_shortlist, match_images_global\n# from .colmap_utils import run_colmap_global, import_into_colmap_cluster\n\n# If not imported, define placeholders here to avoid NameErrors:\n# (These values should match your actual configurations)\n# MIN_MATCHES_FOR_GRAPH_EDGE = 100\n# KEYPOINTS_SUBDIR = 'keypoints'\n# DESCRIPTORS_H5 = 'descriptors.h5'\n# CROP_DATA = 'crop_data.h5'\n# CROP_BOXES_FILE = 'crop_boxes.h5'\n# CROP_PADDING_FACTOR = 0.1\n# MIN_CROP_DIMENSION = 512\n# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ngc.collect()\n\nmax_images = None  # Used For debugging only. Set to None to disable.\ndatasets_to_process = None  # Not the best convention, but None means all datasets.\n\n# is_train and samples are assumed to be defined externally, e.g., from your notebook setup\n# For demonstration:\n# is_train = True\n# class Prediction:\n#     def __init__(self, filename, cluster_index=None, rotation=None, translation=None):\n#         self.filename = filename\n#         self.cluster_index = cluster_index\n#         self.rotation = rotation\n#         self.translation = translation\n# samples = {\n#     'stairs': [Prediction('image001.jpg'), Prediction('image002.jpg'), Prediction('image003.jpg'), Prediction('image004.jpg'), Prediction('image005.jpg')]\n# }\n\n\nif is_train:\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n        # New data.\n        'amy_gardens',\n        'ETs',\n        'fbk_vineyard',\n        'stairs',\n        # Data from IMC 2023 and 2024.\n        # 'imc2024_dioscuri_baalshamin',\n        # 'imc2023_theather_imc2024_church',\n        # 'imc2023_heritage',\n        # 'imc2023_haiper',\n        # 'imc2024_lizard_pond',\n        # Crowdsourced PhotoTourism data.\n        # 'pt_stpeters_stpauls',\n        # 'pt_brandenburg_british_buckingham',\n        # 'pt_piazzasanmarco_grandplace',\n        # 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_augmentation\":[],\n    \"feature_merge\":[],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n\nprint (f\"Extracting on device {DEVICE}\") # Changed to DEVICE for consistency\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # All try-except blocks around main pipeline steps are removed.\n    # We assume successful execution for each step.\n    try:\n\n        t_start_pipeline = time() # Start timer for the whole pipeline for this dataset\n    \n        # 1. Image Pair Shortlisting\n        t = time()\n        index_pairs = get_image_pairs_shortlist(\n            images,\n            sim_th = 0.5,\n            min_pairs = 10,\n            exhaustive_if_less = 20,\n            device=DEVICE\n        )\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n        \n        # 2. Perform initial detection and matching for cropping data\n        # (This step will overwrite crop_data.h5 if it exists)\n        t = time()\n        # pass `feature_dir` as the `data_dir` for initial_detection_and_matching\n        # so it creates its temp files within feature_dir\n        crop_data_file = perform_initial_detection_and_matching(images, index_pairs, data_dir=feature_dir, device=DEVICE)\n        timings['feature_detection'].append(time() - t)\n        print(f'Initial detection for cropping done in {time() - t:.4f} sec')\n        gc.collect()\n        \n        # 3. Calculate crop boxes based on initial match analysis\n        t = time()\n        # `calculate_crop_boxes` only needs `data_dir` which is `feature_dir` in this context\n        # It will find crop_data.h5 within feature_dir and save crop_boxes.h5 there too.\n        # The return value `crop_info_file` is effectively the path to `crop_boxes.h5`\n        crop_info_file = calculate_crop_boxes(data_dir=feature_dir)\n        timings['feature_augmentation'].append(time() - t)\n        print(f'Crop boxes calculated in {time() - t:.4f} sec') # Renamed print output\n        gc.collect()    \n        \n        # 4. Perform multi-variation ALIKED detection, combine features per image, save to .pt/.h5\n        t = time()\n        # `detect_and_combine_features` expects `feature_dir` as the base for its outputs\n        # and `crop_info_file` (which is `crop_boxes.h5`) for crop information.\n        detect_and_combine_features(images, crop_info_file, os.path.join(feature_dir, 'features_combined'), device=DEVICE)\n        timings['feature_merge'].append(time() - t)\n        print(f'Features combined in {time() - t:.4f} sec') # Renamed print output\n        gc.collect()   \n    \n        # 5. Load combined features and perform LightGlue matching, save global matches to .h5\n        t = time()\n        # `match_images_global` expects `data_dir` (where `features_combined` is a subdir)\n        # which is `feature_dir` in this context.\n        match_images_global(images, index_pairs, data_dir=feature_dir, device=DEVICE)\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched globally in {time() - t:.4f} sec') # Renamed print output\n        gc.collect()\n        \n        # 6. Run COLMAP global reconstruction\n        # timings dict is passed by reference, so updates inside run_colmap_global are reflected\n        timings, maps = run_colmap_global(feature_dir, images_dir, timings)\n        timings['Reconstruction'].append(time() - t_start_pipeline) # This timing is for the whole reconstruction part\n        print(f'Global reconstruction done in {time() - t_start_pipeline:.4f} sec') # This measures the whole pipeline\n        print(maps)\n\n        # clear_output(wait=False)\n    \n        registered = 0\n        for map_index, cur_map in maps.items():\n            for index, image in cur_map.images.items():\n                prediction_index = filename_to_index[image.name]\n                predictions[prediction_index].cluster_index = map_index\n                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                registered += 1\n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\n# print('\\nTimings')\n# for k, v in timings.items():\n#      print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:38:40.469392Z","iopub.execute_input":"2025-05-22T22:38:40.469774Z","iopub.status.idle":"2025-05-22T22:56:36.397467Z","shell.execute_reply.started":"2025-05-22T22:38:40.469739Z","shell.execute_reply":"2025-05-22T22:56:36.395869Z"}},"outputs":[{"name":"stdout","text":"Extracting on device cuda\nSkipping \"imc2023_haiper\"\nSkipping \"imc2023_heritage\"\nSkipping \"imc2023_theather_imc2024_church\"\nSkipping \"imc2024_dioscuri_baalshamin\"\nSkipping \"imc2024_lizard_pond\"\nSkipping \"pt_brandenburg_british_buckingham\"\nSkipping \"pt_piazzasanmarco_grandplace\"\nSkipping \"pt_sacrecoeur_trevi_tajmahal\"\nSkipping \"pt_stpeters_stpauls\"\n\nProcessing dataset \"amy_gardens\": 200 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:23<00:00,  8.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1448\nMax:  0.4239\nMean: 0.2737\nStd:  0.0388\n20%:  0.2375\n25%:  0.2439\nUSED 50%:  0.2754\n75%:  0.3017\nShortlisting. Number of pairs to match: 9954. Done in 23.6682 sec\nPerforming initial ALIKED detection (1280) and LightGlue matching for cropping data...\nRunning initial ALIKED detection...\n","output_type":"stream"},{"name":"stderr","text":"Initial ALIKED Detection: 100%|██████████| 200/200 [00:13<00:00, 14.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Running initial LightGlue matching...\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"Initial LightGlue Matching: 100%|██████████| 9954/9954 [12:50<00:00, 12.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Analyzing initial matches for cropping data...\nInitial detection and matching complete. Cropping data saved to /kaggle/working/result/featureout/amy_gardens/crop_data.h5\nInitial detection for cropping done in 786.7096 sec\nCalculating crop boxes from /kaggle/working/result/featureout/amy_gardens/crop_data.h5...\nCrop boxes calculated and saved to /kaggle/working/result/featureout/amy_gardens/crop_boxes.h5\nCrop boxes calculated in 0.0878 sec\nRunning multi-variation ALIKED detection and combining features (with NMS)...\n","output_type":"stream"},{"name":"stderr","text":"Detecting & Combining Features: 100%|██████████| 200/200 [01:56<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Multi-variation detection, combination, and deduplication complete.\nFeatures combined in 116.9322 sec\nLoaded LightGlue model\nPerforming LightGlue matching on combined features...\n","output_type":"stream"},{"name":"stderr","text":"LightGlue Matching:   8%|▊         | 776/9954 [02:26<28:52,  5.30it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-9f8d7b6627a2>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# `match_images_global` expects `data_dir` (where `features_combined` is a subdir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# which is `feature_dir` in this context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mmatch_images_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mtimings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_matching'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Features matched globally in {time() - t:.4f} sec'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Renamed print output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-61-6be8e2a6a88f>\u001b[0m in \u001b[0;36mmatch_images_global\u001b[0;34m(img_fnames, index_pairs, data_dir, device, min_matches, verbose)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlg_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc1_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc2_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mn_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Number of matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/feature/integrated.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, desc1, desc2, lafs1, lafs2, hw1, hw2)\u001b[0m\n\u001b[1;32m    489\u001b[0m             },\n\u001b[1;32m    490\u001b[0m         }\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0mmatches0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmscores0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"matches0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"matching_scores0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches0\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \"\"\"\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mdesc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_assignment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mm0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmscores0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmscores1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py\u001b[0m in \u001b[0;36mfilter_matches\u001b[0;34m(scores, th)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mmutual1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mmax0_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax0_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mmscores0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutual0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax0_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0mmscores1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutual1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmscores0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":68},{"cell_type":"code","source":"# Must Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:56:36.398086Z","iopub.status.idle":"2025-05-22T22:56:36.398410Z","shell.execute_reply":"2025-05-22T22:56:36.398265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Definitely Compute results if running on the training set.\n# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n\nif is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:56:36.399141Z","iopub.status.idle":"2025-05-22T22:56:36.399497Z","shell.execute_reply":"2025-05-22T22:56:36.399372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}