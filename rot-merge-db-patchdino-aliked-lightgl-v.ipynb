{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a2fcab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.008337,
     "end_time": "2025-06-05T23:02:16.508980",
     "exception": false,
     "start_time": "2025-06-05T23:02:16.500643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b12c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:16.525367Z",
     "iopub.status.busy": "2025-06-05T23:02:16.524906Z",
     "iopub.status.idle": "2025-06-05T23:02:22.188174Z",
     "shell.execute_reply": "2025-06-05T23:02:22.187215Z"
    },
    "papermill": {
     "duration": 5.673556,
     "end_time": "2025-06-05T23:02:22.189931",
     "exception": false,
     "start_time": "2025-06-05T23:02:16.516375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74d1214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:22.205535Z",
     "iopub.status.busy": "2025-06-05T23:02:22.205201Z",
     "iopub.status.idle": "2025-06-05T23:02:23.237288Z",
     "shell.execute_reply": "2025-06-05T23:02:23.236135Z"
    },
    "papermill": {
     "duration": 1.041289,
     "end_time": "2025-06-05T23:02:23.238854",
     "exception": false,
     "start_time": "2025-06-05T23:02:22.197565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/depth-save.pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c838ac56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:23.254203Z",
     "iopub.status.busy": "2025-06-05T23:02:23.253914Z",
     "iopub.status.idle": "2025-06-05T23:02:24.434236Z",
     "shell.execute_reply": "2025-06-05T23:02:24.433174Z"
    },
    "papermill": {
     "duration": 1.189792,
     "end_time": "2025-06-05T23:02:24.435979",
     "exception": false,
     "start_time": "2025-06-05T23:02:23.246187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/superpoint-lightglue/superpoint_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/superpoint-lightglue/superpoint_lightglue.pth  /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/superpoint-lightglue/superpoint_v1.pth /root/.cache/torch/hub/checkpoints/superpoint_v1.pth\n",
    "!cp /kaggle/input/superpoint-lightglue/superpoint_v1.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6562c03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:24.452920Z",
     "iopub.status.busy": "2025-06-05T23:02:24.452611Z",
     "iopub.status.idle": "2025-06-05T23:02:24.457482Z",
     "shell.execute_reply": "2025-06-05T23:02:24.456681Z"
    },
    "papermill": {
     "duration": 0.014811,
     "end_time": "2025-06-05T23:02:24.458989",
     "exception": false,
     "start_time": "2025-06-05T23:02:24.444178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/root/.cache/torch/hub/checkpoints/depth-save.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae01447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:24.475785Z",
     "iopub.status.busy": "2025-06-05T23:02:24.475509Z",
     "iopub.status.idle": "2025-06-05T23:02:46.755595Z",
     "shell.execute_reply": "2025-06-05T23:02:46.754669Z"
    },
    "papermill": {
     "duration": 22.290344,
     "end_time": "2025-06-05T23:02:46.757234",
     "exception": false,
     "start_time": "2025-06-05T23:02:24.466890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# from lightglue import DISK\n",
    "from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\n",
    "from scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint,DISK, DoGHardNet, LightGlue, SIFT\n",
    "from fastprogress import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d586bacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:46.773202Z",
     "iopub.status.busy": "2025-06-05T23:02:46.772699Z",
     "iopub.status.idle": "2025-06-05T23:02:46.775924Z",
     "shell.execute_reply": "2025-06-05T23:02:46.775329Z"
    },
    "papermill": {
     "duration": 0.012112,
     "end_time": "2025-06-05T23:02:46.777060",
     "exception": false,
     "start_time": "2025-06-05T23:02:46.764948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c201aa47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:46.791866Z",
     "iopub.status.busy": "2025-06-05T23:02:46.791657Z",
     "iopub.status.idle": "2025-06-05T23:02:46.895839Z",
     "shell.execute_reply": "2025-06-05T23:02:46.894887Z"
    },
    "papermill": {
     "duration": 0.113277,
     "end_time": "2025-06-05T23:02:46.897314",
     "exception": false,
     "start_time": "2025-06-05T23:02:46.784037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb9ab76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:46.912947Z",
     "iopub.status.busy": "2025-06-05T23:02:46.912721Z",
     "iopub.status.idle": "2025-06-05T23:02:46.916468Z",
     "shell.execute_reply": "2025-06-05T23:02:46.915737Z"
    },
    "papermill": {
     "duration": 0.012769,
     "end_time": "2025-06-05T23:02:46.917682",
     "exception": false,
     "start_time": "2025-06-05T23:02:46.904913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14045ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:46.932331Z",
     "iopub.status.busy": "2025-06-05T23:02:46.932052Z",
     "iopub.status.idle": "2025-06-05T23:02:46.935284Z",
     "shell.execute_reply": "2025-06-05T23:02:46.934478Z"
    },
    "papermill": {
     "duration": 0.011818,
     "end_time": "2025-06-05T23:02:46.936455",
     "exception": false,
     "start_time": "2025-06-05T23:02:46.924637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3f9e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:46.951623Z",
     "iopub.status.busy": "2025-06-05T23:02:46.951389Z",
     "iopub.status.idle": "2025-06-05T23:02:46.959583Z",
     "shell.execute_reply": "2025-06-05T23:02:46.958774Z"
    },
    "papermill": {
     "duration": 0.017186,
     "end_time": "2025-06-05T23:02:46.960880",
     "exception": false,
     "start_time": "2025-06-05T23:02:46.943694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : True,\n",
    "        \"filter_iterations\" : 5,\n",
    "        \"filter_threshold\" : 3,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = True\n",
    "    use_disk_lightglue = True\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":0.2\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 50,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":0.2\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":0.2\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de3d4c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:46.975565Z",
     "iopub.status.busy": "2025-06-05T23:02:46.975325Z",
     "iopub.status.idle": "2025-06-05T23:02:54.916924Z",
     "shell.execute_reply": "2025-06-05T23:02:54.915733Z"
    },
    "papermill": {
     "duration": 7.950877,
     "end_time": "2025-06-05T23:02:54.918678",
     "exception": false,
     "start_time": "2025-06-05T23:02:46.967801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m pip install --no-index --find-links=/kaggle/input/pkg-check-orientation/ check_orientation==0.0.5 > /dev/null\n",
    "!cp /kaggle/input/pkg-check-orientation/2020-11-16_resnext50_32x4d.zip /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21321481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:54.936568Z",
     "iopub.status.busy": "2025-06-05T23:02:54.936291Z",
     "iopub.status.idle": "2025-06-05T23:02:57.755374Z",
     "shell.execute_reply": "2025-06-05T23:02:57.754657Z"
    },
    "papermill": {
     "duration": 2.829772,
     "end_time": "2025-06-05T23:02:57.756972",
     "exception": false,
     "start_time": "2025-06-05T23:02:54.927200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnext50_32x4d to current resnext50_32x4d.fb_swsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image as T_read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms as T\n",
    "from check_orientation.pre_trained_models import create_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def convert_rot_k(index):\n",
    "    if index == 0:\n",
    "        return 0\n",
    "    elif index == 1:\n",
    "        return 3\n",
    "    elif index == 2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "class CheckRotationDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgPath = self.files[idx]\n",
    "        image = T_read_image(imgPath, mode=ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def get_CheckRotation_dataloader_crop(images, batch_size=1):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    dataset = CheckRotationDataset(images, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def exec_rotation_detection(img_files, device):\n",
    "    model = create_model(\"swsl_resnext50_32x4d\")\n",
    "    model.eval().to(device);\n",
    "    \n",
    "    dataloader = get_CheckRotation_dataloader_crop(img_files)\n",
    "    \n",
    "    rots = []\n",
    "    for idx, image in enumerate(dataloader):\n",
    "        image = image.to(torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image).detach().cpu().numpy()\n",
    "            detected_rot = prediction[0].argmax()\n",
    "            if prediction[0][detected_rot] > 0.9:\n",
    "                rot_k = convert_rot_k(detected_rot)\n",
    "            else:\n",
    "                rot_k = 0\n",
    "            rots.append(rot_k)\n",
    "            print(f\"{os.path.basename(img_files[idx])} > rot_k={rot_k}\")\n",
    "    return rots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c8984a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:57.773323Z",
     "iopub.status.busy": "2025-06-05T23:02:57.773042Z",
     "iopub.status.idle": "2025-06-05T23:02:57.778978Z",
     "shell.execute_reply": "2025-06-05T23:02:57.778408Z"
    },
    "papermill": {
     "duration": 0.015404,
     "end_time": "2025-06-05T23:02:57.780174",
     "exception": false,
     "start_time": "2025-06-05T23:02:57.764770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def output_rot_images(\n",
    "    paths,\n",
    "    output_dir,\n",
    "    rots,\n",
    "):\n",
    "    corrected_image_paths = []\n",
    "    for rot, path in tqdm(zip(rots, paths), total=len(paths), desc=f\"Rotating images\", dynamic_ncols=True):\n",
    "        try:\n",
    "            img = cv2.imread(str(path))\n",
    "    \n",
    "            if rot == 1:\n",
    "                img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            elif rot == 2:\n",
    "                img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "            elif rot == 3:\n",
    "                img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "            cv2.imwrite(str(output_dir / path.name), img)\n",
    "        except:\n",
    "            shutil.copy(str(path), str(output_dir / path.name))\n",
    "        \n",
    "        corrected_image_paths.append(output_dir / path.name)\n",
    "\n",
    "    return corrected_image_paths\n",
    "\n",
    "def exec_rotation_correction(paths, output_dir):\n",
    "\n",
    "    rots = exec_rotation_detection(paths, device)\n",
    "\n",
    "    corrected_image_paths = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(\n",
    "            output_rot_images,\n",
    "            np.array_split(paths, 2),\n",
    "            itertools.repeat(output_dir),\n",
    "            np.array_split(rots, 2),\n",
    "        )\n",
    "        for data in results:\n",
    "            corrected_image_paths.append(data)\n",
    "\n",
    "    corrected_image_paths = list(itertools.chain.from_iterable(corrected_image_paths))\n",
    "    gc.collect()\n",
    "\n",
    "    return corrected_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5d26ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:57.796332Z",
     "iopub.status.busy": "2025-06-05T23:02:57.796084Z",
     "iopub.status.idle": "2025-06-05T23:02:58.226498Z",
     "shell.execute_reply": "2025-06-05T23:02:58.225783Z"
    },
    "papermill": {
     "duration": 0.440113,
     "end_time": "2025-06-05T23:02:58.227738",
     "exception": false,
     "start_time": "2025-06-05T23:02:57.787625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume these are available from your environment or previous code\n",
    "# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n",
    "# from kornia.feature import ALIKED # Already in your detect_aliked\n",
    "# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n",
    "# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n",
    "# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n",
    "\n",
    "# --- Helper function for image loading (if not already defined) ---\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "def get_dino_patch_features_for_keypoints(img_path, keypoints_xy, dino_processor, dino_model, patch_size=16, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Extracts DINO patch features corresponding to given ALIKED keypoint locations.\n",
    "    It correctly infers the DINO patch grid dimensions from the processed input.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        keypoints_xy (torch.Tensor): Nx2 tensor of (x, y) keypoint coordinates in image pixel space.\n",
    "                                     These keypoints are assumed to be in the original image's coordinate system.\n",
    "        dino_processor: HuggingFace AutoImageProcessor for DINO.\n",
    "        dino_model: HuggingFace AutoModel for DINO.\n",
    "        patch_size (int): The patch size used by the DINO model (e.g., 14 or 16).\n",
    "        device (torch.device): Device to run the models on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: NxD_dino tensor of DINO patch features for each keypoint.\n",
    "                      Returns None if no keypoints or image loading fails.\n",
    "    \"\"\"\n",
    "    if len(keypoints_xy) == 0:\n",
    "        dino_feature_dim = dino_model.config.hidden_size # Get actual DINO hidden size\n",
    "        return torch.empty((0, dino_feature_dim), device=device)\n",
    "\n",
    "    # 1. Load the original image (ALIKED processed this size)\n",
    "    original_img = load_torch_image(img_path, device=device)\n",
    "    original_h, original_w = original_img.shape[-2], original_img.shape[-1]\n",
    "\n",
    "\n",
    "    # 2. Process the image with DINO's processor\n",
    "    #    This step performs resizing, padding, etc., as needed by the DINO model\n",
    "    with torch.inference_mode():\n",
    "        # dino_processor returns a BatchFeature object which includes pixel_values\n",
    "        # and potentially other information like `pixel_mask`\n",
    "        inputs = dino_processor(images=original_img, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "        # Get the actual dimensions of the image as processed by the DINO model\n",
    "        # This is the crucial part: the actual H and W that produced `patch_tokens`\n",
    "        # We can infer this from the `pixel_values` shape\n",
    "        processed_h = inputs['pixel_values'].shape[-2]\n",
    "        processed_w = inputs['pixel_values'].shape[-1]\n",
    "\n",
    "        # Extract patch tokens (excluding the CLS token)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:].squeeze(0) # Shape: (num_patches, hidden_size)\n",
    "\n",
    "        # Calculate the actual grid dimensions based on the *processed* image size\n",
    "        # and the model's patch size.\n",
    "        # This should perfectly match the number of patch_tokens if the model is well-behaved.\n",
    "        num_patches_h = processed_h // patch_size\n",
    "        num_patches_w = processed_w // patch_size\n",
    "\n",
    "        # Safety check: ensure calculated grid matches actual token count\n",
    "        expected_token_count = num_patches_h * num_patches_w\n",
    "        if patch_tokens.shape[0] != expected_token_count:\n",
    "            # This indicates a deeper issue with how the model's output tokens\n",
    "            # map to the spatial grid, or an unexpected patch size/model behavior.\n",
    "            # Some models might have slightly different patch token arrangements.\n",
    "            # DINOv2 typically aligns well.\n",
    "            raise ValueError(\n",
    "                f\"DINO patch token count ({patch_tokens.shape[0]}) does not match \"\n",
    "                f\"expected grid dimensions ({num_patches_h}x{num_patches_w} = {expected_token_count}) \"\n",
    "                f\"for processed image size {processed_w}x{processed_h} with patch size {patch_size}. \"\n",
    "                f\"Please verify DINO model and processor configuration.\"\n",
    "            )\n",
    "\n",
    "        # Reshape patch tokens into a 2D grid\n",
    "        patch_features_grid = patch_tokens.reshape(num_patches_h, num_patches_w, -1)\n",
    "        dino_feature_dim = patch_features_grid.shape[-1] # Actual feature dimension\n",
    "\n",
    "\n",
    "    dino_features_for_kpts = torch.zeros((len(keypoints_xy), dino_feature_dim), device=device)\n",
    "\n",
    "    # 3. Rescale ALIKED keypoints to the DINO *processed* image dimensions\n",
    "    #    ALIKED keypoints are in original_w x original_h coordinates.\n",
    "    #    DINO patches correspond to processed_w x processed_h coordinates.\n",
    "    scale_x = processed_w / original_w\n",
    "    scale_y = processed_h / original_h\n",
    "\n",
    "    scaled_keypoints_xy = keypoints_xy.clone()\n",
    "    scaled_keypoints_xy[:, 0] *= scale_x\n",
    "    scaled_keypoints_xy[:, 1] *= scale_y\n",
    "\n",
    "    # 4. Map scaled keypoints to DINO patch grid indices\n",
    "    keypoint_cols = (scaled_keypoints_xy[:, 0] / patch_size).long()\n",
    "    keypoint_rows = (scaled_keypoints_xy[:, 1] / patch_size).long()\n",
    "\n",
    "    # Clip indices to ensure they are within bounds of the patch grid\n",
    "    keypoint_rows = torch.clamp(keypoint_rows, 0, num_patches_h - 1)\n",
    "    keypoint_cols = torch.clamp(keypoint_cols, 0, num_patches_w - 1)\n",
    "\n",
    "    # Gather DINO features for each keypoint's corresponding patch\n",
    "    dino_features_for_kpts = patch_features_grid[keypoint_rows, keypoint_cols]\n",
    "\n",
    "    return dino_features_for_kpts\n",
    "\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,\n",
    "                  match_confidence_threshold = 0.0,\n",
    "                  verbose=VERBOSE\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    # extractor = extractor_class(max_num_keypoints=num_features, detection_threshold=detection_threshold, \n",
    "    #                             resize=resize_to).eval().to(device, dtype)\n",
    "    # if model_name == 'disk':\n",
    "    #     extractor = DISK(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device).eval()\n",
    "    #     checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    #     extractor.load_state_dict(checkpoint['model'])\n",
    "    # else:\n",
    "    #     extractor_class = dict_model[model_name]\n",
    "    #     extractor = extractor_class(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device, dtype).eval()\n",
    "\n",
    "    extractor_class = dict_model[model_name]\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=detection_threshold,\n",
    "        resize=resize_to\n",
    "    ).to(device, dtype).eval()\n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                            \"filter_threshold\":match_confidence_threshold,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            len1 = len(idxs)\n",
    "            n_matches = len1\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                pass\n",
    "                # if verbose:\n",
    "                #     print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "    match_confidence_threshold = 0.0\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "        match_confidence_threshold = match_confidence_threshold\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t\n",
    "\n",
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    verbose = VERBOSE\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                # if verbose:\n",
    "                #     print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 3, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    if verbose:\n",
    "                        print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "                # print(f\"KKKKKKK KKKKKK {k1} - {k2}: {len(match)} matches\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69b89438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.244771Z",
     "iopub.status.busy": "2025-06-05T23:02:58.244520Z",
     "iopub.status.idle": "2025-06-05T23:02:58.247555Z",
     "shell.execute_reply": "2025-06-05T23:02:58.246915Z"
    },
    "papermill": {
     "duration": 0.012274,
     "end_time": "2025-06-05T23:02:58.248724",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.236450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddff6ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.267392Z",
     "iopub.status.busy": "2025-06-05T23:02:58.267083Z",
     "iopub.status.idle": "2025-06-05T23:02:58.275494Z",
     "shell.execute_reply": "2025-06-05T23:02:58.274577Z"
    },
    "papermill": {
     "duration": 0.019874,
     "end_time": "2025-06-05T23:02:58.277042",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.257168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: Detect ALIKED and Combine with DINO Patch Features ---\n",
    "def detect_aliked_and_combine_with_dino(img_fnames,\n",
    "                                        feature_dir='.featureout',\n",
    "                                        num_features=4096,\n",
    "                                        resize_to=1024,\n",
    "                                        dino_processor=None,\n",
    "                                        dino_model=None,\n",
    "                                        dino_patch_size=16, # Typically 14 or 16 for DINO\n",
    "                                        device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    aliked_extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    aliked_extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='w') as f_desc_aliked, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='w') as f_desc_combined: # New HDF5 for combined features\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = aliked_extractor.extract(image0)\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy() # ALIKED keypoints (x,y)\n",
    "                descs_aliked = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy() # ALIKED descriptors\n",
    "\n",
    "                # Get DINO patch features for these keypoints\n",
    "                kpts_torch = torch.from_numpy(kpts).to(device)\n",
    "                descs_dino_patch = get_dino_patch_features_for_keypoints(\n",
    "                    img_path, kpts_torch, dino_processor, dino_model, dino_patch_size, device\n",
    "                ).detach().cpu().numpy()\n",
    "\n",
    "                # Concatenate ALIKED and DINO features\n",
    "                if len(descs_aliked) > 0 and len(descs_dino_patch) > 0:\n",
    "                    combined_descs = np.concatenate((descs_aliked, descs_dino_patch), axis=1)\n",
    "                elif len(descs_aliked) > 0: # Only ALIKED if no DINO features (shouldn't happen often)\n",
    "                    combined_descs = descs_aliked\n",
    "                else: # No features found\n",
    "                    combined_descs = np.array([]) # Empty array\n",
    "\n",
    "                f_kp[key] = kpts\n",
    "                f_desc_aliked[key] = descs_aliked # Keep ALIKED descriptors for debugging or other uses\n",
    "                f_desc_combined[key] = combined_descs # Store the new combined descriptors\n",
    "    print(f\"Combined features saved to {feature_dir}/descriptors_combined.h5\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f09f4d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.294701Z",
     "iopub.status.busy": "2025-06-05T23:02:58.294463Z",
     "iopub.status.idle": "2025-06-05T23:02:58.598724Z",
     "shell.execute_reply": "2025-06-05T23:02:58.598058Z"
    },
    "papermill": {
     "duration": 0.314711,
     "end_time": "2025-06-05T23:02:58.600254",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.285543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans # MiniBatchKMeans is faster for large datasets\n",
    "\n",
    "# --- VLAD Aggregation Function ---\n",
    "def vlad_encode(descriptors, centroids):\n",
    "    \"\"\"\n",
    "    Performs VLAD encoding.\n",
    "\n",
    "    Args:\n",
    "        descriptors (np.ndarray): NxM array of local descriptors.\n",
    "        centroids (np.ndarray): KxM array of K-Means cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1x(K*M) VLAD descriptor.\n",
    "    \"\"\"\n",
    "    if descriptors.shape[0] == 0:\n",
    "        return np.zeros(centroids.shape[0] * centroids.shape[1], dtype=np.float32)\n",
    "\n",
    "    num_descriptors, desc_dim = descriptors.shape\n",
    "    num_centroids, _ = centroids.shape\n",
    "\n",
    "    # Assign each descriptor to its nearest centroid\n",
    "    # Using cdist for efficiency\n",
    "    distances = np.sqrt(np.sum((descriptors[:, None, :] - centroids[None, :, :])**2, axis=2))\n",
    "    # distances = cdist(descriptors, centroids, 'sqeuclidean') # Could use cdist for sqeuclidean\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Initialize VLAD accumulator\n",
    "    vlad_accumulator = np.zeros((num_centroids, desc_dim), dtype=np.float32)\n",
    "\n",
    "    # Accumulate residuals\n",
    "    for i in range(num_descriptors):\n",
    "        cluster_idx = cluster_assignments[i]\n",
    "        residual = descriptors[i] - centroids[cluster_idx]\n",
    "        vlad_accumulator[cluster_idx] += residual\n",
    "\n",
    "    # Flatten and L2 normalize\n",
    "    vlad_descriptor = vlad_accumulator.flatten()\n",
    "    vlad_descriptor = F.normalize(torch.from_numpy(vlad_descriptor).unsqueeze(0), dim=1, p=2).squeeze(0).numpy()\n",
    "\n",
    "    return vlad_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db3eb93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.616237Z",
     "iopub.status.busy": "2025-06-05T23:02:58.615970Z",
     "iopub.status.idle": "2025-06-05T23:02:58.623603Z",
     "shell.execute_reply": "2025-06-05T23:02:58.622988Z"
    },
    "papermill": {
     "duration": 0.016862,
     "end_time": "2025-06-05T23:02:58.624674",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.607812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- NEW: Get Global Descriptors using K-Means + VLAD ---\n",
    "def get_global_desc_vlad(fnames, feature_dir='.featureout', num_clusters=64, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Generates global descriptors for images using K-Means + VLAD on combined ALIKED+DINO features.\n",
    "\n",
    "    Args:\n",
    "        fnames (list): List of image file paths.\n",
    "        feature_dir (str): Directory where combined descriptors are stored.\n",
    "        num_clusters (int): Number of clusters for K-Means (K in VLAD).\n",
    "        device (torch.device): Not directly used for VLAD computation, but passed for consistency.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Nx(K*M) tensor of global VLAD descriptors.\n",
    "    \"\"\"\n",
    "    all_local_descs = []\n",
    "    keys_order = [] # To maintain order of descriptors with respect to fnames\n",
    "\n",
    "    # 1. Load all combined local descriptors\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Loading combined local descriptors for K-Means\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                if descs.shape[0] > 0:\n",
    "                    all_local_descs.append(descs)\n",
    "                    keys_order.append(key)\n",
    "\n",
    "    if not all_local_descs:\n",
    "        print(\"No combined local descriptors found. Cannot train K-Means or compute VLAD.\")\n",
    "        return torch.empty((0, num_clusters * 0), dtype=torch.float32) # Return empty tensor\n",
    "\n",
    "    # Concatenate all descriptors for K-Means training\n",
    "    all_local_descs_flat = np.concatenate(all_local_descs, axis=0)\n",
    "\n",
    "    # 2. Train K-Means on a subset of descriptors if the dataset is too large\n",
    "    # Or directly on all_local_descs_flat if memory permits\n",
    "    print(f\"Training K-Means with {num_clusters} clusters on {all_local_descs_flat.shape[0]} descriptors...\")\n",
    "    # Use MiniBatchKMeans for efficiency\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, n_init='auto', batch_size=256).fit(all_local_descs_flat)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    print(\"K-Means training complete.\")\n",
    "\n",
    "    # 3. Compute VLAD descriptor for each image\n",
    "    global_descs_vlad = []\n",
    "    # Re-iterate through original fnames to match the output order\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Computing VLAD descriptors\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                vlad_desc = vlad_encode(descs, centroids)\n",
    "                global_descs_vlad.append(torch.from_numpy(vlad_desc).unsqueeze(0))\n",
    "            else:\n",
    "                # Handle cases where an image might not have any combined descriptors\n",
    "                # (e.g., no ALIKED keypoints detected). Append a zero vector of correct size.\n",
    "                print(f\"Warning: No combined descriptors for {key}. Appending zero VLAD descriptor.\")\n",
    "                # Determine descriptor dimension from centroids\n",
    "                desc_dim_per_cluster = centroids.shape[1] if centroids.shape[1] > 0 else 0 # Should not be 0 normally\n",
    "                zero_vlad = np.zeros(num_clusters * desc_dim_per_cluster, dtype=np.float32)\n",
    "                global_descs_vlad.append(torch.from_numpy(zero_vlad).unsqueeze(0))\n",
    "\n",
    "\n",
    "    if not global_descs_vlad:\n",
    "        return torch.empty((0, num_clusters * centroids.shape[1] if centroids.shape[1] > 0 else 0), dtype=torch.float32)\n",
    "\n",
    "    global_descs_vlad = torch.cat(global_descs_vlad, dim=0)\n",
    "    return global_descs_vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f370f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.639775Z",
     "iopub.status.busy": "2025-06-05T23:02:58.639533Z",
     "iopub.status.idle": "2025-06-05T23:02:58.647085Z",
     "shell.execute_reply": "2025-06-05T23:02:58.646323Z"
    },
    "papermill": {
     "duration": 0.016282,
     "end_time": "2025-06-05T23:02:58.648209",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.631927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- RE-DEFINED: get_image_pairs_shortlist to use the new VLAD global descriptor ---\n",
    "def get_image_pairs_shortlist_vlad(fnames,\n",
    "                                   sim_th=0.6, # should be strict\n",
    "                                   min_pairs=30,\n",
    "                                   exhaustive_if_less=20,\n",
    "                                   feature_dir='.featureout', # Pass feature_dir\n",
    "                                   num_clusters_vlad=64, # New parameter for VLAD\n",
    "                                   device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames) # You need to define get_img_pairs_exhaustive if not done.\n",
    "\n",
    "    # Use the new VLAD-based global descriptor\n",
    "    descs = get_global_desc_vlad(fnames, feature_dir=feature_dir, num_clusters=num_clusters_vlad, device=device)\n",
    "\n",
    "    if descs.shape[0] == 0:\n",
    "        print(\"No global descriptors generated. Returning empty matching list.\")\n",
    "        return []\n",
    "\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # \n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 60)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = set() # Use a set for faster lookup of already added pairs\n",
    "\n",
    "    for st_idx in range(num_imgs - 1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold: # Ensure distance is not effectively infinite\n",
    "                pair = tuple(sorted((st_idx, idx.item())))\n",
    "                if pair not in already_there_set:\n",
    "                    matching_list.append(pair)\n",
    "                    already_there_set.add(pair)\n",
    "                    total += 1\n",
    "    matching_list = sorted(list(matching_list)) # Sort the list of tuples\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "895ed804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.663262Z",
     "iopub.status.busy": "2025-06-05T23:02:58.663022Z",
     "iopub.status.idle": "2025-06-05T23:02:58.666583Z",
     "shell.execute_reply": "2025-06-05T23:02:58.665778Z"
    },
    "papermill": {
     "duration": 0.012198,
     "end_time": "2025-06-05T23:02:58.667706",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.655508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "381846a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.683084Z",
     "iopub.status.busy": "2025-06-05T23:02:58.682858Z",
     "iopub.status.idle": "2025-06-05T23:02:58.692759Z",
     "shell.execute_reply": "2025-06-05T23:02:58.691984Z"
    },
    "papermill": {
     "duration": 0.018993,
     "end_time": "2025-06-05T23:02:58.693854",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.674861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th=0.6,\n",
    "                              min_pairs=30,\n",
    "                              max_pairs=100,  #  max_pairs \n",
    "                              exhaustive_if_less=20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "    mask = dm <= np.percentile(dm_flat, 50)\n",
    "\n",
    "    ar = np.arange(num_imgs)\n",
    "    matching_set = set()\n",
    "\n",
    "    for st_idx in range(num_imgs):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "\n",
    "        #  min_pairs \n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        #  max_pairs\n",
    "        sorted_matches = sorted(\n",
    "            [(idx, dm[st_idx, idx]) for idx in to_match if idx != st_idx and dm[st_idx, idx] < threshold],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        for idx, _ in sorted_matches[:max_pairs]:\n",
    "            pair = tuple(sorted((st_idx, idx)))\n",
    "            matching_set.add(pair)\n",
    "\n",
    "    matching_list = sorted(list(matching_set))\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0c013e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.708650Z",
     "iopub.status.busy": "2025-06-05T23:02:58.708434Z",
     "iopub.status.idle": "2025-06-05T23:02:58.728400Z",
     "shell.execute_reply": "2025-06-05T23:02:58.727779Z"
    },
    "papermill": {
     "duration": 0.0288,
     "end_time": "2025-06-05T23:02:58.729708",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.700908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_aliked_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_superpoint_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_disk_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a21e68d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.745069Z",
     "iopub.status.busy": "2025-06-05T23:02:58.744839Z",
     "iopub.status.idle": "2025-06-05T23:02:58.753209Z",
     "shell.execute_reply": "2025-06-05T23:02:58.752427Z"
    },
    "papermill": {
     "duration": 0.017248,
     "end_time": "2025-06-05T23:02:58.754320",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.737072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def find_connected_components(pairs):\n",
    "    graph = defaultdict(set)\n",
    "    for i, j in pairs:\n",
    "        graph[i].add(j)\n",
    "        graph[j].add(i)\n",
    "\n",
    "    visited = set()\n",
    "    components = []\n",
    "\n",
    "    def dfs(u, comp):\n",
    "        visited.add(u)\n",
    "        comp.append(u)\n",
    "        for v in graph[u]:\n",
    "            if v not in visited:\n",
    "                dfs(v, comp)\n",
    "\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            comp = []\n",
    "            dfs(node, comp)\n",
    "            components.append(comp)\n",
    "\n",
    "    return components\n",
    "\n",
    "\n",
    "def affine_matrix_from_points(v0, v1):\n",
    "    v0 = np.array(v0, dtype=np.float64, copy=True)\n",
    "    v1 = np.array(v1, dtype=np.float64, copy=True)\n",
    "    t0 = -np.mean(v0, axis=1)\n",
    "    t1 = -np.mean(v1, axis=1)\n",
    "    v0 += t0.reshape(3, 1)\n",
    "    v1 += t1.reshape(3, 1)\n",
    "    u, s, vh = np.linalg.svd(np.dot(v1, v0.T))\n",
    "    R = np.dot(u, vh)\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R[:, -1] *= -1\n",
    "    scale = np.linalg.norm(v1) / np.linalg.norm(v0)\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = scale * R\n",
    "    T[:3, 3] = np.mean(v1, axis=1) - np.dot(scale * R, np.mean(v0, axis=1))\n",
    "    return T\n",
    "\n",
    "\n",
    "def register_by_Horn(ev_coord, gt_coord, threshold=1.0):\n",
    "    if ev_coord.shape[1] < 3:\n",
    "        return None\n",
    "    T = affine_matrix_from_points(ev_coord, gt_coord)\n",
    "    transformed = (T[:3, :3] @ ev_coord) + T[:3, 3:4]\n",
    "    err = np.linalg.norm(transformed - gt_coord, axis=0)\n",
    "    inliers = err < threshold\n",
    "    if np.sum(inliers) < 3:\n",
    "        return None\n",
    "    T_refined = affine_matrix_from_points(ev_coord[:, inliers], gt_coord[:, inliers])\n",
    "    return T_refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbd95eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.770095Z",
     "iopub.status.busy": "2025-06-05T23:02:58.769887Z",
     "iopub.status.idle": "2025-06-05T23:02:58.781702Z",
     "shell.execute_reply": "2025-06-05T23:02:58.781056Z"
    },
    "papermill": {
     "duration": 0.021036,
     "end_time": "2025-06-05T23:02:58.782819",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.761783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(feature_dir, img_dir):\n",
    "    result = {}\n",
    "    local_timings = {'RANSAC': [], 'Reconstruction': []}\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    t = time()\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    local_timings['RANSAC'].append(time() - t)\n",
    "\n",
    "    t = time()\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    mapper_options.min_model_size = 5\n",
    "    mapper_options.max_num_models = 12\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir,\n",
    "                                        output_path=output_path, options=mapper_options)\n",
    "    print(\"Original results\")\n",
    "    print(maps)\n",
    "    local_timings['Reconstruction'].append(time() - t)\n",
    "\n",
    "    map_ids = list(maps.keys())\n",
    "    map_graph = {}\n",
    "    for i, j in combinations(map_ids, 2):\n",
    "        shared = []\n",
    "        for name in maps[i].images:\n",
    "            if name in maps[j].images:\n",
    "                C_i = -maps[i].images[name].cam_from_world.rotation.matrix().T @ maps[i].images[name].cam_from_world.translation\n",
    "                C_j = -maps[j].images[name].cam_from_world.rotation.matrix().T @ maps[j].images[name].cam_from_world.translation\n",
    "                shared.append((C_i.reshape(3, 1), C_j.reshape(3, 1)))\n",
    "        if len(shared) >= 3:\n",
    "            u, g = zip(*shared)\n",
    "            u = np.concatenate(u, axis=1)\n",
    "            g = np.concatenate(g, axis=1)\n",
    "            T_ij = register_by_Horn(u, g)\n",
    "            if T_ij is not None:\n",
    "                map_graph[(i, j)] = T_ij\n",
    "    print(map_graph)\n",
    "    connected_components = find_connected_components(map_graph.keys())\n",
    "    for group in connected_components:\n",
    "        transforms = {group[0]: np.eye(4)}\n",
    "        queue = [group[0]]\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            for other in group:\n",
    "                if other == current or other in transforms:\n",
    "                    continue\n",
    "                if (current, other) in map_graph:\n",
    "                    transforms[other] = map_graph[(current, other)] @ transforms[current]\n",
    "                    queue.append(other)\n",
    "                elif (other, current) in map_graph:\n",
    "                    transforms[other] = np.linalg.inv(map_graph[(other, current)]) @ transforms[current]\n",
    "                    queue.append(other)\n",
    "\n",
    "        for map_index in group:\n",
    "            result[map_index] = {}\n",
    "            T = transforms.get(map_index, np.eye(4))\n",
    "            for img_id, image in maps[map_index].images.items():\n",
    "                T_cam = np.eye(4)\n",
    "                T_cam[:3, :3] = image.cam_from_world.rotation.matrix()\n",
    "                T_cam[:3, 3] = image.cam_from_world.translation\n",
    "                T_global = T @ T_cam\n",
    "                result[map_index][image.name] = {\n",
    "                    'R': T_global[:3, :3].tolist(),\n",
    "                    't': T_global[:3, 3].tolist()\n",
    "                }\n",
    "\n",
    "    for map_index in maps:\n",
    "        if map_index not in result:\n",
    "            result[map_index] = {}\n",
    "            for img_id, image in maps[map_index].images.items():\n",
    "                result[map_index][image.name] = {\n",
    "                    'R': image.cam_from_world.rotation.matrix().tolist(),\n",
    "                    't': image.cam_from_world.translation.tolist()\n",
    "                }\n",
    "    if VERBOSE:\n",
    "        for map_index in maps:\n",
    "            for img_id, image in maps[map_index].images.items():\n",
    "                print(f\"map {map_index}:{image}\")\n",
    "    return result, local_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdc1f800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.798770Z",
     "iopub.status.busy": "2025-06-05T23:02:58.798558Z",
     "iopub.status.idle": "2025-06-05T23:02:58.959606Z",
     "shell.execute_reply": "2025-06-05T23:02:58.958964Z"
    },
    "papermill": {
     "duration": 0.170359,
     "end_time": "2025-06-05T23:02:58.960825",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.790466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79b5a0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:58.976771Z",
     "iopub.status.busy": "2025-06-05T23:02:58.976569Z",
     "iopub.status.idle": "2025-06-05T23:02:58.990506Z",
     "shell.execute_reply": "2025-06-05T23:02:58.989896Z"
    },
    "papermill": {
     "duration": 0.023016,
     "end_time": "2025-06-05T23:02:58.991660",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.968644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_keypoints_and_matches(images_input, unified_kp_path, remapped_matches_path, feature_dir='visualization_output'):\n",
    "    output_dir = os.path.join(feature_dir, 'visualization_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load images and determine image_keys for HDF5 lookup\n",
    "    if isinstance(images_input[0], str):\n",
    "        loaded_images = [cv2.imread(img_path) for img_path in images_input]\n",
    "        image_keys = [os.path.basename(img_path) for img_path in images_input]\n",
    "    else:\n",
    "        loaded_images = images_input\n",
    "        # If images_input are already arrays, you need to provide the corresponding keys\n",
    "        # This part is crucial: image_keys MUST align with the HDF5 keys\n",
    "        image_keys = image_keys_in_h5 # Use the predefined list for the dummy case\n",
    "\n",
    "    # Load unified keypoints\n",
    "    keypoints_data = {}\n",
    "    with h5py.File(unified_kp_path, 'r') as f_kp:\n",
    "        for img_name_raw in f_kp.keys():\n",
    "            img_name = img_name_raw.decode('utf-8') if isinstance(img_name_raw, bytes) else img_name_raw\n",
    "            keypoints_data[img_name] = f_kp[img_name_raw][()] # Access with raw key if bytes\n",
    "\n",
    "    # Load remapped matches - CORRECTED LOGIC\n",
    "    # Store (img1_key, img2_key) directly with matches for robust iteration\n",
    "    matches_data_pairs = [] # Will store (img1_key, img2_key, matches_array)\n",
    "    with h5py.File(remapped_matches_path, 'r') as f_matches:\n",
    "        print(\"\\n--- Loading remapped matches from HDF5 ---\")\n",
    "        for img1_group_key_candidate in tqdm(f_matches.keys(), desc=\"Loading matches\"):\n",
    "            img1_key = img1_group_key_candidate.decode('utf-8') if isinstance(img1_group_key_candidate, bytes) else img1_group_key_candidate\n",
    "\n",
    "            img1_group = f_matches[img1_group_key_candidate] # Access with raw key\n",
    "\n",
    "            if isinstance(img1_group, h5py.Group):\n",
    "                for img2_dataset_key_candidate in img1_group.keys():\n",
    "                    img2_key = img2_dataset_key_candidate.decode('utf-8') if isinstance(img2_dataset_key_candidate, bytes) else img2_dataset_key_candidate\n",
    "\n",
    "                    try:\n",
    "                        matches_array = img1_group[img2_dataset_key_candidate][()]\n",
    "                        matches_data_pairs.append((img1_key, img2_key, matches_array))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading matches for pair ({img1_key}, {img2_key}): {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Expected '{img1_key}' to be a group, but found {type(img1_group)}. Skipping its contents.\")\n",
    "\n",
    "\n",
    "    # --- Drawing Keypoints ---\n",
    "    print(\"\\n--- Drawing Keypoints ---\")\n",
    "    for i, img_key in enumerate(image_keys):\n",
    "        if img_key in keypoints_data:\n",
    "            img = loaded_images[i].copy()\n",
    "            kpts = keypoints_data[img_key]\n",
    "\n",
    "            for kp in kpts:\n",
    "                x, y = int(kp[0]), int(kp[1])\n",
    "                cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Green circle for keypoint\n",
    "\n",
    "            output_kp_path = os.path.join(output_dir, f\"keypoints_{img_key}\")\n",
    "            if len(img.shape) == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(output_kp_path, img)\n",
    "            print(f\"Keypoints drawn on {img_key}, saved to {output_kp_path}\")\n",
    "        else:\n",
    "            print(f\"No keypoints found for {img_key} in unified keypoints file.\")\n",
    "\n",
    "    # --- Drawing Matches ---\n",
    "    print(\"\\n--- Drawing Matches ---\")\n",
    "    # Iterate through the (img1_key, img2_key, matches) tuples directly\n",
    "    for img_name1, img_name2, matches in matches_data_pairs:\n",
    "        # We no longer need to split img_pair_key, as we have img_name1 and img_name2 directly\n",
    "\n",
    "        # Find the actual image objects and their keypoints using image_keys list\n",
    "        try:\n",
    "            img1_idx = image_keys.index(img_name1)\n",
    "            img2_idx = image_keys.index(img_name2)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: One or both image names not found in the provided 'images' list/keys.\")\n",
    "            continue\n",
    "\n",
    "        img1 = loaded_images[img1_idx].copy()\n",
    "        img2 = loaded_images[img2_idx].copy()\n",
    "\n",
    "        kpts1 = keypoints_data.get(img_name1)\n",
    "        kpts2 = keypoints_data.get(img_name2)\n",
    "\n",
    "        if kpts1 is None or kpts2 is None:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: keypoints not found for one or both images in unified keypoints.\")\n",
    "            continue\n",
    "        if len(matches) == 0:\n",
    "            print(f\"No matches to draw for {img_name1}-{img_name2}.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure images are 3 channels for drawing lines\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "        if len(img2.shape) == 2:\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Create a concatenated image for drawing matches\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        max_h = max(h1, h2)\n",
    "        matched_img = np.zeros((max_h, w1 + w2, 3), dtype=np.uint8)\n",
    "        matched_img[0:h1, 0:w1] = img1\n",
    "        matched_img[0:h2, w1:w1+w2] = img2\n",
    "\n",
    "        num_matches_to_draw = min(len(matches), 200) # Draw up to 200 matches to avoid clutter, adjust as needed\n",
    "\n",
    "        for i in range(num_matches_to_draw):\n",
    "            match = matches[i]\n",
    "            kp1_idx, kp2_idx = int(match[0]), int(match[1])\n",
    "\n",
    "            # Bounds check for keypoint indices\n",
    "            if kp1_idx >= len(kpts1) or kp2_idx >= len(kpts2):\n",
    "                # print(f\"Warning: Match index out of bounds for {img_name1}-{img_name2}. Skipping match {kp1_idx}-{kp2_idx}.\")\n",
    "                continue\n",
    "\n",
    "            pt1 = tuple(map(int, kpts1[kp1_idx][:2]))\n",
    "            pt2 = tuple(map(int, kpts2[kp2_idx][:2]))\n",
    "\n",
    "            # Draw circles on the concatenated image\n",
    "            cv2.circle(matched_img, pt1, 5, (0, 0, 255), 2) # Red circle on img1 side\n",
    "            cv2.circle(matched_img, (pt2[0] + w1, pt2[1]), 5, (255, 0, 0), 2) # Blue circle on img2 side\n",
    "\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            cv2.line(matched_img, pt1, (pt2[0] + w1, pt2[1]), color, 1)\n",
    "\n",
    "        output_match_path = os.path.join(output_dir, f\"matches_{img_name1}_{img_name2}.png\")\n",
    "        cv2.imwrite(output_match_path, matched_img)\n",
    "        print(f\"Matches drawn between {img_name1} and {img_name2}, saved to {output_match_path}\")\n",
    "\n",
    "\n",
    "# Example call (replace with your actual 'images' list)\n",
    "# If your 'images' are file paths:\n",
    "# images_file_paths = ['path/to/your/image1.jpg', 'path/to/your/image2.jpg', ...]\n",
    "# draw_keypoints_and_matches(images_file_paths, unified_kp_path, remapped_matches_path)\n",
    "\n",
    "# If your 'images' are loaded numpy arrays (as in the dummy example above):\n",
    "# draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1392ca9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:02:59.007382Z",
     "iopub.status.busy": "2025-06-05T23:02:59.007174Z",
     "iopub.status.idle": "2025-06-05T23:09:12.209317Z",
     "shell.execute_reply": "2025-06-05T23:09:12.208326Z"
    },
    "papermill": {
     "duration": 373.211815,
     "end_time": "2025-06-05T23:09:12.210704",
     "exception": false,
     "start_time": "2025-06-05T23:02:58.998889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model for patch feature extraction...\n",
      "DINOv2 model loaded.\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n",
      "rotation_detection for 22 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 22/22 [00:02<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "25%:  0.2547\n",
      "60%:  0.2913\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 150. Done in 2.5562 sec\n",
      "Generated 150 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 150. Done in 2.8770 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1656, 2]), descs.shape=torch.Size([1656, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1449, 2]), descs.shape=torch.Size([1449, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1341, 2]), descs.shape=torch.Size([1341, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1395, 2]), descs.shape=torch.Size([1395, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1387, 2]), descs.shape=torch.Size([1387, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1343, 2]), descs.shape=torch.Size([1343, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1422, 2]), descs.shape=torch.Size([1422, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1375, 2]), descs.shape=torch.Size([1375, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1132, 2]), descs.shape=torch.Size([1132, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1022, 2]), descs.shape=torch.Size([1022, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2785, 2]), descs.shape=torch.Size([2785, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2762, 2]), descs.shape=torch.Size([2762, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2473, 2]), descs.shape=torch.Size([2473, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2462, 2]), descs.shape=torch.Size([2462, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2866, 2]), descs.shape=torch.Size([2866, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2248, 2]), descs.shape=torch.Size([2248, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2026, 2]), descs.shape=torch.Size([2026, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2171, 2]), descs.shape=torch.Size([2171, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2480, 2]), descs.shape=torch.Size([2480, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1830, 2]), descs.shape=torch.Size([1830, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2499, 2]), descs.shape=torch.Size([2499, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2361, 2]), descs.shape=torch.Size([2361, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 5/150 [00:00<00:08, 16.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et001.png-another_et_another_et002.png: 962 matches @ 1th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et004.png: 682 matches @ 2th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et005.png: 746 matches @ 3th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et006.png: 342 matches @ 4th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et007.png: 262 matches @ 5th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et008.png: 107 matches @ 6th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|        | 17/150 [00:00<00:04, 30.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et002.png-another_et_another_et003.png: 486 matches @ 7th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et004.png: 707 matches @ 8th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et005.png: 652 matches @ 9th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et006.png: 373 matches @ 10th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et007.png: 296 matches @ 11th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et008.png: 103 matches @ 12th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 33/150 [00:01<00:03, 34.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et003.png-another_et_another_et004.png: 498 matches @ 13th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et005.png: 378 matches @ 14th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et006.png: 259 matches @ 15th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et007.png: 183 matches @ 16th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 45/150 [00:01<00:02, 35.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et004.png-another_et_another_et005.png: 682 matches @ 17th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et004.png-another_et_another_et006.png: 333 matches @ 18th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et004.png-another_et_another_et007.png: 197 matches @ 19th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 54/150 [00:01<00:02, 36.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et005.png-another_et_another_et006.png: 269 matches @ 20th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et005.png-another_et_another_et007.png: 177 matches @ 21th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et006.png-another_et_another_et007.png: 393 matches @ 22th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et006.png-another_et_another_et008.png: 271 matches @ 23th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 70/150 [00:02<00:02, 34.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et007.png-another_et_another_et008.png: 393 matches @ 24th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et007.png-another_et_another_et009.png: 170 matches @ 25th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et007.png-another_et_another_et010.png: 113 matches @ 26th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 82/150 [00:02<00:02, 33.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et008.png-another_et_another_et009.png: 329 matches @ 27th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et008.png-another_et_another_et010.png: 239 matches @ 28th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 94/150 [00:02<00:01, 33.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et009.png-another_et_another_et010.png: 225 matches @ 29th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 102/150 [00:03<00:01, 30.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et000.png-et_et001.png: 1330 matches @ 30th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et002.png: 892 matches @ 31th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et003.png: 1738 matches @ 32th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et004.png: 1003 matches @ 33th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et005.png: 137 matches @ 34th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 109/150 [00:03<00:01, 25.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et000.png-et_et006.png: 135 matches @ 35th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et007.png: 101 matches @ 36th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et002.png: 1416 matches @ 37th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 115/150 [00:03<00:01, 22.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et001.png-et_et003.png: 985 matches @ 38th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et004.png: 1239 matches @ 39th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et005.png: 227 matches @ 40th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et006.png: 270 matches @ 41th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et007.png: 229 matches @ 42th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 121/150 [00:04<00:01, 22.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et002.png-et_et003.png: 680 matches @ 43th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et004.png: 905 matches @ 44th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et005.png: 328 matches @ 45th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et006.png: 403 matches @ 46th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et007.png: 272 matches @ 47th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 127/150 [00:04<00:01, 22.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et002.png-et_et008.png: 204 matches @ 48th pair(aliked+lightglue)\n",
      "aliked> et_et003.png-et_et004.png: 792 matches @ 49th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 133/150 [00:04<00:00, 22.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et004.png-et_et005.png: 208 matches @ 50th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et006.png: 245 matches @ 51th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et007.png: 189 matches @ 52th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et008.png: 141 matches @ 53th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 139/150 [00:04<00:00, 23.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et005.png-et_et006.png: 1235 matches @ 54th pair(aliked+lightglue)\n",
      "aliked> et_et005.png-et_et007.png: 1281 matches @ 55th pair(aliked+lightglue)\n",
      "aliked> et_et005.png-et_et008.png: 933 matches @ 56th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 145/150 [00:05<00:00, 24.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et006.png-et_et007.png: 1362 matches @ 57th pair(aliked+lightglue)\n",
      "aliked> et_et006.png-et_et008.png: 690 matches @ 58th pair(aliked+lightglue)\n",
      "aliked> et_et007.png-et_et008.png: 884 matches @ 59th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:05<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  8.0404 sec (aliked+LightGlue)\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([395, 2]), descs.shape=torch.Size([395, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([218, 2]), descs.shape=torch.Size([218, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([166, 2]), descs.shape=torch.Size([166, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([207, 2]), descs.shape=torch.Size([207, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([262, 2]), descs.shape=torch.Size([262, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([215, 2]), descs.shape=torch.Size([215, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([203, 2]), descs.shape=torch.Size([203, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([163, 2]), descs.shape=torch.Size([163, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([145, 2]), descs.shape=torch.Size([145, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([165, 2]), descs.shape=torch.Size([165, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([324, 2]), descs.shape=torch.Size([324, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([332, 2]), descs.shape=torch.Size([332, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([319, 2]), descs.shape=torch.Size([319, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([239, 2]), descs.shape=torch.Size([239, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([273, 2]), descs.shape=torch.Size([273, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([181, 2]), descs.shape=torch.Size([181, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([206, 2]), descs.shape=torch.Size([206, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([207, 2]), descs.shape=torch.Size([207, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([198, 2]), descs.shape=torch.Size([198, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([88, 2]), descs.shape=torch.Size([88, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([55, 2]), descs.shape=torch.Size([55, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([103, 2]), descs.shape=torch.Size([103, 256])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 4/150 [00:00<00:04, 33.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et001.png-another_et_another_et002.png: 149 matches @ 1th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et001.png-another_et_another_et004.png: 103 matches @ 2th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et001.png-another_et_another_et005.png: 135 matches @ 3th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et001.png-another_et_another_et006.png: 81 matches @ 4th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 19/150 [00:00<00:03, 41.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et002.png-another_et_another_et004.png: 80 matches @ 5th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et002.png-another_et_another_et005.png: 93 matches @ 6th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 44/150 [00:01<00:02, 41.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et004.png-another_et_another_et005.png: 98 matches @ 7th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 58/150 [00:01<00:02, 35.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et006.png-another_et_another_et007.png: 52 matches @ 8th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 72/150 [00:01<00:02, 38.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et007.png-another_et_another_et008.png: 54 matches @ 9th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|  | 107/150 [00:02<00:01, 41.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et000.png-et_et001.png: 163 matches @ 10th pair(superpoint+lightglue)\n",
      "superpoint> et_et000.png-et_et002.png: 122 matches @ 11th pair(superpoint+lightglue)\n",
      "superpoint> et_et000.png-et_et003.png: 184 matches @ 12th pair(superpoint+lightglue)\n",
      "superpoint> et_et000.png-et_et004.png: 111 matches @ 13th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 117/150 [00:02<00:00, 41.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et001.png-et_et002.png: 187 matches @ 14th pair(superpoint+lightglue)\n",
      "superpoint> et_et001.png-et_et003.png: 124 matches @ 15th pair(superpoint+lightglue)\n",
      "superpoint> et_et001.png-et_et004.png: 118 matches @ 16th pair(superpoint+lightglue)\n",
      "superpoint> et_et001.png-et_et006.png: 53 matches @ 17th pair(superpoint+lightglue)\n",
      "superpoint> et_et002.png-et_et003.png: 100 matches @ 18th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 122/150 [00:03<00:00, 41.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et002.png-et_et004.png: 104 matches @ 19th pair(superpoint+lightglue)\n",
      "superpoint> et_et002.png-et_et006.png: 65 matches @ 20th pair(superpoint+lightglue)\n",
      "superpoint> et_et002.png-et_et007.png: 53 matches @ 21th pair(superpoint+lightglue)\n",
      "superpoint> et_et003.png-et_et004.png: 85 matches @ 22th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|| 137/150 [00:03<00:00, 41.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et004.png-et_et006.png: 56 matches @ 23th pair(superpoint+lightglue)\n",
      "superpoint> et_et005.png-et_et006.png: 103 matches @ 24th pair(superpoint+lightglue)\n",
      "superpoint> et_et005.png-et_et007.png: 102 matches @ 25th pair(superpoint+lightglue)\n",
      "superpoint> et_et005.png-et_et008.png: 68 matches @ 26th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 147/150 [00:03<00:00, 41.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et006.png-et_et007.png: 132 matches @ 27th pair(superpoint+lightglue)\n",
      "superpoint> et_et006.png-et_et008.png: 65 matches @ 28th pair(superpoint+lightglue)\n",
      "superpoint> et_et007.png-et_et008.png: 76 matches @ 29th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:03<00:00, 40.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  5.8148 sec (superpoint+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4475, 2]), descs.shape=torch.Size([4475, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3959, 2]), descs.shape=torch.Size([3959, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3945, 2]), descs.shape=torch.Size([3945, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3789, 2]), descs.shape=torch.Size([3789, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3722, 2]), descs.shape=torch.Size([3722, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3725, 2]), descs.shape=torch.Size([3725, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3742, 2]), descs.shape=torch.Size([3742, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3406, 2]), descs.shape=torch.Size([3406, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3319, 2]), descs.shape=torch.Size([3319, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([2928, 2]), descs.shape=torch.Size([2928, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([6018, 2]), descs.shape=torch.Size([6018, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([6118, 2]), descs.shape=torch.Size([6118, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([6009, 2]), descs.shape=torch.Size([6009, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5317, 2]), descs.shape=torch.Size([5317, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5661, 2]), descs.shape=torch.Size([5661, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5496, 2]), descs.shape=torch.Size([5496, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5553, 2]), descs.shape=torch.Size([5553, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5276, 2]), descs.shape=torch.Size([5276, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5578, 2]), descs.shape=torch.Size([5578, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4859, 2]), descs.shape=torch.Size([4859, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4393, 2]), descs.shape=torch.Size([4393, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5047, 2]), descs.shape=torch.Size([5047, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et002.png: 2663 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 2/150 [00:00<00:14, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et004.png: 1886 matches @ 2th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et005.png: 2028 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 4/150 [00:00<00:13, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et006.png: 1334 matches @ 4th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 6/150 [00:00<00:13, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et007.png: 651 matches @ 5th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et008.png: 268 matches @ 6th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 12/150 [00:01<00:14,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et003.png: 1364 matches @ 7th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et004.png: 1855 matches @ 8th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et005.png: 1798 matches @ 9th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 16/150 [00:01<00:12, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et006.png: 1240 matches @ 10th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et007.png: 675 matches @ 11th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et008.png: 288 matches @ 12th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 28/150 [00:02<00:12,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et003.png-another_et_another_et004.png: 1460 matches @ 13th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et005.png: 1169 matches @ 14th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et006.png: 852 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|       | 32/150 [00:03<00:10, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et003.png-another_et_another_et007.png: 461 matches @ 16th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et008.png: 193 matches @ 17th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 41/150 [00:04<00:10, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et004.png-another_et_another_et005.png: 1791 matches @ 18th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et006.png: 1024 matches @ 19th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et007.png: 464 matches @ 20th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 43/150 [00:04<00:10, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et004.png-another_et_another_et008.png: 240 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 49/150 [00:04<00:09, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et005.png-another_et_another_et006.png: 1185 matches @ 22th pair(disk+lightglue)\n",
      "disk> another_et_another_et005.png-another_et_another_et007.png: 405 matches @ 23th pair(disk+lightglue)\n",
      "disk> another_et_another_et005.png-another_et_another_et008.png: 213 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 55/150 [00:05<00:08, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et006.png-another_et_another_et007.png: 1201 matches @ 25th pair(disk+lightglue)\n",
      "disk> another_et_another_et006.png-another_et_another_et008.png: 787 matches @ 26th pair(disk+lightglue)\n",
      "disk> another_et_another_et006.png-another_et_another_et009.png: 160 matches @ 27th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 68/150 [00:06<00:07, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et007.png-another_et_another_et008.png: 1104 matches @ 28th pair(disk+lightglue)\n",
      "disk> another_et_another_et007.png-another_et_another_et009.png: 555 matches @ 29th pair(disk+lightglue)\n",
      "disk> another_et_another_et007.png-another_et_another_et010.png: 292 matches @ 30th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 80/150 [00:07<00:06, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et008.png-another_et_another_et009.png: 993 matches @ 31th pair(disk+lightglue)\n",
      "disk> another_et_another_et008.png-another_et_another_et010.png: 356 matches @ 32th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 90/150 [00:08<00:05, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et009.png-another_et_another_et010.png: 637 matches @ 33th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 102/150 [00:10<00:05,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et000.png-et_et001.png: 3138 matches @ 34th pair(disk+lightglue)\n",
      "disk> et_et000.png-et_et002.png: 2443 matches @ 35th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 104/150 [00:10<00:06,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et000.png-et_et003.png: 3888 matches @ 36th pair(disk+lightglue)\n",
      "disk> et_et000.png-et_et004.png: 2233 matches @ 37th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 111/150 [00:11<00:06,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et001.png-et_et002.png: 3386 matches @ 38th pair(disk+lightglue)\n",
      "disk> et_et001.png-et_et003.png: 2479 matches @ 39th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 113/150 [00:12<00:06,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et001.png-et_et004.png: 2518 matches @ 40th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 119/150 [00:13<00:04,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et003.png: 1981 matches @ 41th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et004.png: 2227 matches @ 42th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 121/150 [00:13<00:04,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et005.png: 302 matches @ 43th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et006.png: 715 matches @ 44th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 123/150 [00:13<00:04,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et007.png: 255 matches @ 45th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 125/150 [00:14<00:03,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et003.png-et_et004.png: 1824 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 131/150 [00:14<00:02,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et004.png-et_et005.png: 184 matches @ 47th pair(disk+lightglue)\n",
      "disk> et_et004.png-et_et006.png: 337 matches @ 48th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 136/150 [00:15<00:02,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et005.png-et_et006.png: 3273 matches @ 49th pair(disk+lightglue)\n",
      "disk> et_et005.png-et_et007.png: 3155 matches @ 50th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 138/150 [00:15<00:01,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et005.png-et_et008.png: 2423 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 141/150 [00:16<00:01,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et006.png-et_et007.png: 3581 matches @ 52th pair(disk+lightglue)\n",
      "disk> et_et006.png-et_et008.png: 2105 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 146/150 [00:17<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et007.png-et_et008.png: 2448 matches @ 54th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:17<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  22.5599 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='150' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [150/150 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_FundamentalMatrix: 3774 matches --> 3771 matches\n",
      "another_et_another_et001.png-another_et_another_et002.png: 3774 --> 3771 matches\n",
      "filter_FundamentalMatrix: 2671 matches --> 2666 matches\n",
      "another_et_another_et001.png-another_et_another_et004.png: 2671 --> 2666 matches\n",
      "filter_FundamentalMatrix: 2909 matches --> 2907 matches\n",
      "another_et_another_et001.png-another_et_another_et005.png: 2909 --> 2907 matches\n",
      "filter_FundamentalMatrix: 1757 matches --> 1707 matches\n",
      "another_et_another_et001.png-another_et_another_et006.png: 1757 --> 1707 matches\n",
      "filter_FundamentalMatrix: 913 matches --> 890 matches\n",
      "another_et_another_et001.png-another_et_another_et007.png: 913 --> 890 matches\n",
      "filter_FundamentalMatrix: 375 matches --> 344 matches\n",
      "another_et_another_et001.png-another_et_another_et008.png: 375 --> 344 matches\n",
      "filter_FundamentalMatrix: 1850 matches --> 1846 matches\n",
      "another_et_another_et002.png-another_et_another_et003.png: 1850 --> 1846 matches\n",
      "filter_FundamentalMatrix: 2642 matches --> 2627 matches\n",
      "another_et_another_et002.png-another_et_another_et004.png: 2642 --> 2627 matches\n",
      "filter_FundamentalMatrix: 2543 matches --> 2527 matches\n",
      "another_et_another_et002.png-another_et_another_et005.png: 2543 --> 2527 matches\n",
      "filter_FundamentalMatrix: 1613 matches --> 1580 matches\n",
      "another_et_another_et002.png-another_et_another_et006.png: 1613 --> 1580 matches\n",
      "filter_FundamentalMatrix: 971 matches --> 943 matches\n",
      "another_et_another_et002.png-another_et_another_et007.png: 971 --> 943 matches\n",
      "filter_FundamentalMatrix: 391 matches --> 373 matches\n",
      "another_et_another_et002.png-another_et_another_et008.png: 391 --> 373 matches\n",
      "filter_FundamentalMatrix: 1958 matches --> 1943 matches\n",
      "another_et_another_et003.png-another_et_another_et004.png: 1958 --> 1943 matches\n",
      "filter_FundamentalMatrix: 1547 matches --> 1536 matches\n",
      "another_et_another_et003.png-another_et_another_et005.png: 1547 --> 1536 matches\n",
      "filter_FundamentalMatrix: 1111 matches --> 1089 matches\n",
      "another_et_another_et003.png-another_et_another_et006.png: 1111 --> 1089 matches\n",
      "filter_FundamentalMatrix: 644 matches --> 613 matches\n",
      "another_et_another_et003.png-another_et_another_et007.png: 644 --> 613 matches\n",
      "filter_FundamentalMatrix: 193 matches --> 186 matches\n",
      "another_et_another_et003.png-another_et_another_et008.png: 193 --> 186 matches\n",
      "filter_FundamentalMatrix: 2571 matches --> 2553 matches\n",
      "another_et_another_et004.png-another_et_another_et005.png: 2571 --> 2553 matches\n",
      "filter_FundamentalMatrix: 1357 matches --> 1335 matches\n",
      "another_et_another_et004.png-another_et_another_et006.png: 1357 --> 1335 matches\n",
      "filter_FundamentalMatrix: 661 matches --> 641 matches\n",
      "another_et_another_et004.png-another_et_another_et007.png: 661 --> 641 matches\n",
      "filter_FundamentalMatrix: 240 matches --> 234 matches\n",
      "another_et_another_et004.png-another_et_another_et008.png: 240 --> 234 matches\n",
      "filter_FundamentalMatrix: 1454 matches --> 1441 matches\n",
      "another_et_another_et005.png-another_et_another_et006.png: 1454 --> 1441 matches\n",
      "filter_FundamentalMatrix: 582 matches --> 557 matches\n",
      "another_et_another_et005.png-another_et_another_et007.png: 582 --> 557 matches\n",
      "filter_FundamentalMatrix: 213 matches --> 193 matches\n",
      "another_et_another_et005.png-another_et_another_et008.png: 213 --> 193 matches\n",
      "filter_FundamentalMatrix: 1646 matches --> 1604 matches\n",
      "another_et_another_et006.png-another_et_another_et007.png: 1646 --> 1604 matches\n",
      "filter_FundamentalMatrix: 1058 matches --> 1043 matches\n",
      "another_et_another_et006.png-another_et_another_et008.png: 1058 --> 1043 matches\n",
      "filter_FundamentalMatrix: 160 matches --> 159 matches\n",
      "another_et_another_et006.png-another_et_another_et009.png: 160 --> 159 matches\n",
      "filter_FundamentalMatrix: 1551 matches --> 1512 matches\n",
      "another_et_another_et007.png-another_et_another_et008.png: 1551 --> 1512 matches\n",
      "filter_FundamentalMatrix: 725 matches --> 712 matches\n",
      "another_et_another_et007.png-another_et_another_et009.png: 725 --> 712 matches\n",
      "filter_FundamentalMatrix: 405 matches --> 403 matches\n",
      "another_et_another_et007.png-another_et_another_et010.png: 405 --> 403 matches\n",
      "filter_FundamentalMatrix: 1322 matches --> 1309 matches\n",
      "another_et_another_et008.png-another_et_another_et009.png: 1322 --> 1309 matches\n",
      "filter_FundamentalMatrix: 595 matches --> 532 matches\n",
      "another_et_another_et008.png-another_et_another_et010.png: 595 --> 532 matches\n",
      "filter_FundamentalMatrix: 862 matches --> 841 matches\n",
      "another_et_another_et009.png-another_et_another_et010.png: 862 --> 841 matches\n",
      "filter_FundamentalMatrix: 4631 matches --> 4630 matches\n",
      "et_et000.png-et_et001.png: 4631 --> 4630 matches\n",
      "filter_FundamentalMatrix: 3457 matches --> 3453 matches\n",
      "et_et000.png-et_et002.png: 3457 --> 3453 matches\n",
      "filter_FundamentalMatrix: 5810 matches --> 5809 matches\n",
      "et_et000.png-et_et003.png: 5810 --> 5809 matches\n",
      "filter_FundamentalMatrix: 3347 matches --> 3337 matches\n",
      "et_et000.png-et_et004.png: 3347 --> 3337 matches\n",
      "filter_FundamentalMatrix: 137 matches --> 134 matches\n",
      "et_et000.png-et_et005.png: 137 --> 134 matches\n",
      "filter_FundamentalMatrix: 135 matches --> 126 matches\n",
      "et_et000.png-et_et006.png: 135 --> 126 matches\n",
      "filter_FundamentalMatrix: 101 matches --> 78 matches\n",
      "et_et000.png-et_et007.png: 101 --> 78 matches\n",
      "filter_FundamentalMatrix: 4989 matches --> 4988 matches\n",
      "et_et001.png-et_et002.png: 4989 --> 4988 matches\n",
      "filter_FundamentalMatrix: 3588 matches --> 3585 matches\n",
      "et_et001.png-et_et003.png: 3588 --> 3585 matches\n",
      "filter_FundamentalMatrix: 3875 matches --> 3860 matches\n",
      "et_et001.png-et_et004.png: 3875 --> 3860 matches\n",
      "filter_FundamentalMatrix: 227 matches --> 224 matches\n",
      "et_et001.png-et_et005.png: 227 --> 224 matches\n",
      "filter_FundamentalMatrix: 323 matches --> 307 matches\n",
      "et_et001.png-et_et006.png: 323 --> 307 matches\n",
      "filter_FundamentalMatrix: 229 matches --> 222 matches\n",
      "et_et001.png-et_et007.png: 229 --> 222 matches\n",
      "filter_FundamentalMatrix: 2761 matches --> 2761 matches\n",
      "et_et002.png-et_et003.png: 2761 --> 2761 matches\n",
      "filter_FundamentalMatrix: 3236 matches --> 3220 matches\n",
      "et_et002.png-et_et004.png: 3236 --> 3220 matches\n",
      "filter_FundamentalMatrix: 630 matches --> 627 matches\n",
      "et_et002.png-et_et005.png: 630 --> 627 matches\n",
      "filter_FundamentalMatrix: 1183 matches --> 1171 matches\n",
      "et_et002.png-et_et006.png: 1183 --> 1171 matches\n",
      "filter_FundamentalMatrix: 580 matches --> 568 matches\n",
      "et_et002.png-et_et007.png: 580 --> 568 matches\n",
      "filter_FundamentalMatrix: 204 matches --> 197 matches\n",
      "et_et002.png-et_et008.png: 204 --> 197 matches\n",
      "filter_FundamentalMatrix: 2701 matches --> 2686 matches\n",
      "et_et003.png-et_et004.png: 2701 --> 2686 matches\n",
      "filter_FundamentalMatrix: 392 matches --> 385 matches\n",
      "et_et004.png-et_et005.png: 392 --> 385 matches\n",
      "filter_FundamentalMatrix: 638 matches --> 606 matches\n",
      "et_et004.png-et_et006.png: 638 --> 606 matches\n",
      "filter_FundamentalMatrix: 189 matches --> 180 matches\n",
      "et_et004.png-et_et007.png: 189 --> 180 matches\n",
      "filter_FundamentalMatrix: 141 matches --> 132 matches\n",
      "et_et004.png-et_et008.png: 141 --> 132 matches\n",
      "filter_FundamentalMatrix: 4611 matches --> 4606 matches\n",
      "et_et005.png-et_et006.png: 4611 --> 4606 matches\n",
      "filter_FundamentalMatrix: 4538 matches --> 4530 matches\n",
      "et_et005.png-et_et007.png: 4538 --> 4530 matches\n",
      "filter_FundamentalMatrix: 3424 matches --> 3418 matches\n",
      "et_et005.png-et_et008.png: 3424 --> 3418 matches\n",
      "filter_FundamentalMatrix: 5075 matches --> 5075 matches\n",
      "et_et006.png-et_et007.png: 5075 --> 5075 matches\n",
      "filter_FundamentalMatrix: 2860 matches --> 2844 matches\n",
      "et_et006.png-et_et008.png: 2860 --> 2844 matches\n",
      "filter_FundamentalMatrix: 3408 matches --> 3403 matches\n",
      "et_et007.png-et_et008.png: 3408 --> 3403 matches\n",
      "Ensembled pairs : 63 pairs\n",
      "Local feature extracting and matching. Done in 40.6192 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19/19 [00:00<00:00, 68.00it/s]\n",
      " 46%|     | 63/136 [00:00<00:00, 3555.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original results\n",
      "{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=9498, num_observations=43011), 1: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=4748, num_observations=24176)}\n",
      "{}\n",
      "map 0:Image(image_id=11, camera_id=11, name=\"et_et000.png\", triangulated=5486/7400)\n",
      "map 0:Image(image_id=12, camera_id=12, name=\"et_et001.png\", triangulated=5448/7243)\n",
      "map 0:Image(image_id=13, camera_id=13, name=\"et_et002.png\", triangulated=4821/6198)\n",
      "map 0:Image(image_id=14, camera_id=14, name=\"et_et003.png\", triangulated=4550/6143)\n",
      "map 0:Image(image_id=15, camera_id=15, name=\"et_et004.png\", triangulated=4359/5737)\n",
      "map 0:Image(image_id=16, camera_id=16, name=\"et_et005.png\", triangulated=4860/6101)\n",
      "map 0:Image(image_id=17, camera_id=17, name=\"et_et006.png\", triangulated=5065/6187)\n",
      "map 0:Image(image_id=18, camera_id=18, name=\"et_et007.png\", triangulated=4907/6177)\n",
      "map 0:Image(image_id=19, camera_id=19, name=\"et_et008.png\", triangulated=3515/4482)\n",
      "map 1:Image(image_id=1, camera_id=1, name=\"another_et_another_et001.png\", triangulated=3499/5018)\n",
      "map 1:Image(image_id=2, camera_id=2, name=\"another_et_another_et002.png\", triangulated=3478/4830)\n",
      "map 1:Image(image_id=3, camera_id=3, name=\"another_et_another_et003.png\", triangulated=2528/3223)\n",
      "map 1:Image(image_id=4, camera_id=4, name=\"another_et_another_et004.png\", triangulated=3315/4290)\n",
      "map 1:Image(image_id=5, camera_id=5, name=\"another_et_another_et005.png\", triangulated=3417/4155)\n",
      "map 1:Image(image_id=6, camera_id=6, name=\"another_et_another_et006.png\", triangulated=2506/3385)\n",
      "map 1:Image(image_id=7, camera_id=7, name=\"another_et_another_et007.png\", triangulated=2179/3020)\n",
      "map 1:Image(image_id=8, camera_id=8, name=\"another_et_another_et008.png\", triangulated=1591/2757)\n",
      "map 1:Image(image_id=9, camera_id=9, name=\"another_et_another_et009.png\", triangulated=944/1868)\n",
      "map 1:Image(image_id=10, camera_id=10, name=\"another_et_another_et010.png\", triangulated=719/1377)\n",
      "Dataset  ETs -> Registered 19 / 22 images with 2 clusters\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n",
      "rotation_detection for 200 images : 0.0001 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/amy_gardens/peach_0000.png\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n",
      "rotation_detection for 163 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/fbk_vineyard/vineyard_split_1_frame_0900.png\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n",
      "rotation_detection for 54 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_haiper/bike_image_004.png\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_heritage\": 209 images\n",
      "rotation_detection for 209 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/209 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_heritage/cyprus_dsc_6480.png\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_theather_imc2024_church\": 76 images\n",
      "rotation_detection for 76 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_theather_imc2024_church/church_00004.png\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_dioscuri_baalshamin\": 138 images\n",
      "rotation_detection for 138 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_dioscuri_baalshamin/baalshamin_182z.png\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_lizard_pond\": 214 images\n",
      "rotation_detection for 214 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/214 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_lizard_pond/lizard_00003.png\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_brandenburg_british_buckingham\": 225 images\n",
      "rotation_detection for 225 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_brandenburg_british_buckingham/brandenburg_gate_01069771_8567470929.png\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_piazzasanmarco_grandplace\": 168 images\n",
      "rotation_detection for 168 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/168 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_piazzasanmarco_grandplace/grand_place_brussels_00460368_4162644685.png\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_sacrecoeur_trevi_tajmahal\": 225 images\n",
      "rotation_detection for 225 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_sacrecoeur_trevi_tajmahal/sacre_coeur_02928139_3448003521.png\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_stpeters_stpauls\": 200 images\n",
      "rotation_detection for 200 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_stpeters_stpauls/st_pauls_cathedral_00162897_2573777698.png\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n",
      "rotation_detection for 51 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:10<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "60%:  0.2868\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 564. Done in 10.6183 sec\n",
      "Generated 564 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 564. Done in 10.9477 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1430, 2]), descs.shape=torch.Size([1430, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1514, 2]), descs.shape=torch.Size([1514, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([851, 2]), descs.shape=torch.Size([851, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([597, 2]), descs.shape=torch.Size([597, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1017, 2]), descs.shape=torch.Size([1017, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1702, 2]), descs.shape=torch.Size([1702, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([392, 2]), descs.shape=torch.Size([392, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1327, 2]), descs.shape=torch.Size([1327, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1172, 2]), descs.shape=torch.Size([1172, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1025, 2]), descs.shape=torch.Size([1025, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([544, 2]), descs.shape=torch.Size([544, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([238, 2]), descs.shape=torch.Size([238, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1467, 2]), descs.shape=torch.Size([1467, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([806, 2]), descs.shape=torch.Size([806, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1209, 2]), descs.shape=torch.Size([1209, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([885, 2]), descs.shape=torch.Size([885, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2303, 2]), descs.shape=torch.Size([2303, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2208, 2]), descs.shape=torch.Size([2208, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2183, 2]), descs.shape=torch.Size([2183, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1486, 2]), descs.shape=torch.Size([1486, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1257, 2]), descs.shape=torch.Size([1257, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1933, 2]), descs.shape=torch.Size([1933, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([976, 2]), descs.shape=torch.Size([976, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([970, 2]), descs.shape=torch.Size([970, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3646, 2]), descs.shape=torch.Size([3646, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([632, 2]), descs.shape=torch.Size([632, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([440, 2]), descs.shape=torch.Size([440, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1128, 2]), descs.shape=torch.Size([1128, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([4333, 2]), descs.shape=torch.Size([4333, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1541, 2]), descs.shape=torch.Size([1541, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1054, 2]), descs.shape=torch.Size([1054, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1133, 2]), descs.shape=torch.Size([1133, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2563, 2]), descs.shape=torch.Size([2563, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2114, 2]), descs.shape=torch.Size([2114, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2995, 2]), descs.shape=torch.Size([2995, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2111, 2]), descs.shape=torch.Size([2111, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2149, 2]), descs.shape=torch.Size([2149, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2257, 2]), descs.shape=torch.Size([2257, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3735, 2]), descs.shape=torch.Size([3735, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1181, 2]), descs.shape=torch.Size([1181, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1078, 2]), descs.shape=torch.Size([1078, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2382, 2]), descs.shape=torch.Size([2382, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2769, 2]), descs.shape=torch.Size([2769, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3269, 2]), descs.shape=torch.Size([3269, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1897, 2]), descs.shape=torch.Size([1897, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1729, 2]), descs.shape=torch.Size([1729, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1319, 2]), descs.shape=torch.Size([1319, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2404, 2]), descs.shape=torch.Size([2404, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3395, 2]), descs.shape=torch.Size([3395, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1450, 2]), descs.shape=torch.Size([1450, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2413, 2]), descs.shape=torch.Size([2413, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/564 [00:00<00:14, 38.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 163 matches @ 1th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 63/564 [00:01<00:13, 36.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 144 matches @ 2th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 76/564 [00:02<00:12, 38.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 141 matches @ 3th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 142/564 [00:03<00:11, 38.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453643106.png-stairs_split_1_1710453963274.png: 131 matches @ 4th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 155/564 [00:04<00:10, 38.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453668718.png: 141 matches @ 5th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 164/564 [00:04<00:11, 34.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 165 matches @ 6th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 201 matches @ 7th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 190/564 [00:05<00:10, 36.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 145 matches @ 8th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 306/564 [00:08<00:10, 23.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 113 matches @ 9th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 331/564 [00:09<00:09, 25.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 266 matches @ 10th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 338/564 [00:10<00:08, 27.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 246 matches @ 11th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 345/564 [00:10<00:07, 29.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 164 matches @ 12th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 366/564 [00:11<00:05, 33.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 115 matches @ 13th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 395/564 [00:11<00:04, 36.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 135 matches @ 14th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 438/564 [00:13<00:05, 21.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 654 matches @ 15th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 474/564 [00:15<00:04, 22.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 431 matches @ 16th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 740 matches @ 17th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 480/564 [00:15<00:03, 21.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 116 matches @ 18th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 110 matches @ 19th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 483/564 [00:15<00:03, 21.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 628 matches @ 20th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 372 matches @ 21th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 490/564 [00:16<00:04, 18.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 661 matches @ 22th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 502/564 [00:16<00:02, 22.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 263 matches @ 23th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 526/564 [00:17<00:02, 18.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 430 matches @ 24th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 171 matches @ 25th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 543/564 [00:18<00:00, 24.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 541 matches @ 26th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 257 matches @ 27th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 549/564 [00:18<00:00, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 146 matches @ 28th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 174 matches @ 29th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 564/564 [00:19<00:00, 29.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  24.1224 sec (aliked+LightGlue)\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([177, 2]), descs.shape=torch.Size([177, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([163, 2]), descs.shape=torch.Size([163, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([117, 2]), descs.shape=torch.Size([117, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([70, 2]), descs.shape=torch.Size([70, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([84, 2]), descs.shape=torch.Size([84, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([94, 2]), descs.shape=torch.Size([94, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([87, 2]), descs.shape=torch.Size([87, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([193, 2]), descs.shape=torch.Size([193, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([122, 2]), descs.shape=torch.Size([122, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([129, 2]), descs.shape=torch.Size([129, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([68, 2]), descs.shape=torch.Size([68, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([4, 2]), descs.shape=torch.Size([4, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([69, 2]), descs.shape=torch.Size([69, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([49, 2]), descs.shape=torch.Size([49, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([122, 2]), descs.shape=torch.Size([122, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([157, 2]), descs.shape=torch.Size([157, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([247, 2]), descs.shape=torch.Size([247, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([136, 2]), descs.shape=torch.Size([136, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([296, 2]), descs.shape=torch.Size([296, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([164, 2]), descs.shape=torch.Size([164, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([208, 2]), descs.shape=torch.Size([208, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([218, 2]), descs.shape=torch.Size([218, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([65, 2]), descs.shape=torch.Size([65, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([97, 2]), descs.shape=torch.Size([97, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([176, 2]), descs.shape=torch.Size([176, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([135, 2]), descs.shape=torch.Size([135, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([55, 2]), descs.shape=torch.Size([55, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([102, 2]), descs.shape=torch.Size([102, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([163, 2]), descs.shape=torch.Size([163, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([191, 2]), descs.shape=torch.Size([191, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([181, 2]), descs.shape=torch.Size([181, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([300, 2]), descs.shape=torch.Size([300, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([255, 2]), descs.shape=torch.Size([255, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([103, 2]), descs.shape=torch.Size([103, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([143, 2]), descs.shape=torch.Size([143, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([206, 2]), descs.shape=torch.Size([206, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([318, 2]), descs.shape=torch.Size([318, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([263, 2]), descs.shape=torch.Size([263, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([137, 2]), descs.shape=torch.Size([137, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([116, 2]), descs.shape=torch.Size([116, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([204, 2]), descs.shape=torch.Size([204, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([300, 2]), descs.shape=torch.Size([300, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([130, 2]), descs.shape=torch.Size([130, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([120, 2]), descs.shape=torch.Size([120, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([170, 2]), descs.shape=torch.Size([170, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([179, 2]), descs.shape=torch.Size([179, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([152, 2]), descs.shape=torch.Size([152, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([140, 2]), descs.shape=torch.Size([140, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([119, 2]), descs.shape=torch.Size([119, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([158, 2]), descs.shape=torch.Size([158, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([218, 2]), descs.shape=torch.Size([218, 256])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 330/564 [00:08<00:05, 41.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 54 matches @ 1th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 340/564 [00:08<00:05, 41.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 69 matches @ 2th pair(superpoint+lightglue)\n",
      "superpoint> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 60 matches @ 3th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 395/564 [00:09<00:04, 41.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 51 matches @ 4th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 420/564 [00:10<00:03, 41.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 53 matches @ 5th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 450/564 [00:11<00:02, 41.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 50 matches @ 6th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 480/564 [00:11<00:02, 40.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 51 matches @ 7th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 490/564 [00:12<00:01, 40.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 58 matches @ 8th pair(superpoint+lightglue)\n",
      "superpoint> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 64 matches @ 9th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 505/564 [00:12<00:01, 40.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 75 matches @ 10th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 564/564 [00:13<00:00, 40.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  18.4566 sec (superpoint+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5898, 2]), descs.shape=torch.Size([5898, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7973, 2]), descs.shape=torch.Size([7973, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7425, 2]), descs.shape=torch.Size([7425, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8111, 2]), descs.shape=torch.Size([8111, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8156, 2]), descs.shape=torch.Size([8156, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7994, 2]), descs.shape=torch.Size([7994, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7698, 2]), descs.shape=torch.Size([7698, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/564 [00:00<02:15,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 388 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 35/564 [00:10<02:13,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453990286.png: 890 matches @ 2th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 38/564 [00:10<02:12,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 182 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 43/564 [00:12<02:11,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 109 matches @ 4th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 54/564 [00:15<02:44,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453985484.png: 301 matches @ 5th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 55/564 [00:15<02:44,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 905 matches @ 6th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 71/564 [00:21<02:42,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 831 matches @ 7th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 78/564 [00:23<02:40,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 437 matches @ 8th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 97/564 [00:29<02:34,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453620694.png-stairs_split_1_1710453626698.png: 247 matches @ 9th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 154/564 [00:48<02:12,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453693529.png: 114 matches @ 10th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 159/564 [00:49<02:09,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 176 matches @ 11th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 160/564 [00:50<02:09,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453947066.png: 440 matches @ 12th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 161/564 [00:50<02:09,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 291 matches @ 13th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 165/564 [00:51<02:08,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 232 matches @ 14th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 166/564 [00:52<02:08,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 126 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 170/564 [00:53<02:06,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 228 matches @ 16th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 179/564 [00:56<02:02,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453871430.png: 186 matches @ 17th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 185/564 [00:58<02:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 378 matches @ 18th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 229/564 [01:12<01:46,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_1_1710453930259.png: 492 matches @ 19th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 232/564 [01:13<01:45,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453739354.png: 148 matches @ 20th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 236/564 [01:14<01:45,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453759963.png: 195 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 238/564 [01:15<01:44,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 146 matches @ 22th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 242/564 [01:16<01:41,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 713 matches @ 23th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 243/564 [01:16<01:41,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453678922.png: 219 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 246/564 [01:17<01:39,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453704934.png: 191 matches @ 25th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 304/564 [01:36<01:23,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 265 matches @ 26th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 311/564 [01:38<01:21,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453759963.png: 224 matches @ 27th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 317/564 [01:40<01:18,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453871430.png: 125 matches @ 28th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 325/564 [01:42<01:15,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 1474 matches @ 29th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 334/564 [01:45<01:14,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 563 matches @ 30th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 341/564 [01:48<01:12,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 518 matches @ 31th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 354/564 [01:52<01:02,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 141 matches @ 32th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 434/564 [02:17<00:41,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 485 matches @ 33th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 437/564 [02:18<00:40,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 1044 matches @ 34th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 438/564 [02:18<00:40,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 233 matches @ 35th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 440/564 [02:19<00:39,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453728949.png: 150 matches @ 36th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 443/564 [02:20<00:38,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 108 matches @ 37th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 470/564 [02:29<00:30,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 187 matches @ 38th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 472/564 [02:29<00:29,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 1574 matches @ 39th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 473/564 [02:30<00:29,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 2431 matches @ 40th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 476/564 [02:31<00:28,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 445 matches @ 41th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 477/564 [02:31<00:27,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 362 matches @ 42th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 478/564 [02:31<00:27,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 128 matches @ 43th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 481/564 [02:32<00:26,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 237 matches @ 44th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 482/564 [02:33<00:26,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 2986 matches @ 45th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 485/564 [02:34<00:25,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 768 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 487/564 [02:34<00:24,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 862 matches @ 47th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 490/564 [02:35<00:23,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 335 matches @ 48th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 499/564 [02:38<00:20,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 838 matches @ 49th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 504/564 [02:40<00:19,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453862225.png: 146 matches @ 50th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 524/564 [02:46<00:12,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 528 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 527/564 [02:47<00:11,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 378 matches @ 52th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 542/564 [02:52<00:07,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 310 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 545/564 [02:53<00:06,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 1008 matches @ 54th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 546/564 [02:53<00:05,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 1564 matches @ 55th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 547/564 [02:53<00:05,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 302 matches @ 56th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 549/564 [02:54<00:04,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 594 matches @ 57th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 550/564 [02:54<00:04,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453871430.png: 232 matches @ 58th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 551/564 [02:55<00:04,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453793579.png: 251 matches @ 59th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 564/564 [02:59<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453805788.png-stairs_split_2_1710453871430.png: 139 matches @ 60th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  193.0861 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='564' class='' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [564/564 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_FundamentalMatrix: 551 matches --> 492 matches\n",
      "stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 551 --> 492 matches\n",
      "filter_FundamentalMatrix: 890 matches --> 788 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453990286.png: 890 --> 788 matches\n",
      "filter_FundamentalMatrix: 182 matches --> 156 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 182 --> 156 matches\n",
      "filter_FundamentalMatrix: 109 matches --> 98 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 109 --> 98 matches\n",
      "filter_FundamentalMatrix: 301 matches --> 285 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453985484.png: 301 --> 285 matches\n",
      "filter_FundamentalMatrix: 1049 matches --> 965 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 1049 --> 965 matches\n",
      "filter_FundamentalMatrix: 972 matches --> 921 matches\n",
      "stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 972 --> 921 matches\n",
      "filter_FundamentalMatrix: 437 matches --> 369 matches\n",
      "stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 437 --> 369 matches\n",
      "filter_FundamentalMatrix: 247 matches --> 236 matches\n",
      "stairs_split_1_1710453620694.png-stairs_split_1_1710453626698.png: 247 --> 236 matches\n",
      "filter_FundamentalMatrix: 131 matches --> 128 matches\n",
      "stairs_split_1_1710453643106.png-stairs_split_1_1710453963274.png: 131 --> 128 matches\n",
      "filter_FundamentalMatrix: 141 matches --> 133 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453668718.png: 141 --> 133 matches\n",
      "filter_FundamentalMatrix: 114 matches --> 93 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453693529.png: 114 --> 93 matches\n",
      "filter_FundamentalMatrix: 341 matches --> 226 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 341 --> 226 matches\n",
      "filter_FundamentalMatrix: 440 matches --> 402 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453947066.png: 440 --> 402 matches\n",
      "filter_FundamentalMatrix: 492 matches --> 435 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 492 --> 435 matches\n",
      "filter_FundamentalMatrix: 232 matches --> 189 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 232 --> 189 matches\n",
      "filter_FundamentalMatrix: 126 matches --> 106 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 126 --> 106 matches\n",
      "filter_FundamentalMatrix: 228 matches --> 190 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 228 --> 190 matches\n",
      "filter_FundamentalMatrix: 186 matches --> 150 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453871430.png: 186 --> 150 matches\n",
      "filter_FundamentalMatrix: 523 matches --> 439 matches\n",
      "stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 523 --> 439 matches\n",
      "filter_FundamentalMatrix: 492 matches --> 352 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_1_1710453930259.png: 492 --> 352 matches\n",
      "filter_FundamentalMatrix: 148 matches --> 135 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453739354.png: 148 --> 135 matches\n",
      "filter_FundamentalMatrix: 195 matches --> 152 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453759963.png: 195 --> 152 matches\n",
      "filter_FundamentalMatrix: 146 matches --> 117 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 146 --> 117 matches\n",
      "filter_FundamentalMatrix: 713 matches --> 651 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 713 --> 651 matches\n",
      "filter_FundamentalMatrix: 219 matches --> 212 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453678922.png: 219 --> 212 matches\n",
      "filter_FundamentalMatrix: 191 matches --> 162 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453704934.png: 191 --> 162 matches\n",
      "filter_FundamentalMatrix: 378 matches --> 231 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 378 --> 231 matches\n",
      "filter_FundamentalMatrix: 224 matches --> 206 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453759963.png: 224 --> 206 matches\n",
      "filter_FundamentalMatrix: 125 matches --> 107 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453871430.png: 125 --> 107 matches\n",
      "filter_FundamentalMatrix: 1794 matches --> 1625 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 1794 --> 1625 matches\n",
      "filter_FundamentalMatrix: 878 matches --> 757 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 878 --> 757 matches\n",
      "filter_FundamentalMatrix: 742 matches --> 605 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 742 --> 605 matches\n",
      "filter_FundamentalMatrix: 141 matches --> 99 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 141 --> 99 matches\n",
      "filter_FundamentalMatrix: 115 matches --> 63 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 115 --> 63 matches\n",
      "filter_FundamentalMatrix: 186 matches --> 140 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 186 --> 140 matches\n",
      "filter_FundamentalMatrix: 53 matches --> 36 matches\n",
      "stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 53 --> 36 matches\n",
      "filter_FundamentalMatrix: 485 matches --> 420 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 485 --> 420 matches\n",
      "filter_FundamentalMatrix: 1698 matches --> 1406 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 1698 --> 1406 matches\n",
      "filter_FundamentalMatrix: 233 matches --> 197 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 233 --> 197 matches\n",
      "filter_FundamentalMatrix: 150 matches --> 138 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453728949.png: 150 --> 138 matches\n",
      "filter_FundamentalMatrix: 158 matches --> 114 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 158 --> 114 matches\n",
      "filter_FundamentalMatrix: 187 matches --> 174 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 187 --> 174 matches\n",
      "filter_FundamentalMatrix: 2056 matches --> 1930 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 2056 --> 1930 matches\n",
      "filter_FundamentalMatrix: 3171 matches --> 2976 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 3171 --> 2976 matches\n",
      "filter_FundamentalMatrix: 445 matches --> 414 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 445 --> 414 matches\n",
      "filter_FundamentalMatrix: 478 matches --> 385 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 478 --> 385 matches\n",
      "filter_FundamentalMatrix: 238 matches --> 146 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 238 --> 146 matches\n",
      "filter_FundamentalMatrix: 237 matches --> 164 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 237 --> 164 matches\n",
      "filter_FundamentalMatrix: 3614 matches --> 3426 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 3614 --> 3426 matches\n",
      "filter_FundamentalMatrix: 1198 matches --> 873 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 1198 --> 873 matches\n",
      "filter_FundamentalMatrix: 1587 matches --> 1454 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 1587 --> 1454 matches\n",
      "filter_FundamentalMatrix: 335 matches --> 219 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 335 --> 219 matches\n",
      "filter_FundamentalMatrix: 1176 matches --> 1060 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 1176 --> 1060 matches\n",
      "filter_FundamentalMatrix: 146 matches --> 130 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453862225.png: 146 --> 130 matches\n",
      "filter_FundamentalMatrix: 958 matches --> 581 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 958 --> 581 matches\n",
      "filter_FundamentalMatrix: 549 matches --> 384 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 549 --> 384 matches\n",
      "filter_FundamentalMatrix: 851 matches --> 723 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 851 --> 723 matches\n",
      "filter_FundamentalMatrix: 1265 matches --> 1212 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 1265 --> 1212 matches\n",
      "filter_FundamentalMatrix: 1710 matches --> 1442 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 1710 --> 1442 matches\n",
      "filter_FundamentalMatrix: 302 matches --> 228 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 302 --> 228 matches\n",
      "filter_FundamentalMatrix: 768 matches --> 638 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 768 --> 638 matches\n",
      "filter_FundamentalMatrix: 232 matches --> 200 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453871430.png: 232 --> 200 matches\n",
      "filter_FundamentalMatrix: 251 matches --> 225 matches\n",
      "stairs_split_2_1710453790978.png-stairs_split_2_1710453793579.png: 251 --> 225 matches\n",
      "filter_FundamentalMatrix: 139 matches --> 95 matches\n",
      "stairs_split_2_1710453805788.png-stairs_split_2_1710453871430.png: 139 --> 95 matches\n",
      "Ensembled pairs : 65 pairs\n",
      "Local feature extracting and matching. Done in 238.8192 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:01<00:00, 22.05it/s]\n",
      " 17%|        | 65/378 [00:00<00:00, 3915.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original results\n",
      "{0: Reconstruction(num_reg_images=16, num_cameras=16, num_points3D=2861, num_observations=6515), 1: Reconstruction(num_reg_images=7, num_cameras=7, num_points3D=587, num_observations=1573)}\n",
      "{}\n",
      "map 0:Image(image_id=2, camera_id=2, name=\"stairs_split_1_1710453601885.png\", triangulated=50/1294)\n",
      "map 0:Image(image_id=9, camera_id=9, name=\"stairs_split_1_1710453651110.png\", triangulated=243/1548)\n",
      "map 0:Image(image_id=11, camera_id=11, name=\"stairs_split_1_1710453668718.png\", triangulated=129/1212)\n",
      "map 0:Image(image_id=14, camera_id=14, name=\"stairs_split_1_1710453689727.png\", triangulated=134/231)\n",
      "map 0:Image(image_id=15, camera_id=15, name=\"stairs_split_1_1710453693529.png\", triangulated=56/322)\n",
      "map 0:Image(image_id=18, camera_id=18, name=\"stairs_split_1_1710453930259.png\", triangulated=131/578)\n",
      "map 0:Image(image_id=20, camera_id=20, name=\"stairs_split_1_1710453955270.png\", triangulated=46/469)\n",
      "map 0:Image(image_id=24, camera_id=24, name=\"stairs_split_2_1710453720741.png\", triangulated=684/1900)\n",
      "map 0:Image(image_id=27, camera_id=27, name=\"stairs_split_2_1710453736752.png\", triangulated=261/2016)\n",
      "map 0:Image(image_id=28, camera_id=28, name=\"stairs_split_2_1710453739354.png\", triangulated=1537/5506)\n",
      "map 0:Image(image_id=29, camera_id=29, name=\"stairs_split_2_1710453740954.png\", triangulated=363/4653)\n",
      "map 0:Image(image_id=31, camera_id=31, name=\"stairs_split_2_1710453759963.png\", triangulated=73/1859)\n",
      "map 0:Image(image_id=33, camera_id=33, name=\"stairs_split_2_1710453783374.png\", triangulated=270/2912)\n",
      "map 0:Image(image_id=34, camera_id=34, name=\"stairs_split_2_1710453786375.png\", triangulated=810/3628)\n",
      "map 0:Image(image_id=38, camera_id=38, name=\"stairs_split_2_1710453805788.png\", triangulated=323/2184)\n",
      "map 0:Image(image_id=40, camera_id=40, name=\"stairs_split_2_1710453871430.png\", triangulated=1405/4845)\n",
      "map 1:Image(image_id=16, camera_id=16, name=\"stairs_split_1_1710453704934.png\", triangulated=247/2406)\n",
      "map 1:Image(image_id=17, camera_id=17, name=\"stairs_split_1_1710453901046.png\", triangulated=120/1695)\n",
      "map 1:Image(image_id=30, camera_id=30, name=\"stairs_split_2_1710453745156.png\", triangulated=359/1646)\n",
      "map 1:Image(image_id=34, camera_id=34, name=\"stairs_split_2_1710453786375.png\", triangulated=53/3628)\n",
      "map 1:Image(image_id=35, camera_id=35, name=\"stairs_split_2_1710453790978.png\", triangulated=514/1730)\n",
      "map 1:Image(image_id=36, camera_id=36, name=\"stairs_split_2_1710453793579.png\", triangulated=195/225)\n",
      "map 1:Image(image_id=39, camera_id=39, name=\"stairs_split_2_1710453862225.png\", triangulated=85/193)\n",
      "Dataset  stairs -> Registered 23 / 51 images with 2 clusters\n",
      "\n",
      "Results\n",
      "Dataset  ETs -> Registered 19 / 22 images with 2 clusters\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "Dataset  stairs -> Registered 23 / 51 images with 2 clusters\n",
      "\n",
      "Timings\n",
      "rotation_detection -> total=0.00 sec.\n",
      "global feature extraction -> total=0.00 sec.\n",
      "shortlisting -> total=27.00 sec.\n",
      "feature_detection -> total=0.00 sec.\n",
      "feature_matching -> total=272.08 sec.\n",
      "RANSAC -> total=2.61 sec.\n",
      "Reconstruction -> total=59.88 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    'rotation_detection':[],\n",
    "    \"global feature extraction\":[],\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "# Load DINOv2 model (for feature extraction, not global descriptor here)\n",
    "print(\"Loading DINOv2 model for patch feature extraction...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = dino_model.eval().to(device)\n",
    "print(\"DINOv2 model loaded.\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    # print (f\"Extracting on device {device}\")\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "        # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "        try:\n",
    "            # --- Pipeline Execution ---\n",
    "            \n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            if CONFIG.ROTATION_CORRECTION:\n",
    "                rots = exec_rotation_detection(images, device)\n",
    "            else:\n",
    "                rots = [ 0 for fname in images ]\n",
    "            # if CONFIG.ROTATION_CORRECTION:\n",
    "            #     rots = exec_rotation_detection(images, device)\n",
    "            # else:\n",
    "            #     rots = [ 0 for fname in images ]\n",
    "            # rots = [ 0 for fname in images ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print(f'rotation_detection for {len(images)} images : {t:.4f} sec')\n",
    "            # print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            # 1. Detect ALIKED features and combine with DINO patch features\n",
    "            t = time()\n",
    "            index_pairs = get_image_pairs_shortlist(\n",
    "                images,\n",
    "                sim_th = 0.3, # should be strict\n",
    "                min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "                max_pairs = 20,\n",
    "                exhaustive_if_less = 20,\n",
    "                device=device\n",
    "            )\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            # print(\"\\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\")\n",
    "            # detect_aliked_and_combine_with_dino(\n",
    "            #     img_fnames=images,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_features=4096,\n",
    "            #     resize_to=1024,\n",
    "            #     dino_processor=dino_processor,\n",
    "            #     dino_model=dino_model,\n",
    "            #     dino_patch_size=14, # Adjust based on your DINO model's patch size (e.g., 14 for DINOv2 base)\n",
    "            #     device=device\n",
    "            # )\n",
    "            # timings['global feature extraction'].append(time() - t)\n",
    "            # print (f'Gloabl feature extracting. Done in {time() - t:.4f} sec')\n",
    "            # gc.collect()\n",
    "            \n",
    "            # # 2. Get image pairs shortlist using VLAD global descriptors\n",
    "            # print(\"\\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\")\n",
    "            # # Adjust num_clusters_vlad as needed (e.g., 64, 128, 256)\n",
    "            # # Higher clusters mean higher dimensionality for global descriptor.\n",
    "            # index_pairs = get_image_pairs_shortlist_vlad(\n",
    "            #     fnames=images,\n",
    "            #     sim_th=0.5,\n",
    "            #     min_pairs=20,\n",
    "            #     exhaustive_if_less=20,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_clusters_vlad=128, # Example: 128 clusters for VLAD\n",
    "            #     device=device\n",
    "            # )\n",
    "            # index_pairs = get_img_pairs_exhaustive(images)\n",
    "            \n",
    "            print(f\"Generated {len(index_pairs)} image pairs using VLAD global descriptor.\")\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################    \n",
    "            t=time()\n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                images, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "            print (f'Local feature extracting and matching. Done in {time() - t:.4f} sec')\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            future = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                feature_dir, images_dir)\n",
    "            maps, local_timings = future.result()\n",
    "            #  timings\n",
    "            for k in local_timings:\n",
    "                timings[k].extend(local_timings[k])\n",
    "            # clear_output(wait=False)\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():  # cur_map: image_name  {'R': list, 't': list}\n",
    "                for image_name, pose in cur_map.items():\n",
    "                    idx = filename_to_index[image_name]\n",
    "                    pred = predictions[idx]\n",
    "                    pred.cluster_index = map_index\n",
    "                    pred.rotation = np.array(pose['R'])  # convert back to np.ndarray\n",
    "                    pred.translation = np.array(pose['t'])\n",
    "                    registered += 1\n",
    "            mapping_result_str = f\"Dataset  {dataset} -> Registered {registered} / {len(images)} images with {len(maps)} clusters\"\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # raise e\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "827bfa41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:09:12.359534Z",
     "iopub.status.busy": "2025-06-05T23:09:12.359289Z",
     "iopub.status.idle": "2025-06-05T23:09:12.539805Z",
     "shell.execute_reply": "2025-06-05T23:09:12.538734Z"
    },
    "papermill": {
     "duration": 0.255973,
     "end_time": "2025-06-05T23:09:12.541410",
     "exception": false,
     "start_time": "2025-06-05T23:09:12.285437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster1,another_et_another_et001.png,0.999849110;-0.004294366;0.016831984;0.003911450;0.999734294;0.022716565;-0.016925065;-0.022647300;0.999600241,-2.822629531;-1.953492576;2.695960766\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster1,another_et_another_et002.png,0.999996025;-0.001946227;0.002040216;0.001948468;0.999997500;-0.001096606;-0.002038077;0.001100577;0.999997317,-2.668250230;-1.173978771;1.165664985\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster1,another_et_another_et003.png,0.997754758;-0.041417366;0.052631207;0.044510346;0.997263911;-0.059021357;-0.050042694;0.061231473;0.996868314,-2.872644605;0.328614109;-0.582123420\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster1,another_et_another_et004.png,0.999187239;-0.011156197;0.038735007;0.005962984;0.991274051;0.131682188;-0.039866079;-0.131344186;0.990534906,-2.781170519;-1.441968480;-0.135439247\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster1,another_et_another_et005.png,0.995148459;0.002514176;0.098352544;-0.008981150;0.997820960;0.065365673;-0.097973889;-0.065931867;0.993002571,-3.306790942;-2.131013419;1.292412974\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster1,another_et_another_et006.png,0.921637030;0.195447622;-0.335239336;-0.220772549;0.974554376;-0.038771764;0.319131113;0.109745136;0.941334870,-0.600243927;-0.732722224;1.036921077\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster1,another_et_another_et007.png,0.784086500;0.266943514;-0.560311986;-0.312461191;0.949808001;0.015256650;0.536261471;0.163113217;0.828141119,1.094790154;-0.440921448;0.579779438\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster1,another_et_another_et008.png,0.572773935;0.314715263;-0.756891223;-0.395777851;0.914779329;0.080862056;0.717836968;0.253245103;0.648519086,2.788893832;-0.761681757;1.364800918\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster1,another_et_another_et009.png,0.323004946;0.349869527;-0.879351533;-0.492775469;0.855441498;0.159349240;0.807985236;0.381852272;0.448718955,4.446579384;-1.083986564;2.014216179\r\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                #  `rotation` is a list of lists, flatten it\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()  # flatten 3x3 list -> 9 elems\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                #  `translation` is a flat list\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "\n",
    "# Preview the output\n",
    "!head {submission_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c15922aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T23:09:12.692036Z",
     "iopub.status.busy": "2025-06-05T23:09:12.691782Z",
     "iopub.status.idle": "2025-06-05T23:09:12.696012Z",
     "shell.execute_reply": "2025-06-05T23:09:12.695326Z"
    },
    "papermill": {
     "duration": 0.079849,
     "end_time": "2025-06-05T23:09:12.697230",
     "exception": false,
     "start_time": "2025-06-05T23:09:12.617381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced54981",
   "metadata": {
    "papermill": {
     "duration": 0.074287,
     "end_time": "2025-06-05T23:09:12.845283",
     "exception": false,
     "start_time": "2025-06-05T23:09:12.770996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7505602,
     "sourceId": 11938492,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7542297,
     "sourceId": 11991336,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 421.96005,
   "end_time": "2025-06-05T23:09:15.881865",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T23:02:13.921815",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
