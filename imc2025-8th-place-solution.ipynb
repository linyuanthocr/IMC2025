{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776f9abc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007223,
     "end_time": "2025-06-11T16:38:10.260912",
     "exception": false,
     "start_time": "2025-06-11T16:38:10.253689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ee6fcd",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:10.274725Z",
     "iopub.status.busy": "2025-06-11T16:38:10.274498Z",
     "iopub.status.idle": "2025-06-11T16:38:20.507082Z",
     "shell.execute_reply": "2025-06-11T16:38:20.506031Z"
    },
    "papermill": {
     "duration": 10.240975,
     "end_time": "2025-06-11T16:38:20.508459",
     "exception": false,
     "start_time": "2025-06-11T16:38:10.267484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package colmap.\r\n",
      "(Reading database ... 127400 files and directories currently installed.)\r\n",
      "Preparing to unpack ./colmap_3.7-2_amd64.deb ...\r\n",
      "Unpacking colmap (3.7-2) ...\r\n",
      "Selecting previously unselected package libamd2:amd64.\r\n",
      "Preparing to unpack .../libamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libcamd2:amd64.\r\n",
      "Preparing to unpack .../libcamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libcamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libccolamd2:amd64.\r\n",
      "Preparing to unpack .../libccolamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libccolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libceres2.\r\n",
      "Preparing to unpack .../libceres2_2.0.0+dfsg1-5_amd64.deb ...\r\n",
      "Unpacking libceres2 (2.0.0+dfsg1-5) ...\r\n",
      "Selecting previously unselected package libcholmod3:amd64.\r\n",
      "Preparing to unpack .../libcholmod3_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libcholmod3:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libcolamd2:amd64.\r\n",
      "Preparing to unpack .../libcolamd2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libcxsparse3:amd64.\r\n",
      "Preparing to unpack .../libcxsparse3_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libcxsparse3:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libevdev2:amd64.\r\n",
      "Preparing to unpack .../libevdev2_1.12.1+dfsg-1_amd64.deb ...\r\n",
      "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\r\n",
      "Selecting previously unselected package libfreeimage3:amd64.\r\n",
      "Preparing to unpack .../libfreeimage3_3.18.0+ds2-6ubuntu5.1_amd64.deb ...\r\n",
      "Unpacking libfreeimage3:amd64 (3.18.0+ds2-6ubuntu5.1) ...\r\n",
      "Selecting previously unselected package libgflags2.2.\r\n",
      "Preparing to unpack .../libgflags2.2_2.2.2-2_amd64.deb ...\r\n",
      "Unpacking libgflags2.2 (2.2.2-2) ...\r\n",
      "Selecting previously unselected package libgoogle-glog0v5.\r\n",
      "Preparing to unpack .../libgoogle-glog0v5_0.5.0+really0.4.0-2_amd64.deb ...\r\n",
      "Unpacking libgoogle-glog0v5 (0.5.0+really0.4.0-2) ...\r\n",
      "Selecting previously unselected package libgudev-1.0-0:amd64.\r\n",
      "Preparing to unpack .../libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\r\n",
      "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\r\n",
      "Selecting previously unselected package libinput10:amd64.\r\n",
      "Preparing to unpack .../libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\r\n",
      "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\r\n",
      "Selecting previously unselected package libinput-bin.\r\n",
      "Preparing to unpack .../libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\r\n",
      "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\r\n",
      "Selecting previously unselected package libmd4c0:amd64.\r\n",
      "Preparing to unpack ./libmd4c0_0.4.8-1_amd64.deb ...\r\n",
      "Unpacking libmd4c0:amd64 (0.4.8-1) ...\r\n",
      "Selecting previously unselected package libmetis5:amd64.\r\n",
      "Preparing to unpack .../libmetis5_5.1.0.dfsg-7build2_amd64.deb ...\r\n",
      "Unpacking libmetis5:amd64 (5.1.0.dfsg-7build2) ...\r\n",
      "Selecting previously unselected package libmtdev1:amd64.\r\n",
      "Preparing to unpack .../libmtdev1_1.1.6-1build4_amd64.deb ...\r\n",
      "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\r\n",
      "Selecting previously unselected package libqt5core5a:amd64.\r\n",
      "Preparing to unpack .../libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libqt5dbus5:amd64.\r\n",
      "Preparing to unpack .../libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libqt5gui5:amd64.\r\n",
      "Preparing to unpack .../libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libqt5network5:amd64.\r\n",
      "Preparing to unpack .../libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libqt5svg5:amd64.\r\n",
      "Preparing to unpack .../libqt5svg5_5.15.3-1_amd64.deb ...\r\n",
      "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\r\n",
      "Selecting previously unselected package libqt5widgets5:amd64.\r\n",
      "Preparing to unpack .../libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libraw20:amd64.\r\n",
      "Preparing to unpack .../libraw20_0.20.2-2ubuntu2.22.04.1_amd64.deb ...\r\n",
      "Unpacking libraw20:amd64 (0.20.2-2ubuntu2.22.04.1) ...\r\n",
      "Selecting previously unselected package libspqr2:amd64.\r\n",
      "Preparing to unpack .../libspqr2_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libspqr2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libsuitesparseconfig5:amd64.\r\n",
      "Preparing to unpack .../libsuitesparseconfig5_1%3a5.10.1+dfsg-4build1_amd64.deb ...\r\n",
      "Unpacking libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Selecting previously unselected package libwacom9:amd64.\r\n",
      "Preparing to unpack ./libwacom9_2.2.0-1_amd64.deb ...\r\n",
      "Unpacking libwacom9:amd64 (2.2.0-1) ...\r\n",
      "Selecting previously unselected package libwacom-bin.\r\n",
      "Preparing to unpack .../libwacom-bin_2.2.0-1_amd64.deb ...\r\n",
      "Unpacking libwacom-bin (2.2.0-1) ...\r\n",
      "Selecting previously unselected package libwacom-common.\r\n",
      "Preparing to unpack .../libwacom-common_2.2.0-1_all.deb ...\r\n",
      "Unpacking libwacom-common (2.2.0-1) ...\r\n",
      "Selecting previously unselected package libxcb-icccm4:amd64.\r\n",
      "Preparing to unpack .../libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\r\n",
      "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\r\n",
      "Selecting previously unselected package libxcb-image0:amd64.\r\n",
      "Preparing to unpack .../libxcb-image0_0.4.0-2_amd64.deb ...\r\n",
      "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\r\n",
      "Selecting previously unselected package libxcb-keysyms1:amd64.\r\n",
      "Preparing to unpack .../libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\r\n",
      "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\r\n",
      "Selecting previously unselected package libxcb-render-util0:amd64.\r\n",
      "Preparing to unpack .../libxcb-render-util0_0.3.9-1build3_amd64.deb ...\r\n",
      "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\r\n",
      "Selecting previously unselected package libxcb-util1:amd64.\r\n",
      "Preparing to unpack .../libxcb-util1_0.4.0-1build2_amd64.deb ...\r\n",
      "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\r\n",
      "Selecting previously unselected package libxcb-xinerama0:amd64.\r\n",
      "Preparing to unpack .../libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\r\n",
      "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\r\n",
      "Selecting previously unselected package libxcb-xinput0:amd64.\r\n",
      "Preparing to unpack .../libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\r\n",
      "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\r\n",
      "Selecting previously unselected package libxcb-xkb1:amd64.\r\n",
      "Preparing to unpack .../libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\r\n",
      "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\r\n",
      "Selecting previously unselected package libxkbcommon-x11-0:amd64.\r\n",
      "Preparing to unpack .../libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\r\n",
      "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\r\n",
      "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\r\n",
      "Preparing to unpack .../qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Selecting previously unselected package qttranslations5-l10n.\r\n",
      "Preparing to unpack .../qttranslations5-l10n_5.15.3-1_all.deb ...\r\n",
      "Unpacking qttranslations5-l10n (5.15.3-1) ...\r\n",
      "Setting up libcxsparse3:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\r\n",
      "Setting up libgflags2.2 (2.2.2-2) ...\r\n",
      "Setting up libgoogle-glog0v5 (0.5.0+really0.4.0-2) ...\r\n",
      "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\r\n",
      "Setting up libmd4c0:amd64 (0.4.8-1) ...\r\n",
      "Setting up libmetis5:amd64 (5.1.0.dfsg-7build2) ...\r\n",
      "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\r\n",
      "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Setting up libraw20:amd64 (0.20.2-2ubuntu2.22.04.1) ...\r\n",
      "Setting up libsuitesparseconfig5:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libwacom-common (2.2.0-1) ...\r\n",
      "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\r\n",
      "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\r\n",
      "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\r\n",
      "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\r\n",
      "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\r\n",
      "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\r\n",
      "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\r\n",
      "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\r\n",
      "Setting up qttranslations5-l10n (5.15.3-1) ...\r\n",
      "Setting up libamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libcamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libccolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libcolamd2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libfreeimage3:amd64 (3.18.0+ds2-6ubuntu5.1) ...\r\n",
      "Setting up libwacom9:amd64 (2.2.0-1) ...\r\n",
      "Setting up libwacom-bin (2.2.0-1) ...\r\n",
      "Setting up libxcb-image0:amd64 (0.4.0-2) ...\r\n",
      "Setting up libcholmod3:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\r\n",
      "Setting up libspqr2:amd64 (1:5.10.1+dfsg-4build1) ...\r\n",
      "Setting up libceres2 (2.0.0+dfsg1-5) ...\r\n",
      "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\r\n",
      "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\r\n",
      "Setting up colmap (3.7-2) ...\r\n",
      "Setting up libqt5svg5:amd64 (5.15.3-1) ...\r\n",
      "Processing triggers for man-db (2.10.2-1) ...\r\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "COLMAP 3.7 -- Structure-from-Motion and Multi-View Stereo\r\n",
      "              (Commit Unknown on Unknown without CUDA)\r\n",
      "\r\n",
      "Usage:\r\n",
      "  colmap [command] [options]\r\n",
      "\r\n",
      "Documentation:\r\n",
      "  https://colmap.github.io/\r\n",
      "\r\n",
      "Example usage:\r\n",
      "  colmap help [ -h, --help ]\r\n",
      "  colmap gui\r\n",
      "  colmap gui -h [ --help ]\r\n",
      "  colmap automatic_reconstructor -h [ --help ]\r\n",
      "  colmap automatic_reconstructor --image_path IMAGES --workspace_path WORKSPACE\r\n",
      "  colmap feature_extractor --image_path IMAGES --database_path DATABASE\r\n",
      "  colmap exhaustive_matcher --database_path DATABASE\r\n",
      "  colmap mapper --image_path IMAGES --database_path DATABASE --output_path MODEL\r\n",
      "  ...\r\n",
      "\r\n",
      "Available commands:\r\n",
      "  help\r\n",
      "  gui\r\n",
      "  automatic_reconstructor\r\n",
      "  bundle_adjuster\r\n",
      "  color_extractor\r\n",
      "  database_cleaner\r\n",
      "  database_creator\r\n",
      "  database_merger\r\n",
      "  delaunay_mesher\r\n",
      "  exhaustive_matcher\r\n",
      "  feature_extractor\r\n",
      "  feature_importer\r\n",
      "  hierarchical_mapper\r\n",
      "  image_deleter\r\n",
      "  image_filterer\r\n",
      "  image_rectifier\r\n",
      "  image_registrator\r\n",
      "  image_undistorter\r\n",
      "  image_undistorter_standalone\r\n",
      "  mapper\r\n",
      "  matches_importer\r\n",
      "  model_aligner\r\n",
      "  model_analyzer\r\n",
      "  model_comparer\r\n",
      "  model_converter\r\n",
      "  model_cropper\r\n",
      "  model_merger\r\n",
      "  model_orientation_aligner\r\n",
      "  model_splitter\r\n",
      "  model_transformer\r\n",
      "  patch_match_stereo\r\n",
      "  point_filtering\r\n",
      "  point_triangulator\r\n",
      "  poisson_mesher\r\n",
      "  project_generator\r\n",
      "  rig_bundle_adjuster\r\n",
      "  sequential_matcher\r\n",
      "  spatial_matcher\r\n",
      "  stereo_fusion\r\n",
      "  transitive_matcher\r\n",
      "  vocab_tree_builder\r\n",
      "  vocab_tree_matcher\r\n",
      "  vocab_tree_retriever\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# install colmap(CPU)\n",
    "!cd /kaggle/input/pkg-colmap/colmap_offline && dpkg -i ./*.deb\n",
    "\n",
    "# test\n",
    "!colmap -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983bbeab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:20.531667Z",
     "iopub.status.busy": "2025-06-11T16:38:20.531428Z",
     "iopub.status.idle": "2025-06-11T16:38:25.909174Z",
     "shell.execute_reply": "2025-06-11T16:38:25.908211Z"
    },
    "papermill": {
     "duration": 5.390667,
     "end_time": "2025-06-11T16:38:25.910750",
     "exception": false,
     "start_time": "2025-06-11T16:38:20.520083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n32/1/aliked-n32.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16rot/1/aliked-n16rot.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a02608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:25.935699Z",
     "iopub.status.busy": "2025-06-11T16:38:25.935438Z",
     "iopub.status.idle": "2025-06-11T16:38:50.836979Z",
     "shell.execute_reply": "2025-06-11T16:38:50.836289Z"
    },
    "papermill": {
     "duration": 24.915082,
     "end_time": "2025-06-11T16:38:50.838440",
     "exception": false,
     "start_time": "2025-06-11T16:38:25.923358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os, glob\n",
    "from tqdm import tqdm\n",
    "from fastprogress import progress_bar\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import networkx as nx\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "#device = K.utils.get_cuda_device_if_available(0)\n",
    "#print(f'{device=}')\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76a74d",
   "metadata": {
    "papermill": {
     "duration": 0.01116,
     "end_time": "2025-06-11T16:38:50.861634",
     "exception": false,
     "start_time": "2025-06-11T16:38:50.850474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68518bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:50.885334Z",
     "iopub.status.busy": "2025-06-11T16:38:50.884749Z",
     "iopub.status.idle": "2025-06-11T16:38:50.889526Z",
     "shell.execute_reply": "2025-06-11T16:38:50.888938Z"
    },
    "papermill": {
     "duration": 0.017762,
     "end_time": "2025-06-11T16:38:50.890693",
     "exception": false,
     "start_time": "2025-06-11T16:38:50.872931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Param_AlikedLightGlue:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_matches = 15,\n",
    "        max_num_keypoints = 4096,\n",
    "        image_size = 1024,\n",
    "    ):\n",
    "        self.min_matches = min_matches\n",
    "        self.max_num_keypoints = max_num_keypoints\n",
    "        self.image_size = image_size\n",
    "        \n",
    "class CONFIG:\n",
    "    # Image pairs\n",
    "    sim_th = 2.0\n",
    "    min_pairs = 20\n",
    "    exhaustive_if_less = 20\n",
    "    GLOBAL_TOPK    = 150 # 50 # 近傍候補数\n",
    "    N_KEYPOINTS    = 2048          # 1画像あたり上限\n",
    "    RATIO_THR      = 1.0               # Lowe ratio\n",
    "    MATCH_THRESH   = 15                # good‑matches 閾値\n",
    "\n",
    "    # Image Matching\n",
    "    params_alikedlg = [\n",
    "        #Param_AlikedLightGlue( min_matches = 15, max_num_keypoints = 4096, image_size = 512 ),\n",
    "        #Param_AlikedLightGlue( min_matches = 15, max_num_keypoints = 4096, image_size = 1024 ),\n",
    "        #Param_AlikedLightGlue( min_matches = 15, max_num_keypoints = 8192, image_size = 1536 ),\n",
    "        Param_AlikedLightGlue( min_matches = 50, max_num_keypoints = 8192, image_size = 1536 ),\n",
    "        #Param_AlikedLightGlue( min_matches = 50, max_num_keypoints = 4096*3, image_size = 1536 ),\n",
    "    ]\n",
    "\n",
    "    roma_max_num_keypoints = 512\n",
    "    roma_image_size = 140\n",
    "    roma_batch_size = 20\n",
    "\n",
    "    # SfM\n",
    "    num_pallalel_sfm = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9674da80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:50.913887Z",
     "iopub.status.busy": "2025-06-11T16:38:50.913626Z",
     "iopub.status.idle": "2025-06-11T16:38:50.917350Z",
     "shell.execute_reply": "2025-06-11T16:38:50.916563Z"
    },
    "papermill": {
     "duration": 0.016578,
     "end_time": "2025-06-11T16:38:50.918539",
     "exception": false,
     "start_time": "2025-06-11T16:38:50.901961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device0=torch.device('cuda:0')\n",
    "device1=torch.device('cuda:1')\n",
    "\n",
    "def switch_gpu(device, device_index):\n",
    "    if device is None:\n",
    "        return device0, 0\n",
    "    elif device == device0:\n",
    "        return device1, 1\n",
    "    else:\n",
    "        return device0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d42af",
   "metadata": {
    "papermill": {
     "duration": 0.011044,
     "end_time": "2025-06-11T16:38:50.940706",
     "exception": false,
     "start_time": "2025-06-11T16:38:50.929662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Colmap Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ed1d76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:50.963903Z",
     "iopub.status.busy": "2025-06-11T16:38:50.963657Z",
     "iopub.status.idle": "2025-06-11T16:38:50.967292Z",
     "shell.execute_reply": "2025-06-11T16:38:50.966639Z"
    },
    "papermill": {
     "duration": 0.016593,
     "end_time": "2025-06-11T16:38:50.968490",
     "exception": false,
     "start_time": "2025-06-11T16:38:50.951897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-radial', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc377d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:50.991501Z",
     "iopub.status.busy": "2025-06-11T16:38:50.991307Z",
     "iopub.status.idle": "2025-06-11T16:38:50.998782Z",
     "shell.execute_reply": "2025-06-11T16:38:50.998169Z"
    },
    "papermill": {
     "duration": 0.020408,
     "end_time": "2025-06-11T16:38:50.999975",
     "exception": false,
     "start_time": "2025-06-11T16:38:50.979567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_keypoints_with_scene(db, scene, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        if not filename in scene:\n",
    "            continue\n",
    "        \n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches_with_scene(db, scene, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            if not key_1 in scene:\n",
    "                continue\n",
    "\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                if not key_2 in scene:\n",
    "                    continue\n",
    "                    \n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "def import_into_colmap_with_scene(img_dir, scene, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-radial', single_camera)\n",
    "    #fname_to_id = add_keypoints_with_scene(db, scene, feature_dir, img_dir, '', 'simple-radial', single_camera)\n",
    "    add_matches_with_scene(\n",
    "        db,\n",
    "        scene,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234630d",
   "metadata": {
    "papermill": {
     "duration": 0.011304,
     "end_time": "2025-06-11T16:38:51.022468",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.011164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pairs Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc59515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.045813Z",
     "iopub.status.busy": "2025-06-11T16:38:51.045582Z",
     "iopub.status.idle": "2025-06-11T16:38:51.058316Z",
     "shell.execute_reply": "2025-06-11T16:38:51.057684Z"
    },
    "papermill": {
     "duration": 0.02545,
     "end_time": "2025-06-11T16:38:51.059360",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.033910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, torch, urllib.request, tempfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# ---------- Global embeddings ---------- #\n",
    "def _extract_dino_embeddings(fnames, device = torch.device('cuda')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "def _build_topk_lists(global_feats, device):\n",
    "    \"\"\"\n",
    "    global_feats : (N, D)  L2‑normed\n",
    "    returns      : list of length‑≤(N‑1) neighbor indices for each image\n",
    "    \"\"\"\n",
    "    g   = global_feats.to(device)          # (N,D)\n",
    "    sim = g @ g.T                          # cosine similarity\n",
    "    sim.fill_diagonal_(-1)\n",
    "\n",
    "    N   = sim.size(0)\n",
    "    k   = min(CONFIG.GLOBAL_TOPK, N - 1)   # ★ ここが修正点\n",
    "    k   = max(k, 1)                        # 万一 N==2 でも k=1 を確保\n",
    "\n",
    "    topk = torch.topk(sim, k, dim=1).indices.cpu()\n",
    "    return [row.tolist() for row in topk]\n",
    "\n",
    "\n",
    "# ---------- ALIKED local features ---------- #\n",
    "def _get_aliked_model(device, num_features, resize_to=1024, detection_threshold=0.01):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(\n",
    "        model_name=\"aliked-n16\",\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=detection_threshold, \n",
    "        resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "    extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    return extractor\n",
    "    \n",
    "@torch.no_grad()\n",
    "def _extract_local_features(img_paths, model, device):\n",
    "    dtype = torch.float32\n",
    "    descs = []\n",
    "    for p in tqdm(img_paths, desc=\"ALIKED\"):\n",
    "        image0 = load_torch_image(p, device=device).to(dtype)\n",
    "        h, w = image0.shape[2], image0.shape[3]\n",
    "        feats0 = model.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "        d = feats0['descriptors'].reshape(-1, 128).detach()\n",
    "        descs.append(torch.nn.functional.normalize(d, dim=1).half().cpu())\n",
    "    return descs\n",
    "\n",
    "\n",
    "# ---------- good‑match カウント (Torch) ---------- #\n",
    "@torch.no_grad()\n",
    "def _mutual_nn_score(desc1, desc2, device):\n",
    "    if desc1.size(0) == 0 or desc2.size(0) == 0:\n",
    "        return 0\n",
    "    d12 = torch.cdist(desc1.to(device), desc2.to(device), p=2)\n",
    "    min_val, _ = torch.min(d12, 1)\n",
    "    n_matches = np.sum( min_val.cpu().numpy() < (CONFIG.RATIO_THR ** 2) )\n",
    "    return n_matches\n",
    "\n",
    "def _build_final_pairs(topk_lists, descs, device):\n",
    "    pairs = []\n",
    "    for i, nbrs in enumerate(tqdm(topk_lists, desc=\"Local verify\")):\n",
    "        for j in nbrs:\n",
    "            if i < j:\n",
    "                score = _mutual_nn_score(descs[i], descs[j], device)\n",
    "                if score >= CONFIG.MATCH_THRESH:\n",
    "                    pairs.append((i, j))\n",
    "                    #pairs.append((i, j, score))\n",
    "    pairs = sorted(list(set(pairs)))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "def get_image_pairs(fnames, device=torch.device(\"cuda\")):\n",
    "    \"\"\"\n",
    "    fnames: list[str] - 画像ファイルパスのリスト\n",
    "    device: torch.device - 計算に使用する GPU / CPU\n",
    "    returns: list[tuple[int,int]] - マッチングすべき画像 index ペア\n",
    "    \"\"\"\n",
    "    assert len(fnames) > 1, \"fnames must contain at least two images\"\n",
    "\n",
    "    # 1) DINO global features\n",
    "    global_feats = _extract_dino_embeddings(fnames, device)\n",
    "    topk_lists   = _build_topk_lists(global_feats, device)\n",
    "    cnt = 0\n",
    "    for topk_list in topk_lists:\n",
    "        cnt += len(topk_list)\n",
    "    print(cnt)\n",
    "\n",
    "    # 2) ALIKED local descriptors\n",
    "    aliked = _get_aliked_model(device, CONFIG.N_KEYPOINTS)\n",
    "    descs  = _extract_local_features(fnames, aliked, device)\n",
    "\n",
    "    # 3) local verification\n",
    "    pairs  = _build_final_pairs(topk_lists, descs, device)\n",
    "\n",
    "    del aliked\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return pairs\n",
    "# ========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e5f44",
   "metadata": {
    "papermill": {
     "duration": 0.011392,
     "end_time": "2025-06-11T16:38:51.082446",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.071054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6dfc30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.105805Z",
     "iopub.status.busy": "2025-06-11T16:38:51.105574Z",
     "iopub.status.idle": "2025-06-11T16:38:51.111983Z",
     "shell.execute_reply": "2025-06-11T16:38:51.111356Z"
    },
    "papermill": {
     "duration": 0.019386,
     "end_time": "2025-06-11T16:38:51.113211",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.093825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cuda')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cuda')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cuda')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    matching_list = get_image_pairs(fnames, device)\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0b8ee",
   "metadata": {
    "papermill": {
     "duration": 0.010963,
     "end_time": "2025-06-11T16:38:51.135414",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.124451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Aliked+LightGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6f839b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.158755Z",
     "iopub.status.busy": "2025-06-11T16:38:51.158547Z",
     "iopub.status.idle": "2025-06-11T16:38:51.172438Z",
     "shell.execute_reply": "2025-06-11T16:38:51.171869Z"
    },
    "papermill": {
     "duration": 0.027016,
     "end_time": "2025-06-11T16:38:51.173600",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.146584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T # np.array([rx, ry]).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T # np.array([rx, ry]).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T # np.array([rx, ry]).T\n",
    "\n",
    "def matching_aliked_lightglue_rot(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    rot,\n",
    "    saved_file,\n",
    "    feature_dir = '.featureout',\n",
    "    num_features = 4096,\n",
    "    resize_to = 1024,\n",
    "    device=torch.device('cuda'),\n",
    "    min_matches=15,\n",
    "    verbose=True,\n",
    "):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(\n",
    "        model_name=\"aliked-n16\",\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=0.01, #0.001, \n",
    "        resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "    print(\"aliked> image size =\", extractor.preprocess_conf[\"resize\"], \"-->\", resize_to )\n",
    "    extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for img_path in img_fnames:\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            rot_k = 0\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            dict_kpts_cuda[f\"{key}_{rot_k}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}_{rot_k}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"aliked_rot> rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "\n",
    "        if rot != 0:\n",
    "            with torch.inference_mode():\n",
    "                rot_k = rot\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                h, w = image0.shape[2], image0.shape[3]\n",
    "                image1 = torch.rot90(image0, rot, [2, 3])\n",
    "                feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "                kpts = convert_coord(kpts, w, h, rot_k)\n",
    "                dict_kpts_cuda[f\"{key}_{rot_k}\"] = kpts\n",
    "                dict_descs_cuda[f\"{key}_{rot_k}\"] = descs\n",
    "                if verbose:\n",
    "                    print(f\"aliked_rot> rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(\n",
    "        \"aliked\", {\n",
    "            \"width_confidence\": -1,\n",
    "            \"depth_confidence\": -1,\n",
    "            #\"filter_threshold\": 0.5,\n",
    "            \"mp\": True if 'cuda' in str(device) else False\n",
    "        }\n",
    "    ).eval().to(device).half()\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(saved_file, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            if (\"outliers\" in fname1) or ( \"outliers\" in fname2 ):\n",
    "                continue\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[f\"{key1}_0\"]\n",
    "            desc1 = dict_descs_cuda[f\"{key1}_0\"]\n",
    "                        \n",
    "            kp2 = dict_kpts_cuda[f\"{key2}_{rot}\"]\n",
    "            desc2 = dict_descs_cuda[f\"{key2}_{rot}\"]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1.half(),\n",
    "                                     desc2.half(),\n",
    "                                     KF.laf_from_center_scale_ori(kp1.half()[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2.half()[None]))\n",
    "            #print( f\"dists.shape={dists.shape}\")\n",
    "            #print( f\"type(dists)={type(dists)}\")\n",
    "            #print( f\"dists.mean()={dists.mean()}, {dists.max()}, {dists.min()}\")\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            confs = (1.0-dists).cpu().numpy().reshape(-1, 1).astype(np.float32)\n",
    "            n_matches = kp1.shape[0]\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                matches = np.concatenate([kp1, kp2, confs], axis=1)\n",
    "                #print(f\"@@@@ matches.shape = {matches.shape}\")\n",
    "                group.create_dataset(key2, data=matches)\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'aliked_rot> {key1}-{key2}@rot={rot}: {n_matches} matches @ {cnt_pairs}th pair')            \n",
    "            else:\n",
    "                if verbose:\n",
    "                    print (f'aliked_rot> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef456f9",
   "metadata": {
    "papermill": {
     "duration": 0.011055,
     "end_time": "2025-06-11T16:38:51.195761",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.184706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints merger & Matching Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7f73b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.218964Z",
     "iopub.status.busy": "2025-06-11T16:38:51.218709Z",
     "iopub.status.idle": "2025-06-11T16:38:51.263179Z",
     "shell.execute_reply": "2025-06-11T16:38:51.262604Z"
    },
    "papermill": {
     "duration": 0.057455,
     "end_time": "2025-06-11T16:38:51.264349",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.206894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if mkpts is not None:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        for data in list_mkpts:\n",
    "            print(f\"--> \", data.shape)\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    min_matches=15,\n",
    "):\n",
    "    print( files_keypoints )\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "    \n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < min_matches:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            \n",
    "            print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts[:, :4])\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def get_adjacent_edges_with_weights(G, node):\n",
    "    \"\"\"\n",
    "    指定されたノードに隣接するエッジとその重みをリストで返す関数\n",
    "    \n",
    "    :param G: networkxのグラフオブジェクト\n",
    "    :param node: 対象となるノード\n",
    "    :return: (隣接ノード, 重み)のタプルのリスト、重みの降順でソートされる\n",
    "    \"\"\"\n",
    "    adjacent_edges = [(neighbor, G[node][neighbor]['weight']) for neighbor in G.neighbors(node)]\n",
    "    return sorted(adjacent_edges, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def filter_mkpts(\n",
    "    file_mkpts, \n",
    "    save_file,\n",
    "    th_num_pairs,\n",
    "):\n",
    "    sleep(10)\n",
    "    G = nx.Graph()\n",
    "    pairs = []\n",
    "    with h5py.File(save_file, mode='w') as f_match, h5py.File(file_mkpts, \"r\") as fp:\n",
    "        for key1 in fp.keys():\n",
    "            for key2 in fp[key1].keys():\n",
    "                mkpts = fp[key1][key2]\n",
    "                G.add_edge(key1, key2, weight=mkpts.shape[0])\n",
    "                pairs.append([key1, key2, mkpts])\n",
    "\n",
    "        for (key1, key2, mkpts) in pairs:\n",
    "            neibors1 = get_adjacent_edges_with_weights(G, key1)\n",
    "            neibors2 = get_adjacent_edges_with_weights(G, key2)\n",
    "            if (key2 in [ val[0] for val in neibors1[:th_num_pairs] ]) or (key1 in [ val[0] for val in neibors2[:th_num_pairs] ]):\n",
    "                group  = f_match.require_group(key1)\n",
    "                group.create_dataset(key2, data=mkpts)\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    print(f\"files_keypoints = {files_keypoints}\")\n",
    "    save_file0 = f'{feature_dir}/merge_tmp0.h5'\n",
    "    !rm -rf {save_file0}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file0,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "\n",
    "    th_num_pairs = 6\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    filter_mkpts(\n",
    "        save_file0, \n",
    "        save_file,\n",
    "        th_num_pairs,\n",
    "    )\n",
    "    \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1160cc8",
   "metadata": {
    "papermill": {
     "duration": 0.011141,
     "end_time": "2025-06-11T16:38:51.286884",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.275743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pipeline: Image matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b23091c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.309733Z",
     "iopub.status.busy": "2025-06-11T16:38:51.309531Z",
     "iopub.status.idle": "2025-06-11T16:38:51.315307Z",
     "shell.execute_reply": "2025-06-11T16:38:51.314664Z"
    },
    "papermill": {
     "duration": 0.018587,
     "end_time": "2025-06-11T16:38:51.316492",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.297905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exec_feature_extraction(params):\n",
    "    images = params.images\n",
    "    feature_dir = params.feature_dir\n",
    "    device = params.device\n",
    "    \n",
    "    t = time()\n",
    "    index_pairs = get_image_pairs_shortlist(\n",
    "        images,\n",
    "        sim_th = CONFIG.sim_th, # should be strict\n",
    "        min_pairs = CONFIG.min_pairs, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "        exhaustive_if_less = CONFIG.exhaustive_if_less,\n",
    "        device=device\n",
    "    )\n",
    "    params.laptime_image_pairs = time() - t\n",
    "    params.image_pairs = index_pairs\n",
    "    print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {params.laptime_image_pairs:.4f} sec')\n",
    "    gc.collect()\n",
    "\n",
    "    t = time()\n",
    "    files_keypoints = []\n",
    "    for i_param, imc_param in enumerate(CONFIG.params_alikedlg):        \n",
    "        for rot in range(1):\n",
    "            file_keypoints = f'{feature_dir}/aliked_lightglue_param{i_param}_rot{rot}_mkpts.h5'\n",
    "            max_num_keypoints = imc_param.max_num_keypoints\n",
    "            resize_to = imc_param.image_size\n",
    "            min_matches = imc_param.min_matches\n",
    "            matching_aliked_lightglue_rot(\n",
    "                images,\n",
    "                index_pairs,\n",
    "                rot,\n",
    "                file_keypoints,\n",
    "                feature_dir, \n",
    "                max_num_keypoints, \n",
    "                resize_to=resize_to,\n",
    "                device=device,\n",
    "                min_matches=min_matches,\n",
    "                verbose=True,\n",
    "            )\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            gc.collect()\n",
    "    params.laptime_lightglue_4rots = time() - t\n",
    "    print(f'Features matched in {params.laptime_lightglue_4rots:.4f} sec')\n",
    "\n",
    "    # RoMa\n",
    "    \"\"\"\n",
    "    t = time()\n",
    "    file_keypoints = f'{feature_dir}/roma_mkpts.h5'\n",
    "    matching_roma(\n",
    "        images,\n",
    "        index_pairs,\n",
    "        file_keypoints,\n",
    "        feature_dir,\n",
    "        num_features=CONFIG.roma_max_num_keypoints,\n",
    "        resize_to=CONFIG.roma_image_size,\n",
    "        device=device,\n",
    "        min_matches=CONFIG.min_matches,\n",
    "        verbose=True,\n",
    "    )\n",
    "    files_keypoints.append( file_keypoints )\n",
    "    gc.collect()\n",
    "    params.laptime_roma = time() - t\n",
    "    print(f'Features matched in {params.laptime_roma:.4f} sec')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge mkpts and create keypoints/matches\n",
    "    sleep(10)\n",
    "    keypoints_merger(\n",
    "        images,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir,\n",
    "        filter_FundamentalMatrix = False,\n",
    "        filter_iterations = 10,\n",
    "        filter_threshold = 8,\n",
    "    )\n",
    "    \n",
    "    gc.collect()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb6845",
   "metadata": {
    "papermill": {
     "duration": 0.011081,
     "end_time": "2025-06-11T16:38:51.338749",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.327668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SfM Maps Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c8f3a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.361887Z",
     "iopub.status.busy": "2025-06-11T16:38:51.361642Z",
     "iopub.status.idle": "2025-06-11T16:38:51.373344Z",
     "shell.execute_reply": "2025-06-11T16:38:51.372725Z"
    },
    "papermill": {
     "duration": 0.024649,
     "end_time": "2025-06-11T16:38:51.374514",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.349865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge maps of colmap\n",
    "import argparse, shutil, subprocess, tempfile, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import pycolmap  # pip install pycolmap\n",
    "\n",
    "# ---------- util ----------------------------------------------------------------\n",
    "def read_image_names(model_dir: Path):\n",
    "    \"\"\"\n",
    "    pycolmap 0.6.x では Reconstruction() で読み込む。\n",
    "    返り値: 画像ファイル名(set[str])\n",
    "    \"\"\"\n",
    "    rec = pycolmap.Reconstruction(str(model_dir))   # ← これで images.bin をパース\n",
    "    return {img.name for img in rec.images.values()}\n",
    "\n",
    "#def read_image_names(model_dir: Path):\n",
    "#    images = pycolmap.read_images_binary(model_dir / \"images.bin\")\n",
    "#    return {img.name for img in images.values()}\n",
    "\n",
    "def build_overlap_graph(image_sets, min_common):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(image_sets)))\n",
    "    for i in range(len(image_sets)):\n",
    "        for j in range(i + 1, len(image_sets)):\n",
    "            if len(image_sets[i] & image_sets[j]) >= min_common:\n",
    "                G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    \"\"\"stdout+stderr をストリーム出力しつつ実行\"\"\"\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    for line in proc.stdout:\n",
    "        print(line, end=\"\")\n",
    "    proc.wait()\n",
    "    if proc.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(proc.returncode, cmd)\n",
    "\n",
    "# ---------- COLMAP wrappers ------------------------------------------------------\n",
    "def merge_two_models(colmap_bin, ref_model, src_model, out_dir):\n",
    "    cmd = [\n",
    "        colmap_bin, \"model_merger\",\n",
    "        \"--input_path1\", str(ref_model),\n",
    "        \"--input_path2\", str(src_model),\n",
    "        \"--output_path\", str(out_dir),\n",
    "    ]\n",
    "    try:\n",
    "        run_cmd(cmd)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "def run_bundle_adjuster(colmap_bin, model_dir, num_threads=-1):\n",
    "    \"\"\"\n",
    "    実行例:\n",
    "        colmap bundle_adjuster --input_path scene_000 --output_path scene_000 \\\n",
    "            --BundleAdjustment.refine_principal_point 1\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        colmap_bin, \"bundle_adjuster\",\n",
    "        \"--input_path\", str(model_dir),\n",
    "        \"--output_path\", str(model_dir),\n",
    "        \"--BundleAdjustment.refine_focal_length\", \"1\",\n",
    "        \"--BundleAdjustment.refine_principal_point\", \"1\",\n",
    "        \"--BundleAdjustment.refine_extra_params\", \"1\",\n",
    "    ]\n",
    "    if num_threads > 0:\n",
    "        cmd += [\"--Mapper.num_threads\", str(num_threads)]\n",
    "    run_cmd(cmd)     # raise on error\n",
    "\n",
    "# ---------- main logic -----------------------------------------------------------\n",
    "def cluster_and_merge(\n",
    "    model_dirs,\n",
    "    out_root,\n",
    "    colmap_bin=\"colmap\",\n",
    "    min_common=10,\n",
    "    run_bundle_adjustment=True,\n",
    "    keep_tmp=False,\n",
    "):\n",
    "    out_root = Path(out_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) 画像集合\n",
    "    image_sets = [read_image_names(Path(m)) for m in model_dirs]\n",
    "\n",
    "    # 2) 重複グラフ → 連結成分\n",
    "    clusters = list(nx.connected_components(build_overlap_graph(image_sets, min_common)))\n",
    "    print(f\"num of clusters = {len(clusters)}\")\n",
    "    \n",
    "    merged_paths = []\n",
    "    for cidx, comp in enumerate(clusters):\n",
    "        comp = list(comp)\n",
    "        if len(comp) == 1:\n",
    "            src = Path(model_dirs[comp[0]])\n",
    "            dst = out_root / f\"scene_{cidx:03d}\"\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "            merged_paths.append(dst)\n",
    "            if run_bundle_adjustment:\n",
    "                run_bundle_adjuster(colmap_bin, dst)\n",
    "            continue\n",
    "\n",
    "        # 3) 画像数の多い順に\n",
    "        comp_sorted = sorted(comp, key=lambda i: len(image_sets[i]), reverse=True)\n",
    "        work_dir = Path(model_dirs[comp_sorted[0]])\n",
    "\n",
    "        # 4) 順次マージ\n",
    "        failed_sources = []\n",
    "        for midx in comp_sorted[1:]:\n",
    "            tmp_dir = Path(tempfile.mkdtemp(prefix=f\"merge_s{cidx}_\"))\n",
    "            ok = merge_two_models(colmap_bin, work_dir, Path(model_dirs[midx]), tmp_dir)\n",
    "            if ok:\n",
    "                work_dir = tmp_dir\n",
    "            else:\n",
    "                failed_sources.append(midx)\n",
    "\n",
    "        # 5) 出力 & BA\n",
    "        final_dir = out_root / f\"scene_{cidx:03d}\"\n",
    "        shutil.move(str(work_dir), final_dir)\n",
    "        merged_paths.append(final_dir)\n",
    "\n",
    "        if run_bundle_adjustment:\n",
    "            run_bundle_adjuster(colmap_bin, final_dir)\n",
    "\n",
    "        # 6) マージ失敗分を別シーンとしてコピー\n",
    "        for fidx in failed_sources:\n",
    "            dst = out_root / f\"scene_{cidx}_{fidx}\"\n",
    "            shutil.copytree(model_dirs[fidx], dst, dirs_exist_ok=True)\n",
    "            merged_paths.append(dst)\n",
    "            if run_bundle_adjustment:\n",
    "                run_bundle_adjuster(colmap_bin, dst)\n",
    "\n",
    "    return merged_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d1304",
   "metadata": {
    "papermill": {
     "duration": 0.011007,
     "end_time": "2025-06-11T16:38:51.396906",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.385899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Update submission values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa345b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.420394Z",
     "iopub.status.busy": "2025-06-11T16:38:51.420193Z",
     "iopub.status.idle": "2025-06-11T16:38:51.426189Z",
     "shell.execute_reply": "2025-06-11T16:38:51.425539Z"
    },
    "papermill": {
     "duration": 0.018727,
     "end_time": "2025-06-11T16:38:51.427297",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.408570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_prediction(params, result_map_dirs, retry):\n",
    "    # Parse\n",
    "    feature_dir = params.feature_dir\n",
    "    images_dir = params.images_dir\n",
    "    filename_to_index = params.filename_to_index\n",
    "    predictions = params.predictions\n",
    "    dataset = params.dataset\n",
    "\n",
    "    maps = {}\n",
    "    for idx, result_map_dir in enumerate(result_map_dirs):\n",
    "        maps[idx] = pycolmap.Reconstruction(result_map_dir)\n",
    "    \n",
    "    print (\"Counting map size...\")\n",
    "    list_num_images = []\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            list_num_images.append( len(rec.images) )\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if params.list_model_size is None:\n",
    "        params.list_model_size = [list_num_images]\n",
    "    else:\n",
    "        params.list_model_size.append( list_num_images )\n",
    "    sort_idx = np.argsort( np.array(list_num_images) )\n",
    "    \n",
    "    registered = 0\n",
    "    for map_index, cur_idx in enumerate(sort_idx):\n",
    "        cur_map = maps[cur_idx]\n",
    "        cur_map_size = list_num_images[cur_idx]\n",
    "        if cur_map_size < 4:\n",
    "            continue\n",
    "        for index, image in cur_map.images.items():\n",
    "            prediction_index = filename_to_index[image.name]\n",
    "            if (cur_map_size > predictions[prediction_index].map_size) and (cur_map_size > 5):\n",
    "                predictions[prediction_index].map_size = cur_map_size\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].retry_index = retry\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                registered += 1\n",
    "    del maps\n",
    "    gc.collect()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebadb0a",
   "metadata": {
    "papermill": {
     "duration": 0.01136,
     "end_time": "2025-06-11T16:38:51.450005",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.438645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70a7c9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.473243Z",
     "iopub.status.busy": "2025-06-11T16:38:51.473043Z",
     "iopub.status.idle": "2025-06-11T16:38:51.813369Z",
     "shell.execute_reply": "2025-06-11T16:38:51.812646Z"
    },
    "papermill": {
     "duration": 0.353677,
     "end_time": "2025-06-11T16:38:51.814900",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.461223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from typing import List, Tuple, Iterable\n",
    "\n",
    "def get_network_from_matches_h5( file, images, num_max_keypoints = 8192, th_matches=150):\n",
    "    image_to_index = {file:i for i,file in enumerate(images)}\n",
    "    index_to_image = {i:file for i,file in enumerate(images)}\n",
    "\n",
    "    edges = []\n",
    "    with h5py.File(file, \"r\") as f_mat:\n",
    "        for key1 in f_mat.keys():\n",
    "            for key2 in f_mat[key1].keys():\n",
    "                if f_mat[ key1 ][ key2 ].shape[0] >=th_matches:\n",
    "                    edges.append( (key1, key2, f_mat[key1][key2].shape[0] / num_max_keypoints) )\n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(edges, weight=\"weight\")\n",
    "    return G, image_to_index, index_to_image\n",
    "\n",
    "def get_components(\n",
    "    G: nx.Graph | nx.DiGraph,\n",
    "    *,\n",
    "    directed_mode: str = \"auto\"  # \"auto\" | \"weak\" | \"strong\"\n",
    ") -> Tuple[int, List[List]]:\n",
    "    # --- 無向グラフ ----------------------------------------------------\n",
    "    if not G.is_directed():\n",
    "        comp_iter: Iterable[set] = nx.connected_components(G)\n",
    "\n",
    "    # --- 有向グラフ ----------------------------------------------------\n",
    "    else:\n",
    "        if directed_mode == \"strong\":\n",
    "            comp_iter = nx.strongly_connected_components(G)\n",
    "        elif directed_mode in {\"weak\", \"auto\"}:\n",
    "            comp_iter = nx.weakly_connected_components(G)\n",
    "        else:\n",
    "            raise ValueError(\"directed_mode must be 'auto', 'weak', or 'strong'\")\n",
    "\n",
    "    components = [sorted(list(c)) for c in comp_iter]\n",
    "    n_components = len(components)\n",
    "    return n_components, components\n",
    "\n",
    "def get_scenes_from_graph(G, random_state=42, th=0.2):\n",
    "    communities = louvain_communities(\n",
    "        G,\n",
    "        weight=\"weight\",\n",
    "        resolution=th,\n",
    "        seed=random_state\n",
    "    )\n",
    "    \n",
    "    scenes = []\n",
    "    print(f\"#clusters = {len(communities)}\")\n",
    "    for cid, members in enumerate(communities):\n",
    "        scene = set(members)\n",
    "        print(f\"Cluster {cid}: {len(members)} | {sorted(members)}\")\n",
    "        if len(scene) >= 5:\n",
    "            scenes.append( scene )\n",
    "        print(\"=\"*50)\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281dcdb2",
   "metadata": {
    "papermill": {
     "duration": 0.011142,
     "end_time": "2025-06-11T16:38:51.838051",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.826909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pipeline: Multi-Scene SfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22f4de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.861396Z",
     "iopub.status.busy": "2025-06-11T16:38:51.861115Z",
     "iopub.status.idle": "2025-06-11T16:38:51.873039Z",
     "shell.execute_reply": "2025-06-11T16:38:51.872423Z"
    },
    "papermill": {
     "duration": 0.02499,
     "end_time": "2025-06-11T16:38:51.874309",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.849319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exec_reconstruction(params):\n",
    "    feature_dir = params.feature_dir\n",
    "    params.laptime_calc_fmat = []\n",
    "    params.laptime_sfm = []\n",
    "\n",
    "    # Clustering\n",
    "    feature_dir = params.feature_dir\n",
    "    fnames = [ p.filename for p in params.predictions ]\n",
    "\n",
    "    matches_file = f'{feature_dir}/matches.h5'\n",
    "    scenes = []\n",
    "    num_retry = 4\n",
    "    for _ in range(num_retry):\n",
    "        G, images_to_index, index_to_image = get_network_from_matches_h5( \n",
    "            matches_file, fnames, num_max_keypoints = 8192, th_matches=150,\n",
    "        )\n",
    "        n_components, components = get_components(G)\n",
    "        for component in components:\n",
    "            if len(component) > 4:\n",
    "                scenes.append( set(component) )\n",
    "\n",
    "    # SfM for each scne cluster\n",
    "    for retry, scene in enumerate(scenes):\n",
    "        params = reconstruction_single(params, scene, retry, 2)\n",
    "\n",
    "    # merge models\n",
    "    input_map_dirs = sorted( glob.glob( f\"{feature_dir}/colmap_rec_*/*/cameras.bin\"))\n",
    "    input_map_dirs = [ \n",
    "        os.path.dirname(x)\n",
    "        for x in input_map_dirs\n",
    "        if len(pycolmap.Reconstruction( os.path.dirname(x) ).images) > 8\n",
    "    ]\n",
    "    input_map_dirs = [ \n",
    "        x \n",
    "        for x in input_map_dirs\n",
    "        if pycolmap.Reconstruction(x).compute_mean_reprojection_error() < 2.0\n",
    "    ]\n",
    "\n",
    "    if len(input_map_dirs) > 1:\n",
    "        output_dir = f\"{feature_dir}/colmap_rec_merged/\"\n",
    "        COLMAP_BIN = \"/usr/bin/colmap\"\n",
    "        MERGE_MIN_COMMON = 6\n",
    "        MERGE_RUN_BA = True\n",
    "        result_map_dirs = cluster_and_merge(\n",
    "            input_map_dirs,\n",
    "            output_dir,\n",
    "            colmap_bin=COLMAP_BIN,\n",
    "            min_common=MERGE_MIN_COMMON,\n",
    "            run_bundle_adjustment=MERGE_RUN_BA,\n",
    "            keep_tmp=False,\n",
    "        )\n",
    "    \n",
    "        # update motion info\n",
    "        update_prediction(params, result_map_dirs, len(scenes) )\n",
    "    clear_output(wait=False)\n",
    "    return params\n",
    "\n",
    "def reconstruction_single(params, scene, retry, min_model_size):\n",
    "    # Parse\n",
    "    feature_dir = params.feature_dir\n",
    "    images_dir = params.images_dir\n",
    "    filename_to_index = params.filename_to_index\n",
    "    predictions = params.predictions\n",
    "\n",
    "    # Execute process\n",
    "    database_path = os.path.join(feature_dir, f'colmap_{retry}.db')\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    sleep(1)\n",
    "    #import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    import_into_colmap_with_scene(images_dir, scene, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec_{retry}'\n",
    "    \n",
    "    t = time()\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    params.laptime_calc_fmat = time() - t\n",
    "    print(f'Ran RANSAC in {params.laptime_calc_fmat:.4f} sec')\n",
    "    \n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "    # Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    mapper_options.min_model_size = min_model_size #25 # 30 # 20 # 10 # 3\n",
    "    mapper_options.max_num_models = 15 # 25 # 25\n",
    "\n",
    "    # Strict initial pairs\n",
    "    #mapper_options.mapper.init_min_num_inliers = 200 # default:100\n",
    "    #mapper_options.mapper.init_max_error = 2         # default:4\n",
    "    #mapper_options.mapper.init_min_tri_angle = 10    # default:16\n",
    "\n",
    "    ##\n",
    "    #mapper_options.max_model_overlap = 30 # 20\n",
    "    #mapper_options.min_num_matches   = 50\n",
    "    ##\n",
    "    mapper_options.init_num_trials = 1000 # 200\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    t = time()\n",
    "    maps = pycolmap.incremental_mapping(\n",
    "        database_path=database_path, \n",
    "        image_path=images_dir,\n",
    "        output_path=output_path,\n",
    "        options=mapper_options)\n",
    "    sleep(1)\n",
    "    params.laptime_sfm = time() - t\n",
    "    print(f'Reconstruction done in  {params.laptime_sfm:.4f} sec')\n",
    "    print(maps)\n",
    "\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    #print(\"merge maps...\")\n",
    "    #print(f\"before: len(maps) = \", len(maps))\n",
    "    #maps = merge_models_dict(maps)\n",
    "    #print(f\"after: len(maps) = \", len(maps))\n",
    "\n",
    "    print (\"Counting map size...\")\n",
    "    list_num_images = []\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            list_num_images.append( len(rec.images) )\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if params.list_model_size is None:\n",
    "        params.list_model_size = [list_num_images]\n",
    "    else:\n",
    "        params.list_model_size.append( list_num_images )\n",
    "    sort_idx = np.argsort( np.array(list_num_images) )\n",
    "\n",
    "    registered = 0\n",
    "    for map_index, cur_idx in enumerate(sort_idx):\n",
    "        cur_map = maps[cur_idx]\n",
    "        cur_map_size = list_num_images[cur_idx]\n",
    "        for index, image in cur_map.images.items():\n",
    "            prediction_index = filename_to_index[image.name]\n",
    "            if (cur_map_size > predictions[prediction_index].map_size) and (cur_map_size > 5):\n",
    "                predictions[prediction_index].map_size = cur_map_size\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].retry_index = retry\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                registered += 1\n",
    "    gc.collect()\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b261067",
   "metadata": {
    "papermill": {
     "duration": 0.011075,
     "end_time": "2025-06-11T16:38:51.897153",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.886078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be9b0a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:51.920689Z",
     "iopub.status.busy": "2025-06-11T16:38:51.920409Z",
     "iopub.status.idle": "2025-06-11T16:38:52.080260Z",
     "shell.execute_reply": "2025-06-11T16:38:52.079561Z"
    },
    "papermill": {
     "duration": 0.173085,
     "end_time": "2025-06-11T16:38:52.081530",
     "exception": false,
     "start_time": "2025-06-11T16:38:51.908445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    map_size: int = -1\n",
    "    cluster_index: int | None = None\n",
    "    retry_index: int = -1\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DatasetParams:\n",
    "    dataset: str\n",
    "    feature_dir: str | None = None\n",
    "    images_dir: str | None = None\n",
    "    filename_to_index: dict | None = None\n",
    "    predictions: list | None = None\n",
    "    images: list | None = None\n",
    "    device: str | None = None\n",
    "    laptime_image_pairs: float = -1.0\n",
    "    laptime_lightglue_4rots: float = -1.0\n",
    "    laptime_roma: float = -1.0\n",
    "    laptime_calc_fmat: float = -1.0\n",
    "    laptime_sfm: float = -1.0\n",
    "    image_pairs: list | None = None\n",
    "    list_model_size: list | None = None\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"[Summary]\")\n",
    "        print(f\"- Dataset                   : {self.dataset}\")\n",
    "        if self.images is not None:\n",
    "            print(f\"- images                    : {len(self.images)}\")\n",
    "        if self.image_pairs is not None:\n",
    "            print(f\"- pairs                     : {len(self.image_pairs)}\")\n",
    "        print(f\"- laptime_image_pairs       : {self.laptime_image_pairs:.2f}\")\n",
    "        print(f\"- laptime_lightglue_4rots   : {self.laptime_lightglue_4rots:.2f}\")\n",
    "        print(f\"- laptime_roma              : {self.laptime_roma:.2f}\")\n",
    "        print(f\"- laptime_calc_fmat         : {self.laptime_calc_fmat:.2f}\")\n",
    "        print(f\"- laptime_sfm               : {self.laptime_sfm:.2f}\")\n",
    "        print(f\"- list_model_size           : {self.list_model_size}\")\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/tmp/kaggle/working/result/'\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "dataset_logs = []\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "if (not is_train) and (competition_data.shape[0] == 1945):\n",
    "    competition_data = competition_data[ competition_data[\"dataset\"].isin([\"ETs\", \"stairs\"]) ]\n",
    "    workdir = '/kaggle/working/result/'\n",
    "\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    prediction = Prediction(\n",
    "        image_id=None if is_train else row.image_id,\n",
    "        dataset=row.dataset,\n",
    "        filename=row.image\n",
    "    )\n",
    "\n",
    "    #if len(samples[row.dataset]) < 10:\n",
    "    samples[row.dataset].append( prediction )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e943cb03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T16:38:52.105482Z",
     "iopub.status.busy": "2025-06-11T16:38:52.105248Z",
     "iopub.status.idle": "2025-06-11T19:44:01.723298Z",
     "shell.execute_reply": "2025-06-11T19:44:01.722433Z"
    },
    "papermill": {
     "duration": 11109.631567,
     "end_time": "2025-06-11T19:44:01.724850",
     "exception": false,
     "start_time": "2025-06-11T16:38:52.093283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "# Enqeue feature extraction processing\n",
    "futures_gpu0 = {}\n",
    "futures_gpu1 = {}\n",
    "device = None\n",
    "device_index = None\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executors_gpu0, \\\n",
    "     concurrent.futures.ThreadPoolExecutor(max_workers=1) as executors_gpu1:\n",
    "\n",
    "    executors_gpus = [executors_gpu0, executors_gpu1]\n",
    "    futures_gpus = [futures_gpu0, futures_gpu1]\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "        # Switch device\n",
    "        device, device_index = switch_gpu(device, device_index)\n",
    "        \n",
    "        # Dataset parameter\n",
    "        dataset_params = DatasetParams(\n",
    "            dataset = dataset,\n",
    "            feature_dir = feature_dir,\n",
    "            images_dir = images_dir,\n",
    "            predictions = predictions,\n",
    "            filename_to_index = filename_to_index,\n",
    "            images = images,  # image filepaths\n",
    "            device = device,\n",
    "        )\n",
    "\n",
    "        # Enqueue feature extraction\n",
    "        futures_gpus[device_index][dataset] = executors_gpus[device_index].submit(\n",
    "            exec_feature_extraction, dataset_params,\n",
    "        )\n",
    "        #dataset_params = exec_feature_extraction(dataset_params)\n",
    "\n",
    "    # Enqeue reconstruction processing\n",
    "    futures_cpu  = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG.num_pallalel_sfm) as executors:\n",
    "        for dataset, predictions in samples.items():\n",
    "            if datasets_to_process and (dataset not in datasets_to_process):\n",
    "                print(f'Skipping \"{dataset}\"')\n",
    "                continue\n",
    "                \n",
    "            if dataset in futures_gpu0.keys():\n",
    "                future = futures_gpu0[dataset]\n",
    "            else:\n",
    "                future = futures_gpu1[dataset]\n",
    "    \n",
    "            # Wait for feature extraction\n",
    "            print(f\"waiting feature extraction at dataset = {dataset}\")\n",
    "            dataset_params = future.result()\n",
    "    \n",
    "            # if feature extraction is failed:\n",
    "            if dataset_params is None:\n",
    "                continue\n",
    "                \n",
    "            # Enqueue reconstruction\n",
    "            gc.collect()\n",
    "            futures_cpu[dataset] = executors.submit(\n",
    "                exec_reconstruction, dataset_params,\n",
    "            )\n",
    "\n",
    "        # Wait to reconstruction\n",
    "        for dataset, predictions in samples.items():\n",
    "            if datasets_to_process and (dataset not in datasets_to_process):\n",
    "                print(f'Skipping \"{dataset}\"')\n",
    "                continue\n",
    "            gc.collect()\n",
    "            dataset_params = futures_cpu[dataset].result()\n",
    "            samples[dataset] = dataset_params.predictions\n",
    "            dataset_logs.append( dataset_params )\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e8078",
   "metadata": {
    "papermill": {
     "duration": 0.011493,
     "end_time": "2025-06-11T19:44:01.749114",
     "exception": false,
     "start_time": "2025-06-11T19:44:01.737621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485e7858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T19:44:01.772925Z",
     "iopub.status.busy": "2025-06-11T19:44:01.772667Z",
     "iopub.status.idle": "2025-06-11T19:44:01.984084Z",
     "shell.execute_reply": "2025-06-11T19:44:01.983242Z"
    },
    "papermill": {
     "duration": 0.225049,
     "end_time": "2025-06-11T19:44:01.985531",
     "exception": false,
     "start_time": "2025-06-11T19:44:01.760482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_116.png,0.880507352;0.207250227;-0.426326339;-0.400200331;0.807021309;-0.434230701;0.254060029;0.552959266;0.793529805,0.377703176;-0.830395448;1.877378152\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_108.png,0.923497054;-0.096939017;0.371154978;0.288735017;0.812665770;-0.506168387;-0.252557480;0.574610453;0.778484134,0.324277148;-0.713830153;1.657845616\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_101.png,0.617611449;-0.211264131;0.757577432;0.592367689;0.758586672;-0.271379405;-0.517355409;0.616371421;0.593657859,-0.115211860;-0.997006955;1.827099474\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_082.png,-0.984465367;-0.123827651;0.124477527;-0.001547297;0.715046025;0.699075667;-0.175572059;0.688023179;-0.704129645,0.097153656;-1.739598219;4.323943106\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_071.png,-0.788365929;0.132336763;-0.600804580;-0.343986576;0.714842919;0.608829070;0.510051368;0.686648806;-0.518035732,0.198248442;-1.905070392;4.099812728\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_025.png,0.864238070;-0.174809390;0.471735345;0.211181047;0.977132310;-0.024799495;-0.456612662;0.121054232;0.881391372,-0.434934182;-1.802194404;3.456549165\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_000.png,-0.704193683;-0.264269059;0.658994023;0.249544905;0.776812782;0.578177518;-0.664709408;0.571597557;-0.481079656,-0.532386041;-2.151668194;5.463241606\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_007.png,-0.268895553;-0.418127645;0.867677621;0.428602768;0.754803490;0.496559522;-0.862551360;0.505411677;-0.023752636,-0.684046716;-2.516999947;4.156276695\r\n",
      "imc2023_haiper,retry1cluster0,fountain_image_012.png,0.048490651;-0.399623957;0.915395734;0.425752183;0.837315311;0.342984183;-0.903539560;0.373100206;0.210742736,-0.534473322;-1.995632339;3.543107626\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'retry{prediction.retry_index}cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'retry{prediction.retry_index}cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858050d",
   "metadata": {
    "papermill": {
     "duration": 0.011467,
     "end_time": "2025-06-11T19:44:02.009267",
     "exception": false,
     "start_time": "2025-06-11T19:44:01.997800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Running logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd153c31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T19:44:02.033798Z",
     "iopub.status.busy": "2025-06-11T19:44:02.033515Z",
     "iopub.status.idle": "2025-06-11T19:44:02.045568Z",
     "shell.execute_reply": "2025-06-11T19:44:02.044704Z"
    },
    "papermill": {
     "duration": 0.025347,
     "end_time": "2025-06-11T19:44:02.046708",
     "exception": false,
     "start_time": "2025-06-11T19:44:02.021361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[Summary]\n",
      "- Dataset                   : imc2023_haiper\n",
      "- images                    : 54\n",
      "- pairs                     : 1431\n",
      "- laptime_image_pairs       : 46.12\n",
      "- laptime_lightglue_4rots   : 442.32\n",
      "- laptime_roma              : -1.00\n",
      "- laptime_calc_fmat         : 6.51\n",
      "- laptime_sfm               : 93.12\n",
      "- list_model_size           : [array([31]), array([23]), array([31]), array([23]), array([31]), array([23]), array([31]), array([23]), array([31, 23])]\n",
      "====================================================================================================\n",
      "[Summary]\n",
      "- Dataset                   : amy_gardens\n",
      "- images                    : 200\n",
      "- pairs                     : 14500\n",
      "- laptime_image_pairs       : 76.08\n",
      "- laptime_lightglue_4rots   : 4697.38\n",
      "- laptime_roma              : -1.00\n",
      "- laptime_calc_fmat         : 18.59\n",
      "- laptime_sfm               : 893.42\n",
      "- list_model_size           : [array([ 3, 79, 75, 49, 27,  7]), array([ 3, 78, 74, 49,  9,  7]), array([ 3, 77, 75, 49, 28,  7]), array([ 3, 78, 94, 36,  7]), array([170])]\n",
      "====================================================================================================\n",
      "[Summary]\n",
      "- Dataset                   : fbk_vineyard\n",
      "- images                    : 163\n",
      "- pairs                     : 12188\n",
      "- laptime_image_pairs       : 36.71\n",
      "- laptime_lightglue_4rots   : 3840.17\n",
      "- laptime_roma              : -1.00\n",
      "- laptime_calc_fmat         : 1.45\n",
      "- laptime_sfm               : 145.70\n",
      "- list_model_size           : [array([110,   2]), array([48]), array([115]), array([48]), array([113,   2]), array([48]), array([115]), array([48]), array([115,  48])]\n",
      "====================================================================================================\n",
      "[Summary]\n",
      "- Dataset                   : ETs\n",
      "- images                    : 22\n",
      "- pairs                     : 231\n",
      "- laptime_image_pairs       : 7.37\n",
      "- laptime_lightglue_4rots   : 62.57\n",
      "- laptime_roma              : -1.00\n",
      "- laptime_calc_fmat         : 1.23\n",
      "- laptime_sfm               : 34.12\n",
      "- list_model_size           : [array([10]), array([9]), array([10]), array([9]), array([10]), array([9]), array([10]), array([9]), array([10,  9])]\n",
      "====================================================================================================\n",
      "[Summary]\n",
      "- Dataset                   : stairs\n",
      "- images                    : 51\n",
      "- pairs                     : 1275\n",
      "- laptime_image_pairs       : 16.56\n",
      "- laptime_lightglue_4rots   : 417.97\n",
      "- laptime_roma              : -1.00\n",
      "- laptime_calc_fmat         : 0.64\n",
      "- laptime_sfm               : 7.46\n",
      "- list_model_size           : [array([20,  7, 20,  2]), array([7]), array([21,  2, 20,  2]), array([7]), array([26, 22,  2]), array([7]), array([25,  6,  2]), array([7]), array([28])]\n"
     ]
    }
   ],
   "source": [
    "for dataset_params in dataset_logs:\n",
    "    print(\"=\" * 100)\n",
    "    dataset_params.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f5a4d",
   "metadata": {
    "papermill": {
     "duration": 0.011422,
     "end_time": "2025-06-11T19:44:02.069914",
     "exception": false,
     "start_time": "2025-06-11T19:44:02.058492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e51d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T19:44:02.093982Z",
     "iopub.status.busy": "2025-06-11T19:44:02.093739Z",
     "iopub.status.idle": "2025-06-11T19:51:04.563589Z",
     "shell.execute_reply": "2025-06-11T19:51:04.562686Z"
    },
    "papermill": {
     "duration": 422.483488,
     "end_time": "2025-06-11T19:51:04.565015",
     "exception": false,
     "start_time": "2025-06-11T19:44:02.081527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=65.06% (mAA=66.67%, clusterness=63.53%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "amy_gardens: score=31.08% (mAA=18.40%, clusterness=100.00%)\n",
      "fbk_vineyard: score=44.23% (mAA=43.18%, clusterness=45.32%)\n",
      "ETs: score=64.94% (mAA=48.08%, clusterness=100.00%)\n",
      "stairs: score=3.23% (mAA=1.67%, clusterness=50.00%)\n",
      "Average over all datasets: score=16.04% (mAA=13.69%, clusterness=89.14%)\n",
      "Computed metric in: 422.46 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5866093",
   "metadata": {
    "papermill": {
     "duration": 0.01211,
     "end_time": "2025-06-11T19:51:04.590298",
     "exception": false,
     "start_time": "2025-06-11T19:51:04.578188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 234271505,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 237741314,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14634,
     "sourceId": 17578,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14635,
     "sourceId": 17579,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11581.141336,
   "end_time": "2025-06-11T19:51:08.704110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-11T16:38:07.562774",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
