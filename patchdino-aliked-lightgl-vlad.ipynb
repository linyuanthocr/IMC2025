{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11217117,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Example submission\n\nImage Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n\nThis notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n\nRemember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# IMPORTANT \n#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:16.891293Z","iopub.execute_input":"2025-05-22T00:18:16.891643Z","iopub.status.idle":"2025-05-22T00:18:18.903086Z","shell.execute_reply.started":"2025-05-22T00:18:16.891587Z","shell.execute_reply":"2025-05-22T00:18:18.902050Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nkornia is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nkornia-moons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nkornia-rs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nlightglue is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\npycolmap is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nrerun-sdk is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\n\nfrom lightglue import DISK\nfrom kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\nfrom scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n\n# IMPORTANT Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:18.904521Z","iopub.execute_input":"2025-05-22T00:18:18.904809Z","iopub.status.idle":"2025-05-22T00:18:26.630542Z","shell.execute_reply.started":"2025-05-22T00:18:18.904785Z","shell.execute_reply":"2025-05-22T00:18:26.629758Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nprint(\"PyTorch version:\", torch.__version__)\nimport sys\nprint(\"Python version:\", sys.version)\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"Device count:\", torch.cuda.device_count())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.632002Z","iopub.execute_input":"2025-05-22T00:18:26.632583Z","iopub.status.idle":"2025-05-22T00:18:26.723984Z","shell.execute_reply.started":"2025-05-22T00:18:26.632544Z","shell.execute_reply":"2025-05-22T00:18:26.723157Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu121\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nCUDA available: True\nCUDA version: 12.1\nDevice count: 2\nCurrent device: 0\nDevice name: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Do not forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.725355Z","iopub.execute_input":"2025-05-22T00:18:26.725641Z","iopub.status.idle":"2025-05-22T00:18:26.730318Z","shell.execute_reply.started":"2025-05-22T00:18:26.725621Z","shell.execute_reply":"2025-05-22T00:18:26.729335Z"}},"outputs":[{"name":"stdout","text":"device=device(type='cuda', index=0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport os\nimport h5py\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, AutoModel\nimport sys\n\n# Assume these are available from your environment or previous code\n# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n# from kornia.feature import ALIKED # Already in your detect_aliked\n# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n\n# --- Helper function for image loading (if not already defined) ---\ndef load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\ndef get_dino_patch_features_for_keypoints(img_path, keypoints_xy, dino_processor, dino_model, patch_size=16, device=torch.device('cpu')):\n    \"\"\"\n    Extracts DINO patch features corresponding to given ALIKED keypoint locations.\n    It correctly infers the DINO patch grid dimensions from the processed input.\n\n    Args:\n        img_path (str): Path to the image file.\n        keypoints_xy (torch.Tensor): Nx2 tensor of (x, y) keypoint coordinates in image pixel space.\n                                     These keypoints are assumed to be in the original image's coordinate system.\n        dino_processor: HuggingFace AutoImageProcessor for DINO.\n        dino_model: HuggingFace AutoModel for DINO.\n        patch_size (int): The patch size used by the DINO model (e.g., 14 or 16).\n        device (torch.device): Device to run the models on.\n\n    Returns:\n        torch.Tensor: NxD_dino tensor of DINO patch features for each keypoint.\n                      Returns None if no keypoints or image loading fails.\n    \"\"\"\n    if len(keypoints_xy) == 0:\n        dino_feature_dim = dino_model.config.hidden_size # Get actual DINO hidden size\n        return torch.empty((0, dino_feature_dim), device=device)\n\n    # 1. Load the original image (ALIKED processed this size)\n    original_img = load_torch_image(img_path, device=device)\n    original_h, original_w = original_img.shape[-2], original_img.shape[-1]\n\n\n    # 2. Process the image with DINO's processor\n    #    This step performs resizing, padding, etc., as needed by the DINO model\n    with torch.inference_mode():\n        # dino_processor returns a BatchFeature object which includes pixel_values\n        # and potentially other information like `pixel_mask`\n        inputs = dino_processor(images=original_img, return_tensors=\"pt\", do_rescale=False).to(device)\n        outputs = dino_model(**inputs)\n\n        # Get the actual dimensions of the image as processed by the DINO model\n        # This is the crucial part: the actual H and W that produced `patch_tokens`\n        # We can infer this from the `pixel_values` shape\n        processed_h = inputs['pixel_values'].shape[-2]\n        processed_w = inputs['pixel_values'].shape[-1]\n\n        # Extract patch tokens (excluding the CLS token)\n        patch_tokens = outputs.last_hidden_state[:, 1:].squeeze(0) # Shape: (num_patches, hidden_size)\n\n        # Calculate the actual grid dimensions based on the *processed* image size\n        # and the model's patch size.\n        # This should perfectly match the number of patch_tokens if the model is well-behaved.\n        num_patches_h = processed_h // patch_size\n        num_patches_w = processed_w // patch_size\n\n        # Safety check: ensure calculated grid matches actual token count\n        expected_token_count = num_patches_h * num_patches_w\n        if patch_tokens.shape[0] != expected_token_count:\n            # This indicates a deeper issue with how the model's output tokens\n            # map to the spatial grid, or an unexpected patch size/model behavior.\n            # Some models might have slightly different patch token arrangements.\n            # DINOv2 typically aligns well.\n            raise ValueError(\n                f\"DINO patch token count ({patch_tokens.shape[0]}) does not match \"\n                f\"expected grid dimensions ({num_patches_h}x{num_patches_w} = {expected_token_count}) \"\n                f\"for processed image size {processed_w}x{processed_h} with patch size {patch_size}. \"\n                f\"Please verify DINO model and processor configuration.\"\n            )\n\n        # Reshape patch tokens into a 2D grid\n        patch_features_grid = patch_tokens.reshape(num_patches_h, num_patches_w, -1)\n        dino_feature_dim = patch_features_grid.shape[-1] # Actual feature dimension\n\n\n    dino_features_for_kpts = torch.zeros((len(keypoints_xy), dino_feature_dim), device=device)\n\n    # 3. Rescale ALIKED keypoints to the DINO *processed* image dimensions\n    #    ALIKED keypoints are in original_w x original_h coordinates.\n    #    DINO patches correspond to processed_w x processed_h coordinates.\n    scale_x = processed_w / original_w\n    scale_y = processed_h / original_h\n\n    scaled_keypoints_xy = keypoints_xy.clone()\n    scaled_keypoints_xy[:, 0] *= scale_x\n    scaled_keypoints_xy[:, 1] *= scale_y\n\n    # 4. Map scaled keypoints to DINO patch grid indices\n    keypoint_cols = (scaled_keypoints_xy[:, 0] / patch_size).long()\n    keypoint_rows = (scaled_keypoints_xy[:, 1] / patch_size).long()\n\n    # Clip indices to ensure they are within bounds of the patch grid\n    keypoint_rows = torch.clamp(keypoint_rows, 0, num_patches_h - 1)\n    keypoint_cols = torch.clamp(keypoint_cols, 0, num_patches_w - 1)\n\n    # Gather DINO features for each keypoint's corresponding patch\n    dino_features_for_kpts = patch_features_grid[keypoint_rows, keypoint_cols]\n\n    return dino_features_for_kpts\n\n\n# --- MODIFIED: Detect ALIKED and Combine with DINO Patch Features ---\ndef detect_aliked_and_combine_with_dino(img_fnames,\n                                        feature_dir='.featureout',\n                                        num_features=4096,\n                                        resize_to=1024,\n                                        dino_processor=None,\n                                        dino_model=None,\n                                        dino_patch_size=16, # Typically 14 or 16 for DINO\n                                        device=torch.device('cpu')):\n    dtype = torch.float32 # ALIKED has issues with float16\n    aliked_extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n    aliked_extractor.preprocess_conf[\"resize\"] = resize_to\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='w') as f_desc_aliked, \\\n         h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='w') as f_desc_combined: # New HDF5 for combined features\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = aliked_extractor.extract(image0)\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy() # ALIKED keypoints (x,y)\n                descs_aliked = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy() # ALIKED descriptors\n\n                # Get DINO patch features for these keypoints\n                kpts_torch = torch.from_numpy(kpts).to(device)\n                descs_dino_patch = get_dino_patch_features_for_keypoints(\n                    img_path, kpts_torch, dino_processor, dino_model, dino_patch_size, device\n                ).detach().cpu().numpy()\n\n                # Concatenate ALIKED and DINO features\n                if len(descs_aliked) > 0 and len(descs_dino_patch) > 0:\n                    combined_descs = np.concatenate((descs_aliked, descs_dino_patch), axis=1)\n                elif len(descs_aliked) > 0: # Only ALIKED if no DINO features (shouldn't happen often)\n                    combined_descs = descs_aliked\n                else: # No features found\n                    combined_descs = np.array([]) # Empty array\n\n                f_kp[key] = kpts\n                f_desc_aliked[key] = descs_aliked # Keep ALIKED descriptors for debugging or other uses\n                f_desc_combined[key] = combined_descs # Store the new combined descriptors\n    print(f\"Combined features saved to {feature_dir}/descriptors_combined.h5\")\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.731340Z","iopub.execute_input":"2025-05-22T00:18:26.731700Z","iopub.status.idle":"2025-05-22T00:18:26.753047Z","shell.execute_reply.started":"2025-05-22T00:18:26.731668Z","shell.execute_reply":"2025-05-22T00:18:26.752337Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans # MiniBatchKMeans is faster for large datasets\n\n# --- VLAD Aggregation Function ---\ndef vlad_encode(descriptors, centroids):\n    \"\"\"\n    Performs VLAD encoding.\n\n    Args:\n        descriptors (np.ndarray): NxM array of local descriptors.\n        centroids (np.ndarray): KxM array of K-Means cluster centroids.\n\n    Returns:\n        np.ndarray: 1x(K*M) VLAD descriptor.\n    \"\"\"\n    if descriptors.shape[0] == 0:\n        return np.zeros(centroids.shape[0] * centroids.shape[1], dtype=np.float32)\n\n    num_descriptors, desc_dim = descriptors.shape\n    num_centroids, _ = centroids.shape\n\n    # Assign each descriptor to its nearest centroid\n    # Using cdist for efficiency\n    distances = np.sqrt(np.sum((descriptors[:, None, :] - centroids[None, :, :])**2, axis=2))\n    # distances = cdist(descriptors, centroids, 'sqeuclidean') # Could use cdist for sqeuclidean\n    cluster_assignments = np.argmin(distances, axis=1)\n\n    # Initialize VLAD accumulator\n    vlad_accumulator = np.zeros((num_centroids, desc_dim), dtype=np.float32)\n\n    # Accumulate residuals\n    for i in range(num_descriptors):\n        cluster_idx = cluster_assignments[i]\n        residual = descriptors[i] - centroids[cluster_idx]\n        vlad_accumulator[cluster_idx] += residual\n\n    # Flatten and L2 normalize\n    vlad_descriptor = vlad_accumulator.flatten()\n    vlad_descriptor = F.normalize(torch.from_numpy(vlad_descriptor).unsqueeze(0), dim=1, p=2).squeeze(0).numpy()\n\n    return vlad_descriptor\n\n\n# --- NEW: Get Global Descriptors using K-Means + VLAD ---\ndef get_global_desc_vlad(fnames, feature_dir='.featureout', num_clusters=64, device=torch.device('cpu')):\n    \"\"\"\n    Generates global descriptors for images using K-Means + VLAD on combined ALIKED+DINO features.\n\n    Args:\n        fnames (list): List of image file paths.\n        feature_dir (str): Directory where combined descriptors are stored.\n        num_clusters (int): Number of clusters for K-Means (K in VLAD).\n        device (torch.device): Not directly used for VLAD computation, but passed for consistency.\n\n    Returns:\n        torch.Tensor: Nx(K*M) tensor of global VLAD descriptors.\n    \"\"\"\n    all_local_descs = []\n    keys_order = [] # To maintain order of descriptors with respect to fnames\n\n    # 1. Load all combined local descriptors\n    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n        for img_path in tqdm(fnames, desc=\"Loading combined local descriptors for K-Means\"):\n            key = img_path.split('/')[-1]\n            if key in f_desc_combined:\n                descs = f_desc_combined[key][...]\n                if descs.shape[0] > 0:\n                    all_local_descs.append(descs)\n                    keys_order.append(key)\n\n    if not all_local_descs:\n        print(\"No combined local descriptors found. Cannot train K-Means or compute VLAD.\")\n        return torch.empty((0, num_clusters * 0), dtype=torch.float32) # Return empty tensor\n\n    # Concatenate all descriptors for K-Means training\n    all_local_descs_flat = np.concatenate(all_local_descs, axis=0)\n\n    # 2. Train K-Means on a subset of descriptors if the dataset is too large\n    # Or directly on all_local_descs_flat if memory permits\n    print(f\"Training K-Means with {num_clusters} clusters on {all_local_descs_flat.shape[0]} descriptors...\")\n    # Use MiniBatchKMeans for efficiency\n    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, n_init='auto', batch_size=256).fit(all_local_descs_flat)\n    centroids = kmeans.cluster_centers_\n    print(\"K-Means training complete.\")\n\n    # 3. Compute VLAD descriptor for each image\n    global_descs_vlad = []\n    # Re-iterate through original fnames to match the output order\n    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n        for img_path in tqdm(fnames, desc=\"Computing VLAD descriptors\"):\n            key = img_path.split('/')[-1]\n            if key in f_desc_combined:\n                descs = f_desc_combined[key][...]\n                vlad_desc = vlad_encode(descs, centroids)\n                global_descs_vlad.append(torch.from_numpy(vlad_desc).unsqueeze(0))\n            else:\n                # Handle cases where an image might not have any combined descriptors\n                # (e.g., no ALIKED keypoints detected). Append a zero vector of correct size.\n                print(f\"Warning: No combined descriptors for {key}. Appending zero VLAD descriptor.\")\n                # Determine descriptor dimension from centroids\n                desc_dim_per_cluster = centroids.shape[1] if centroids.shape[1] > 0 else 0 # Should not be 0 normally\n                zero_vlad = np.zeros(num_clusters * desc_dim_per_cluster, dtype=np.float32)\n                global_descs_vlad.append(torch.from_numpy(zero_vlad).unsqueeze(0))\n\n\n    if not global_descs_vlad:\n        return torch.empty((0, num_clusters * centroids.shape[1] if centroids.shape[1] > 0 else 0), dtype=torch.float32)\n\n    global_descs_vlad = torch.cat(global_descs_vlad, dim=0)\n    return global_descs_vlad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.753995Z","iopub.execute_input":"2025-05-22T00:18:26.754266Z","iopub.status.idle":"2025-05-22T00:18:26.833159Z","shell.execute_reply.started":"2025-05-22T00:18:26.754236Z","shell.execute_reply":"2025-05-22T00:18:26.832455Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- RE-DEFINED: get_image_pairs_shortlist to use the new VLAD global descriptor ---\ndef get_image_pairs_shortlist_vlad(fnames,\n                                   sim_th=0.6, # should be strict\n                                   min_pairs=30,\n                                   exhaustive_if_less=20,\n                                   feature_dir='.featureout', # Pass feature_dir\n                                   num_clusters_vlad=64, # New parameter for VLAD\n                                   device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames) # You need to define get_img_pairs_exhaustive if not done.\n\n    # Use the new VLAD-based global descriptor\n    descs = get_global_desc_vlad(fnames, feature_dir=feature_dir, num_clusters=num_clusters_vlad, device=device)\n\n    if descs.shape[0] == 0:\n        print(\"No global descriptors generated. Returning empty matching list.\")\n        return []\n\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n\n    # 只分析上三角（去掉对角线），避免重复\n    triu_indices = np.triu_indices_from(dm, k=1)\n    dm_flat = dm[triu_indices]\n    \n    # 打印统计信息\n    print(\"Distance Matrix Statistics:\")\n    print(f\"Min:  {dm_flat.min():.4f}\")\n    print(f\"Max:  {dm_flat.max():.4f}\")\n    print(f\"Mean: {dm_flat.mean():.4f}\")\n    print(f\"Std:  {dm_flat.std():.4f}\")\n    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n    print(f\"50%:  {np.percentile(dm_flat, 50):.4f}\")\n    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n\n    # removing half\n    mask = dm <= np.percentile(dm_flat, 50)\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = set() # Use a set for faster lookup of already added pairs\n\n    for st_idx in range(num_imgs - 1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]\n\n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < threshold: # Ensure distance is not effectively infinite\n                pair = tuple(sorted((st_idx, idx.item())))\n                if pair not in already_there_set:\n                    matching_list.append(pair)\n                    already_there_set.add(pair)\n                    total += 1\n    matching_list = sorted(list(matching_list)) # Sort the list of tuples\n    return matching_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.833965Z","iopub.execute_input":"2025-05-22T00:18:26.834213Z","iopub.status.idle":"2025-05-22T00:18:26.842767Z","shell.execute_reply.started":"2025-05-22T00:18:26.834192Z","shell.execute_reply":"2025-05-22T00:18:26.841831Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# def load_torch_image(fname, device=torch.device('cpu')):\n#     img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n#     return img\n\n\n# # Must Use efficientnet global descriptor to get matching shortlists.\n# def get_global_desc(fnames, device = torch.device('cpu')):\n#     processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n#     model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n#     model = model.eval()\n#     model = model.to(device)\n#     global_descs_dinov2 = []\n#     for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n#         key = os.path.splitext(os.path.basename(img_fname_full))[0]\n#         timg = load_torch_image(img_fname_full)\n#         with torch.inference_mode():\n#             inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n#             outputs = model(**inputs)\n#             dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n#         global_descs_dinov2.append(dino_mac.detach().cpu())\n#     global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n#     return global_descs_dinov2\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\n# def get_image_pairs_shortlist(fnames,\n#                               sim_th = 0.6, # should be strict\n#                               min_pairs = 30,\n#                               exhaustive_if_less = 20,\n#                               device=torch.device('cpu')):\n#     num_imgs = len(fnames)\n#     if num_imgs <= exhaustive_if_less:\n#         return get_img_pairs_exhaustive(fnames)\n#     descs = get_global_desc(fnames, device=device)\n#     dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n\n#     # 只分析上三角（去掉对角线），避免重复\n#     triu_indices = np.triu_indices_from(dm, k=1)\n#     dm_flat = dm[triu_indices]\n    \n#     # 打印统计信息\n#     print(\"Distance Matrix Statistics:\")\n#     print(f\"Min:  {dm_flat.min():.4f}\")\n#     print(f\"Max:  {dm_flat.max():.4f}\")\n#     print(f\"Mean: {dm_flat.mean():.4f}\")\n#     print(f\"Std:  {dm_flat.std():.4f}\")\n#     print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n#     print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n#     print(f\"50%:  {np.percentile(dm_flat, 50):.4f}\")\n#     print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n#     threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n#     # removing half\n#     # thrd = min(np.percentile(dm_flat, 60),sim_th)\n#     # print(f\"USED threshold: :  {thrd:.4f}\")\n#     mask = dm <= np.percentile(dm_flat, 50)\n#     total = 0\n#     matching_list = []\n#     ar = np.arange(num_imgs)\n#     already_there_set = []\n#     for st_idx in range(num_imgs-1):\n#         mask_idx = mask[st_idx]\n#         to_match = ar[mask_idx]\n#         if len(to_match) < min_pairs:\n#             to_match = np.argsort(dm[st_idx])[:min_pairs]  \n#         for idx in to_match:\n#             if st_idx == idx:\n#                 continue\n#             if dm[st_idx, idx] < threshold:\n#                 matching_list.append(tuple(sorted((st_idx, idx.item()))))\n#                 total+=1\n#     matching_list = sorted(list(set(matching_list)))\n#     return matching_list\n\ndef detect_aliked(img_fnames,\n                  feature_dir = '.featureout',\n                  num_features = 4096,\n                  resize_to = 1024,\n                  device=torch.device('cpu')):\n    dtype = torch.float32 # ALIKED has issues with float16\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n    extractor.preprocess_conf[\"resize\"] = resize_to\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    h5_kp_path = os.path.join(feature_dir, 'keypoints_aliked.h5')\n    h5_desc_path = os.path.join(feature_dir, 'descriptors_aliked.h5')\n    with h5py.File(h5_kp_path, mode='w') as f_kp, \\\n         h5py.File(h5_desc_path, mode='w') as f_desc:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return h5_kp_path, h5_desc_path\n\n# def match_with_lightglue_aliked(img_fnames,\n#                    index_pairs,\n#                    feature_dir = '.featureout',\n#                    device=torch.device('cpu'),\n#                    min_matches=20,verbose=True):\n#     lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n#                                                 \"depth_confidence\": -1,\n#                                                  \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n#     output_matches_path = os.path.join(feature_dir, 'matches_aliked.h5')\n#     with h5py.File(f'{feature_dir}/keypoints_aliked.h5', mode='r') as f_kp, \\\n#         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='r') as f_desc, \\\n#         h5py.File(output_matches_path, mode='w') as f_match:\n#         for pair_idx in tqdm(index_pairs):\n#             idx1, idx2 = pair_idx\n#             fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n#             key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n#             kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n#             kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n#             desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n#             desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n#             with torch.inference_mode():\n#                 dists, idxs = lg_matcher(desc1,\n#                                          desc2,\n#                                          KF.laf_from_center_scale_ori(kp1[None]),\n#                                          KF.laf_from_center_scale_ori(kp2[None]))\n#             if len(idxs)  == 0:\n#                 continue\n#             n_matches = len(idxs)\n#             if verbose:\n#                 print (f'{key1}-{key2}: {n_matches} matches')\n#             group  = f_match.require_group(key1)\n#             if n_matches >= min_matches:\n#                  group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n#     return output_matches_path\n\ndef match_with_lightglue_aliked(img_fnames,\n                                index_pairs,\n                                feature_kp_path, # Path to ALIKED keypoints H5\n                                feature_desc_path, # Path to ALIKED descriptors H5\n                                output_matches_h5='matches_aliked_lightglue.h5', # Specific output file\n                                device=torch.device('cpu'),\n                                min_matches=20,\n                                verbose=True):\n    \"\"\"\n    Performs feature matching using LightGlue with ALIKED descriptors.\n\n    Args:\n        img_fnames (list): List of original image file paths.\n        index_pairs (list): List of (idx1, idx2) tuples representing image pairs to match.\n        feature_kp_path (str): Path to the HDF5 file containing ALIKED keypoints.\n        feature_desc_path (str): Path to the HDF5 file containing ALIKED descriptors.\n        output_matches_h5 (str): Name of the HDF5 file to save the matches.\n        device (torch.device): Device to run the matcher on.\n        min_matches (int): Minimum number of matches required to save a pair.\n        verbose (bool): If True, print progress and match counts.\n\n    Returns:\n        str: Path to the HDF5 file containing the saved matches.\n    \"\"\"\n    # Initialize LightGlueMatcher for ALIKED features.\n    # Note: KF needs to be imported or mocked from Kornia.feature\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                \"depth_confidence\": -1,\n                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    \n    output_matches_path = os.path.join(os.path.dirname(feature_kp_path), output_matches_h5)\n    \n    with h5py.File(feature_kp_path, mode='r') as f_kp, \\\n         h5py.File(feature_desc_path, mode='r') as f_desc, \\\n         h5py.File(output_matches_path, mode='w') as f_match:\n        \n        for pair_idx in tqdm(index_pairs, desc=\"Matching ALIKED with LightGlue\"):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n\n            # Check if features exist for both images\n            if key1 not in f_kp or key2 not in f_kp or key1 not in f_desc or key2 not in f_desc:\n                if verbose:\n                    print(f\"Skipping {key1}-{key2}: features not found in HDF5.\")\n                continue\n\n            # Load keypoints and descriptors\n            # Convert to PyTorch tensors and move to device\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n\n            # Skip if either image has no keypoints or descriptors\n            if kp1.shape[0] == 0 or kp2.shape[0] == 0 or desc1.shape[0] == 0 or desc2.shape[0] == 0:\n                if verbose:\n                    print(f'{key1}-{key2}: Skipping due to no features detected.')\n                continue\n\n            with torch.inference_mode():\n                # Convert keypoints to Local Affine Frames (LAFs) as required by LightGlueMatcher\n                # Note: laf_from_center_scale_ori needs to be imported from kornia.geometry\n                lafs1 = KF.laf_from_center_scale_ori(kp1[None]) # [None] adds a batch dimension\n                lafs2 = KF.laf_from_center_scale_ori(kp2[None])\n\n                # Perform matching\n                dists, idxs = lg_matcher(desc1, desc2, lafs1, lafs2)\n            \n            # If no matches are found, continue to the next pair\n            if len(idxs) == 0:\n                continue\n            \n            n_matches = len(idxs)\n            if verbose:\n                print (f'{key1}-{key2}: {n_matches} matches')\n            \n            # Create a group for the first image if it doesn't exist\n            group = f_match.require_group(key1)\n            \n            # Save matches if they meet the minimum count\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n                 \n    print(f\"ALIKED + LightGlue matches saved to {output_matches_path}\")\n    return output_matches_path\n\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.843702Z","iopub.execute_input":"2025-05-22T00:18:26.844017Z","iopub.status.idle":"2025-05-22T00:18:26.865459Z","shell.execute_reply.started":"2025-05-22T00:18:26.843967Z","shell.execute_reply":"2025-05-22T00:18:26.864548Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# --- NEW: Detect DISK features ---\ndef detect_disk(img_fnames, feature_dir = '.featureout', num_features = 4096, resize_to = 1024, device=torch.device('cpu')):\n    \"\"\"\n    Detects DISK features using the LightGlue-provided DISK wrapper,\n    which accepts max_num_keypoints in its constructor and uses .extract().\n\n    Args:\n        img_fnames (list): List of image file paths.\n        feature_dir (str): Directory to save output HDF5 files.\n        num_features (int): Maximum number of features to detect per image.\n        resize_to (int): Image size to resize to for feature detection.\n        device (torch.device): Device to run the model on.\n\n    Returns:\n        tuple: Paths to the keypoints H5 file and descriptors H5 file.\n    \"\"\"\n    # Initialize DISK as it appears in the LightGlue library's usage pattern.\n    # This version correctly uses `max_num_keypoints` in the constructor\n    # and calls `.extract()` on the extractor.\n    extractor = DISK(max_num_keypoints=num_features, # This is the argument LightGlue's wrapper expects\n                     detection_threshold=0.001, # From notebook's CONFIG.params_disk_lightglue\n                     resize=resize_to # Pass resize to the constructor\n                    ).eval().to(device)\n    \n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n        \n    h5_kp_path = os.path.join(feature_dir, 'keypoints_disk.h5')\n    h5_desc_path = os.path.join(feature_dir, 'descriptors_disk.h5')\n\n    with h5py.File(h5_kp_path, mode='w') as f_kp, \\\n         h5py.File(h5_desc_path, mode='w') as f_desc:\n        for img_path in tqdm(img_fnames, desc=\"Detecting DISK features\"):\n            key = os.path.basename(img_path)\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device) # Load as Kornia expects (RGB, float, 0-1)\n                \n                # Call .extract() on the extractor, as seen in the notebook\n                feats0 = extractor.extract(image0) \n                \n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n\n                # Ensure empty arrays are correctly shaped if no features found\n                if len(kpts) == 0:\n                    kpts = np.array([], dtype=np.float32).reshape(0, 2)\n                    # Use the actual descriptor dimension found, or default to 256\n                    descs = np.array([], dtype=np.float32).reshape(0, descs.shape[-1] if descs.shape[-1] > 0 else 256)\n                \n                f_kp[key] = kpts\n                f_desc[key] = descs\n                \n    print(f\"DISK features saved to {h5_kp_path} and {h5_desc_path}\")\n    return h5_kp_path, h5_desc_path\n\n\n# --- MODIFIED: Detect SIFT features using OpenCV ---\ndef detect_sift(img_fnames,\n                feature_dir = '.featureout',\n                num_features = 4096,\n                resize_to = 1024, # SIFT in OpenCV usually works best on grayscale\n                device=torch.device('cpu')): # Device parameter is not directly used by OpenCV, but kept for consistency\n    \n    # Initialize OpenCV SIFT detector\n    # max_num_keypoints can be controlled via nfeatures\n    sift_extractor = cv2.SIFT_create(nfeatures=num_features)\n    \n    if not os.path.isdir(feature_dir): os.makedirs(feature_dir)\n    h5_kp_path = os.path.join(feature_dir, 'keypoints_sift.h5')\n    h5_desc_path = os.path.join(feature_dir, 'descriptors_sift.h5')\n\n    with h5py.File(h5_kp_path, mode='w') as f_kp, \\\n         h5py.File(h5_desc_path, mode='w') as f_desc:\n        for img_path in tqdm(img_fnames, desc=\"Detecting SIFT features (OpenCV)\"):\n            key = os.path.basename(img_path)\n            \n            # Load image using PIL, convert to grayscale NumPy array for OpenCV\n            pil_img = Image.open(img_path).convert('L') # Convert to grayscale\n            np_img = np.array(pil_img) # H, W, uint8\n\n            # Optional: Resize image before SIFT detection if resize_to is specified\n            # OpenCV SIFT handles scaling internally, but if you want to limit input size:\n            if resize_to is not None:\n                original_h, original_w = np_img.shape\n                # Calculate new dimensions preserving aspect ratio and fitting within resize_to\n                scale = resize_to / max(original_h, original_w)\n                new_w, new_h = int(original_w * scale), int(original_h * scale)\n                np_img = cv2.resize(np_img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n\n            # Detect keypoints and compute descriptors\n            # kp will be a list of cv2.KeyPoint objects\n            # descs will be a NumPy array of shape (num_keypoints, 128)\n            kp_list, descs = sift_extractor.detectAndCompute(np_img, None)\n\n            # Convert cv2.KeyPoint objects to a NumPy array of (x, y) coordinates\n            # KeyPoint.pt gives (x, y) tuple\n            kpts = np.array([kp.pt for kp in kp_list], dtype=np.float32).reshape(-1, 2)\n            \n            # If no descriptors, set to empty array (important for HDF5)\n            if descs is None:\n                descs = np.array([], dtype=np.float32).reshape(0, 128) # SIFT descriptors are 128-dim\n\n            # If resized, rescale keypoint coordinates back to original image size\n            if resize_to is not None and len(kpts) > 0:\n                scale_back_x = original_w / np_img.shape[1]\n                scale_back_y = original_h / np_img.shape[0]\n                kpts[:, 0] *= scale_back_x\n                kpts[:, 1] *= scale_back_y\n\n\n            f_kp[key] = kpts\n            f_desc[key] = descs\n    print(f\"SIFT features (OpenCV) saved to {h5_kp_path} and {h5_desc_path}\")\n    return h5_kp_path, h5_desc_path\n\n\n# # --- NEW: Detect SIFT features (using Kornia's SIFT, which wraps OpenCV/PyTorch backend) ---\n# def detect_sift(img_fnames,\n#                 feature_dir = '.featureout',\n#                 num_features = 4096,\n#                 resize_to = 1024,\n#                 device=torch.device('cpu')):\n#     # Kornia SIFT supports both torch and opencv backend. Using torch by default.\n#     extractor = KorniaSIFT(num_features=num_features,\n#                           upright=False, # Standard SIFT is not upright\n#                           edge_threshold=10,\n#                           pyr_levels=5,\n#                           sigma=1.6).eval().to(device)\n#     if not os.path.isdir(feature_dir):\n#         os.makedirs(feature_dir)\n#     # Use specific filenames for SIFT features\n#     h5_kp_path = os.path.join(feature_dir, 'keypoints_sift.h5')\n#     h5_desc_path = os.path.join(feature_dir, 'descriptors_sift.h5')\n\n#     with h5py.File(h5_kp_path, mode='w') as f_kp, \\\n#          h5py.File(h5_desc_path, mode='w') as f_desc:\n#         for img_path in tqdm(img_fnames, desc=\"Detecting SIFT features\"):\n#             img_fname = os.path.basename(img_path)\n#             key = img_fname\n#             with torch.inference_mode():\n#                 image0 = load_torch_image(img_path, device=device).mean(dim=0, keepdim=True) # SIFT usually works on grayscale, Kornia's SIFT expects 1-channel or 3-channel\n#                 # Make sure image is in float32 and [0,1] or [0,255] range as expected by Kornia SIFT.\n#                 # load_torch_image already gives float32 [0,1].\n#                 kpts_laf, descs = extractor(image0) # Kornia SIFT returns LAF and descriptors\n\n#                 # Convert LAF to simple (x,y) keypoints\n#                 # LAF is (1, N, 2, 3), keypoints are center (x,y) at [:, :, :, 2]\n#                 kpts = kpts_laf[0, :, :2, 2].detach().cpu().numpy() # [batch_idx, keypoint_idx, center_x_y, 3rd_dim_for_laf_matrix]\n#                 descs = descs[0].detach().cpu().numpy() # [batch_idx, descriptor_data]\n\n#                 f_kp[key] = kpts\n#                 f_desc[key] = descs\n#     print(f\"SIFT features saved to {h5_kp_path} and {h5_desc_path}\")\n#     return h5_kp_path, h5_desc_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.867521Z","iopub.execute_input":"2025-05-22T00:18:26.867821Z","iopub.status.idle":"2025-05-22T00:18:26.891223Z","shell.execute_reply.started":"2025-05-22T00:18:26.867786Z","shell.execute_reply":"2025-05-22T00:18:26.890342Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --- NEW: Match DISK with LightGlue ---\ndef match_with_lightglue_disk(img_fnames,\n                              index_pairs,\n                              feature_kp_path, # Path to DISK keypoints H5\n                              feature_desc_path, # Path to DISK descriptors H5\n                              output_matches_h5='matches_disk.h5', # Specific output file\n                              device=torch.device('cpu'),\n                              min_matches=20,\n                              verbose=True):\n    lg_matcher = KF_LightGlueMatcher(\"disk\", {\"width_confidence\": -1, # Using 'disk' type for LightGlue\n                                                \"depth_confidence\": -1,\n                                                \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    output_matches_path = os.path.join(os.path.dirname(feature_kp_path), output_matches_h5)\n    with h5py.File(feature_kp_path, mode='r') as f_kp, \\\n         h5py.File(feature_desc_path, mode='r') as f_desc, \\\n         h5py.File(output_matches_path, mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs, desc=\"Matching DISK with LightGlue\"):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n\n            if key1 not in f_kp or key2 not in f_kp or key1 not in f_desc or key2 not in f_desc:\n                if verbose: print(f\"Skipping {key1}-{key2}: features not found in HDF5.\")\n                continue\n\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n\n            if kp1.shape[0] == 0 or kp2.shape[0] == 0 or desc1.shape[0] == 0 or desc2.shape[0] == 0:\n                if verbose: print(f'{key1}-{key2}: Skipping due to no features detected.')\n                continue\n\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1,\n                                         desc2,\n                                         KF.laf_from_center_scale_ori(kp1[None]), # DISK returns (x,y), so same LAF conversion\n                                         KF.laf_from_center_scale_ori(kp2[None]))\n            if len(idxs)  == 0: continue\n            n_matches = len(idxs)\n            if verbose: print (f'{key1}-{key2}: {n_matches} matches')\n            group  = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n    print(f\"DISK + LightGlue matches saved to {output_matches_path}\")\n    return output_matches_path\n\n# --- NEW: Match SIFT with FLANN + Ratio Test ---\ndef match_sift_flann(img_fnames,\n                     index_pairs,\n                     feature_kp_path,        # Path to SIFT keypoints H5\n                     feature_desc_path,      # Path to SIFT descriptors H5\n                     output_matches_h5='matches_sift.h5',\n                     ratio_test_threshold=0.8,\n                     device=torch.device('cpu'),  # unused but preserved\n                     min_matches=20,\n                     verbose=True):\n\n    # FLANN index params for SIFT (uses KD-Tree)\n    index_params = dict(algorithm=1, trees=5)  # 1 = FLANN_INDEX_KDTREE\n    search_params = dict(checks=50)           # higher = more accurate but slower\n    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n\n    output_matches_path = os.path.join(os.path.dirname(feature_kp_path), output_matches_h5)\n\n    with h5py.File(feature_kp_path, mode='r') as f_kp, \\\n         h5py.File(feature_desc_path, mode='r') as f_desc, \\\n         h5py.File(output_matches_path, mode='w') as f_match:\n\n        for pair_idx in tqdm(index_pairs, desc=\"Matching SIFT with FLANN\"):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n\n            if key1 not in f_kp or key2 not in f_kp or key1 not in f_desc or key2 not in f_desc:\n                if verbose:\n                    print(f\"Skipping {key1}-{key2}: features not found in HDF5.\")\n                continue\n\n            desc1 = f_desc[key1][...].astype(np.float32)\n            desc2 = f_desc[key2][...].astype(np.float32)\n\n            if desc1.shape[0] == 0 or desc2.shape[0] == 0:\n                if verbose:\n                    print(f'{key1}-{key2}: Skipping due to no features detected.')\n                continue\n\n            # FLANN requires descriptors to be float32 and non-empty\n            try:\n                matches = matcher.knnMatch(desc1, desc2, k=2)\n            except cv2.error as e:\n                if verbose:\n                    print(f\"{key1}-{key2}: FLANN matching failed with error: {e}\")\n                continue\n\n            good_matches = [m for m, n in matches if m.distance < ratio_test_threshold * n.distance]\n\n            n_matches = len(good_matches)\n            if n_matches == 0:\n                continue\n\n            if verbose:\n                print(f'{key1}-{key2}: {n_matches} SIFT FLANN matches')\n\n            idxs = np.array([[m.queryIdx, m.trainIdx] for m in good_matches], dtype=np.int32)\n\n            group = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                group.create_dataset(key2, data=idxs)\n\n    print(f\"SIFT + FLANN matches saved to {output_matches_path}\")\n    return output_matches_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.892248Z","iopub.execute_input":"2025-05-22T00:18:26.892553Z","iopub.status.idle":"2025-05-22T00:18:26.911745Z","shell.execute_reply.started":"2025-05-22T00:18:26.892521Z","shell.execute_reply":"2025-05-22T00:18:26.910671Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Collect vital info from the dataset\n\n@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None\n\n# Set is_train=True to run the notebook on the training data.\n# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\nis_train = True\ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:26.912511Z","iopub.execute_input":"2025-05-22T00:18:26.912787Z","iopub.status.idle":"2025-05-22T00:18:27.070231Z","shell.execute_reply.started":"2025-05-22T00:18:26.912765Z","shell.execute_reply":"2025-05-22T00:18:27.069126Z"}},"outputs":[{"name":"stdout","text":"Dataset \"imc2023_haiper\" -> num_images=54\nDataset \"imc2023_heritage\" -> num_images=209\nDataset \"imc2023_theather_imc2024_church\" -> num_images=76\nDataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\nDataset \"imc2024_lizard_pond\" -> num_images=214\nDataset \"pt_brandenburg_british_buckingham\" -> num_images=225\nDataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\nDataset \"pt_stpeters_stpauls\" -> num_images=200\nDataset \"amy_gardens\" -> num_images=200\nDataset \"fbk_vineyard\" -> num_images=163\nDataset \"ETs\" -> num_images=22\nDataset \"stairs\" -> num_images=51\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- Core Ensemble and Remapping Function ---\ndef ensemble_and_remap_matches(img_fnames,\n                               match_h5_paths_by_detector, # Dict: {'method_name': 'path/to/matches.h5'}\n                               kp_h5_paths_by_detector, # Dict: {'detector_name': 'path/to/keypoints.h5'}\n                               output_unified_kp_h5='keypoints_unified.h5',\n                               output_remapped_matches_h5='matches_final_ensemble.h5',\n                               kpt_merge_threshold=2.0, # Pixels\n                               min_matches_per_pair=1):\n    print(\"\\n--- Ensembling and Remapping Matches ---\")\n\n    # Determine common output directory\n    output_dir = os.path.dirname(list(match_h5_paths_by_detector.values())[0])\n    unified_kp_path = os.path.join(output_dir, output_unified_kp_h5)\n    remapped_matches_path = os.path.join(output_dir, output_remapped_matches_h5)\n\n    image_keys = [os.path.basename(f) for f in img_fnames]\n\n    unified_keypoints_per_image = {}\n    old_to_new_index_maps_per_image = {}\n\n    # --- Phase 1: Create Unified Keypoints and Build Index Maps ---\n    for img_key in tqdm(image_keys, desc=\"Phase 1: Unifying keypoints per image\"):\n        all_kpts_for_img = []\n        \n        # Collect keypoints from all detectors for the current image\n        detector_kpts_list = [] # List of (detector_name, kpts_array)\n        for detector_name, kp_path in kp_h5_paths_by_detector.items():\n            with h5py.File(kp_path, 'r') as f_kp:\n                if img_key in f_kp and f_kp[img_key].shape[0] > 0:\n                    kpts_detector = f_kp[img_key][...]\n                    all_kpts_for_img.append(kpts_detector)\n                    detector_kpts_list.append((detector_name, kpts_detector))\n        \n        if not all_kpts_for_img:\n            unified_keypoints_per_image[img_key] = np.array([], dtype=np.float32).reshape(0,2) # Ensure empty (0,2) array\n            old_to_new_index_maps_per_image[img_key] = {}\n            continue\n\n        concatenated_kpts = np.concatenate(all_kpts_for_img, axis=0)\n        \n        # Build KDTree for efficient merging\n        tree = cKDTree(concatenated_kpts)\n        close_pairs = tree.query_pairs(kpt_merge_threshold)\n\n        # Disjoint Set Union (DSU) for clustering close keypoints\n        parent = list(range(len(concatenated_kpts)))\n        def find(i):\n            if parent[i] == i: return i\n            parent[i] = find(parent[i])\n            return parent[i]\n        def union(i, j):\n            root_i, root_j = find(i), find(j)\n            if root_i != root_j: parent[root_j] = root_i; return True\n            return False\n        for i, j in close_pairs: union(i, j)\n\n        unified_kpts_raw_groups = {} # root_idx -> list of kpts\n        for i in range(len(concatenated_kpts)):\n            root_idx = find(i)\n            if root_idx not in unified_kpts_raw_groups:\n                unified_kpts_raw_groups[root_idx] = []\n            unified_kpts_raw_groups[root_idx].append(concatenated_kpts[i])\n\n        final_unified_kpts_list = []\n        unified_idx_map = {} # root_idx -> new_unified_idx\n        for root_idx, kpts_group in unified_kpts_raw_groups.items():\n            unified_pos = np.mean(kpts_group, axis=0) # Average position\n            new_unified_idx = len(final_unified_kpts_list)\n            final_unified_kpts_list.append(unified_pos)\n            unified_idx_map[root_idx] = new_unified_idx\n        \n        final_unified_kpts = np.array(final_unified_kpts_list, dtype=np.float32)\n        unified_keypoints_per_image[img_key] = final_unified_kpts\n\n        # Build old_idx -> new_idx mapping for each original detector's keypoints\n        detector_to_new_idx_map = {}\n        current_kpt_offset = 0 # Offset into concatenated_kpts\n        for detector_name, original_kpts_array in detector_kpts_list:\n            old_to_new_map_for_detector = np.full(original_kpts_array.shape[0], -1, dtype=np.int32)\n            for original_local_idx in range(original_kpts_array.shape[0]):\n                concatenated_idx = current_kpt_offset + original_local_idx\n                root_idx = find(concatenated_idx)\n                old_to_new_map_for_detector[original_local_idx] = unified_idx_map[root_idx]\n            detector_to_new_idx_map[detector_name] = old_to_new_map_for_detector\n            current_kpt_offset += original_kpts_array.shape[0]\n        old_to_new_index_maps_per_image[img_key] = detector_to_new_idx_map\n\n    # Save unified keypoints\n    with h5py.File(unified_kp_path, 'w') as f_unified_kp:\n        for img_key, unified_kpts_array in unified_keypoints_per_image.items():\n            f_unified_kp.create_dataset(img_key, data=unified_kpts_array)\n    print(f\"Unified keypoints saved to {unified_kp_path}\")\n\n    # --- Phase 2: Remap and Ensemble Matches ---\n    final_ensembled_remapped_matches_dict = {} # Key: (img1_key, img2_key) -> set of (new_idx1, new_idx2)\n\n    # Need image_keys to iterate over pairs in a consistent order\n    for i in tqdm(range(len(image_keys)), desc=\"Phase 2: Remapping and Ensembling Matches\"):\n        img1_key = image_keys[i]\n        for j in range(i + 1, len(image_keys)):\n            img2_key = image_keys[j]\n\n            current_pair_remapped_matches_set = set() # Use a set to handle union and avoid duplicates\n\n            # Iterate through each individual matcher's results\n            for method_name, match_path in match_h5_paths_by_detector.items():\n                detector_name = method_name.split('_')[0] # e.g., 'aliked', 'disk', 'sift', 'dedode'\n\n                with h5py.File(match_path, 'r') as f_current_matches:\n                    if img1_key in f_current_matches and img2_key in f_current_matches[img1_key]:\n                        original_matches = f_current_matches[img1_key][img2_key][...] # (N, 2)\n                        \n                        # Get the remapping maps for this image pair and this detector\n                        map1 = old_to_new_index_maps_per_image[img1_key].get(detector_name)\n                        map2 = old_to_new_index_maps_per_image[img2_key].get(detector_name)\n\n                        # Check if mappings exist and are not empty\n                        if map1 is not None and map2 is not None and map1.shape[0] > 0 and map2.shape[0] > 0:\n                            for orig_idx1, orig_idx2 in original_matches:\n                                # Ensure original indices are within bounds of the map\n                                if orig_idx1 < map1.shape[0] and orig_idx2 < map2.shape[0]:\n                                    new_idx1 = map1[orig_idx1]\n                                    new_idx2 = map2[orig_idx2]\n                                    if new_idx1 != -1 and new_idx2 != -1: # Valid remapped indices\n                                        current_pair_remapped_matches_set.add(tuple(sorted((new_idx1, new_idx2)))) # Store sorted for canonical form\n                                else:\n                                    # This indicates a potential issue with original match indices or mappings\n                                    # print(f\"Warning: Match index out of bounds for {img1_key}-{img2_key} from {method_name}\")\n                                    pass # Skip problematic match\n\n\n            if len(current_pair_remapped_matches_set) >= min_matches_per_pair:\n                final_ensembled_remapped_matches_dict[(img1_key, img2_key)] = list(current_pair_remapped_matches_set)\n\n    # Save the final ensembled and remapped matches\n    with h5py.File(remapped_matches_path, 'w') as f_remapped_matches:\n        for (img1_key, img2_key), matches_list in tqdm(final_ensembled_remapped_matches_dict.items(), desc=\"Saving remapped ensembled matches\"):\n            if matches_list:\n                matches_array = np.array(matches_list, dtype=np.int32)\n                group = f_remapped_matches.require_group(img1_key)\n                group.create_dataset(img2_key, data=matches_array)\n            else: # Create empty dataset if no matches, for consistency\n                group = f_remapped_matches.require_group(img1_key)\n                group.create_dataset(img2_key, data=np.array([]), dtype=np.int32, shape=(0,2))\n\n    print(f\"Ensembled and remapped matches saved to {remapped_matches_path}\")\n    return unified_kp_path, remapped_matches_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:27.071324Z","iopub.execute_input":"2025-05-22T00:18:27.071640Z","iopub.status.idle":"2025-05-22T00:18:27.089470Z","shell.execute_reply.started":"2025-05-22T00:18:27.071587Z","shell.execute_reply":"2025-05-22T00:18:27.088364Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\n\nmax_images = None  # Used For debugging only. Set to None to disable.\ndatasets_to_process = None  # Not the best convention, but None means all datasets.\n\nif is_train:\n    # max_images = 5\n\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n    \t# New data.\n    \t# 'amy_gardens',\n    \t'ETs',\n    \t# 'fbk_vineyard',\n    \t# 'stairs',\n    \t# Data from IMC 2023 and 2024.\n    \t# 'imc2024_dioscuri_baalshamin',\n    \t# 'imc2023_theather_imc2024_church',\n    \t# 'imc2023_heritage',\n    \t# 'imc2023_haiper',\n    \t# 'imc2024_lizard_pond',\n    \t# Crowdsourced PhotoTourism data.\n    \t# 'pt_stpeters_stpauls',\n    \t# 'pt_brandenburg_british_buckingham',\n    \t# 'pt_piazzasanmarco_grandplace',\n    \t# 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"global feature extraction\":[],\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n# Load DINOv2 model (for feature extraction, not global descriptor here)\nprint(\"Loading DINOv2 model for patch feature extraction...\")\ndino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\ndino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\ndino_model = dino_model.eval().to(device)\nprint(\"DINOv2 model loaded.\")\n\nprint (f\"Extracting on device {device}\")\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n    try:\n\n    # --- Pipeline Execution ---\n\n        # 1. Detect ALIKED features and combine with DINO patch features\n        t = time()\n        print(\"\\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\")\n        detect_aliked_and_combine_with_dino(\n            img_fnames=images,\n            feature_dir=feature_dir,\n            num_features=4096,\n            resize_to=1024,\n            dino_processor=dino_processor,\n            dino_model=dino_model,\n            dino_patch_size=14, # Adjust based on your DINO model's patch size (e.g., 14 for DINOv2 base)\n            device=device\n        )\n        timings['global feature extraction'].append(time() - t)\n        print (f'Gloabl feature extracting. Done in {time() - t:.4f} sec')\n        gc.collect()\n        \n        # 2. Get image pairs shortlist using VLAD global descriptors\n        print(\"\\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\")\n        # Adjust num_clusters_vlad as needed (e.g., 64, 128, 256)\n        # Higher clusters mean higher dimensionality for global descriptor.\n        index_pairs = get_image_pairs_shortlist_vlad(\n            fnames=images,\n            sim_th=0.1,\n            min_pairs=10,\n            exhaustive_if_less=20,\n            feature_dir=feature_dir,\n            num_clusters_vlad=128, # Example: 128 clusters for VLAD\n            device=device\n        )\n        print(f\"Generated {len(index_pairs)} image pairs using VLAD global descriptor.\")\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n    \n        t = time()\n\n        # --- Step 3.1: Detect Features for each type ---\n        print(\"\\n--- Step 3.1: Detecting Features ---\")\n        kp_h5_paths_by_detector = {}\n        desc_h5_paths_by_detector = {}\n        aliked_kp_path, aliked_desc_path = detect_aliked(images, feature_dir=feature_dir, device=device)\n        kp_h5_paths_by_detector['aliked'] = aliked_kp_path\n        desc_h5_paths_by_detector['aliked'] = aliked_desc_path\n        gc.collect()\n        print(\"detect aliked done\")\n        disk_kp_path, disk_desc_path = detect_disk(images, feature_dir=feature_dir, device=device)\n        kp_h5_paths_by_detector['disk'] = disk_kp_path\n        desc_h5_paths_by_detector['disk'] = disk_desc_path\n        print(\"detect disk done\")\n        gc.collect()\n        sift_kp_path, sift_desc_path = detect_sift(images, feature_dir=feature_dir, device=device)\n        kp_h5_paths_by_detector['sift'] = sift_kp_path\n        desc_h5_paths_by_detector['sift'] = sift_desc_path\n        print(\"detect sift done\")\n        gc.collect()\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n        \n        # Dedode v2 features would be detected here if you had the code for it.\n        # dedode_v2_kp_path, dedode_v2_desc_path = detect_dedode_v2(...)\n    \n    \n        # --- Step 3.2: Perform Matching for each configuration ---\n        print(\"\\n--- Step 3.2: Performing Individual Matchings ---\")\n        match_paths = []\n        \n        match_h5_paths_by_detector = {}\n    \n        # 3.2.1. ALIKED + LightGlue (your baseline)\n        print(\"Matching ALIKED + LightGlue...\")\n        t = time()\n        matches_aliked_lg_path = match_with_lightglue_aliked(\n            images, index_pairs,\n            feature_kp_path=aliked_kp_path, feature_desc_path=aliked_desc_path,\n            output_matches_h5='matches_aliked_lightglue.h5',\n            device=device\n        )\n        \n        match_h5_paths_by_detector['aliked'] = matches_aliked_lg_path # Use 'aliked' as key for consistency with kp_h5_paths\n        gc.collect()\n        \n        # 3.2.2. DISK + LightGlue\n        print(\"Matching DISK + LightGlue...\")\n        matches_disk_lg_path = match_with_lightglue_disk(\n            images, index_pairs,\n            feature_kp_path=disk_kp_path, feature_desc_path=disk_desc_path,\n            output_matches_h5='matches_disk_lightglue.h5',\n            device=device\n        )\n        match_h5_paths_by_detector['disk'] = matches_disk_lg_path\n        gc.collect()\n        \n        # 3.2.3. SIFT + Nearest Neighbor\n        print(\"Matching SIFT + Nearest Neighbor...\")\n        matches_sift_nn_path = match_sift_flann(\n            images, index_pairs,\n            feature_kp_path=sift_kp_path, feature_desc_path=sift_desc_path,\n            output_matches_h5='matches_sift_nn.h5',\n            device=device # This param is mostly ignored for OpenCV, but for consistency\n        )\n        match_h5_paths_by_detector['sift'] = matches_sift_nn_path\n        gc.collect()\n        \n        # # 3.2.4. Dedode v2 + Dual Softmax (Placeholder)\n        # print(\"Matching Dedode v2 + Dual Softmax (Placeholder)...\")\n        # matches_dedode_v2_ds_path = match_dedode_v2_dual_softmax(\n        #     img_fnames, index_pairs_to_match,\n        #     feature_dir=feature_output_dir, # Assuming Dedode v2 features are saved here\n        #     output_matches_h5='matches_dedode_v2_dualsoftmax.h5',\n        #     device=device\n        # )\n        # match_paths.append(matches_dedode_v2_ds_path)\n    \n    \n        # --- Step 3.3: Ensemble the Matching Results ---\n        print(\"\\n--- Step 3.3: Ensembling and Remapping all matching results ---\")\n        unified_kp_path, remapped_matches_path = ensemble_and_remap_matches(\n            images,\n            match_h5_paths_by_detector=match_h5_paths_by_detector,\n            kp_h5_paths_by_detector=kp_h5_paths_by_detector,\n            output_unified_kp_h5='keypoints.h5',\n            output_remapped_matches_h5='matches.h5',\n            kpt_merge_threshold=2.0 # Merge keypoints closer than 2 pixels\n        )\n        print(f\"Final unified keypoints saved to: {unified_kp_path}\")\n        print(f\"Final ensembled and remapped matches saved to: {remapped_matches_path}\")\n\n\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n        gc.collect()\n        # --- Step 3.4: Import into COLMAP (using the ensembled matches) ---\n        print(\"\\n--- Step 3.4: Importing ensembled results into COLMAP ---\")\n        database_path = os.path.join(feature_dir, 'colmap.db')\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n        gc.collect()\n        sleep(1)\n        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n        output_path = f'{feature_dir}/colmap_rec_aliked'\n        \n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        timings['RANSAC'].append(time() - t)\n        print(f'Ran RANSAC in {time() - t:.4f} sec')\n        \n        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n        # Lower it to 3.\n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 8\n        mapper_options.max_num_models = 25\n        # mapper_options.mapper.filter_max_reproj_error\t = 9.0\n\n        os.makedirs(output_path, exist_ok=True)\n        t = time()\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path, \n            image_path=images_dir,\n            output_path=output_path,\n            options=mapper_options)\n        sleep(1)\n        timings['Reconstruction'].append(time() - t)\n        print(f'Reconstruction done in  {time() - t:.4f} sec')\n        print(maps)\n\n        # clear_output(wait=False)\n    \n        registered = 0\n        for map_index, cur_map in maps.items():\n            for index, image in cur_map.images.items():\n                prediction_index = filename_to_index[image.name]\n                predictions[prediction_index].cluster_index = map_index\n                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                registered += 1\n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T00:18:27.090520Z","iopub.execute_input":"2025-05-22T00:18:27.090867Z","execution_failed":"2025-05-22T00:19:28.505Z"}},"outputs":[{"name":"stdout","text":"Loading DINOv2 model for patch feature extraction...\nDINOv2 model loaded.\nExtracting on device cuda:0\nSkipping \"imc2023_haiper\"\nSkipping \"imc2023_heritage\"\nSkipping \"imc2023_theather_imc2024_church\"\nSkipping \"imc2024_dioscuri_baalshamin\"\nSkipping \"imc2024_lizard_pond\"\nSkipping \"pt_brandenburg_british_buckingham\"\nSkipping \"pt_piazzasanmarco_grandplace\"\nSkipping \"pt_sacrecoeur_trevi_tajmahal\"\nSkipping \"pt_stpeters_stpauls\"\nSkipping \"amy_gardens\"\nSkipping \"fbk_vineyard\"\n\nProcessing dataset \"ETs\": 22 images\n\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22/22 [00:03<00:00,  7.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Combined features saved to /kaggle/working/result/featureout/ETs/descriptors_combined.h5\nGloabl feature extracting. Done in 3.3335 sec\n\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\n","output_type":"stream"},{"name":"stderr","text":"Loading combined local descriptors for K-Means: 100%|██████████| 22/22 [00:00<00:00, 206.08it/s]","output_type":"stream"},{"name":"stdout","text":"Training K-Means with 128 clusters on 42485 descriptors...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"K-Means training complete.\n","output_type":"stream"},{"name":"stderr","text":"Computing VLAD descriptors: 100%|██████████| 22/22 [00:13<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  1.2267\nMax:  1.6611\nMean: 1.4444\nStd:  0.0494\n20%:  1.4192\n25%:  1.4215\n50%:  1.4318\n75%:  1.4588\nGenerated 124 image pairs using VLAD global descriptor.\nShortlisting. Number of pairs to match: 124. Done in 18.6404 sec\n\n--- Step 3.1: Detecting Features ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22/22 [00:01<00:00, 21.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"detect aliked done\n","output_type":"stream"},{"name":"stderr","text":"Detecting DISK features: 100%|██████████| 22/22 [00:03<00:00,  5.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"DISK features saved to /kaggle/working/result/featureout/ETs/keypoints_disk.h5 and /kaggle/working/result/featureout/ETs/descriptors_disk.h5\ndetect disk done\n","output_type":"stream"},{"name":"stderr","text":"Detecting SIFT features (OpenCV): 100%|██████████| 22/22 [00:04<00:00,  4.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"SIFT features (OpenCV) saved to /kaggle/working/result/featureout/ETs/keypoints_sift.h5 and /kaggle/working/result/featureout/ETs/descriptors_sift.h5\ndetect sift done\nFeatures detected in 10.6268 sec\n\n--- Step 3.2: Performing Individual Matchings ---\nMatching ALIKED + LightGlue...\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:   2%|▏         | 3/124 [00:00<00:09, 12.25it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-outliers_out_et003.png: 11 matches\noutliers_out_et001.png-et_et007.png: 18 matches\noutliers_out_et001.png-et_et004.png: 11 matches\noutliers_out_et001.png-et_et002.png: 25 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:   6%|▋         | 8/124 [00:00<00:05, 19.44it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-et_et008.png: 21 matches\noutliers_out_et001.png-et_et005.png: 24 matches\noutliers_out_et001.png-another_et_another_et006.png: 41 matches\noutliers_out_et001.png-another_et_another_et002.png: 49 matches\noutliers_out_et001.png-another_et_another_et004.png: 23 matches\noutliers_out_et001.png-another_et_another_et007.png: 21 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  12%|█▏        | 15/124 [00:00<00:04, 25.04it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et008.png: 14 matches\noutliers_out_et001.png-another_et_another_et003.png: 42 matches\noutliers_out_et001.png-another_et_another_et005.png: 49 matches\noutliers_out_et001.png-another_et_another_et001.png: 66 matches\noutliers_out_et003.png-outliers_out_et002.png: 25 matches\noutliers_out_et003.png-et_et003.png: 45 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  17%|█▋        | 21/124 [00:00<00:04, 23.69it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-et_et006.png: 6 matches\noutliers_out_et003.png-et_et001.png: 47 matches\noutliers_out_et003.png-et_et002.png: 28 matches\noutliers_out_et003.png-another_et_another_et010.png: 19 matches\noutliers_out_et003.png-another_et_another_et005.png: 10 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  22%|██▏       | 27/124 [00:01<00:03, 25.49it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-another_et_another_et009.png: 30 matches\noutliers_out_et002.png-et_et008.png: 17 matches\noutliers_out_et002.png-another_et_another_et006.png: 42 matches\noutliers_out_et002.png-another_et_another_et002.png: 8 matches\noutliers_out_et002.png-another_et_another_et010.png: 40 matches\noutliers_out_et002.png-another_et_another_et004.png: 10 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  27%|██▋       | 33/124 [00:01<00:03, 26.33it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et008.png: 20 matches\noutliers_out_et002.png-another_et_another_et003.png: 11 matches\noutliers_out_et002.png-another_et_another_et005.png: 14 matches\noutliers_out_et002.png-another_et_another_et001.png: 10 matches\noutliers_out_et002.png-another_et_another_et009.png: 28 matches\net_et007.png-et_et006.png: 1385 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  32%|███▏      | 40/124 [00:01<00:02, 28.42it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-et_et005.png: 1298 matches\net_et007.png-another_et_another_et006.png: 42 matches\net_et007.png-another_et_another_et010.png: 7 matches\net_et007.png-another_et_another_et008.png: 31 matches\net_et007.png-another_et_another_et003.png: 43 matches\net_et007.png-another_et_another_et005.png: 49 matches\net_et007.png-another_et_another_et009.png: 5 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  37%|███▋      | 46/124 [00:01<00:02, 27.59it/s]","output_type":"stream"},{"name":"stdout","text":"et_et003.png-et_et000.png: 1751 matches\net_et003.png-another_et_another_et006.png: 37 matches\net_et003.png-another_et_another_et002.png: 21 matches\net_et003.png-another_et_another_et010.png: 28 matches\net_et003.png-another_et_another_et007.png: 38 matches\net_et003.png-another_et_another_et008.png: 28 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  40%|████      | 50/124 [00:02<00:02, 28.63it/s]","output_type":"stream"},{"name":"stdout","text":"et_et003.png-another_et_another_et005.png: 24 matches\net_et003.png-another_et_another_et009.png: 23 matches\net_et006.png-another_et_another_et006.png: 49 matches\net_et006.png-another_et_another_et002.png: 83 matches\net_et006.png-another_et_another_et010.png: 30 matches\net_et006.png-another_et_another_et007.png: 57 matches\net_et006.png-another_et_another_et008.png: 23 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  46%|████▌     | 57/124 [00:02<00:02, 28.54it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et005.png: 58 matches\net_et006.png-another_et_another_et001.png: 65 matches\net_et006.png-another_et_another_et009.png: 5 matches\net_et001.png-et_et002.png: 1447 matches\net_et001.png-another_et_another_et006.png: 39 matches\net_et001.png-another_et_another_et010.png: 15 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  51%|█████     | 63/124 [00:02<00:02, 27.16it/s]","output_type":"stream"},{"name":"stdout","text":"et_et001.png-another_et_another_et004.png: 47 matches\net_et001.png-another_et_another_et007.png: 59 matches\net_et001.png-another_et_another_et008.png: 21 matches\net_et001.png-another_et_another_et005.png: 41 matches\net_et001.png-another_et_another_et009.png: 10 matches\net_et004.png-another_et_another_et006.png: 20 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  56%|█████▌    | 69/124 [00:02<00:02, 26.55it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et002.png: 16 matches\net_et004.png-another_et_another_et010.png: 10 matches\net_et004.png-another_et_another_et004.png: 14 matches\net_et004.png-another_et_another_et007.png: 17 matches\net_et004.png-another_et_another_et008.png: 22 matches\net_et004.png-another_et_another_et003.png: 12 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  60%|██████    | 75/124 [00:02<00:01, 25.84it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et005.png: 26 matches\net_et004.png-another_et_another_et001.png: 13 matches\net_et004.png-another_et_another_et009.png: 16 matches\net_et002.png-another_et_another_et006.png: 55 matches\net_et002.png-another_et_another_et002.png: 21 matches\net_et002.png-another_et_another_et010.png: 38 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  65%|██████▌   | 81/124 [00:03<00:01, 27.10it/s]","output_type":"stream"},{"name":"stdout","text":"et_et002.png-another_et_another_et004.png: 53 matches\net_et002.png-another_et_another_et007.png: 41 matches\net_et002.png-another_et_another_et008.png: 26 matches\net_et002.png-another_et_another_et005.png: 59 matches\net_et002.png-another_et_another_et001.png: 54 matches\net_et002.png-another_et_another_et009.png: 22 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  70%|███████   | 87/124 [00:03<00:01, 27.91it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et006.png: 14 matches\net_et008.png-another_et_another_et002.png: 36 matches\net_et008.png-another_et_another_et010.png: 42 matches\net_et008.png-another_et_another_et007.png: 34 matches\net_et008.png-another_et_another_et008.png: 13 matches\net_et008.png-another_et_another_et003.png: 28 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  75%|███████▌  | 93/124 [00:03<00:01, 28.36it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et005.png: 48 matches\net_et008.png-another_et_another_et001.png: 36 matches\net_et008.png-another_et_another_et009.png: 11 matches\net_et005.png-another_et_another_et006.png: 35 matches\net_et005.png-another_et_another_et010.png: 15 matches\net_et005.png-another_et_another_et007.png: 43 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  81%|████████  | 100/124 [00:03<00:00, 28.82it/s]","output_type":"stream"},{"name":"stdout","text":"et_et005.png-another_et_another_et008.png: 36 matches\net_et005.png-another_et_another_et005.png: 61 matches\net_et005.png-another_et_another_et001.png: 52 matches\net_et005.png-another_et_another_et009.png: 14 matches\net_et000.png-another_et_another_et006.png: 29 matches\net_et000.png-another_et_another_et002.png: 16 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  85%|████████▌ | 106/124 [00:04<00:00, 27.08it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et010.png: 7 matches\net_et000.png-another_et_another_et004.png: 17 matches\net_et000.png-another_et_another_et007.png: 27 matches\net_et000.png-another_et_another_et008.png: 32 matches\net_et000.png-another_et_another_et005.png: 26 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  91%|█████████ | 113/124 [00:04<00:00, 28.35it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et001.png: 26 matches\net_et000.png-another_et_another_et009.png: 25 matches\nanother_et_another_et006.png-another_et_another_et002.png: 407 matches\nanother_et_another_et006.png-another_et_another_et004.png: 367 matches\nanother_et_another_et006.png-another_et_another_et007.png: 424 matches\nanother_et_another_et006.png-another_et_another_et005.png: 310 matches\nanother_et_another_et006.png-another_et_another_et001.png: 400 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue:  97%|█████████▋| 120/124 [00:04<00:00, 29.71it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et002.png-another_et_another_et004.png: 726 matches\nanother_et_another_et002.png-another_et_another_et005.png: 672 matches\nanother_et_another_et002.png-another_et_another_et001.png: 980 matches\nanother_et_another_et010.png-another_et_another_et003.png: 22 matches\nanother_et_another_et004.png-another_et_another_et003.png: 525 matches\nanother_et_another_et004.png-another_et_another_et001.png: 709 matches\nanother_et_another_et007.png-another_et_another_et008.png: 427 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching ALIKED with LightGlue: 100%|██████████| 124/124 [00:04<00:00, 26.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"another_et_another_et008.png-another_et_another_et003.png: 98 matches\nanother_et_another_et008.png-another_et_another_et009.png: 358 matches\nanother_et_another_et003.png-another_et_another_et009.png: 82 matches\nanother_et_another_et005.png-another_et_another_et001.png: 757 matches\nALIKED + LightGlue matches saved to /kaggle/working/result/featureout/ETs/matches_aliked_lightglue.h5\nMatching DISK + LightGlue...\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:   2%|▏         | 2/124 [00:00<00:11, 10.51it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-outliers_out_et003.png: 1 matches\noutliers_out_et001.png-et_et004.png: 1 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:   5%|▍         | 6/124 [00:00<00:11, 10.33it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-et_et002.png: 9 matches\noutliers_out_et001.png-et_et008.png: 2 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:   6%|▋         | 8/124 [00:00<00:11, 10.42it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et006.png: 3 matches\noutliers_out_et001.png-another_et_another_et002.png: 101 matches\noutliers_out_et001.png-another_et_another_et004.png: 7 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  10%|▉         | 12/124 [00:01<00:10, 10.74it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et007.png: 1 matches\noutliers_out_et001.png-another_et_another_et008.png: 3 matches\noutliers_out_et001.png-another_et_another_et003.png: 4 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  11%|█▏        | 14/124 [00:01<00:10, 10.66it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et005.png: 121 matches\noutliers_out_et001.png-another_et_another_et001.png: 96 matches\noutliers_out_et003.png-outliers_out_et002.png: 4 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  15%|█▍        | 18/124 [00:01<00:10, 10.47it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-et_et003.png: 7 matches\noutliers_out_et003.png-et_et006.png: 8 matches\noutliers_out_et003.png-et_et001.png: 4 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  16%|█▌        | 20/124 [00:01<00:09, 10.70it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-et_et002.png: 1 matches\noutliers_out_et003.png-another_et_another_et010.png: 3 matches\noutliers_out_et003.png-another_et_another_et005.png: 13 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  19%|█▉        | 24/124 [00:02<00:09, 10.81it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-another_et_another_et009.png: 10 matches\noutliers_out_et002.png-et_et008.png: 2 matches\noutliers_out_et002.png-another_et_another_et006.png: 28 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  21%|██        | 26/124 [00:02<00:08, 10.99it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et002.png: 41 matches\noutliers_out_et002.png-another_et_another_et010.png: 2 matches\noutliers_out_et002.png-another_et_another_et004.png: 37 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  24%|██▍       | 30/124 [00:02<00:08, 10.92it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et008.png: 16 matches\noutliers_out_et002.png-another_et_another_et003.png: 8 matches\noutliers_out_et002.png-another_et_another_et005.png: 78 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  26%|██▌       | 32/124 [00:02<00:08, 10.95it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et001.png: 157 matches\noutliers_out_et002.png-another_et_another_et009.png: 2 matches\net_et007.png-et_et006.png: 2765 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  29%|██▉       | 36/124 [00:03<00:08, 11.00it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-et_et005.png: 2499 matches\net_et007.png-another_et_another_et006.png: 6 matches\net_et007.png-another_et_another_et010.png: 14 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  31%|███       | 38/124 [00:03<00:07, 10.94it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-another_et_another_et008.png: 1 matches\net_et007.png-another_et_another_et005.png: 19 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  34%|███▍      | 42/124 [00:03<00:07, 10.92it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-another_et_another_et009.png: 2 matches\net_et003.png-et_et000.png: 2896 matches\net_et003.png-another_et_another_et006.png: 9 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  35%|███▌      | 44/124 [00:04<00:07, 11.05it/s]","output_type":"stream"},{"name":"stdout","text":"et_et003.png-another_et_another_et002.png: 7 matches\net_et003.png-another_et_another_et010.png: 6 matches\net_et003.png-another_et_another_et007.png: 10 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  39%|███▊      | 48/124 [00:04<00:06, 11.18it/s]","output_type":"stream"},{"name":"stdout","text":"et_et003.png-another_et_another_et008.png: 5 matches\net_et003.png-another_et_another_et005.png: 5 matches\net_et003.png-another_et_another_et009.png: 7 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  40%|████      | 50/124 [00:04<00:06, 11.01it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et006.png: 12 matches\net_et006.png-another_et_another_et002.png: 4 matches\net_et006.png-another_et_another_et010.png: 5 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  44%|████▎     | 54/124 [00:04<00:06, 11.22it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et007.png: 4 matches\net_et006.png-another_et_another_et008.png: 2 matches\net_et006.png-another_et_another_et005.png: 8 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  45%|████▌     | 56/124 [00:05<00:06, 11.15it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et001.png: 3 matches\net_et006.png-another_et_another_et009.png: 9 matches\net_et001.png-et_et002.png: 2379 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  48%|████▊     | 60/124 [00:05<00:05, 11.12it/s]","output_type":"stream"},{"name":"stdout","text":"et_et001.png-another_et_another_et006.png: 3 matches\net_et001.png-another_et_another_et010.png: 5 matches\net_et001.png-another_et_another_et004.png: 13 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  50%|█████     | 62/124 [00:05<00:05, 11.14it/s]","output_type":"stream"},{"name":"stdout","text":"et_et001.png-another_et_another_et007.png: 4 matches\net_et001.png-another_et_another_et008.png: 11 matches\net_et001.png-another_et_another_et005.png: 2 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  53%|█████▎    | 66/124 [00:06<00:05, 11.01it/s]","output_type":"stream"},{"name":"stdout","text":"et_et001.png-another_et_another_et009.png: 9 matches\net_et004.png-another_et_another_et006.png: 66 matches\net_et004.png-another_et_another_et002.png: 2 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  55%|█████▍    | 68/124 [00:06<00:04, 11.20it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et010.png: 1 matches\net_et004.png-another_et_another_et004.png: 8 matches\net_et004.png-another_et_another_et007.png: 2 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  58%|█████▊    | 72/124 [00:06<00:04, 11.05it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et008.png: 2 matches\net_et004.png-another_et_another_et003.png: 26 matches\net_et004.png-another_et_another_et005.png: 5 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  60%|█████▉    | 74/124 [00:06<00:04, 10.98it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et001.png: 15 matches\net_et004.png-another_et_another_et009.png: 7 matches\net_et002.png-another_et_another_et006.png: 7 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  63%|██████▎   | 78/124 [00:07<00:04, 11.09it/s]","output_type":"stream"},{"name":"stdout","text":"et_et002.png-another_et_another_et010.png: 16 matches\net_et002.png-another_et_another_et004.png: 11 matches\net_et002.png-another_et_another_et007.png: 1 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  66%|██████▌   | 82/124 [00:07<00:03, 10.93it/s]","output_type":"stream"},{"name":"stdout","text":"et_et002.png-another_et_another_et008.png: 4 matches\net_et002.png-another_et_another_et005.png: 10 matches\net_et002.png-another_et_another_et001.png: 11 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  68%|██████▊   | 84/124 [00:07<00:03, 11.02it/s]","output_type":"stream"},{"name":"stdout","text":"et_et002.png-another_et_another_et009.png: 9 matches\net_et008.png-another_et_another_et006.png: 17 matches\net_et008.png-another_et_another_et002.png: 3 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  71%|███████   | 88/124 [00:08<00:03, 11.10it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et010.png: 5 matches\net_et008.png-another_et_another_et007.png: 2 matches\net_et008.png-another_et_another_et008.png: 4 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  73%|███████▎  | 90/124 [00:08<00:03, 10.94it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et003.png: 4 matches\net_et008.png-another_et_another_et005.png: 7 matches\net_et008.png-another_et_another_et001.png: 4 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  76%|███████▌  | 94/124 [00:08<00:02, 11.14it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et009.png: 15 matches\net_et005.png-another_et_another_et006.png: 1 matches\net_et005.png-another_et_another_et010.png: 14 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  79%|███████▉  | 98/124 [00:08<00:02, 10.94it/s]","output_type":"stream"},{"name":"stdout","text":"et_et005.png-another_et_another_et008.png: 22 matches\net_et005.png-another_et_another_et005.png: 9 matches\net_et005.png-another_et_another_et001.png: 5 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  81%|████████  | 100/124 [00:09<00:02, 11.05it/s]","output_type":"stream"},{"name":"stdout","text":"et_et005.png-another_et_another_et009.png: 22 matches\net_et000.png-another_et_another_et006.png: 10 matches\net_et000.png-another_et_another_et002.png: 8 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  84%|████████▍ | 104/124 [00:09<00:01, 11.01it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et010.png: 13 matches\net_et000.png-another_et_another_et004.png: 5 matches\net_et000.png-another_et_another_et007.png: 2 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  85%|████████▌ | 106/124 [00:09<00:01, 11.07it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et008.png: 5 matches\net_et000.png-another_et_another_et005.png: 14 matches\net_et000.png-another_et_another_et001.png: 8 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  89%|████████▊ | 110/124 [00:10<00:01, 11.10it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et009.png: 10 matches\nanother_et_another_et006.png-another_et_another_et002.png: 1311 matches\nanother_et_another_et006.png-another_et_another_et004.png: 1115 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  90%|█████████ | 112/124 [00:10<00:01, 11.27it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et006.png-another_et_another_et007.png: 1284 matches\nanother_et_another_et006.png-another_et_another_et005.png: 1275 matches\nanother_et_another_et006.png-another_et_another_et001.png: 1360 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  94%|█████████▎| 116/124 [00:10<00:00, 10.96it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et002.png-another_et_another_et004.png: 1920 matches\nanother_et_another_et002.png-another_et_another_et005.png: 1875 matches\nanother_et_another_et002.png-another_et_another_et001.png: 2617 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  95%|█████████▌| 118/124 [00:10<00:00, 11.19it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et010.png-another_et_another_et003.png: 5 matches\nanother_et_another_et004.png-another_et_another_et003.png: 1532 matches\nanother_et_another_et004.png-another_et_another_et001.png: 1860 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue:  98%|█████████▊| 122/124 [00:11<00:00, 11.55it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et007.png-another_et_another_et008.png: 1204 matches\nanother_et_another_et008.png-another_et_another_et003.png: 252 matches\nanother_et_another_et008.png-another_et_another_et009.png: 1077 matches\n","output_type":"stream"},{"name":"stderr","text":"Matching DISK with LightGlue: 100%|██████████| 124/124 [00:11<00:00, 11.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"another_et_another_et003.png-another_et_another_et009.png: 66 matches\nanother_et_another_et005.png-another_et_another_et001.png: 1977 matches\nDISK + LightGlue matches saved to /kaggle/working/result/featureout/ETs/matches_disk_lightglue.h5\nMatching SIFT + Nearest Neighbor...\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:   2%|▏         | 3/124 [00:00<00:04, 24.30it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-outliers_out_et003.png: 47 SIFT FLANN matches\noutliers_out_et001.png-et_et007.png: 50 SIFT FLANN matches\noutliers_out_et001.png-et_et004.png: 53 SIFT FLANN matches\noutliers_out_et001.png-et_et002.png: 43 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:   5%|▍         | 6/124 [00:00<00:04, 25.88it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-et_et008.png: 51 SIFT FLANN matches\noutliers_out_et001.png-et_et005.png: 47 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:   7%|▋         | 9/124 [00:00<00:04, 26.31it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et006.png: 55 SIFT FLANN matches\noutliers_out_et001.png-another_et_another_et002.png: 53 SIFT FLANN matches\noutliers_out_et001.png-another_et_another_et004.png: 58 SIFT FLANN matches\noutliers_out_et001.png-another_et_another_et007.png: 63 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  10%|▉         | 12/124 [00:00<00:04, 27.21it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et008.png: 67 SIFT FLANN matches\noutliers_out_et001.png-another_et_another_et003.png: 46 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  12%|█▏        | 15/124 [00:00<00:04, 24.16it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et001.png-another_et_another_et005.png: 64 SIFT FLANN matches\noutliers_out_et001.png-another_et_another_et001.png: 68 SIFT FLANN matches\noutliers_out_et003.png-outliers_out_et002.png: 52 SIFT FLANN matches\noutliers_out_et003.png-et_et003.png: 66 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  15%|█▍        | 18/124 [00:00<00:04, 21.83it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-et_et006.png: 61 SIFT FLANN matches\noutliers_out_et003.png-et_et001.png: 56 SIFT FLANN matches\noutliers_out_et003.png-et_et002.png: 63 SIFT FLANN matches\noutliers_out_et003.png-another_et_another_et010.png: 59 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  17%|█▋        | 21/124 [00:00<00:04, 20.92it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et003.png-another_et_another_et005.png: 61 SIFT FLANN matches\noutliers_out_et003.png-another_et_another_et009.png: 69 SIFT FLANN matches\noutliers_out_et002.png-et_et008.png: 108 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  19%|█▉        | 24/124 [00:01<00:05, 18.82it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et006.png: 83 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  21%|██        | 26/124 [00:01<00:05, 17.41it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et002.png: 99 SIFT FLANN matches\noutliers_out_et002.png-another_et_another_et010.png: 125 SIFT FLANN matches\noutliers_out_et002.png-another_et_another_et004.png: 107 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  23%|██▎       | 28/124 [00:01<00:05, 16.48it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et008.png: 106 SIFT FLANN matches\noutliers_out_et002.png-another_et_another_et003.png: 91 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  24%|██▍       | 30/124 [00:01<00:06, 15.55it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et005.png: 108 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  26%|██▌       | 32/124 [00:01<00:06, 14.69it/s]","output_type":"stream"},{"name":"stdout","text":"outliers_out_et002.png-another_et_another_et001.png: 99 SIFT FLANN matches\noutliers_out_et002.png-another_et_another_et009.png: 119 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  27%|██▋       | 34/124 [00:01<00:05, 15.84it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-et_et006.png: 688 SIFT FLANN matches\net_et007.png-et_et005.png: 400 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  29%|██▉       | 36/124 [00:01<00:05, 16.80it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-another_et_another_et006.png: 80 SIFT FLANN matches\net_et007.png-another_et_another_et010.png: 68 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  31%|███       | 38/124 [00:02<00:04, 17.21it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-another_et_another_et008.png: 74 SIFT FLANN matches\net_et007.png-another_et_another_et003.png: 60 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  32%|███▏      | 40/124 [00:02<00:04, 17.90it/s]","output_type":"stream"},{"name":"stdout","text":"et_et007.png-another_et_another_et005.png: 59 SIFT FLANN matches\net_et007.png-another_et_another_et009.png: 68 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  35%|███▍      | 43/124 [00:02<00:04, 19.02it/s]","output_type":"stream"},{"name":"stdout","text":"et_et003.png-et_et000.png: 522 SIFT FLANN matches\net_et003.png-another_et_another_et006.png: 55 SIFT FLANN matches\net_et003.png-another_et_another_et002.png: 53 SIFT FLANN matches\net_et003.png-another_et_another_et010.png: 58 SIFT FLANN matches\net_et003.png-another_et_another_et007.png: 54 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  37%|███▋      | 46/124 [00:02<00:03, 20.80it/s]","output_type":"stream"},{"name":"stdout","text":"et_et003.png-another_et_another_et008.png: 57 SIFT FLANN matches\net_et003.png-another_et_another_et005.png: 47 SIFT FLANN matches\net_et003.png-another_et_another_et009.png: 40 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  40%|███▉      | 49/124 [00:02<00:03, 21.63it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et006.png: 70 SIFT FLANN matches\net_et006.png-another_et_another_et002.png: 62 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  42%|████▏     | 52/124 [00:02<00:03, 22.29it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et010.png: 54 SIFT FLANN matches\net_et006.png-another_et_another_et007.png: 69 SIFT FLANN matches\net_et006.png-another_et_another_et008.png: 68 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  44%|████▍     | 55/124 [00:02<00:03, 22.09it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et005.png: 64 SIFT FLANN matches\net_et006.png-another_et_another_et001.png: 72 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  47%|████▋     | 58/124 [00:02<00:03, 21.88it/s]","output_type":"stream"},{"name":"stdout","text":"et_et006.png-another_et_another_et009.png: 61 SIFT FLANN matches\net_et001.png-et_et002.png: 642 SIFT FLANN matches\net_et001.png-another_et_another_et006.png: 85 SIFT FLANN matches\net_et001.png-another_et_another_et010.png: 93 SIFT FLANN matches\net_et001.png-another_et_another_et004.png: 61 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  49%|████▉     | 61/124 [00:03<00:02, 21.41it/s]","output_type":"stream"},{"name":"stdout","text":"et_et001.png-another_et_another_et007.png: 87 SIFT FLANN matches\net_et001.png-another_et_another_et008.png: 74 SIFT FLANN matches\net_et001.png-another_et_another_et005.png: 87 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  52%|█████▏    | 64/124 [00:03<00:02, 21.12it/s]","output_type":"stream"},{"name":"stdout","text":"et_et001.png-another_et_another_et009.png: 88 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  54%|█████▍    | 67/124 [00:03<00:02, 19.64it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et006.png: 72 SIFT FLANN matches\net_et004.png-another_et_another_et002.png: 58 SIFT FLANN matches\net_et004.png-another_et_another_et010.png: 94 SIFT FLANN matches\net_et004.png-another_et_another_et004.png: 51 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  57%|█████▋    | 71/124 [00:03<00:03, 17.65it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et007.png: 95 SIFT FLANN matches\net_et004.png-another_et_another_et008.png: 74 SIFT FLANN matches\net_et004.png-another_et_another_et003.png: 72 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  60%|██████    | 75/124 [00:03<00:02, 16.56it/s]","output_type":"stream"},{"name":"stdout","text":"et_et004.png-another_et_another_et005.png: 71 SIFT FLANN matches\net_et004.png-another_et_another_et001.png: 59 SIFT FLANN matches\net_et004.png-another_et_another_et009.png: 71 SIFT FLANN matches\net_et002.png-another_et_another_et006.png: 79 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  63%|██████▎   | 78/124 [00:04<00:02, 18.09it/s]","output_type":"stream"},{"name":"stdout","text":"et_et002.png-another_et_another_et002.png: 68 SIFT FLANN matches\net_et002.png-another_et_another_et010.png: 61 SIFT FLANN matches\net_et002.png-another_et_another_et004.png: 62 SIFT FLANN matches\net_et002.png-another_et_another_et007.png: 84 SIFT FLANN matches\net_et002.png-another_et_another_et008.png: 61 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  68%|██████▊   | 84/124 [00:04<00:02, 19.50it/s]","output_type":"stream"},{"name":"stdout","text":"et_et002.png-another_et_another_et005.png: 70 SIFT FLANN matches\net_et002.png-another_et_another_et001.png: 52 SIFT FLANN matches\net_et002.png-another_et_another_et009.png: 77 SIFT FLANN matches\net_et008.png-another_et_another_et006.png: 66 SIFT FLANN matches\net_et008.png-another_et_another_et002.png: 66 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  73%|███████▎  | 90/124 [00:04<00:01, 20.46it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et010.png: 61 SIFT FLANN matches\net_et008.png-another_et_another_et007.png: 79 SIFT FLANN matches\net_et008.png-another_et_another_et008.png: 60 SIFT FLANN matches\net_et008.png-another_et_another_et003.png: 78 SIFT FLANN matches\net_et008.png-another_et_another_et005.png: 61 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  75%|███████▌  | 93/124 [00:04<00:01, 20.95it/s]","output_type":"stream"},{"name":"stdout","text":"et_et008.png-another_et_another_et001.png: 57 SIFT FLANN matches\net_et008.png-another_et_another_et009.png: 48 SIFT FLANN matches\net_et005.png-another_et_another_et006.png: 60 SIFT FLANN matches\net_et005.png-another_et_another_et010.png: 38 SIFT FLANN matches\net_et005.png-another_et_another_et007.png: 51 SIFT FLANN matches\net_et005.png-another_et_another_et008.png: 39 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  81%|████████  | 100/124 [00:05<00:01, 23.36it/s]","output_type":"stream"},{"name":"stdout","text":"et_et005.png-another_et_another_et005.png: 49 SIFT FLANN matches\net_et005.png-another_et_another_et001.png: 35 SIFT FLANN matches\net_et005.png-another_et_another_et009.png: 33 SIFT FLANN matches\net_et000.png-another_et_another_et006.png: 89 SIFT FLANN matches\net_et000.png-another_et_another_et002.png: 78 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  83%|████████▎ | 103/124 [00:05<00:00, 21.22it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et010.png: 78 SIFT FLANN matches\net_et000.png-another_et_another_et004.png: 82 SIFT FLANN matches\net_et000.png-another_et_another_et007.png: 87 SIFT FLANN matches\net_et000.png-another_et_another_et008.png: 75 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  88%|████████▊ | 109/124 [00:05<00:00, 19.20it/s]","output_type":"stream"},{"name":"stdout","text":"et_et000.png-another_et_another_et005.png: 68 SIFT FLANN matches\net_et000.png-another_et_another_et001.png: 86 SIFT FLANN matches\net_et000.png-another_et_another_et009.png: 68 SIFT FLANN matches\nanother_et_another_et006.png-another_et_another_et002.png: 192 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  90%|█████████ | 112/124 [00:05<00:00, 20.48it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et006.png-another_et_another_et004.png: 134 SIFT FLANN matches\nanother_et_another_et006.png-another_et_another_et007.png: 201 SIFT FLANN matches\nanother_et_another_et006.png-another_et_another_et005.png: 158 SIFT FLANN matches\nanother_et_another_et006.png-another_et_another_et001.png: 166 SIFT FLANN matches\nanother_et_another_et002.png-another_et_another_et004.png: 449 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN:  95%|█████████▌| 118/124 [00:05<00:00, 20.73it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et002.png-another_et_another_et005.png: 429 SIFT FLANN matches\nanother_et_another_et002.png-another_et_another_et001.png: 792 SIFT FLANN matches\nanother_et_another_et010.png-another_et_another_et003.png: 40 SIFT FLANN matches\nanother_et_another_et004.png-another_et_another_et003.png: 263 SIFT FLANN matches\nanother_et_another_et004.png-another_et_another_et001.png: 406 SIFT FLANN matches\n","output_type":"stream"},{"name":"stderr","text":"Matching SIFT with FLANN: 100%|██████████| 124/124 [00:06<00:00, 19.99it/s]","output_type":"stream"},{"name":"stdout","text":"another_et_another_et007.png-another_et_another_et008.png: 169 SIFT FLANN matches\nanother_et_another_et008.png-another_et_another_et003.png: 68 SIFT FLANN matches\nanother_et_another_et008.png-another_et_another_et009.png: 171 SIFT FLANN matches\nanother_et_another_et003.png-another_et_another_et009.png: 60 SIFT FLANN matches\nanother_et_another_et005.png-another_et_another_et001.png: 504 SIFT FLANN matches\nSIFT + FLANN matches saved to /kaggle/working/result/featureout/ETs/matches_sift_nn.h5\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n--- Step 3.3: Ensembling and Remapping all matching results ---\n\n--- Ensembling and Remapping Matches ---\n","output_type":"stream"},{"name":"stderr","text":"Phase 1: Unifying keypoints per image: 100%|██████████| 22/22 [00:01<00:00, 11.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unified keypoints saved to /kaggle/working/result/featureout/ETs/keypoints.h5\n","output_type":"stream"},{"name":"stderr","text":"Phase 2: Remapping and Ensembling Matches: 100%|██████████| 22/22 [00:00<00:00, 32.06it/s]\nSaving remapped ensembled matches: 100%|██████████| 124/124 [00:00<00:00, 2555.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ensembled and remapped matches saved to /kaggle/working/result/featureout/ETs/matches.h5\nFinal unified keypoints saved to: /kaggle/working/result/featureout/ETs/keypoints.h5\nFinal ensembled and remapped matches saved to: /kaggle/working/result/featureout/ETs/matches.h5\nFeatures matched in 26.1384 sec\n\n--- Step 3.4: Importing ensembled results into COLMAP ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22/22 [00:00<00:00, 82.12it/s]\n 65%|██████▌   | 124/190 [00:00<00:00, 4798.97it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Must Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-22T00:19:28.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Definitely Compute results if running on the training set.\n# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n\nif is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-22T00:19:28.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}