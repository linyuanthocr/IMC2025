{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11217117,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Example submission\n\nImage Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n\nThis notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n\nRemember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# IMPORTANT \n#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:12:53.461905Z","iopub.execute_input":"2025-05-15T06:12:53.462253Z","iopub.status.idle":"2025-05-15T06:12:55.293059Z","shell.execute_reply.started":"2025-05-15T06:12:53.462219Z","shell.execute_reply":"2025-05-15T06:12:55.292108Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nkornia is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nkornia-moons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nkornia-rs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nlightglue is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\npycolmap is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nrerun-sdk is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\n\n# IMPORTANT Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:12:55.294288Z","iopub.execute_input":"2025-05-15T06:12:55.294510Z","iopub.status.idle":"2025-05-15T06:13:01.841279Z","shell.execute_reply.started":"2025-05-15T06:12:55.294491Z","shell.execute_reply":"2025-05-15T06:13:01.840402Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nprint(\"PyTorch version:\", torch.__version__)\nimport sys\nprint(\"Python version:\", sys.version)\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"Device count:\", torch.cuda.device_count())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:01.843145Z","iopub.execute_input":"2025-05-15T06:13:01.843808Z","iopub.status.idle":"2025-05-15T06:13:01.942070Z","shell.execute_reply.started":"2025-05-15T06:13:01.843772Z","shell.execute_reply":"2025-05-15T06:13:01.940882Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu121\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nCUDA available: True\nCUDA version: 12.1\nDevice count: 2\nCurrent device: 0\nDevice name: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Do not forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:01.943435Z","iopub.execute_input":"2025-05-15T06:13:01.943769Z","iopub.status.idle":"2025-05-15T06:13:01.949202Z","shell.execute_reply.started":"2025-05-15T06:13:01.943738Z","shell.execute_reply":"2025-05-15T06:13:01.948130Z"}},"outputs":[{"name":"stdout","text":"device=device(type='cuda', index=0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !zip -r /kaggle/working/result/featureout/ETs/featurept.zip /kaggle/working/result/featureout/ETs/featurept\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:01.949918Z","iopub.execute_input":"2025-05-15T06:13:01.950223Z","iopub.status.idle":"2025-05-15T06:13:01.969567Z","shell.execute_reply.started":"2025-05-15T06:13:01.950198Z","shell.execute_reply":"2025-05-15T06:13:01.968771Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from pathlib import Path\n\ndef draw_and_save_feature_points(image_path, keypoints, result_folder):\n    \"\"\"\n    Draw feature points on the image and save to result folder.\n\n    Args:\n        image_path (str or Path): Path to the input image.\n        keypoints (np.ndarray): (N, 2) array of (x, y) coordinates.\n        result_folder (str or Path): Folder to save the output image.\n    \"\"\"\n    # Load image in BGR\n    return\n    image = cv2.imread(str(image_path))\n    if image is None:\n        raise ValueError(f\"Cannot read image from {image_path}\")\n\n    # Draw keypoints\n    for (x, y) in keypoints.astype(int):\n        cv2.circle(image, (x, y), radius=2, color=(0, 255, 0), thickness=-1)  # Green dots\n\n    img_fname = image_path.split('/')[-1]\n\n    result_folder = Path(result_folder)\n    img_fname = Path(image_path).stem  # no extension\n    output_path = result_folder / f\"{img_fname}_fe.png\"\n\n    cv2.imwrite(str(output_path), image)\n    print(f\"Saved: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:01.970457Z","iopub.execute_input":"2025-05-15T06:13:01.970798Z","iopub.status.idle":"2025-05-15T06:13:01.988111Z","shell.execute_reply.started":"2025-05-15T06:13:01.970768Z","shell.execute_reply":"2025-05-15T06:13:01.987213Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\n\n# Must Use efficientnet global descriptor to get matching shortlists.\ndef get_global_desc(fnames, device = torch.device('cpu')):\n    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = model.eval()\n    model = model.to(device)\n    global_descs_dinov2 = []\n    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n        timg = load_torch_image(img_fname_full)\n        with torch.inference_mode():\n            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs)\n            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n        global_descs_dinov2.append(dino_mac.detach().cpu())\n    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n    return global_descs_dinov2\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 10,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu'),\n                              max_pairs = 30):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc(fnames, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n    # print(dm)\n    # 只分析上三角（去掉对角线），避免重复\n    triu_indices = np.triu_indices_from(dm, k=1)\n    dm_flat = dm[triu_indices]\n    \n    # 打印统计信息\n    print(\"Distance Matrix Statistics:\")\n    print(f\"Min:  {dm_flat.min():.4f}\")\n    print(f\"Max:  {dm_flat.max():.4f}\")\n    print(f\"Mean: {dm_flat.mean():.4f}\")\n    print(f\"Std:  {dm_flat.std():.4f}\")\n    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n    print(f\"30%:  {np.percentile(dm_flat, 30):.4f}\")\n    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n    # removing half\n    mask = dm <= np.percentile(dm_flat, 50)\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs :\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        if len(to_match) >= max_pairs:\n            to_match = np.argsort(dm[st_idx])[:max_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < threshold:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\ndef detect_aliked(img_fnames,\n                  feature_dir = '.featureout',\n                  num_features = 4096,\n                  resize_to = 2048,\n                  device=torch.device('cpu')):\n    dtype = torch.float32 # ALIKED has issues with float16\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n    extractor.preprocess_conf[\"resize\"] = resize_to\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n\n    # Calculate the expected scale factor ALIKED will apply\n    # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n    # Input image tensor size will be (H, W) after Kornia loading/conversion\n    \n    draw_feature_dir = os.path.join(feature_dir, 'featurept')\n    os.makedirs(draw_feature_dir, exist_ok=True)\n    \n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n                draw_and_save_feature_points(img_path, kpts, draw_feature_dir)\n    return\n\ndef match_with_lightglue(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout',\n                   device=torch.device('cpu'),\n                   min_matches=30,\n                   verbose=False,\n                   match_score_thresh = 0.15):\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                \"depth_confidence\": -1,\n                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1,\n                                         desc2,\n                                         KF.laf_from_center_scale_ori(kp1[None]),\n                                         KF.laf_from_center_scale_ori(kp2[None]))\n            if len(idxs)  == 0:\n                continue\n            n_matches = len(idxs)\n            # if verbose:\n            #     print (f'{key1}-{key2}: {n_matches} matches')\n            # group  = f_match.require_group(key1)\n            # if n_matches >= min_matches:\n            #      group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n            # Filter by match score (distance)\n            \n            mask = dists > match_score_thresh\n            idxs_filtered = idxs[mask.squeeze(1)]\n    \n            n_matches = len(idxs_filtered)\n            if n_matches == 0:\n                continue\n    \n            if verbose:\n                print(f'{key1}-{key2}: {n_matches} matches (filtered from {len(idxs)})')\n    \n            group = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                group.create_dataset(key2, data=idxs_filtered.detach().cpu().numpy().reshape(-1, 2))\n\n    return\n\n\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:01.989245Z","iopub.execute_input":"2025-05-15T06:13:01.989523Z","iopub.status.idle":"2025-05-15T06:13:02.022625Z","shell.execute_reply.started":"2025-05-15T06:13:01.989486Z","shell.execute_reply":"2025-05-15T06:13:02.020568Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import networkx as nx\n\nimport networkx as nx\nfrom tqdm import tqdm\n\ndef match_with_lightglue_and_cluster(\n    img_fnames,\n    index_pairs,\n    feature_dir='.featureout',\n    device=torch.device('cpu'),\n    min_matches=20,\n    aliked_dis_min=100,\n    verbose=True\n):\n    # 初始化 LightGlue 匹配器\n    lg_matcher = KF.LightGlueMatcher(\n        \"aliked\", {\n            \"width_confidence\": -1,\n            \"depth_confidence\": -1,\n            \"mp\": 'cuda' in str(device)\n        }\n    ).eval().to(device)\n\n    num_imgs = len(img_fnames)\n    match_graph = nx.Graph()\n    match_graph.add_nodes_from(range(num_imgs))\n\n    # 加载关键点和描述子，准备写入匹配结果\n    with h5py.File(os.path.join(feature_dir, 'keypoints.h5'), 'r') as f_kp, \\\n         h5py.File(os.path.join(feature_dir, 'descriptors.h5'), 'r') as f_desc, \\\n         h5py.File(os.path.join(feature_dir, 'matches.h5'), 'w') as f_match:\n\n        # Step 1: 匹配每对图像\n        for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n            img1 = os.path.basename(img_fnames[idx1])\n            img2 = os.path.basename(img_fnames[idx2])\n\n            kp1 = torch.from_numpy(f_kp[img1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[img2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[img1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[img2][...]).to(device)\n\n            with torch.inference_mode():\n                _, idxs = lg_matcher(\n                    desc1, desc2,\n                    KF.laf_from_center_scale_ori(kp1[None]),\n                    KF.laf_from_center_scale_ori(kp2[None])\n                )\n\n            if len(idxs) == 0:\n                continue\n\n            n_matches = len(idxs)\n            if verbose:\n                print(f'{img1} - {img2}: {n_matches} matches')\n\n            if n_matches >= min_matches:\n                match_graph.add_edge(idx1, idx2, weight=n_matches)\n                group = f_match.require_group(img1)\n                group.create_dataset(img2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n\n        # Step 2: 提取聚类（connected components）\n        raw_clusters = list(nx.connected_components(match_graph))\n        final_clusters = []\n        outliers = set()\n\n        for cluster in raw_clusters:\n            cluster = list(cluster)\n            subgraph = match_graph.subgraph(cluster)\n            valid_nodes = []\n\n            for node in cluster:\n                strong_neighbors = [\n                    neighbor for neighbor in subgraph.neighbors(node)\n                    if subgraph[node][neighbor]['weight'] >= aliked_dis_min\n                ]\n                if strong_neighbors:\n                    valid_nodes.append(node)\n                else:\n                    outliers.add(node)\n\n            if len(valid_nodes) >= 2:\n                final_clusters.append(valid_nodes)\n            else:\n                outliers.update(valid_nodes)\n\n        # Step 3: 清理掉涉及 outlier 的匹配项（保留聚类内匹配）\n        # outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n\n        # for group_name in f_match.keys():\n        #     group = f_match[group_name]\n        #     for dataset_name in list(group.keys()):\n        #         if group_name in outlier_names or dataset_name in outlier_names:\n        #             del group[dataset_name]\n        #             if verbose:\n        #                 print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n        \n        # Step 3: 清理掉涉及 outlier 的匹配项\n        outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n\n        groups_to_delete = []\n\n        for group_name in list(f_match.keys()):\n            group = f_match[group_name]\n            datasets_to_delete = []\n\n            for dataset_name in list(group.keys()):\n                # 删除与 outlier 有关的 match\n                if group_name in outlier_names or dataset_name in outlier_names:\n                    datasets_to_delete.append(dataset_name)\n                    if verbose:\n                        print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n\n            # 先删除标记的 dataset\n            for ds in datasets_to_delete:\n                del group[ds]\n\n            # 新增逻辑：如果剩下不到 3 个 match，就删掉整个 group\n            if len(group.keys()) < 3:\n                groups_to_delete.append(group_name)\n\n        # 删除 group 本身\n        for gname in groups_to_delete:\n            del f_match[gname]\n            if verbose:\n                print(f\"Deleted group: {gname} (too few matches)\")\n\n    if True:\n        print(list(outliers))\n        for i, cluster in enumerate(final_clusters):\n            print(f\"Cluster {i} ({len(cluster)} images):\")\n    return final_clusters, sorted(list(outliers))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:02.024696Z","iopub.execute_input":"2025-05-15T06:13:02.024936Z","iopub.status.idle":"2025-05-15T06:13:02.217832Z","shell.execute_reply.started":"2025-05-15T06:13:02.024913Z","shell.execute_reply":"2025-05-15T06:13:02.216722Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Collect vital info from the dataset\n\n@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None\n\n# Set is_train=True to run the notebook on the training data.\n# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\nis_train = True\n\ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:02.219045Z","iopub.execute_input":"2025-05-15T06:13:02.219756Z","iopub.status.idle":"2025-05-15T06:13:02.364833Z","shell.execute_reply.started":"2025-05-15T06:13:02.219708Z","shell.execute_reply":"2025-05-15T06:13:02.363951Z"}},"outputs":[{"name":"stdout","text":"Dataset \"imc2023_haiper\" -> num_images=54\nDataset \"imc2023_heritage\" -> num_images=209\nDataset \"imc2023_theather_imc2024_church\" -> num_images=76\nDataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\nDataset \"imc2024_lizard_pond\" -> num_images=214\nDataset \"pt_brandenburg_british_buckingham\" -> num_images=225\nDataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\nDataset \"pt_stpeters_stpauls\" -> num_images=200\nDataset \"amy_gardens\" -> num_images=200\nDataset \"fbk_vineyard\" -> num_images=163\nDataset \"ETs\" -> num_images=22\nDataset \"stairs\" -> num_images=51\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\ngc.collect()\n\nmax_images = None  # Used For debugging only. Set to None to disable.\ndatasets_to_process = None  # Not the best convention, but None means all datasets.\n\nif is_train:\n    # max_images = 5\n\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n    \t# New data.\n    \t'amy_gardens',\n    \t'ETs',\n    \t'fbk_vineyard',\n    \t'stairs', \n    \t# Data from IMC 2023 and 2024.\n    \t# 'imc2024_dioscuri_baalshamin',\n    \t# 'imc2023_theather_imc2024_church',\n    \t'imc2023_heritage',\n    \t# 'imc2023_haiper',\n    \t# 'imc2024_lizard_pond',\n    \t# Crowdsourced PhotoTourism data.\n    \t# 'pt_stpeters_stpauls',\n    \t'pt_brandenburg_british_buckingham',\n    \t# 'pt_piazzasanmarco_grandplace',\n    \t# 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n\nprint (f\"Extracting on device {device}\")\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n    try:\n        t = time()\n        index_pairs = get_image_pairs_shortlist(\n            images,\n            sim_th = 0.5, # should be strict\n            min_pairs = 8, # we should select at least min_pairs PER IMAGE with biggest similarity\n            exhaustive_if_less = 20,\n            device=device\n        )\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n    \n        t = time()\n\n        detect_aliked(images, feature_dir, 8192, device=device)\n        gc.collect()\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n        \n        t = time()\n        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        # match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n\n        database_path = os.path.join(feature_dir, 'colmap.db')\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n        gc.collect()\n        sleep(1)\n        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n        output_path = f'{feature_dir}/colmap_rec_aliked'\n        \n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        timings['RANSAC'].append(time() - t)\n        print(f'Ran RANSAC in {time() - t:.4f} sec')\n        \n        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n        # Lower it to 3.\n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 5\n        mapper_options.max_num_models = 25\n        mapper_options.mapper.filter_max_reproj_error\t = 8.0\n        # mapper_options.min_num_matches\t = 60\n        # mapper_options.ba_local_max_num_iterations = 100\n        # mapper_options.ba_local_num_images = 10\n        # mapper_options.ba_global_images_freq = 5\n        \n\n        os.makedirs(output_path, exist_ok=True)\n        t = time()\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path, \n            image_path=images_dir,\n            output_path=output_path,\n            options=mapper_options)\n        sleep(1)\n        timings['Reconstruction'].append(time() - t)\n        print(f'Reconstruction done in  {time() - t:.4f} sec')\n        print(maps)\n\n        # clear_output(wait=False)\n    \n        registered = 0\n        for map_index, cur_map in maps.items():\n            img_list =[]\n            for index, image in cur_map.images.items():\n                prediction_index = filename_to_index[image.name]\n                predictions[prediction_index].cluster_index = map_index\n                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                img_list.append(image.name)\n                registered += 1\n            img_list_str = ' '.join(img_list) \n            print(f\"map_index = {map_index}\", img_list_str)\n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:13:02.365770Z","iopub.execute_input":"2025-05-15T06:13:02.366118Z","iopub.status.idle":"2025-05-15T08:06:06.215828Z","shell.execute_reply.started":"2025-05-15T06:13:02.366089Z","shell.execute_reply":"2025-05-15T08:06:06.215044Z"}},"outputs":[{"name":"stdout","text":"Extracting on device cuda:0\nSkipping \"imc2023_haiper\"\n\nProcessing dataset \"imc2023_heritage\": 209 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 209/209 [04:29<00:00,  1.29s/it]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1159\nMax:  0.5188\nMean: 0.3667\nStd:  0.0539\n20%:  0.3260\n30%:  0.3430\nUSED 50%:  0.3727\n75%:  0.4056\nShortlisting. Number of pairs to match: 3806. Done in 269.6785 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 209/209 [01:26<00:00,  2.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features detected in 87.0946 sec\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3806/3806 [19:16<00:00,  3.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features matched in 1156.6067 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 209/209 [01:07<00:00,  3.12it/s]\n  2%|▏         | 516/21115 [00:00<00:05, 3683.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ran RANSAC in 11.0998 sec\nReconstruction done in  377.1497 sec\n{0: Reconstruction(num_reg_images=43, num_cameras=43, num_points3D=61984, num_observations=244665), 1: Reconstruction(num_reg_images=58, num_cameras=58, num_points3D=34179, num_observations=147824), 2: Reconstruction(num_reg_images=8, num_cameras=8, num_points3D=4720, num_observations=16037), 3: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=5186, num_observations=14885), 4: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=4128, num_observations=10348)}\nmap_index = 0 wall_dsc_4973_acr.png wall_dsc_4976_acr.png wall_dsc_4979_acr.png wall_dsc_4982_acr.png wall_dsc_4991_acr.png wall_dsc_4994_acr.png wall_dsc_4997_acr.png wall_dsc_5000_acr.png wall_dsc_5003_acr.png wall_dsc_5006_acr.png wall_dsc_5009_acr.png wall_dsc_5012_acr.png wall_dsc_5015_acr.png wall_dsc_5018_acr.png wall_dsc_5021_acr.png wall_dsc_5024_acr.png wall_dsc_5027_acr.png wall_dsc_5030_acr.png wall_dsc_5033_acr.png wall_dsc_5039_acr.png wall_dsc_5042_acr.png wall_dsc_5045_acr.png wall_dsc_5048_acr.png wall_dsc_5051_acr.png wall_dsc_5054_acr.png wall_dsc_5057_acr.png wall_dsc_5060_acr.png wall_dsc_5063_acr.png wall_dsc_4928_acr.png wall_dsc_4931_acr.png wall_dsc_4934_acr.png wall_dsc_4937_acr.png wall_dsc_4940_acr.png wall_dsc_4943_acr.png wall_dsc_4946_acr.png wall_dsc_4952_acr.png wall_dsc_4955_acr.png wall_dsc_4958_acr.png wall_dsc_4961_acr.png wall_dsc_4964_acr.png wall_dsc_4967_acr.png wall_dsc_4970_acr.png wall_dsc_4985_acr.png\nmap_index = 1 dioscuri_3dom_fbk_img_1516.png dioscuri_3dom_fbk_img_1520.png dioscuri_3dom_fbk_img_1524.png dioscuri_3dom_fbk_img_1548.png dioscuri_3dom_fbk_img_1561.png dioscuri_3dom_fbk_img_1563.png dioscuri_3dom_fbk_img_1566.png dioscuri_archive_0003.png dioscuri_archive_0004.png dioscuri_archive_0010.png dioscuri_archive_0015.png dioscuri_archive_0171.png dioscuri_archive_0230.png dioscuri_img_0055.png dioscuri_img_0068.png dioscuri_img_0079.png dioscuri_img_0081.png dioscuri_img_0092.png dioscuri_img_0095.png dioscuri_img_0105.png dioscuri_img_0112.png dioscuri_img_0115.png dioscuri_img_0132.png dioscuri_img_0161.png dioscuri_img_0163.png dioscuri_img_0181.png dioscuri_img_0187.png dioscuri_img_0188.png dioscuri_img_0200.png dioscuri_img_0209.png dioscuri_img_0213.png dioscuri_img_0216.png dioscuri_img_0227.png dioscuri_img_0230.png dioscuri_img_0237.png dioscuri_img_0238.png dioscuri_img_0246.png dioscuri_img_0258.png dioscuri_img_0266.png dioscuri_img_0276.png dioscuri_img_0277.png dioscuri_img_0283.png dioscuri_img_0314.png dioscuri_img_0338.png dioscuri_img_0339.png dioscuri_img_0348.png dioscuri_img_0354.png dioscuri_img_0359.png dioscuri_img_0364.png dioscuri_img_0369.png dioscuri_img_0370.png dioscuri_img_0376.png dioscuri_img_0401.png dioscuri_img_0412.png dioscuri_img_0418.png dioscuri_img_0424.png dioscuri_img_0444.png dioscuri_img_0460.png\nmap_index = 2 dioscuri_archive_0028.png dioscuri_archive_0033.png dioscuri_archive_0068.png dioscuri_archive_0069.png dioscuri_archive_0070.png dioscuri_archive_0071.png dioscuri_archive_0072.png dioscuri_archive_0092.png\nmap_index = 3 cyprus_dsc_6488.png cyprus_dsc_6492.png cyprus_dsc_6496.png cyprus_dsc_6500.png cyprus_dsc_6512.png cyprus_dsc_6520.png cyprus_dsc_6524.png cyprus_dsc_6528.png cyprus_dsc_6540.png cyprus_dsc_6548.png cyprus_dsc_6565.png cyprus_dsc_6609.png cyprus_dsc_6633.png\nmap_index = 4 cyprus_dsc_6569.png cyprus_dsc_6589.png cyprus_dsc_6593.png cyprus_dsc_6597.png cyprus_dsc_6601.png cyprus_dsc_6605.png\nDataset \"imc2023_heritage\" -> Registered 128 / 209 images with 5 clusters\nSkipping \"imc2023_theather_imc2024_church\"\nSkipping \"imc2024_dioscuri_baalshamin\"\nSkipping \"imc2024_lizard_pond\"\n\nProcessing dataset \"pt_brandenburg_british_buckingham\": 225 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 225/225 [00:29<00:00,  7.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1324\nMax:  0.4310\nMean: 0.2733\nStd:  0.0364\n20%:  0.2439\n30%:  0.2586\nUSED 50%:  0.2762\n75%:  0.2974\nShortlisting. Number of pairs to match: 4393. Done in 29.7488 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 225/225 [00:37<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features detected in 37.5751 sec\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4393/4393 [16:48<00:00,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features matched in 1009.1865 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 225/225 [00:05<00:00, 40.54it/s]\n 18%|█▊        | 4312/24090 [00:01<00:05, 3896.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ran RANSAC in 32.9906 sec\nReconstruction done in  752.0593 sec\n{0: Reconstruction(num_reg_images=75, num_cameras=75, num_points3D=20081, num_observations=330937), 1: Reconstruction(num_reg_images=75, num_cameras=75, num_points3D=15417, num_observations=299350), 2: Reconstruction(num_reg_images=74, num_cameras=74, num_points3D=13428, num_observations=260767)}\nmap_index = 0 buckingham_palace_04861657_8587228495.png buckingham_palace_05394210_3325092555.png buckingham_palace_05879148_4721852797.png buckingham_palace_06170639_4557285016.png buckingham_palace_08413293_4591289960.png buckingham_palace_10097113_4343681151.png buckingham_palace_11220321_6429817645.png buckingham_palace_11272171_837310675.png buckingham_palace_11299827_3665838648.png buckingham_palace_12842704_470297372.png buckingham_palace_14258385_9378732166.png buckingham_palace_16774617_219184039.png buckingham_palace_19226918_112883362.png buckingham_palace_20371484_12209806.png buckingham_palace_21134535_11614071253.png buckingham_palace_21167644_8707271738.png buckingham_palace_21697383_6430594307.png buckingham_palace_25910232_4557319964.png buckingham_palace_26606950_5411314264.png buckingham_palace_28542033_5434317589.png buckingham_palace_34791488_2406713663.png buckingham_palace_35128922_3585104193.png buckingham_palace_35301556_5755414976.png buckingham_palace_35654194_9287834913.png buckingham_palace_35686206_4081926571.png buckingham_palace_36576167_9378677290.png buckingham_palace_37524858_3739802247.png buckingham_palace_37797945_4824569303.png buckingham_palace_38217522_12120715573.png buckingham_palace_38257186_6252086513.png buckingham_palace_39291857_1523464373.png buckingham_palace_39528348_6563861043.png buckingham_palace_40942590_10086689274.png buckingham_palace_42572558_3847795752.png buckingham_palace_44573130_5715131949.png buckingham_palace_46109345_5162840183.png buckingham_palace_46810479_13785513864.png buckingham_palace_47315604_5047670100.png buckingham_palace_48484647_7883607208.png buckingham_palace_49269748_4041495348.png buckingham_palace_51699589_9343866373.png buckingham_palace_52402986_5523235865.png buckingham_palace_52410261_4937469812.png buckingham_palace_54621533_151517675.png buckingham_palace_59781164_3740564586.png buckingham_palace_59875279_3803024920.png buckingham_palace_61805239_2253432740.png buckingham_palace_62469402_3202824067.png buckingham_palace_62660707_10596782846.png buckingham_palace_64822253_2217151982.png buckingham_palace_66580063_2204250033.png buckingham_palace_66580194_6996358425.png buckingham_palace_67053735_5824219898.png buckingham_palace_69568614_12055226536.png buckingham_palace_70462905_5669092857.png buckingham_palace_70979018_2078157897.png buckingham_palace_73465264_3740611434.png buckingham_palace_74296459_103436516.png buckingham_palace_76669421_1878873201.png buckingham_palace_77667400_439062740.png buckingham_palace_80761089_1260341347.png buckingham_palace_81876166_676507850.png buckingham_palace_82116100_4565320676.png buckingham_palace_82600926_8760297100.png buckingham_palace_84522886_5954681667.png buckingham_palace_85181405_8612250269.png buckingham_palace_85556619_3740631958.png buckingham_palace_86418271_9241595670.png buckingham_palace_89205589_356802584.png buckingham_palace_89367054_672839242.png buckingham_palace_91446393_39141513.png buckingham_palace_91676499_3739813019.png buckingham_palace_93037627_13638822993.png buckingham_palace_94455955_3929007141.png buckingham_palace_95865994_275402759.png\nmap_index = 1 british_museum_03576546_367645446.png british_museum_04790098_4924719784.png british_museum_09218102_3818070255.png british_museum_09362272_3871121693.png british_museum_09740642_11372944273.png british_museum_15144115_117839432.png british_museum_17406933_4349448982.png british_museum_17892330_2218662345.png british_museum_19254317_158538587.png british_museum_19811771_3152186510.png british_museum_21021803_3610519149.png british_museum_21326450_90598151.png british_museum_21757263_3848724084.png british_museum_25308247_4757972250.png british_museum_26677707_11389887745.png british_museum_27732591_4349424252.png british_museum_27779894_6489328657.png british_museum_28309287_5482270912.png british_museum_29840064_6240820088.png british_museum_30495805_5912735553.png british_museum_31925345_5986804667.png british_museum_32161143_8079875050.png british_museum_32520806_97760578.png british_museum_39461890_106560712.png british_museum_39667919_12715742453.png british_museum_42803655_6429621523.png british_museum_45839934_4117745134.png british_museum_51111273_4980544542.png british_museum_52641281_5921667676.png british_museum_53881364_5029031037.png british_museum_54432186_6732914439.png british_museum_56818398_67603532.png british_museum_59356487_3468534134.png british_museum_60624404_7093768535.png british_museum_61744140_838726075.png british_museum_61892850_2364933018.png british_museum_62774028_4348685003.png british_museum_63647221_4349439258.png british_museum_66361736_3945101598.png british_museum_66393650_5300086486.png british_museum_66747696_4734591579.png british_museum_67471600_1224757581.png british_museum_67521166_4735236552.png british_museum_67772160_2655952816.png british_museum_67894974_9704498288.png british_museum_68914717_4070962405.png british_museum_72728072_3851668437.png british_museum_73361450_117839447.png british_museum_73406890_5043569825.png british_museum_73748105_4531531922.png british_museum_74626542_2453940576.png british_museum_76518628_3747715512.png british_museum_77196526_11613990374.png british_museum_78137733_4059821853.png british_museum_78242267_4404579754.png british_museum_78567520_2219457772.png british_museum_80976991_8342057400.png british_museum_83025239_9530039386.png british_museum_84019782_6926081795.png british_museum_84932226_4352179060.png british_museum_85835963_3490488375.png british_museum_87893778_8781019493.png british_museum_89208997_5875216727.png british_museum_90026426_4349430506.png british_museum_90095149_11613863483.png british_museum_91782790_275554656.png british_museum_92242343_4351333520.png british_museum_93658023_4980549800.png british_museum_94308218_4349427490.png british_museum_95591790_8233423442.png british_museum_96255967_5091378744.png british_museum_98018049_7167451918.png british_museum_99480770_3945403640.png british_museum_99662030_3945401118.png british_museum_99981082_8904760272.png\nmap_index = 2 brandenburg_gate_01069771_8567470929.png brandenburg_gate_02936509_94852264.png brandenburg_gate_03300929_2790010816.png brandenburg_gate_04684290_2698969364.png brandenburg_gate_04705241_3760208106.png brandenburg_gate_06809631_5499605152.png brandenburg_gate_06935805_2163586273.png brandenburg_gate_06950806_3245016486.png brandenburg_gate_08455944_11844596793.png brandenburg_gate_08655339_301940774.png brandenburg_gate_09870003_7184351941.png brandenburg_gate_18348543_5140101294.png brandenburg_gate_19431740_4837182516.png brandenburg_gate_20133057_3035445116.png brandenburg_gate_22465257_94852237.png brandenburg_gate_23128078_9800439215.png brandenburg_gate_23898581_3418292777.png brandenburg_gate_24072578_3313430186.png brandenburg_gate_26655336_6192911644.png brandenburg_gate_28221825_5139496129.png brandenburg_gate_33797050_227462369.png brandenburg_gate_35200934_73110776.png brandenburg_gate_36555276_225381350.png brandenburg_gate_38891238_3944459389.png brandenburg_gate_41213067_5831563503.png brandenburg_gate_42723789_7775854318.png brandenburg_gate_43929418_4177129462.png brandenburg_gate_47510282_2492277573.png brandenburg_gate_47521158_5096477796.png brandenburg_gate_48127186_5425905321.png brandenburg_gate_52599802_11381897224.png brandenburg_gate_52630012_12435805525.png brandenburg_gate_53208747_4264002612.png brandenburg_gate_54714905_6154442539.png brandenburg_gate_54744886_13665893353.png brandenburg_gate_59713287_6809504619.png brandenburg_gate_59826471_8014732885.png brandenburg_gate_60553649_2254779884.png brandenburg_gate_60863296_2846812457.png brandenburg_gate_68654335_2476618452.png brandenburg_gate_69037173_2735815523.png brandenburg_gate_69366784_6629915323.png brandenburg_gate_71033172_8108767503.png brandenburg_gate_71214740_51142248.png brandenburg_gate_72396943_4654565284.png brandenburg_gate_73039718_4294835156.png brandenburg_gate_74077013_184987611.png brandenburg_gate_74576406_1433639911.png brandenburg_gate_75037828_1434507967.png brandenburg_gate_76173312_1901163610.png brandenburg_gate_76177997_2293073399.png brandenburg_gate_78329459_8088722344.png brandenburg_gate_79167472_5876853278.png brandenburg_gate_80142447_1434506947.png brandenburg_gate_82089809_2789175595.png brandenburg_gate_82618295_1916872032.png brandenburg_gate_82818583_7062231077.png brandenburg_gate_85067470_51142901.png brandenburg_gate_85386984_3990647856.png brandenburg_gate_90142764_8281118641.png brandenburg_gate_90954945_7168342309.png brandenburg_gate_91333167_11075684623.png brandenburg_gate_91824386_51142061.png brandenburg_gate_92841931_3067495942.png brandenburg_gate_92884278_346751510.png brandenburg_gate_93110830_1932760362.png brandenburg_gate_93385231_5436268017.png brandenburg_gate_94029235_930442362.png brandenburg_gate_94045317_3752214176.png brandenburg_gate_97371403_4823878472.png brandenburg_gate_98157145_9159024129.png brandenburg_gate_99467148_7775852980.png brandenburg_gate_99778055_7859904484.png brandenburg_gate_99933312_2768771111.png\nDataset \"pt_brandenburg_british_buckingham\" -> Registered 224 / 225 images with 3 clusters\nSkipping \"pt_piazzasanmarco_grandplace\"\nSkipping \"pt_sacrecoeur_trevi_tajmahal\"\nSkipping \"pt_stpeters_stpauls\"\n\nProcessing dataset \"amy_gardens\": 200 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:25<00:00,  7.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1448\nMax:  0.4239\nMean: 0.2737\nStd:  0.0388\n20%:  0.2375\n30%:  0.2505\nUSED 50%:  0.2754\n75%:  0.3017\nShortlisting. Number of pairs to match: 3892. Done in 25.5625 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:31<00:00,  6.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features detected in 31.3570 sec\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3892/3892 [20:16<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features matched in 1216.3057 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:04<00:00, 40.47it/s]\n  3%|▎         | 581/19110 [00:00<00:04, 4005.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ran RANSAC in 9.6746 sec\nReconstruction done in  292.3406 sec\n{0: Reconstruction(num_reg_images=3, num_cameras=3, num_points3D=0, num_observations=0), 1: Reconstruction(num_reg_images=68, num_cameras=68, num_points3D=41805, num_observations=200947), 2: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=10195, num_observations=42071), 3: Reconstruction(num_reg_images=16, num_cameras=16, num_points3D=6183, num_observations=22946), 4: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=6411, num_observations=24311), 5: Reconstruction(num_reg_images=37, num_cameras=37, num_points3D=12536, num_observations=48501), 6: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=5715, num_observations=20142), 7: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=3993, num_observations=12706), 8: Reconstruction(num_reg_images=5, num_cameras=5, num_points3D=774, num_observations=2338), 9: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=2455, num_observations=7775)}\nmap_index = 0 peach_0094.png peach_0108.png peach_0110.png\nmap_index = 1 peach_0003.png peach_0004.png peach_0197.png peach_0005.png peach_0199.png peach_0008.png peach_0009.png peach_0010.png peach_0019.png peach_0020.png peach_0026.png peach_0027.png peach_0031.png peach_0033.png peach_0037.png peach_0038.png peach_0040.png peach_0044.png peach_0046.png peach_0055.png peach_0056.png peach_0057.png peach_0058.png peach_0062.png peach_0063.png peach_0065.png peach_0067.png peach_0073.png peach_0075.png peach_0076.png peach_0079.png peach_0081.png peach_0087.png peach_0089.png peach_0090.png peach_0091.png peach_0094.png peach_0095.png peach_0099.png peach_0100.png peach_0104.png peach_0106.png peach_0108.png peach_0110.png peach_0111.png peach_0112.png peach_0122.png peach_0125.png peach_0127.png peach_0128.png peach_0134.png peach_0139.png peach_0141.png peach_0143.png peach_0146.png peach_0148.png peach_0149.png peach_0150.png peach_0160.png peach_0162.png peach_0166.png peach_0167.png peach_0176.png peach_0179.png peach_0182.png peach_0183.png peach_0185.png peach_0193.png\nmap_index = 2 peach_0196.png peach_0007.png peach_0034.png peach_0047.png peach_0060.png peach_0064.png peach_0102.png peach_0114.png peach_0117.png peach_0131.png peach_0145.png peach_0156.png peach_0174.png peach_0177.png peach_0191.png\nmap_index = 3 peach_0032.png peach_0068.png peach_0069.png peach_0093.png peach_0133.png peach_0142.png peach_0147.png peach_0151.png peach_0154.png peach_0157.png peach_0158.png peach_0168.png peach_0169.png peach_0171.png peach_0178.png peach_0186.png\nmap_index = 4 peach_0016.png peach_0025.png peach_0028.png peach_0042.png peach_0086.png peach_0088.png peach_0103.png peach_0155.png peach_0161.png peach_0188.png peach_0189.png\nmap_index = 5 peach_0001.png peach_0002.png peach_0010.png peach_0013.png peach_0015.png peach_0029.png peach_0032.png peach_0041.png peach_0047.png peach_0048.png peach_0050.png peach_0068.png peach_0069.png peach_0083.png peach_0085.png peach_0093.png peach_0113.png peach_0115.png peach_0119.png peach_0129.png peach_0133.png peach_0135.png peach_0136.png peach_0142.png peach_0147.png peach_0151.png peach_0154.png peach_0157.png peach_0158.png peach_0159.png peach_0168.png peach_0169.png peach_0171.png peach_0172.png peach_0178.png peach_0186.png peach_0187.png\nmap_index = 6 peach_0195.png peach_0014.png peach_0021.png peach_0053.png peach_0072.png peach_0074.png peach_0077.png peach_0098.png peach_0138.png peach_0152.png\nmap_index = 7 peach_0194.png peach_0018.png peach_0035.png peach_0051.png peach_0061.png peach_0071.png peach_0092.png peach_0097.png peach_0101.png peach_0124.png peach_0164.png\nmap_index = 8 peach_0006.png peach_0024.png peach_0084.png peach_0116.png peach_0118.png\nmap_index = 9 peach_0045.png peach_0080.png peach_0107.png peach_0121.png peach_0126.png peach_0180.png\nDataset \"amy_gardens\" -> Registered 182 / 200 images with 10 clusters\n\nProcessing dataset \"fbk_vineyard\": 163 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 163/163 [00:09<00:00, 16.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1510\nMax:  0.3338\nMean: 0.2232\nStd:  0.0303\n20%:  0.1980\n30%:  0.2047\nUSED 50%:  0.2173\n75%:  0.2396\nShortlisting. Number of pairs to match: 3107. Done in 10.2027 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 163/163 [00:22<00:00,  7.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features detected in 22.7333 sec\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3107/3107 [16:23<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features matched in 983.9188 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 163/163 [00:01<00:00, 94.19it/s]\n  3%|▎         | 329/12561 [00:00<00:03, 3585.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ran RANSAC in 2.5929 sec\nReconstruction done in  192.7317 sec\n{0: Reconstruction(num_reg_images=42, num_cameras=42, num_points3D=31718, num_observations=117842), 1: Reconstruction(num_reg_images=29, num_cameras=29, num_points3D=13538, num_observations=46953), 2: Reconstruction(num_reg_images=34, num_cameras=34, num_points3D=18328, num_observations=61623), 3: Reconstruction(num_reg_images=12, num_cameras=12, num_points3D=5811, num_observations=19516), 4: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=5839, num_observations=18833), 5: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=6961, num_observations=22287)}\nmap_index = 0 vineyard_split_3_frame_0070.png vineyard_split_3_frame_0075.png vineyard_split_3_frame_0080.png vineyard_split_3_frame_0085.png vineyard_split_3_frame_0090.png vineyard_split_3_frame_0095.png vineyard_split_3_frame_0100.png vineyard_split_3_frame_0105.png vineyard_split_3_frame_0110.png vineyard_split_3_frame_0115.png vineyard_split_3_frame_0120.png vineyard_split_3_frame_0125.png vineyard_split_3_frame_0130.png vineyard_split_3_frame_0135.png vineyard_split_3_frame_0140.png vineyard_split_3_frame_0150.png vineyard_split_3_frame_0155.png vineyard_split_3_frame_0160.png vineyard_split_3_frame_0165.png vineyard_split_3_frame_0170.png vineyard_split_3_frame_0175.png vineyard_split_3_frame_0180.png vineyard_split_3_frame_0185.png vineyard_split_3_frame_0190.png vineyard_split_3_frame_0195.png vineyard_split_3_frame_0200.png vineyard_split_3_frame_0205.png vineyard_split_3_frame_0210.png vineyard_split_3_frame_0215.png vineyard_split_3_frame_0220.png vineyard_split_3_frame_0225.png vineyard_split_3_frame_0230.png vineyard_split_3_frame_0240.png vineyard_split_3_frame_0245.png vineyard_split_3_frame_0255.png vineyard_split_3_frame_0260.png vineyard_split_3_frame_0265.png vineyard_split_3_frame_0270.png vineyard_split_3_frame_0275.png vineyard_split_3_frame_0280.png vineyard_split_3_frame_0285.png vineyard_split_3_frame_0295.png\nmap_index = 1 vineyard_split_1_frame_1000.png vineyard_split_1_frame_1010.png vineyard_split_1_frame_1015.png vineyard_split_1_frame_1020.png vineyard_split_1_frame_1025.png vineyard_split_1_frame_1030.png vineyard_split_1_frame_1035.png vineyard_split_1_frame_1040.png vineyard_split_1_frame_1045.png vineyard_split_1_frame_1050.png vineyard_split_1_frame_1055.png vineyard_split_1_frame_1065.png vineyard_split_1_frame_1070.png vineyard_split_1_frame_1075.png vineyard_split_1_frame_1080.png vineyard_split_1_frame_1095.png vineyard_split_1_frame_1100.png vineyard_split_1_frame_1105.png vineyard_split_1_frame_1110.png vineyard_split_3_frame_1450.png vineyard_split_3_frame_1460.png vineyard_split_3_frame_1465.png vineyard_split_3_frame_1470.png vineyard_split_3_frame_1475.png vineyard_split_3_frame_1480.png vineyard_split_3_frame_1485.png vineyard_split_3_frame_1490.png vineyard_split_3_frame_1495.png vineyard_split_3_frame_1500.png\nmap_index = 2 vineyard_split_2_frame_1150.png vineyard_split_2_frame_1155.png vineyard_split_2_frame_1160.png vineyard_split_2_frame_1165.png vineyard_split_2_frame_1170.png vineyard_split_2_frame_1175.png vineyard_split_2_frame_1180.png vineyard_split_2_frame_1185.png vineyard_split_2_frame_1195.png vineyard_split_2_frame_1200.png vineyard_split_2_frame_1205.png vineyard_split_2_frame_1210.png vineyard_split_2_frame_1215.png vineyard_split_2_frame_1220.png vineyard_split_2_frame_1225.png vineyard_split_2_frame_1230.png vineyard_split_2_frame_1235.png vineyard_split_2_frame_1240.png vineyard_split_2_frame_1245.png vineyard_split_2_frame_1250.png vineyard_split_2_frame_1255.png vineyard_split_2_frame_1260.png vineyard_split_2_frame_1265.png vineyard_split_2_frame_1270.png vineyard_split_2_frame_1275.png vineyard_split_2_frame_1280.png vineyard_split_2_frame_1285.png vineyard_split_2_frame_1290.png vineyard_split_2_frame_1295.png vineyard_split_2_frame_1300.png vineyard_split_2_frame_1305.png vineyard_split_2_frame_1310.png vineyard_split_2_frame_1315.png vineyard_split_2_frame_1320.png\nmap_index = 3 vineyard_split_1_frame_0940.png vineyard_split_1_frame_0945.png vineyard_split_1_frame_0950.png vineyard_split_1_frame_0955.png vineyard_split_1_frame_0960.png vineyard_split_1_frame_0965.png vineyard_split_1_frame_0970.png vineyard_split_1_frame_0975.png vineyard_split_1_frame_0980.png vineyard_split_1_frame_0985.png vineyard_split_1_frame_0990.png vineyard_split_1_frame_1000.png\nmap_index = 4 vineyard_split_3_frame_1510.png vineyard_split_3_frame_1515.png vineyard_split_3_frame_1520.png vineyard_split_3_frame_1525.png vineyard_split_3_frame_1530.png vineyard_split_3_frame_1535.png vineyard_split_3_frame_1540.png vineyard_split_3_frame_1545.png vineyard_split_3_frame_1550.png vineyard_split_3_frame_1555.png vineyard_split_3_frame_1560.png vineyard_split_3_frame_1565.png vineyard_split_3_frame_1570.png\nmap_index = 5 vineyard_split_1_frame_0900.png vineyard_split_1_frame_0905.png vineyard_split_1_frame_0910.png vineyard_split_1_frame_0915.png vineyard_split_1_frame_0920.png vineyard_split_1_frame_0925.png vineyard_split_1_frame_0930.png vineyard_split_1_frame_0935.png vineyard_split_1_frame_0940.png vineyard_split_1_frame_0945.png vineyard_split_1_frame_0950.png vineyard_split_1_frame_0955.png vineyard_split_1_frame_0960.png vineyard_split_1_frame_0965.png vineyard_split_1_frame_0970.png\nDataset \"fbk_vineyard\" -> Registered 145 / 163 images with 6 clusters\n\nProcessing dataset \"ETs\": 22 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22/22 [00:01<00:00, 17.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1504\nMax:  0.4104\nMean: 0.2817\nStd:  0.0495\n20%:  0.2356\n30%:  0.2647\nUSED 50%:  0.2804\n75%:  0.3260\nShortlisting. Number of pairs to match: 141. Done in 1.4897 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22/22 [00:02<00:00,  7.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features detected in 3.2189 sec\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 141/141 [00:09<00:00, 14.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features matched in 10.1593 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 22/22 [00:00<00:00, 91.20it/s]\n 30%|███       | 64/210 [00:00<00:00, 4239.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ran RANSAC in 1.0671 sec\nReconstruction done in  10.8713 sec\n{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=4202, num_observations=20248), 1: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=1538, num_observations=8704)}\nmap_index = 0 et_et000.png et_et001.png et_et002.png et_et003.png et_et004.png et_et005.png et_et006.png et_et007.png et_et008.png\nmap_index = 1 another_et_another_et001.png another_et_another_et002.png another_et_another_et003.png another_et_another_et004.png another_et_another_et005.png another_et_another_et006.png another_et_another_et007.png another_et_another_et008.png another_et_another_et009.png another_et_another_et010.png outliers_out_et001.png\nDataset \"ETs\" -> Registered 20 / 22 images with 2 clusters\n\nProcessing dataset \"stairs\": 51 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 51/51 [00:09<00:00,  5.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1598\nMax:  0.4240\nMean: 0.2807\nStd:  0.0451\n20%:  0.2433\n30%:  0.2557\nUSED 50%:  0.2767\n75%:  0.3089\nShortlisting. Number of pairs to match: 640. Done in 9.3396 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 51/51 [00:09<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features detected in 9.6825 sec\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 640/640 [01:10<00:00,  9.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Features matched in 70.3864 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 51/51 [00:02<00:00, 21.69it/s]\n 11%|█         | 125/1176 [00:00<00:00, 3744.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Ran RANSAC in 1.5749 sec\nReconstruction done in  19.7608 sec\n{0: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=1358, num_observations=3558), 1: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=868, num_observations=2534)}\nmap_index = 0 stairs_split_1_1710453601885.png stairs_split_2_1710453805788.png stairs_split_2_1710453871430.png stairs_split_1_1710453693529.png stairs_split_2_1710453720741.png stairs_split_2_1710453739354.png stairs_split_2_1710453740954.png stairs_split_2_1710453759963.png stairs_split_2_1710453783374.png stairs_split_2_1710453786375.png\nmap_index = 1 stairs_split_2_1710453862225.png stairs_split_1_1710453675921.png stairs_split_1_1710453704934.png stairs_split_1_1710453901046.png stairs_split_1_1710453947066.png stairs_split_1_1710453990286.png stairs_split_2_1710453745156.png stairs_split_2_1710453790978.png stairs_split_2_1710453793579.png stairs_split_2_1710453798181.png\nDataset \"stairs\" -> Registered 20 / 51 images with 2 clusters\n\nResults\nDataset \"imc2023_heritage\" -> Registered 128 / 209 images with 5 clusters\nDataset \"pt_brandenburg_british_buckingham\" -> Registered 224 / 225 images with 3 clusters\nDataset \"amy_gardens\" -> Registered 182 / 200 images with 10 clusters\nDataset \"fbk_vineyard\" -> Registered 145 / 163 images with 6 clusters\nDataset \"ETs\" -> Registered 20 / 22 images with 2 clusters\nDataset \"stairs\" -> Registered 20 / 51 images with 2 clusters\n\nTimings\nshortlisting -> total=346.02 sec.\nfeature_detection -> total=191.66 sec.\nfeature_matching -> total=4446.56 sec.\nRANSAC -> total=59.00 sec.\nReconstruction -> total=1644.91 sec.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Must Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:06:06.216643Z","iopub.execute_input":"2025-05-15T08:06:06.216902Z","iopub.status.idle":"2025-05-15T08:06:06.426525Z","shell.execute_reply.started":"2025-05-15T08:06:06.216879Z","shell.execute_reply":"2025-05-15T08:06:06.425676Z"}},"outputs":[{"name":"stdout","text":"dataset,scene,image,rotation_matrix,translation_vector\nimc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Definitely Compute results if running on the training set.\n# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n\nif is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:06:06.427594Z","iopub.execute_input":"2025-05-15T08:06:06.427843Z","iopub.status.idle":"2025-05-15T08:13:01.174631Z","shell.execute_reply.started":"2025-05-15T08:06:06.427820Z","shell.execute_reply":"2025-05-15T08:13:01.173640Z"}},"outputs":[{"name":"stdout","text":"imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2023_heritage: score=56.70% (mAA=39.57%, clusterness=100.00%)\nimc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_brandenburg_british_buckingham: score=86.61% (mAA=76.39%, clusterness=100.00%)\npt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\namy_gardens: score=26.62% (mAA=15.36%, clusterness=100.00%)\nfbk_vineyard: score=44.16% (mAA=29.22%, clusterness=90.38%)\nETs: score=48.64% (mAA=32.69%, clusterness=95.00%)\nstairs: score=0.00% (mAA=0.00%, clusterness=65.00%)\nAverage over all datasets: score=20.21% (mAA=14.86%, clusterness=42.34%)\nComputed metric in: 414.74 sec.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}