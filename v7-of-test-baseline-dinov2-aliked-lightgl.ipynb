{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47267cd8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004223,
     "end_time": "2025-05-14T06:53:22.465070",
     "exception": false,
     "start_time": "2025-05-14T06:53:22.460847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655fb80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:53:22.473276Z",
     "iopub.status.busy": "2025-05-14T06:53:22.472960Z",
     "iopub.status.idle": "2025-05-14T06:53:28.958874Z",
     "shell.execute_reply": "2025-05-14T06:53:28.957906Z"
    },
    "papermill": {
     "duration": 6.491758,
     "end_time": "2025-05-14T06:53:28.960749",
     "exception": false,
     "start_time": "2025-05-14T06:53:22.468991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfeb636",
   "metadata": {
    "papermill": {
     "duration": 0.004078,
     "end_time": "2025-05-14T06:53:28.969507",
     "exception": false,
     "start_time": "2025-05-14T06:53:28.965429",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f6d255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:53:28.978516Z",
     "iopub.status.busy": "2025-05-14T06:53:28.978235Z",
     "iopub.status.idle": "2025-05-14T06:54:04.187653Z",
     "shell.execute_reply": "2025-05-14T06:54:04.186953Z"
    },
    "papermill": {
     "duration": 35.215539,
     "end_time": "2025-05-14T06:54:04.189038",
     "exception": false,
     "start_time": "2025-05-14T06:53:28.973499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde9a148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:04.198730Z",
     "iopub.status.busy": "2025-05-14T06:54:04.198215Z",
     "iopub.status.idle": "2025-05-14T06:54:04.318932Z",
     "shell.execute_reply": "2025-05-14T06:54:04.317941Z"
    },
    "papermill": {
     "duration": 0.126865,
     "end_time": "2025-05-14T06:54:04.320398",
     "exception": false,
     "start_time": "2025-05-14T06:54:04.193533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669bdba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:04.329703Z",
     "iopub.status.busy": "2025-05-14T06:54:04.329487Z",
     "iopub.status.idle": "2025-05-14T06:54:04.333003Z",
     "shell.execute_reply": "2025-05-14T06:54:04.332159Z"
    },
    "papermill": {
     "duration": 0.009463,
     "end_time": "2025-05-14T06:54:04.334201",
     "exception": false,
     "start_time": "2025-05-14T06:54:04.324738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b5f3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:04.343280Z",
     "iopub.status.busy": "2025-05-14T06:54:04.343031Z",
     "iopub.status.idle": "2025-05-14T06:54:04.348050Z",
     "shell.execute_reply": "2025-05-14T06:54:04.347302Z"
    },
    "papermill": {
     "duration": 0.010872,
     "end_time": "2025-05-14T06:54:04.349313",
     "exception": false,
     "start_time": "2025-05-14T06:54:04.338441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pil_image(fname):\n",
    "    \"\"\"Loads an image using PIL.\"\"\"\n",
    "    return Image.open(fname).convert('RGB')\n",
    "\n",
    "def get_image_size(fname):\n",
    "    \"\"\"Gets image size (width, height) using PIL.\"\"\"\n",
    "    with Image.open(fname) as img:\n",
    "        return img.size # (width, height)\n",
    "\n",
    "def get_original_coords(kp_coords, img_orig_size, variation_info):\n",
    "    \"\"\"\n",
    "    Transforms keypoint coordinates from variation space back to original image space.\n",
    "\n",
    "    Args:\n",
    "        kp_coords (np.ndarray): Keypoint coordinates [N, 2] in the variation space.\n",
    "        img_orig_size (tuple): Original image size (width, height).\n",
    "        variation_info (dict): Dictionary containing 'type' ('orig' or 'crop'),\n",
    "                               'scale_factor' (scale used for resize),\n",
    "                               'crop_box' ([x, y, w, h] in original coords, None if type is 'orig').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n",
    "    \"\"\"\n",
    "    if len(kp_coords) == 0:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    coords = kp_coords.copy() # Work on a copy\n",
    "\n",
    "    # 1. Reverse scaling\n",
    "    scale_factor = variation_info['scale_factor']\n",
    "    coords /= scale_factor # Now coords are in the space of the original/cropped image (before resize)\n",
    "\n",
    "    # 2. Reverse cropping offset\n",
    "    if variation_info['type'] == 'crop' and variation_info['crop_box'] is not None:\n",
    "        x_crop, y_crop, _, _ = variation_info['crop_box']\n",
    "        coords[:, 0] += x_crop\n",
    "        coords[:, 1] += y_crop\n",
    "\n",
    "    # Ensure points are within original image bounds (optional, but good practice)\n",
    "    # coords[:, 0] = np.clip(coords[:, 0], 0, img_orig_size[0] - 1)\n",
    "    # coords[:, 1] = np.clip(coords[:, 1], 0, img_orig_size[1] - 1)\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82fb5101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:04.358329Z",
     "iopub.status.busy": "2025-05-14T06:54:04.358069Z",
     "iopub.status.idle": "2025-05-14T06:54:04.363438Z",
     "shell.execute_reply": "2025-05-14T06:54:04.362640Z"
    },
    "papermill": {
     "duration": 0.011303,
     "end_time": "2025-05-14T06:54:04.364757",
     "exception": false,
     "start_time": "2025-05-14T06:54:04.353454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_kornia_resize_scale(original_size_hw, target_resize):\n",
    "    \"\"\"\n",
    "    Calculates the scale factor Kornia's default resize applies.\n",
    "    Assumes aspect ratio is maintained and longer side is scaled to target_resize,\n",
    "    only if longer side > target_resize.\n",
    "\n",
    "    Args:\n",
    "        original_size_hw (tuple): Original image size (H, W).\n",
    "        target_resize (int): The target size for the longer side.\n",
    "\n",
    "    Returns:\n",
    "        float: The scale factor applied (processed_size / original_size).\n",
    "    \"\"\"\n",
    "    h_orig, w_orig = original_size_hw\n",
    "    max_orig_dim = max(h_orig, w_orig)\n",
    "\n",
    "    if target_resize is None or target_resize <= 0 or max_orig_dim <= target_resize:\n",
    "        # No resizing or scaling up is needed based on default logic\n",
    "        return 1.0\n",
    "    else:\n",
    "        # Scale down the longer side to target_resize\n",
    "        return target_resize / max_orig_dim\n",
    "\n",
    "def transform_points_from_processed(kp_processed, processed_scale_factor, crop_box=None):\n",
    "    \"\"\"\n",
    "    Transforms keypoint coordinates from the 'processed' scale space\n",
    "    back to the original image space, accounting for scaling and cropping.\n",
    "\n",
    "    Args:\n",
    "        kp_processed (np.ndarray): Keypoint coordinates [N, 2] in the processed space (after scaling by ALIKED).\n",
    "        processed_scale_factor (float): The calculated scale factor applied by ALIKED (processed_size / original_or_cropped_size).\n",
    "        crop_box (list): [x, y, w, h] of the crop in original image coords, or None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n",
    "    \"\"\"\n",
    "    if len(kp_processed) == 0:\n",
    "        return np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "    coords = kp_processed.copy().astype(np.float32) # Ensure float32\n",
    "\n",
    "    # 1. Reverse scaling (from processed scale back to original or cropped scale)\n",
    "    if processed_scale_factor > 0:\n",
    "        coords /= processed_scale_factor\n",
    "    # else: scale_factor is 1.0, no change needed here\n",
    "\n",
    "    # 2. Add cropping offset (from cropped coordinates back to original coordinates)\n",
    "    if crop_box is not None and len(crop_box) == 4 and crop_box[2] > 0 and crop_box[3] > 0:\n",
    "        x_crop, y_crop, _, _ = crop_box\n",
    "        coords[:, 0] += x_crop\n",
    "        coords[:, 1] += y_crop\n",
    "\n",
    "    # Note: We don't clip to original bounds here, as that might discard valid points near edges.\n",
    "    # Downstream steps should handle points outside bounds if necessary.\n",
    "\n",
    "    return coords\n",
    "\n",
    "# Remove the old get_keypoint_original_coords function entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c4f901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:04.373776Z",
     "iopub.status.busy": "2025-05-14T06:54:04.373550Z",
     "iopub.status.idle": "2025-05-14T06:54:04.392340Z",
     "shell.execute_reply": "2025-05-14T06:54:04.391545Z"
    },
    "papermill": {
     "duration": 0.024777,
     "end_time": "2025-05-14T06:54:04.393610",
     "exception": false,
     "start_time": "2025-05-14T06:54:04.368833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "\n",
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 30,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 50)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list\n",
    "\n",
    "def detect_aliked(img_fnames,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    # Calculate the expected scale factor ALIKED will apply\n",
    "    # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n",
    "    # Input image tensor size will be (H, W) after Kornia loading/conversion\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            # Load original PIL image to get its size\n",
    "            img_orig_pil = load_pil_image(img_path)\n",
    "            if img_orig_pil is None:\n",
    "                 print(f\"Skipping ALIKED for {img_path}: PIL image loading failed.\")\n",
    "                 continue\n",
    "            original_pil_size = img_orig_pil.size # (W, H)\n",
    "            input_tensor_size_hw = (original_pil_size[1], original_pil_size[0]) # Convert (W, H) to (H, W)\n",
    "            calculated_scale_factor = calculate_kornia_resize_scale(input_tensor_size_hw, resize_to)\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                kpts = transform_points_from_processed(\n",
    "                                 kpts, calculated_scale_factor, crop_box=None)\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return\n",
    "\n",
    "def match_with_lightglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=20,verbose=True):\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                         desc2,\n",
    "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches')\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ebfd52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:04.402530Z",
     "iopub.status.busy": "2025-05-14T06:54:04.402324Z",
     "iopub.status.idle": "2025-05-14T06:54:05.421094Z",
     "shell.execute_reply": "2025-05-14T06:54:05.420367Z"
    },
    "papermill": {
     "duration": 1.025058,
     "end_time": "2025-05-14T06:54:05.422696",
     "exception": false,
     "start_time": "2025-05-14T06:54:04.397638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "def match_with_lightglue_and_cluster(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    feature_dir='.featureout',\n",
    "    device=torch.device('cpu'),\n",
    "    min_matches=20,\n",
    "    aliked_dis_min=100,\n",
    "    verbose=True\n",
    "):\n",
    "    # 初始化 LightGlue 匹配器\n",
    "    lg_matcher = KF.LightGlueMatcher(\n",
    "        \"aliked\", {\n",
    "            \"width_confidence\": -1,\n",
    "            \"depth_confidence\": -1,\n",
    "            \"mp\": 'cuda' in str(device)\n",
    "        }\n",
    "    ).eval().to(device)\n",
    "\n",
    "    num_imgs = len(img_fnames)\n",
    "    match_graph = nx.Graph()\n",
    "    match_graph.add_nodes_from(range(num_imgs))\n",
    "\n",
    "    # 加载关键点和描述子，准备写入匹配结果\n",
    "    with h5py.File(os.path.join(feature_dir, 'keypoints.h5'), 'r') as f_kp, \\\n",
    "         h5py.File(os.path.join(feature_dir, 'descriptors.h5'), 'r') as f_desc, \\\n",
    "         h5py.File(os.path.join(feature_dir, 'matches.h5'), 'w') as f_match:\n",
    "\n",
    "        # Step 1: 匹配每对图像\n",
    "        for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n",
    "            img1 = os.path.basename(img_fnames[idx1])\n",
    "            img2 = os.path.basename(img_fnames[idx2])\n",
    "\n",
    "            kp1 = torch.from_numpy(f_kp[img1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[img2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[img1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[img2][...]).to(device)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                _, idxs = lg_matcher(\n",
    "                    desc1, desc2,\n",
    "                    KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                    KF.laf_from_center_scale_ori(kp2[None])\n",
    "                )\n",
    "\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "\n",
    "            n_matches = len(idxs)\n",
    "            if verbose:\n",
    "                print(f'{img1} - {img2}: {n_matches} matches')\n",
    "\n",
    "            if n_matches >= min_matches:\n",
    "                match_graph.add_edge(idx1, idx2, weight=n_matches)\n",
    "                group = f_match.require_group(img1)\n",
    "                group.create_dataset(img2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "\n",
    "        # Step 2: 提取聚类（connected components）\n",
    "        raw_clusters = list(nx.connected_components(match_graph))\n",
    "        final_clusters = []\n",
    "        outliers = set()\n",
    "\n",
    "        for cluster in raw_clusters:\n",
    "            cluster = list(cluster)\n",
    "            subgraph = match_graph.subgraph(cluster)\n",
    "            valid_nodes = []\n",
    "\n",
    "            for node in cluster:\n",
    "                strong_neighbors = [\n",
    "                    neighbor for neighbor in subgraph.neighbors(node)\n",
    "                    if subgraph[node][neighbor]['weight'] >= aliked_dis_min\n",
    "                ]\n",
    "                if strong_neighbors:\n",
    "                    valid_nodes.append(node)\n",
    "                else:\n",
    "                    outliers.add(node)\n",
    "\n",
    "            if len(valid_nodes) >= 2:\n",
    "                final_clusters.append(valid_nodes)\n",
    "            else:\n",
    "                outliers.update(valid_nodes)\n",
    "\n",
    "        # Step 3: 清理掉涉及 outlier 的匹配项（保留聚类内匹配）\n",
    "        # outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n",
    "\n",
    "        # for group_name in f_match.keys():\n",
    "        #     group = f_match[group_name]\n",
    "        #     for dataset_name in list(group.keys()):\n",
    "        #         if group_name in outlier_names or dataset_name in outlier_names:\n",
    "        #             del group[dataset_name]\n",
    "        #             if verbose:\n",
    "        #                 print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n",
    "        \n",
    "        # Step 3: 清理掉涉及 outlier 的匹配项\n",
    "        outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n",
    "\n",
    "        groups_to_delete = []\n",
    "\n",
    "        for group_name in list(f_match.keys()):\n",
    "            group = f_match[group_name]\n",
    "            datasets_to_delete = []\n",
    "\n",
    "            for dataset_name in list(group.keys()):\n",
    "                # 删除与 outlier 有关的 match\n",
    "                if group_name in outlier_names or dataset_name in outlier_names:\n",
    "                    datasets_to_delete.append(dataset_name)\n",
    "                    if verbose:\n",
    "                        print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n",
    "\n",
    "            # 先删除标记的 dataset\n",
    "            for ds in datasets_to_delete:\n",
    "                del group[ds]\n",
    "\n",
    "            # 新增逻辑：如果剩下不到 3 个 match，就删掉整个 group\n",
    "            if len(group.keys()) < 3:\n",
    "                groups_to_delete.append(group_name)\n",
    "\n",
    "        # 删除 group 本身\n",
    "        for gname in groups_to_delete:\n",
    "            del f_match[gname]\n",
    "            if verbose:\n",
    "                print(f\"Deleted group: {gname} (too few matches)\")\n",
    "\n",
    "    if True:\n",
    "        print(list(outliers))\n",
    "        for i, cluster in enumerate(final_clusters):\n",
    "            print(f\"Cluster {i} ({len(cluster)} images):\")\n",
    "    return final_clusters, sorted(list(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc8797",
   "metadata": {
    "papermill": {
     "duration": 0.004607,
     "end_time": "2025-05-14T06:54:05.432374",
     "exception": false,
     "start_time": "2025-05-14T06:54:05.427767",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36534cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:05.442956Z",
     "iopub.status.busy": "2025-05-14T06:54:05.442384Z",
     "iopub.status.idle": "2025-05-14T06:54:05.629797Z",
     "shell.execute_reply": "2025-05-14T06:54:05.629126Z"
    },
    "papermill": {
     "duration": 0.194015,
     "end_time": "2025-05-14T06:54:05.631033",
     "exception": false,
     "start_time": "2025-05-14T06:54:05.437018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4db2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:54:05.640412Z",
     "iopub.status.busy": "2025-05-14T06:54:05.640157Z",
     "iopub.status.idle": "2025-05-14T06:56:19.906965Z",
     "shell.execute_reply": "2025-05-14T06:56:19.906040Z"
    },
    "papermill": {
     "duration": 134.294298,
     "end_time": "2025-05-14T06:56:19.929647",
     "exception": false,
     "start_time": "2025-05-14T06:54:05.635349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting on device cuda:0\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:03<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "25%:  0.2547\n",
      "USED 50%:  0.2804\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 142. Done in 8.7356 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 2.6437 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:05<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 5.7915 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 72.24it/s]\n",
      " 63%|██████▎   | 120/190 [00:00<00:00, 4991.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 2.7208 sec\n",
      "Reconstruction done in  14.4038 sec\n",
      "{0: Reconstruction(num_reg_images=20, num_cameras=20, num_points3D=4097, num_observations=19371)}\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images with 1 clusters\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/amy_gardens/peach_0000.png\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/fbk_vineyard/vineyard_split_1_frame_0900.png\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_haiper/bike_image_004.png\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_heritage\": 209 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/209 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_heritage/cyprus_dsc_6480.png\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_theather_imc2024_church\": 76 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_theather_imc2024_church/church_00004.png\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_dioscuri_baalshamin\": 138 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_dioscuri_baalshamin/baalshamin_182z.png\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_lizard_pond\": 214 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/214 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_lizard_pond/lizard_00003.png\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_brandenburg_british_buckingham\": 225 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_brandenburg_british_buckingham/brandenburg_gate_01069771_8567470929.png\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_piazzasanmarco_grandplace\": 168 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/168 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_piazzasanmarco_grandplace/grand_place_brussels_00460368_4162644685.png\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_sacrecoeur_trevi_tajmahal\": 225 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_sacrecoeur_trevi_tajmahal/sacre_coeur_02928139_3448003521.png\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_stpeters_stpauls\": 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_stpeters_stpauls/st_pauls_cathedral_00162897_2573777698.png\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:10<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "USED 50%:  0.2767\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 650. Done in 10.3727 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:06<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 6.7174 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 650/650 [00:22<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 22.7902 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 21.35it/s]\n",
      " 24%|██▍       | 296/1225 [00:00<00:00, 5582.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 7.7415 sec\n",
      "Reconstruction done in  42.4735 sec\n",
      "{0: Reconstruction(num_reg_images=17, num_cameras=17, num_points3D=1876, num_observations=5801), 1: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=606, num_observations=1961), 2: Reconstruction(num_reg_images=18, num_cameras=18, num_points3D=753, num_observations=2333)}\n",
      "Dataset \"stairs\" -> Registered 44 / 51 images with 3 clusters\n",
      "\n",
      "Results\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images with 1 clusters\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "Dataset \"stairs\" -> Registered 44 / 51 images with 3 clusters\n",
      "\n",
      "Timings\n",
      "shortlisting -> total=19.11 sec.\n",
      "feature_detection -> total=9.36 sec.\n",
      "feature_matching -> total=28.58 sec.\n",
      "RANSAC -> total=10.46 sec.\n",
      "Reconstruction -> total=56.88 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "print (f\"Extracting on device {device}\")\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "    try:\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.5, # should be strict\n",
    "            min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "    \n",
    "        t = time()\n",
    "\n",
    "        detect_aliked(images, feature_dir, 4096, device=device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "        \n",
    "        t = time()\n",
    "        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        # match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'Ran RANSAC in {time() - t:.4f} sec')\n",
    "        \n",
    "        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "        # Lower it to 3.\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = 8\n",
    "        mapper_options.max_num_models = 25\n",
    "        mapper_options.mapper.filter_max_reproj_error\t = 10.0\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path, \n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options)\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction done in  {time() - t:.4f} sec')\n",
    "        print(maps)\n",
    "\n",
    "        # clear_output(wait=False)\n",
    "    \n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                registered += 1\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6728172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:56:19.977893Z",
     "iopub.status.busy": "2025-05-14T06:56:19.977550Z",
     "iopub.status.idle": "2025-05-14T06:56:20.151407Z",
     "shell.execute_reply": "2025-05-14T06:56:20.150384Z"
    },
    "papermill": {
     "duration": 0.20007,
     "end_time": "2025-05-14T06:56:20.152929",
     "exception": false,
     "start_time": "2025-05-14T06:56:19.952859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster0,another_et_another_et001.png,0.661268918;-0.440250366;0.607373883;0.582461861;0.811561516;-0.045892112;-0.472717250;0.384119149;0.793089453,-1.575978893;-1.284916307;0.892816218\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster0,another_et_another_et002.png,0.668225169;-0.443927327;0.596995520;0.586943608;0.807661318;-0.056394998;-0.457134908;0.388087262;0.800259929,-1.454967468;-0.716854956;-0.038404077\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster0,another_et_another_et003.png,0.612977954;-0.458353214;0.643560689;0.634644128;0.770808755;-0.055503994;-0.470621779;0.442454737;0.763379949,-1.673201420;0.399983865;-1.206991713\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster0,another_et_another_et004.png,0.643454782;-0.437293645;0.628283544;0.525599679;0.849098629;0.052692464;-0.556516775;0.296320411;0.776197973,-1.571552970;-0.985321229;-0.919301776\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster0,another_et_another_et005.png,0.627810600;-0.403403370;0.665672270;0.556018414;0.830908276;-0.020855691;-0.544699343;0.383219464;0.745952725,-1.971217006;-1.441989250;0.075683593\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster0,another_et_another_et006.png,0.876359358;-0.378989614;0.297256031;0.449924963;0.864430483;-0.224337842;-0.171935462;0.330343476;0.928068632,0.179825377;-0.450914101;0.124331600\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster0,another_et_another_et007.png,0.936407344;-0.348256793;0.043110230;0.348444300;0.908219063;-0.231785898;0.041567481;0.232067531;0.971811096,1.494795721;-0.325471530;-0.035870819\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster0,another_et_another_et008.png,0.925114751;-0.293002211;-0.241479611;0.238478160;0.943290244;-0.230936535;0.295450276;0.156055182;0.942526347,2.793527977;-0.656061456;0.762291277\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster0,another_et_another_et009.png,0.849333563;-0.201302331;-0.487965031;0.099710417;0.968968605;-0.226180624;0.518353482;0.143447599;0.843049497,4.002073823;-1.005266749;1.432749332\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c711e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:56:20.201073Z",
     "iopub.status.busy": "2025-05-14T06:56:20.200809Z",
     "iopub.status.idle": "2025-05-14T06:56:20.205030Z",
     "shell.execute_reply": "2025-05-14T06:56:20.204432Z"
    },
    "papermill": {
     "duration": 0.029292,
     "end_time": "2025-05-14T06:56:20.206320",
     "exception": false,
     "start_time": "2025-05-14T06:56:20.177028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe4127",
   "metadata": {
    "papermill": {
     "duration": 0.023093,
     "end_time": "2025-05-14T06:56:20.252379",
     "exception": false,
     "start_time": "2025-05-14T06:56:20.229286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11217117,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 185.333238,
   "end_time": "2025-05-14T06:56:24.260163",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-14T06:53:18.926925",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
