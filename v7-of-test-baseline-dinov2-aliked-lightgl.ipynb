{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2bb81d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003648,
     "end_time": "2025-05-15T21:30:02.376809",
     "exception": false,
     "start_time": "2025-05-15T21:30:02.373161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ca35c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:02.384102Z",
     "iopub.status.busy": "2025-05-15T21:30:02.383742Z",
     "iopub.status.idle": "2025-05-15T21:30:08.045718Z",
     "shell.execute_reply": "2025-05-15T21:30:08.044721Z"
    },
    "papermill": {
     "duration": 5.6674,
     "end_time": "2025-05-15T21:30:08.047508",
     "exception": false,
     "start_time": "2025-05-15T21:30:02.380108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c538bf57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:08.056134Z",
     "iopub.status.busy": "2025-05-15T21:30:08.055882Z",
     "iopub.status.idle": "2025-05-15T21:30:30.723691Z",
     "shell.execute_reply": "2025-05-15T21:30:30.723029Z"
    },
    "papermill": {
     "duration": 22.673643,
     "end_time": "2025-05-15T21:30:30.725250",
     "exception": false,
     "start_time": "2025-05-15T21:30:08.051607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fccee7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:30.734372Z",
     "iopub.status.busy": "2025-05-15T21:30:30.733884Z",
     "iopub.status.idle": "2025-05-15T21:30:30.736937Z",
     "shell.execute_reply": "2025-05-15T21:30:30.736311Z"
    },
    "papermill": {
     "duration": 0.008863,
     "end_time": "2025-05-15T21:30:30.738226",
     "exception": false,
     "start_time": "2025-05-15T21:30:30.729363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(\"PyTorch version:\", torch.__version__)\n",
    "# import sys\n",
    "# print(\"Python version:\", sys.version)\n",
    "\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA version:\", torch.version.cuda)\n",
    "# print(\"Device count:\", torch.cuda.device_count())\n",
    "# print(\"Current device:\", torch.cuda.current_device())\n",
    "# print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9ef958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:30.746343Z",
     "iopub.status.busy": "2025-05-15T21:30:30.746097Z",
     "iopub.status.idle": "2025-05-15T21:30:30.799669Z",
     "shell.execute_reply": "2025-05-15T21:30:30.798846Z"
    },
    "papermill": {
     "duration": 0.059127,
     "end_time": "2025-05-15T21:30:30.801059",
     "exception": false,
     "start_time": "2025-05-15T21:30:30.741932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194858fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:30.809877Z",
     "iopub.status.busy": "2025-05-15T21:30:30.809608Z",
     "iopub.status.idle": "2025-05-15T21:30:30.812595Z",
     "shell.execute_reply": "2025-05-15T21:30:30.811975Z"
    },
    "papermill": {
     "duration": 0.008331,
     "end_time": "2025-05-15T21:30:30.813668",
     "exception": false,
     "start_time": "2025-05-15T21:30:30.805337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r /kaggle/working/result/featureout/ETs/featurept.zip /kaggle/working/result/featureout/ETs/featurept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5ef95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:30.821675Z",
     "iopub.status.busy": "2025-05-15T21:30:30.821470Z",
     "iopub.status.idle": "2025-05-15T21:30:30.826247Z",
     "shell.execute_reply": "2025-05-15T21:30:30.825577Z"
    },
    "papermill": {
     "duration": 0.010011,
     "end_time": "2025-05-15T21:30:30.827413",
     "exception": false,
     "start_time": "2025-05-15T21:30:30.817402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def draw_and_save_feature_points(image_path, keypoints, result_folder):\n",
    "    \"\"\"\n",
    "    Draw feature points on the image and save to result folder.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the input image.\n",
    "        keypoints (np.ndarray): (N, 2) array of (x, y) coordinates.\n",
    "        result_folder (str or Path): Folder to save the output image.\n",
    "    \"\"\"\n",
    "    # Load image in BGR\n",
    "    return\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Cannot read image from {image_path}\")\n",
    "\n",
    "    # Draw keypoints\n",
    "    for (x, y) in keypoints.astype(int):\n",
    "        cv2.circle(image, (x, y), radius=2, color=(0, 255, 0), thickness=-1)  # Green dots\n",
    "\n",
    "    img_fname = image_path.split('/')[-1]\n",
    "\n",
    "    result_folder = Path(result_folder)\n",
    "    img_fname = Path(image_path).stem  # no extension\n",
    "    output_path = result_folder / f\"{img_fname}_fe.png\"\n",
    "\n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2fec8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:30.835706Z",
     "iopub.status.busy": "2025-05-15T21:30:30.835503Z",
     "iopub.status.idle": "2025-05-15T21:30:30.855074Z",
     "shell.execute_reply": "2025-05-15T21:30:30.854428Z"
    },
    "papermill": {
     "duration": 0.024929,
     "end_time": "2025-05-15T21:30:30.856161",
     "exception": false,
     "start_time": "2025-05-15T21:30:30.831232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.normalize(torch.mean(x.clamp(min=eps).pow(p), dim=1).pow(1/p), p=2, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu'), is_max = True):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            if is_max:\n",
    "                dino_fea = gem(outputs.last_hidden_state[:,1:], p=3)\n",
    "            else:\n",
    "                dino_fea = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_fea.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 10,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu'),\n",
    "                              max_pairs = 30):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device, is_max = False)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # print(dm)\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"30%:  {np.percentile(dm_flat, 30):.4f}\")\n",
    "    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    \n",
    "    thr = np.percentile(dm_flat, 50)\n",
    "    threshold = max(dm_flat.mean() + np.sqrt(3) * dm_flat.std(), thr)\n",
    "    # removing half\n",
    "    mask = dm <= thr\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs :\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        if len(to_match) >= max_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:max_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list\n",
    "\n",
    "def detect_aliked(img_fnames,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 2048,\n",
    "                  device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.15).eval().to(device, dtype)\n",
    "    extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    # Calculate the expected scale factor ALIKED will apply\n",
    "    # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n",
    "    # Input image tensor size will be (H, W) after Kornia loading/conversion\n",
    "    \n",
    "    draw_feature_dir = os.path.join(feature_dir, 'featurept')\n",
    "    os.makedirs(draw_feature_dir, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "                draw_and_save_feature_points(img_path, kpts, draw_feature_dir)\n",
    "    return\n",
    "\n",
    "def match_with_lightglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=30,\n",
    "                   verbose=False,\n",
    "                   match_score_thresh = 0.25):\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                         desc2,\n",
    "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            # if verbose:\n",
    "            #     print (f'{key1}-{key2}: {n_matches} matches')\n",
    "            # group  = f_match.require_group(key1)\n",
    "            # if n_matches >= min_matches:\n",
    "            #      group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "            # Filter by match score (distance)\n",
    "            \n",
    "            mask = dists > match_score_thresh\n",
    "            idxs_filtered = idxs[mask.squeeze(1)]\n",
    "    \n",
    "            n_matches = len(idxs_filtered)\n",
    "            if n_matches == 0:\n",
    "                continue\n",
    "    \n",
    "            if verbose:\n",
    "                print(f'{key1}-{key2}: {n_matches} matches (filtered from {len(idxs)})')\n",
    "    \n",
    "            group = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=idxs_filtered.detach().cpu().numpy().reshape(-1, 2))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6f7ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:30.864035Z",
     "iopub.status.busy": "2025-05-15T21:30:30.863766Z",
     "iopub.status.idle": "2025-05-15T21:30:31.005352Z",
     "shell.execute_reply": "2025-05-15T21:30:31.004557Z"
    },
    "papermill": {
     "duration": 0.146932,
     "end_time": "2025-05-15T21:30:31.006653",
     "exception": false,
     "start_time": "2025-05-15T21:30:30.859721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09e104f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T21:30:31.015322Z",
     "iopub.status.busy": "2025-05-15T21:30:31.015104Z",
     "iopub.status.idle": "2025-05-16T00:08:36.239108Z",
     "shell.execute_reply": "2025-05-16T00:08:36.238189Z"
    },
    "papermill": {
     "duration": 9485.229809,
     "end_time": "2025-05-16T00:08:36.240345",
     "exception": false,
     "start_time": "2025-05-15T21:30:31.010536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting on device cuda:0\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:19<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1691\n",
      "Max:  0.4170\n",
      "Mean: 0.3158\n",
      "Std:  0.0524\n",
      "20%:  0.2605\n",
      "30%:  0.2852\n",
      "USED 50%:  0.3285\n",
      "75%:  0.3588\n",
      "Shortlisting. Number of pairs to match: 710. Done in 24.1036 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:09<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 9.4312 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [08:29<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 509.9425 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:04<00:00, 11.50it/s]\n",
      " 18%|█▊        | 244/1326 [00:00<00:00, 4341.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 6.7178 sec\n",
      "Reconstruction done in  82.1359 sec\n",
      "{0: Reconstruction(num_reg_images=23, num_cameras=23, num_points3D=20460, num_observations=82430), 1: Reconstruction(num_reg_images=31, num_cameras=31, num_points3D=12500, num_observations=41758)}\n",
      "map_index = 0 fountain_image_000.png fountain_image_007.png fountain_image_012.png fountain_image_025.png fountain_image_033.png fountain_image_041.png fountain_image_056.png fountain_image_071.png fountain_image_082.png fountain_image_101.png fountain_image_108.png fountain_image_116.png fountain_image_129.png fountain_image_136.png fountain_image_143.png fountain_image_155.png fountain_image_163.png fountain_image_166.png fountain_image_173.png fountain_image_186.png fountain_image_199.png fountain_image_214.png fountain_image_230.png\n",
      "map_index = 1 bike_image_004.png bike_image_029.png bike_image_038.png bike_image_049.png bike_image_062.png bike_image_076.png bike_image_088.png bike_image_094.png bike_image_101.png bike_image_115.png bike_image_119.png bike_image_128.png bike_image_137.png bike_image_139.png bike_image_150.png chairs_image_004.png chairs_image_020.png chairs_image_035.png chairs_image_045.png chairs_image_051.png chairs_image_073.png chairs_image_094.png chairs_image_103.png chairs_image_115.png chairs_image_122.png chairs_image_131.png chairs_image_141.png chairs_image_144.png chairs_image_152.png chairs_image_155.png chairs_image_160.png\n",
      "Dataset \"imc2023_haiper\" -> Registered 54 / 54 images with 2 clusters\n",
      "\n",
      "Processing dataset \"imc2023_heritage\": 209 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [04:24<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1159\n",
      "Max:  0.5188\n",
      "Mean: 0.3667\n",
      "Std:  0.0539\n",
      "20%:  0.3260\n",
      "30%:  0.3430\n",
      "USED 50%:  0.3727\n",
      "75%:  0.4056\n",
      "Shortlisting. Number of pairs to match: 3815. Done in 264.6938 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [01:13<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 73.5992 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3815/3815 [42:22<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 2542.7445 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [01:06<00:00,  3.14it/s]\n",
      "  2%|▏         | 480/20910 [00:00<00:05, 3611.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 8.4097 sec\n",
      "Reconstruction done in  296.2004 sec\n",
      "{0: Reconstruction(num_reg_images=43, num_cameras=43, num_points3D=61495, num_observations=242754), 1: Reconstruction(num_reg_images=58, num_cameras=58, num_points3D=31591, num_observations=134553), 2: Reconstruction(num_reg_images=8, num_cameras=8, num_points3D=4642, num_observations=15623), 3: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=4988, num_observations=13746), 4: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=4088, num_observations=9944), 5: Reconstruction(num_reg_images=5, num_cameras=5, num_points3D=902, num_observations=2522)}\n",
      "map_index = 0 wall_dsc_4973_acr.png wall_dsc_4976_acr.png wall_dsc_4979_acr.png wall_dsc_4982_acr.png wall_dsc_4991_acr.png wall_dsc_4994_acr.png wall_dsc_4997_acr.png wall_dsc_5000_acr.png wall_dsc_5003_acr.png wall_dsc_5006_acr.png wall_dsc_5009_acr.png wall_dsc_5012_acr.png wall_dsc_5015_acr.png wall_dsc_5018_acr.png wall_dsc_5021_acr.png wall_dsc_5024_acr.png wall_dsc_5027_acr.png wall_dsc_5030_acr.png wall_dsc_5033_acr.png wall_dsc_5039_acr.png wall_dsc_5042_acr.png wall_dsc_5045_acr.png wall_dsc_5048_acr.png wall_dsc_5051_acr.png wall_dsc_5054_acr.png wall_dsc_5057_acr.png wall_dsc_5060_acr.png wall_dsc_5063_acr.png wall_dsc_4928_acr.png wall_dsc_4931_acr.png wall_dsc_4934_acr.png wall_dsc_4937_acr.png wall_dsc_4940_acr.png wall_dsc_4943_acr.png wall_dsc_4946_acr.png wall_dsc_4952_acr.png wall_dsc_4955_acr.png wall_dsc_4958_acr.png wall_dsc_4961_acr.png wall_dsc_4964_acr.png wall_dsc_4967_acr.png wall_dsc_4970_acr.png wall_dsc_4985_acr.png\n",
      "map_index = 1 dioscuri_3dom_fbk_img_1516.png dioscuri_3dom_fbk_img_1520.png dioscuri_3dom_fbk_img_1524.png dioscuri_3dom_fbk_img_1548.png dioscuri_3dom_fbk_img_1561.png dioscuri_3dom_fbk_img_1563.png dioscuri_3dom_fbk_img_1566.png dioscuri_archive_0003.png dioscuri_archive_0004.png dioscuri_archive_0010.png dioscuri_archive_0015.png dioscuri_archive_0171.png dioscuri_archive_0230.png dioscuri_img_0055.png dioscuri_img_0068.png dioscuri_img_0079.png dioscuri_img_0081.png dioscuri_img_0092.png dioscuri_img_0095.png dioscuri_img_0105.png dioscuri_img_0112.png dioscuri_img_0115.png dioscuri_img_0132.png dioscuri_img_0161.png dioscuri_img_0163.png dioscuri_img_0181.png dioscuri_img_0187.png dioscuri_img_0188.png dioscuri_img_0200.png dioscuri_img_0209.png dioscuri_img_0213.png dioscuri_img_0216.png dioscuri_img_0227.png dioscuri_img_0230.png dioscuri_img_0237.png dioscuri_img_0238.png dioscuri_img_0246.png dioscuri_img_0258.png dioscuri_img_0266.png dioscuri_img_0276.png dioscuri_img_0277.png dioscuri_img_0283.png dioscuri_img_0314.png dioscuri_img_0338.png dioscuri_img_0339.png dioscuri_img_0348.png dioscuri_img_0354.png dioscuri_img_0359.png dioscuri_img_0364.png dioscuri_img_0369.png dioscuri_img_0370.png dioscuri_img_0376.png dioscuri_img_0401.png dioscuri_img_0412.png dioscuri_img_0418.png dioscuri_img_0424.png dioscuri_img_0444.png dioscuri_img_0460.png\n",
      "map_index = 2 dioscuri_archive_0028.png dioscuri_archive_0033.png dioscuri_archive_0068.png dioscuri_archive_0069.png dioscuri_archive_0070.png dioscuri_archive_0071.png dioscuri_archive_0072.png dioscuri_archive_0092.png\n",
      "map_index = 3 cyprus_dsc_6488.png cyprus_dsc_6492.png cyprus_dsc_6496.png cyprus_dsc_6500.png cyprus_dsc_6512.png cyprus_dsc_6520.png cyprus_dsc_6524.png cyprus_dsc_6528.png cyprus_dsc_6540.png cyprus_dsc_6548.png cyprus_dsc_6565.png cyprus_dsc_6609.png cyprus_dsc_6633.png\n",
      "map_index = 4 cyprus_dsc_6569.png cyprus_dsc_6589.png cyprus_dsc_6593.png cyprus_dsc_6597.png cyprus_dsc_6601.png cyprus_dsc_6605.png\n",
      "map_index = 5 outliers_dsc_4120_thumb.png outliers_dsc_4122_thumb.png outliers_img_8630_thumb.png outliers_img_8632.png outliers_img_8634.png\n",
      "Dataset \"imc2023_heritage\" -> Registered 133 / 209 images with 6 clusters\n",
      "Skipping \"imc2023_theather_imc2024_church\"\n",
      "Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Skipping \"imc2024_lizard_pond\"\n",
      "Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Skipping \"pt_stpeters_stpauls\"\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1448\n",
      "Max:  0.4239\n",
      "Mean: 0.2737\n",
      "Std:  0.0388\n",
      "20%:  0.2375\n",
      "30%:  0.2505\n",
      "USED 50%:  0.2754\n",
      "75%:  0.3017\n",
      "Shortlisting. Number of pairs to match: 3894. Done in 21.9896 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 20.1068 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3894/3894 [45:50<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 2750.8123 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 38.21it/s]\n",
      "  3%|▎         | 519/18721 [00:00<00:04, 3851.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 6.4977 sec\n",
      "Reconstruction done in  265.6455 sec\n",
      "{0: Reconstruction(num_reg_images=3, num_cameras=3, num_points3D=0, num_observations=0), 1: Reconstruction(num_reg_images=68, num_cameras=68, num_points3D=37888, num_observations=179412), 2: Reconstruction(num_reg_images=14, num_cameras=14, num_points3D=9286, num_observations=38168), 3: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=5482, num_observations=20262), 4: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=5801, num_observations=22178), 5: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=5031, num_observations=17732), 6: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=5645, num_observations=17788), 7: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=3416, num_observations=10938), 8: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=678, num_observations=2107), 9: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=2090, num_observations=6746), 10: Reconstruction(num_reg_images=17, num_cameras=17, num_points3D=5452, num_observations=21666)}\n",
      "map_index = 0 peach_0094.png peach_0108.png peach_0110.png\n",
      "map_index = 1 peach_0003.png peach_0004.png peach_0197.png peach_0005.png peach_0199.png peach_0008.png peach_0009.png peach_0010.png peach_0019.png peach_0020.png peach_0026.png peach_0027.png peach_0031.png peach_0033.png peach_0037.png peach_0038.png peach_0040.png peach_0044.png peach_0046.png peach_0055.png peach_0056.png peach_0057.png peach_0058.png peach_0062.png peach_0063.png peach_0065.png peach_0067.png peach_0073.png peach_0075.png peach_0076.png peach_0079.png peach_0081.png peach_0087.png peach_0089.png peach_0090.png peach_0091.png peach_0094.png peach_0095.png peach_0099.png peach_0100.png peach_0104.png peach_0106.png peach_0108.png peach_0110.png peach_0111.png peach_0112.png peach_0122.png peach_0125.png peach_0127.png peach_0128.png peach_0134.png peach_0139.png peach_0141.png peach_0143.png peach_0146.png peach_0148.png peach_0149.png peach_0150.png peach_0160.png peach_0162.png peach_0166.png peach_0167.png peach_0176.png peach_0179.png peach_0182.png peach_0183.png peach_0185.png peach_0193.png\n",
      "map_index = 2 peach_0196.png peach_0007.png peach_0034.png peach_0060.png peach_0064.png peach_0102.png peach_0114.png peach_0117.png peach_0131.png peach_0145.png peach_0156.png peach_0174.png peach_0177.png peach_0191.png\n",
      "map_index = 3 peach_0032.png peach_0068.png peach_0069.png peach_0093.png peach_0133.png peach_0142.png peach_0147.png peach_0151.png peach_0154.png peach_0158.png peach_0168.png peach_0169.png peach_0171.png peach_0178.png peach_0186.png\n",
      "map_index = 4 peach_0016.png peach_0025.png peach_0028.png peach_0042.png peach_0086.png peach_0088.png peach_0103.png peach_0155.png peach_0161.png peach_0188.png peach_0189.png\n",
      "map_index = 5 peach_0195.png peach_0014.png peach_0021.png peach_0053.png peach_0072.png peach_0074.png peach_0077.png peach_0098.png peach_0138.png peach_0152.png\n",
      "map_index = 6 peach_0001.png peach_0013.png peach_0029.png peach_0041.png peach_0050.png peach_0083.png peach_0085.png peach_0113.png peach_0136.png peach_0159.png peach_0172.png\n",
      "map_index = 7 peach_0194.png peach_0018.png peach_0035.png peach_0051.png peach_0061.png peach_0071.png peach_0092.png peach_0097.png peach_0101.png peach_0124.png peach_0164.png\n",
      "map_index = 8 peach_0006.png peach_0024.png peach_0084.png peach_0116.png peach_0118.png peach_0137.png\n",
      "map_index = 9 peach_0045.png peach_0080.png peach_0107.png peach_0121.png peach_0126.png peach_0180.png\n",
      "map_index = 10 peach_0001.png peach_0002.png peach_0010.png peach_0013.png peach_0015.png peach_0029.png peach_0041.png peach_0047.png peach_0050.png peach_0083.png peach_0085.png peach_0093.png peach_0113.png peach_0115.png peach_0136.png peach_0159.png peach_0172.png\n",
      "Dataset \"amy_gardens\" -> Registered 172 / 200 images with 11 clusters\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:09<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1510\n",
      "Max:  0.3338\n",
      "Mean: 0.2232\n",
      "Std:  0.0303\n",
      "20%:  0.1980\n",
      "30%:  0.2047\n",
      "USED 50%:  0.2173\n",
      "75%:  0.2396\n",
      "Shortlisting. Number of pairs to match: 3113. Done in 9.3067 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:13<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 14.0948 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3113/3113 [37:06<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 2227.0568 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:01<00:00, 96.19it/s]\n",
      "  2%|▏         | 281/12561 [00:00<00:03, 3556.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 1.9856 sec\n",
      "Reconstruction done in  117.2586 sec\n",
      "{0: Reconstruction(num_reg_images=42, num_cameras=42, num_points3D=28600, num_observations=106004), 1: Reconstruction(num_reg_images=29, num_cameras=29, num_points3D=14705, num_observations=49602), 2: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=7119, num_observations=24401), 3: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=3630, num_observations=12649), 4: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=5003, num_observations=16088), 5: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=6046, num_observations=19391), 6: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=7462, num_observations=25422)}\n",
      "map_index = 0 vineyard_split_3_frame_0070.png vineyard_split_3_frame_0075.png vineyard_split_3_frame_0080.png vineyard_split_3_frame_0085.png vineyard_split_3_frame_0090.png vineyard_split_3_frame_0095.png vineyard_split_3_frame_0100.png vineyard_split_3_frame_0105.png vineyard_split_3_frame_0110.png vineyard_split_3_frame_0115.png vineyard_split_3_frame_0120.png vineyard_split_3_frame_0125.png vineyard_split_3_frame_0130.png vineyard_split_3_frame_0135.png vineyard_split_3_frame_0140.png vineyard_split_3_frame_0150.png vineyard_split_3_frame_0155.png vineyard_split_3_frame_0160.png vineyard_split_3_frame_0165.png vineyard_split_3_frame_0170.png vineyard_split_3_frame_0175.png vineyard_split_3_frame_0180.png vineyard_split_3_frame_0185.png vineyard_split_3_frame_0190.png vineyard_split_3_frame_0195.png vineyard_split_3_frame_0200.png vineyard_split_3_frame_0205.png vineyard_split_3_frame_0210.png vineyard_split_3_frame_0215.png vineyard_split_3_frame_0220.png vineyard_split_3_frame_0225.png vineyard_split_3_frame_0230.png vineyard_split_3_frame_0240.png vineyard_split_3_frame_0245.png vineyard_split_3_frame_0255.png vineyard_split_3_frame_0260.png vineyard_split_3_frame_0265.png vineyard_split_3_frame_0270.png vineyard_split_3_frame_0275.png vineyard_split_3_frame_0280.png vineyard_split_3_frame_0285.png vineyard_split_3_frame_0295.png\n",
      "map_index = 1 vineyard_split_2_frame_1150.png vineyard_split_2_frame_1155.png vineyard_split_2_frame_1160.png vineyard_split_2_frame_1165.png vineyard_split_2_frame_1170.png vineyard_split_2_frame_1175.png vineyard_split_2_frame_1180.png vineyard_split_2_frame_1185.png vineyard_split_2_frame_1195.png vineyard_split_2_frame_1200.png vineyard_split_2_frame_1205.png vineyard_split_2_frame_1210.png vineyard_split_2_frame_1215.png vineyard_split_2_frame_1220.png vineyard_split_2_frame_1225.png vineyard_split_2_frame_1230.png vineyard_split_2_frame_1235.png vineyard_split_2_frame_1240.png vineyard_split_2_frame_1245.png vineyard_split_2_frame_1250.png vineyard_split_2_frame_1255.png vineyard_split_2_frame_1260.png vineyard_split_2_frame_1265.png vineyard_split_2_frame_1270.png vineyard_split_2_frame_1275.png vineyard_split_2_frame_1280.png vineyard_split_2_frame_1285.png vineyard_split_2_frame_1290.png vineyard_split_2_frame_1295.png\n",
      "map_index = 2 vineyard_split_1_frame_1000.png vineyard_split_1_frame_1010.png vineyard_split_1_frame_1015.png vineyard_split_1_frame_1020.png vineyard_split_1_frame_1025.png vineyard_split_1_frame_1030.png vineyard_split_1_frame_1035.png vineyard_split_1_frame_1040.png vineyard_split_1_frame_1045.png vineyard_split_1_frame_1050.png vineyard_split_1_frame_1055.png vineyard_split_1_frame_1065.png vineyard_split_1_frame_1070.png vineyard_split_1_frame_1075.png vineyard_split_1_frame_1080.png\n",
      "map_index = 3 vineyard_split_1_frame_0970.png vineyard_split_1_frame_0975.png vineyard_split_1_frame_0980.png vineyard_split_1_frame_0985.png vineyard_split_1_frame_0990.png vineyard_split_1_frame_1000.png\n",
      "map_index = 4 vineyard_split_3_frame_1510.png vineyard_split_3_frame_1515.png vineyard_split_3_frame_1520.png vineyard_split_3_frame_1525.png vineyard_split_3_frame_1530.png vineyard_split_3_frame_1535.png vineyard_split_3_frame_1540.png vineyard_split_3_frame_1545.png vineyard_split_3_frame_1550.png vineyard_split_3_frame_1555.png vineyard_split_3_frame_1560.png vineyard_split_3_frame_1565.png vineyard_split_3_frame_1570.png\n",
      "map_index = 5 vineyard_split_1_frame_0900.png vineyard_split_1_frame_0905.png vineyard_split_1_frame_0910.png vineyard_split_1_frame_0915.png vineyard_split_1_frame_0920.png vineyard_split_1_frame_0925.png vineyard_split_1_frame_0930.png vineyard_split_1_frame_0935.png vineyard_split_1_frame_0940.png vineyard_split_1_frame_0945.png vineyard_split_1_frame_0950.png vineyard_split_1_frame_0955.png vineyard_split_1_frame_0960.png vineyard_split_1_frame_0965.png vineyard_split_1_frame_0970.png\n",
      "map_index = 6 vineyard_split_3_frame_1390.png vineyard_split_3_frame_1395.png vineyard_split_3_frame_1400.png vineyard_split_3_frame_1405.png vineyard_split_3_frame_1410.png vineyard_split_3_frame_1415.png vineyard_split_3_frame_1420.png vineyard_split_3_frame_1425.png vineyard_split_3_frame_1430.png vineyard_split_3_frame_1435.png vineyard_split_3_frame_1440.png vineyard_split_3_frame_1445.png vineyard_split_3_frame_1450.png\n",
      "Dataset \"fbk_vineyard\" -> Registered 133 / 163 images with 7 clusters\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:01<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "30%:  0.2647\n",
      "USED 50%:  0.2804\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 150. Done in 1.5901 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:01<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 2.1152 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:15<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 16.0994 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 90.80it/s]\n",
      " 30%|██▉       | 62/210 [00:00<00:00, 4127.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 0.8544 sec\n",
      "Reconstruction done in  7.3126 sec\n",
      "{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=3400, num_observations=15654), 1: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=1078, num_observations=5775)}\n",
      "map_index = 0 et_et000.png et_et001.png et_et002.png et_et003.png et_et004.png et_et005.png et_et006.png et_et007.png et_et008.png\n",
      "map_index = 1 another_et_another_et001.png another_et_another_et002.png another_et_another_et003.png another_et_another_et004.png another_et_another_et005.png another_et_another_et006.png another_et_another_et007.png another_et_another_et008.png another_et_another_et009.png another_et_another_et010.png outliers_out_et001.png\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images with 2 clusters\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "30%:  0.2557\n",
      "USED 50%:  0.2767\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 647. Done in 8.9654 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:06<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 6.4830 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [01:25<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 85.9812 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 22.36it/s]\n",
      "  7%|▋         | 80/1176 [00:00<00:00, 4075.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 0.5918 sec\n",
      "Reconstruction done in  9.0782 sec\n",
      "{0: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=1070, num_observations=3051), 1: Reconstruction(num_reg_images=5, num_cameras=5, num_points3D=312, num_observations=874)}\n",
      "map_index = 0 stairs_split_2_1710453805788.png stairs_split_2_1710453871430.png stairs_split_1_1710453689727.png stairs_split_2_1710453720741.png stairs_split_2_1710453736752.png stairs_split_2_1710453739354.png stairs_split_2_1710453740954.png stairs_split_2_1710453756762.png stairs_split_2_1710453759963.png stairs_split_2_1710453783374.png stairs_split_2_1710453786375.png\n",
      "map_index = 1 stairs_split_1_1710453704934.png stairs_split_1_1710453901046.png stairs_split_2_1710453745156.png stairs_split_2_1710453790978.png stairs_split_2_1710453793579.png\n",
      "Dataset \"stairs\" -> Registered 16 / 51 images with 2 clusters\n",
      "\n",
      "Results\n",
      "Dataset \"imc2023_haiper\" -> Registered 54 / 54 images with 2 clusters\n",
      "Dataset \"imc2023_heritage\" -> Registered 133 / 209 images with 6 clusters\n",
      "Dataset \"amy_gardens\" -> Registered 172 / 200 images with 11 clusters\n",
      "Dataset \"fbk_vineyard\" -> Registered 133 / 163 images with 7 clusters\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images with 2 clusters\n",
      "Dataset \"stairs\" -> Registered 16 / 51 images with 2 clusters\n",
      "\n",
      "Timings\n",
      "shortlisting -> total=330.65 sec.\n",
      "feature_detection -> total=125.83 sec.\n",
      "feature_matching -> total=8132.64 sec.\n",
      "RANSAC -> total=25.06 sec.\n",
      "Reconstruction -> total=777.63 sec.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs', \n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t'imc2023_heritage',\n",
    "    \t'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "print (f\"Extracting on device {device}\")\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "    try:\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.5, # should be strict\n",
    "            min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "    \n",
    "        t = time()\n",
    "\n",
    "        detect_aliked(images, feature_dir, 8192, device=device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "        \n",
    "        t = time()\n",
    "        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        # match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'Ran RANSAC in {time() - t:.4f} sec')\n",
    "        \n",
    "        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "        # Lower it to 3.\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = 5\n",
    "        mapper_options.max_num_models = 25\n",
    "        mapper_options.mapper.filter_max_reproj_error\t = 6.0\n",
    "        # mapper_options.min_num_matches\t = 50\n",
    "        # mapper_options.ba_local_max_num_iterations = 100\n",
    "        # mapper_options.ba_local_num_images = 10\n",
    "        mapper_options.ba_global_images_freq = 5\n",
    "        \n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path, \n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options)\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction done in  {time() - t:.4f} sec')\n",
    "        print(maps)\n",
    "\n",
    "        # clear_output(wait=False)\n",
    "    \n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            img_list =[]\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                img_list.append(image.name)\n",
    "                registered += 1\n",
    "            img_list_str = ' '.join(img_list) \n",
    "            print(f\"map_index = {map_index}\", img_list_str)\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcdb33b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T00:08:37.477985Z",
     "iopub.status.busy": "2025-05-16T00:08:37.477640Z",
     "iopub.status.idle": "2025-05-16T00:08:37.711380Z",
     "shell.execute_reply": "2025-05-16T00:08:37.710362Z"
    },
    "papermill": {
     "duration": 0.883833,
     "end_time": "2025-05-16T00:08:37.712991",
     "exception": false,
     "start_time": "2025-05-16T00:08:36.829158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,cluster0,fountain_image_116.png,0.869129317;0.232353614;-0.436607408;-0.403378095;0.843817073;-0.353919284;0.286182361;0.483719490;0.827112514,0.358965655;-0.843690342;1.845886880\r\n",
      "imc2023_haiper,cluster0,fountain_image_108.png,0.933615065;-0.135776390;0.331553437;0.282382009;0.848417909;-0.447713583;-0.220506940;0.511616872;0.830436551,0.330236871;-0.729795670;1.630062740\r\n",
      "imc2023_haiper,cluster0,fountain_image_101.png,0.639016636;-0.278117785;0.717152869;0.592606595;0.772403491;-0.228495670;-0.490382669;0.571002054;0.658393113,-0.092049249;-1.001903594;1.791716316\r\n",
      "imc2023_haiper,cluster0,fountain_image_082.png,-0.981584690;-0.123141475;0.146039973;0.029802770;0.656417776;0.753808661;-0.188688345;0.744279437;-0.640659682,0.099493123;-1.709364600;4.183197031\r\n",
      "imc2023_haiper,cluster0,fountain_image_071.png,-0.805769103;0.188254240;-0.561512683;-0.314870977;0.666844519;0.675407029;0.501589892;0.721026164;-0.478046913,0.178477587;-1.875633578;3.970399397\r\n",
      "imc2023_haiper,cluster0,fountain_image_025.png,0.877177113;-0.221259294;0.426150956;0.222688072;0.973746551;0.047198285;-0.425406083;0.053497479;0.903419993,-0.413132443;-1.793658088;3.396198621\r\n",
      "imc2023_haiper,cluster0,fountain_image_000.png,-0.685731387;-0.310005797;0.658535399;0.277990571;0.724624056;0.630587996;-0.672676526;0.615480612;-0.410718770,-0.506126079;-2.120284395;5.302792078\r\n",
      "imc2023_haiper,cluster0,fountain_image_007.png,-0.245625890;-0.485077716;0.839266067;0.454718041;0.706966009;0.541692317;-0.856095454;0.514683079;0.046924434,-0.646739822;-2.482136527;4.037055613\r\n",
      "imc2023_haiper,cluster0,fountain_image_012.png,0.073324695;-0.473644537;0.877658442;0.447078165;0.802259202;0.395602437;-0.891484494;0.363374497;0.270581542,-0.497749189;-1.973010805;3.455070793\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b06d6c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T00:08:38.943125Z",
     "iopub.status.busy": "2025-05-16T00:08:38.942582Z",
     "iopub.status.idle": "2025-05-16T00:10:18.997764Z",
     "shell.execute_reply": "2025-05-16T00:10:18.996980Z"
    },
    "papermill": {
     "duration": 100.702021,
     "end_time": "2025-05-16T00:10:18.999204",
     "exception": false,
     "start_time": "2025-05-16T00:08:38.297183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=63.15% (mAA=62.78%, clusterness=63.53%)\n",
      "imc2023_heritage: score=56.89% (mAA=39.75%, clusterness=100.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "amy_gardens: score=25.47% (mAA=14.59%, clusterness=100.00%)\n",
      "fbk_vineyard: score=43.65% (mAA=27.92%, clusterness=100.00%)\n",
      "ETs: score=44.25% (mAA=28.85%, clusterness=95.00%)\n",
      "stairs: score=4.32% (mAA=2.22%, clusterness=75.00%)\n",
      "Average over all datasets: score=18.29% (mAA=13.55%, clusterness=41.04%)\n",
      "Computed metric in: 100.05 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec6f3a",
   "metadata": {
    "papermill": {
     "duration": 0.579248,
     "end_time": "2025-05-16T00:10:20.218966",
     "exception": false,
     "start_time": "2025-05-16T00:10:19.639718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11217117,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9624.709754,
   "end_time": "2025-05-16T00:10:24.399650",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-15T21:29:59.689896",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
