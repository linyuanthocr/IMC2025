{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde2250d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004379,
     "end_time": "2025-05-15T08:36:36.106009",
     "exception": false,
     "start_time": "2025-05-15T08:36:36.101630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009d271e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:36:36.114190Z",
     "iopub.status.busy": "2025-05-15T08:36:36.113891Z",
     "iopub.status.idle": "2025-05-15T08:36:41.350558Z",
     "shell.execute_reply": "2025-05-15T08:36:41.349458Z"
    },
    "papermill": {
     "duration": 5.242445,
     "end_time": "2025-05-15T08:36:41.352099",
     "exception": false,
     "start_time": "2025-05-15T08:36:36.109654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f7bf83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:36:41.361533Z",
     "iopub.status.busy": "2025-05-15T08:36:41.361244Z",
     "iopub.status.idle": "2025-05-15T08:37:04.653419Z",
     "shell.execute_reply": "2025-05-15T08:37:04.652668Z"
    },
    "papermill": {
     "duration": 23.298457,
     "end_time": "2025-05-15T08:37:04.654910",
     "exception": false,
     "start_time": "2025-05-15T08:36:41.356453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b67458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:04.664385Z",
     "iopub.status.busy": "2025-05-15T08:37:04.663861Z",
     "iopub.status.idle": "2025-05-15T08:37:04.774410Z",
     "shell.execute_reply": "2025-05-15T08:37:04.773444Z"
    },
    "papermill": {
     "duration": 0.116517,
     "end_time": "2025-05-15T08:37:04.775808",
     "exception": false,
     "start_time": "2025-05-15T08:37:04.659291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0159e19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:04.785437Z",
     "iopub.status.busy": "2025-05-15T08:37:04.785141Z",
     "iopub.status.idle": "2025-05-15T08:37:04.788998Z",
     "shell.execute_reply": "2025-05-15T08:37:04.788359Z"
    },
    "papermill": {
     "duration": 0.009614,
     "end_time": "2025-05-15T08:37:04.790111",
     "exception": false,
     "start_time": "2025-05-15T08:37:04.780497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f897f503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:04.798764Z",
     "iopub.status.busy": "2025-05-15T08:37:04.798534Z",
     "iopub.status.idle": "2025-05-15T08:37:04.801675Z",
     "shell.execute_reply": "2025-05-15T08:37:04.800905Z"
    },
    "papermill": {
     "duration": 0.008655,
     "end_time": "2025-05-15T08:37:04.802765",
     "exception": false,
     "start_time": "2025-05-15T08:37:04.794110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r /kaggle/working/result/featureout/ETs/featurept.zip /kaggle/working/result/featureout/ETs/featurept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7412985f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:04.811283Z",
     "iopub.status.busy": "2025-05-15T08:37:04.811044Z",
     "iopub.status.idle": "2025-05-15T08:37:04.815963Z",
     "shell.execute_reply": "2025-05-15T08:37:04.815203Z"
    },
    "papermill": {
     "duration": 0.010531,
     "end_time": "2025-05-15T08:37:04.817231",
     "exception": false,
     "start_time": "2025-05-15T08:37:04.806700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def draw_and_save_feature_points(image_path, keypoints, result_folder):\n",
    "    \"\"\"\n",
    "    Draw feature points on the image and save to result folder.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the input image.\n",
    "        keypoints (np.ndarray): (N, 2) array of (x, y) coordinates.\n",
    "        result_folder (str or Path): Folder to save the output image.\n",
    "    \"\"\"\n",
    "    # Load image in BGR\n",
    "    return\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Cannot read image from {image_path}\")\n",
    "\n",
    "    # Draw keypoints\n",
    "    for (x, y) in keypoints.astype(int):\n",
    "        cv2.circle(image, (x, y), radius=2, color=(0, 255, 0), thickness=-1)  # Green dots\n",
    "\n",
    "    img_fname = image_path.split('/')[-1]\n",
    "\n",
    "    result_folder = Path(result_folder)\n",
    "    img_fname = Path(image_path).stem  # no extension\n",
    "    output_path = result_folder / f\"{img_fname}_fe.png\"\n",
    "\n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018d8d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:04.826339Z",
     "iopub.status.busy": "2025-05-15T08:37:04.826086Z",
     "iopub.status.idle": "2025-05-15T08:37:04.845284Z",
     "shell.execute_reply": "2025-05-15T08:37:04.844698Z"
    },
    "papermill": {
     "duration": 0.025007,
     "end_time": "2025-05-15T08:37:04.846412",
     "exception": false,
     "start_time": "2025-05-15T08:37:04.821405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "\n",
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 10,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu'),\n",
    "                              max_pairs = 30):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # print(dm)\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"30%:  {np.percentile(dm_flat, 30):.4f}\")\n",
    "    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 50)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs :\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        if len(to_match) >= max_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:max_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list\n",
    "\n",
    "def detect_aliked(img_fnames,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 2048,\n",
    "                  device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    # Calculate the expected scale factor ALIKED will apply\n",
    "    # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n",
    "    # Input image tensor size will be (H, W) after Kornia loading/conversion\n",
    "    \n",
    "    draw_feature_dir = os.path.join(feature_dir, 'featurept')\n",
    "    os.makedirs(draw_feature_dir, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "                draw_and_save_feature_points(img_path, kpts, draw_feature_dir)\n",
    "    return\n",
    "\n",
    "def match_with_lightglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=30,\n",
    "                   verbose=False,\n",
    "                   match_score_thresh = 0.15):\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                         desc2,\n",
    "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            # if verbose:\n",
    "            #     print (f'{key1}-{key2}: {n_matches} matches')\n",
    "            # group  = f_match.require_group(key1)\n",
    "            # if n_matches >= min_matches:\n",
    "            #      group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "            # Filter by match score (distance)\n",
    "            \n",
    "            mask = dists > match_score_thresh\n",
    "            idxs_filtered = idxs[mask.squeeze(1)]\n",
    "    \n",
    "            n_matches = len(idxs_filtered)\n",
    "            if n_matches == 0:\n",
    "                continue\n",
    "    \n",
    "            if verbose:\n",
    "                print(f'{key1}-{key2}: {n_matches} matches (filtered from {len(idxs)})')\n",
    "    \n",
    "            group = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=idxs_filtered.detach().cpu().numpy().reshape(-1, 2))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc1e8ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:04.854978Z",
     "iopub.status.busy": "2025-05-15T08:37:04.854744Z",
     "iopub.status.idle": "2025-05-15T08:37:05.560796Z",
     "shell.execute_reply": "2025-05-15T08:37:05.560101Z"
    },
    "papermill": {
     "duration": 0.712061,
     "end_time": "2025-05-15T08:37:05.562387",
     "exception": false,
     "start_time": "2025-05-15T08:37:04.850326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "def match_with_lightglue_and_cluster(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    feature_dir='.featureout',\n",
    "    device=torch.device('cpu'),\n",
    "    min_matches=20,\n",
    "    aliked_dis_min=100,\n",
    "    verbose=True\n",
    "):\n",
    "    # 初始化 LightGlue 匹配器\n",
    "    lg_matcher = KF.LightGlueMatcher(\n",
    "        \"aliked\", {\n",
    "            \"width_confidence\": -1,\n",
    "            \"depth_confidence\": -1,\n",
    "            \"mp\": 'cuda' in str(device)\n",
    "        }\n",
    "    ).eval().to(device)\n",
    "\n",
    "    num_imgs = len(img_fnames)\n",
    "    match_graph = nx.Graph()\n",
    "    match_graph.add_nodes_from(range(num_imgs))\n",
    "\n",
    "    # 加载关键点和描述子，准备写入匹配结果\n",
    "    with h5py.File(os.path.join(feature_dir, 'keypoints.h5'), 'r') as f_kp, \\\n",
    "         h5py.File(os.path.join(feature_dir, 'descriptors.h5'), 'r') as f_desc, \\\n",
    "         h5py.File(os.path.join(feature_dir, 'matches.h5'), 'w') as f_match:\n",
    "\n",
    "        # Step 1: 匹配每对图像\n",
    "        for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n",
    "            img1 = os.path.basename(img_fnames[idx1])\n",
    "            img2 = os.path.basename(img_fnames[idx2])\n",
    "\n",
    "            kp1 = torch.from_numpy(f_kp[img1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[img2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[img1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[img2][...]).to(device)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                _, idxs = lg_matcher(\n",
    "                    desc1, desc2,\n",
    "                    KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                    KF.laf_from_center_scale_ori(kp2[None])\n",
    "                )\n",
    "\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "\n",
    "            n_matches = len(idxs)\n",
    "            if verbose:\n",
    "                print(f'{img1} - {img2}: {n_matches} matches')\n",
    "\n",
    "            if n_matches >= min_matches:\n",
    "                match_graph.add_edge(idx1, idx2, weight=n_matches)\n",
    "                group = f_match.require_group(img1)\n",
    "                group.create_dataset(img2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "\n",
    "        # Step 2: 提取聚类（connected components）\n",
    "        raw_clusters = list(nx.connected_components(match_graph))\n",
    "        final_clusters = []\n",
    "        outliers = set()\n",
    "\n",
    "        for cluster in raw_clusters:\n",
    "            cluster = list(cluster)\n",
    "            subgraph = match_graph.subgraph(cluster)\n",
    "            valid_nodes = []\n",
    "\n",
    "            for node in cluster:\n",
    "                strong_neighbors = [\n",
    "                    neighbor for neighbor in subgraph.neighbors(node)\n",
    "                    if subgraph[node][neighbor]['weight'] >= aliked_dis_min\n",
    "                ]\n",
    "                if strong_neighbors:\n",
    "                    valid_nodes.append(node)\n",
    "                else:\n",
    "                    outliers.add(node)\n",
    "\n",
    "            if len(valid_nodes) >= 2:\n",
    "                final_clusters.append(valid_nodes)\n",
    "            else:\n",
    "                outliers.update(valid_nodes)\n",
    "\n",
    "        # Step 3: 清理掉涉及 outlier 的匹配项（保留聚类内匹配）\n",
    "        # outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n",
    "\n",
    "        # for group_name in f_match.keys():\n",
    "        #     group = f_match[group_name]\n",
    "        #     for dataset_name in list(group.keys()):\n",
    "        #         if group_name in outlier_names or dataset_name in outlier_names:\n",
    "        #             del group[dataset_name]\n",
    "        #             if verbose:\n",
    "        #                 print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n",
    "        \n",
    "        # Step 3: 清理掉涉及 outlier 的匹配项\n",
    "        outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n",
    "\n",
    "        groups_to_delete = []\n",
    "\n",
    "        for group_name in list(f_match.keys()):\n",
    "            group = f_match[group_name]\n",
    "            datasets_to_delete = []\n",
    "\n",
    "            for dataset_name in list(group.keys()):\n",
    "                # 删除与 outlier 有关的 match\n",
    "                if group_name in outlier_names or dataset_name in outlier_names:\n",
    "                    datasets_to_delete.append(dataset_name)\n",
    "                    if verbose:\n",
    "                        print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n",
    "\n",
    "            # 先删除标记的 dataset\n",
    "            for ds in datasets_to_delete:\n",
    "                del group[ds]\n",
    "\n",
    "            # 新增逻辑：如果剩下不到 3 个 match，就删掉整个 group\n",
    "            if len(group.keys()) < 3:\n",
    "                groups_to_delete.append(group_name)\n",
    "\n",
    "        # 删除 group 本身\n",
    "        for gname in groups_to_delete:\n",
    "            del f_match[gname]\n",
    "            if verbose:\n",
    "                print(f\"Deleted group: {gname} (too few matches)\")\n",
    "\n",
    "    if True:\n",
    "        print(list(outliers))\n",
    "        for i, cluster in enumerate(final_clusters):\n",
    "            print(f\"Cluster {i} ({len(cluster)} images):\")\n",
    "    return final_clusters, sorted(list(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b250c9",
   "metadata": {
    "papermill": {
     "duration": 0.003897,
     "end_time": "2025-05-15T08:37:05.570716",
     "exception": false,
     "start_time": "2025-05-15T08:37:05.566819",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf045ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:05.579765Z",
     "iopub.status.busy": "2025-05-15T08:37:05.579207Z",
     "iopub.status.idle": "2025-05-15T08:37:05.725560Z",
     "shell.execute_reply": "2025-05-15T08:37:05.724860Z"
    },
    "papermill": {
     "duration": 0.152325,
     "end_time": "2025-05-15T08:37:05.726959",
     "exception": false,
     "start_time": "2025-05-15T08:37:05.574634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377e2b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:37:05.736525Z",
     "iopub.status.busy": "2025-05-15T08:37:05.736292Z",
     "iopub.status.idle": "2025-05-15T09:33:42.838547Z",
     "shell.execute_reply": "2025-05-15T09:33:42.837663Z"
    },
    "papermill": {
     "duration": 3397.108605,
     "end_time": "2025-05-15T09:33:42.839984",
     "exception": false,
     "start_time": "2025-05-15T08:37:05.731379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting on device cuda:0\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:20<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1691\n",
      "Max:  0.4170\n",
      "Mean: 0.3158\n",
      "Std:  0.0524\n",
      "20%:  0.2605\n",
      "30%:  0.2852\n",
      "USED 50%:  0.3285\n",
      "75%:  0.3588\n",
      "Shortlisting. Number of pairs to match: 710. Done in 24.0409 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:12<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 12.4786 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [03:37<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 217.9268 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:04<00:00, 11.31it/s]\n",
      " 20%|█▉        | 265/1326 [00:00<00:00, 4236.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 9.0942 sec\n",
      "Reconstruction done in  92.7773 sec\n",
      "{0: Reconstruction(num_reg_images=23, num_cameras=23, num_points3D=21712, num_observations=82960), 1: Reconstruction(num_reg_images=31, num_cameras=31, num_points3D=13654, num_observations=42922)}\n",
      "map_index = 0 fountain_image_000.png fountain_image_007.png fountain_image_012.png fountain_image_025.png fountain_image_033.png fountain_image_041.png fountain_image_056.png fountain_image_071.png fountain_image_082.png fountain_image_101.png fountain_image_108.png fountain_image_116.png fountain_image_129.png fountain_image_136.png fountain_image_143.png fountain_image_155.png fountain_image_163.png fountain_image_166.png fountain_image_173.png fountain_image_186.png fountain_image_199.png fountain_image_214.png fountain_image_230.png\n",
      "map_index = 1 bike_image_004.png bike_image_029.png bike_image_038.png bike_image_049.png bike_image_062.png bike_image_076.png bike_image_088.png bike_image_094.png bike_image_101.png bike_image_115.png bike_image_119.png bike_image_128.png bike_image_137.png bike_image_139.png bike_image_150.png chairs_image_004.png chairs_image_020.png chairs_image_035.png chairs_image_045.png chairs_image_051.png chairs_image_073.png chairs_image_094.png chairs_image_103.png chairs_image_115.png chairs_image_122.png chairs_image_131.png chairs_image_141.png chairs_image_144.png chairs_image_152.png chairs_image_155.png chairs_image_160.png\n",
      "Dataset \"imc2023_haiper\" -> Registered 54 / 54 images with 2 clusters\n",
      "Skipping \"imc2023_heritage\"\n",
      "Skipping \"imc2023_theather_imc2024_church\"\n",
      "Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Skipping \"imc2024_lizard_pond\"\n",
      "Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Skipping \"pt_stpeters_stpauls\"\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:24<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1448\n",
      "Max:  0.4239\n",
      "Mean: 0.2737\n",
      "Std:  0.0388\n",
      "20%:  0.2375\n",
      "30%:  0.2505\n",
      "USED 50%:  0.2754\n",
      "75%:  0.3017\n",
      "Shortlisting. Number of pairs to match: 3894. Done in 25.0776 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:32<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 32.6243 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3894/3894 [20:07<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 1207.9643 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 40.93it/s]\n",
      "  3%|▎         | 581/19110 [00:00<00:04, 4008.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 9.7433 sec\n",
      "Reconstruction done in  378.2742 sec\n",
      "{0: Reconstruction(num_reg_images=3, num_cameras=3, num_points3D=0, num_observations=0), 1: Reconstruction(num_reg_images=68, num_cameras=68, num_points3D=42121, num_observations=191533), 2: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=10340, num_observations=41167), 3: Reconstruction(num_reg_images=16, num_cameras=16, num_points3D=6220, num_observations=22355), 4: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=6170, num_observations=23227), 5: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=5602, num_observations=16740), 6: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=5665, num_observations=19475), 7: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=4000, num_observations=12458), 8: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=824, num_observations=2453), 9: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=2446, num_observations=7630), 10: Reconstruction(num_reg_images=25, num_cameras=25, num_points3D=10520, num_observations=35722)}\n",
      "map_index = 0 peach_0094.png peach_0108.png peach_0110.png\n",
      "map_index = 1 peach_0003.png peach_0004.png peach_0197.png peach_0005.png peach_0199.png peach_0008.png peach_0009.png peach_0010.png peach_0019.png peach_0020.png peach_0026.png peach_0027.png peach_0031.png peach_0033.png peach_0037.png peach_0038.png peach_0040.png peach_0044.png peach_0046.png peach_0055.png peach_0056.png peach_0057.png peach_0058.png peach_0062.png peach_0063.png peach_0065.png peach_0067.png peach_0073.png peach_0075.png peach_0076.png peach_0079.png peach_0081.png peach_0087.png peach_0089.png peach_0090.png peach_0091.png peach_0094.png peach_0095.png peach_0099.png peach_0100.png peach_0104.png peach_0106.png peach_0108.png peach_0110.png peach_0111.png peach_0112.png peach_0122.png peach_0125.png peach_0127.png peach_0128.png peach_0134.png peach_0139.png peach_0141.png peach_0143.png peach_0146.png peach_0148.png peach_0149.png peach_0150.png peach_0160.png peach_0162.png peach_0166.png peach_0167.png peach_0176.png peach_0179.png peach_0182.png peach_0183.png peach_0185.png peach_0193.png\n",
      "map_index = 2 peach_0196.png peach_0007.png peach_0034.png peach_0047.png peach_0060.png peach_0064.png peach_0102.png peach_0114.png peach_0117.png peach_0131.png peach_0145.png peach_0156.png peach_0174.png peach_0177.png peach_0191.png\n",
      "map_index = 3 peach_0032.png peach_0068.png peach_0069.png peach_0093.png peach_0133.png peach_0142.png peach_0147.png peach_0151.png peach_0154.png peach_0157.png peach_0158.png peach_0168.png peach_0169.png peach_0171.png peach_0178.png peach_0186.png\n",
      "map_index = 4 peach_0016.png peach_0025.png peach_0028.png peach_0042.png peach_0086.png peach_0088.png peach_0103.png peach_0155.png peach_0161.png peach_0188.png peach_0189.png\n",
      "map_index = 5 peach_0001.png peach_0013.png peach_0029.png peach_0041.png peach_0050.png peach_0083.png peach_0085.png peach_0113.png peach_0136.png peach_0159.png peach_0172.png\n",
      "map_index = 6 peach_0195.png peach_0014.png peach_0021.png peach_0053.png peach_0072.png peach_0074.png peach_0077.png peach_0098.png peach_0138.png peach_0152.png\n",
      "map_index = 7 peach_0194.png peach_0018.png peach_0035.png peach_0051.png peach_0061.png peach_0071.png peach_0092.png peach_0097.png peach_0101.png peach_0124.png peach_0164.png\n",
      "map_index = 8 peach_0006.png peach_0024.png peach_0084.png peach_0116.png peach_0118.png peach_0137.png\n",
      "map_index = 9 peach_0045.png peach_0080.png peach_0107.png peach_0121.png peach_0126.png peach_0180.png\n",
      "map_index = 10 peach_0001.png peach_0002.png peach_0010.png peach_0013.png peach_0015.png peach_0029.png peach_0041.png peach_0048.png peach_0050.png peach_0083.png peach_0085.png peach_0093.png peach_0113.png peach_0115.png peach_0129.png peach_0136.png peach_0142.png peach_0151.png peach_0154.png peach_0159.png peach_0169.png peach_0171.png peach_0172.png peach_0178.png peach_0186.png\n",
      "Dataset \"amy_gardens\" -> Registered 182 / 200 images with 11 clusters\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:09<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1510\n",
      "Max:  0.3338\n",
      "Mean: 0.2232\n",
      "Std:  0.0303\n",
      "20%:  0.1980\n",
      "30%:  0.2047\n",
      "USED 50%:  0.2173\n",
      "75%:  0.2396\n",
      "Shortlisting. Number of pairs to match: 3113. Done in 9.8664 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:24<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 24.5327 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3113/3113 [16:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 973.6544 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:01<00:00, 95.97it/s]\n",
      "  3%|▎         | 329/12561 [00:00<00:03, 3761.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 2.5207 sec\n",
      "Reconstruction done in  212.7372 sec\n",
      "{0: Reconstruction(num_reg_images=42, num_cameras=42, num_points3D=31742, num_observations=116664), 1: Reconstruction(num_reg_images=29, num_cameras=29, num_points3D=13803, num_observations=47175), 2: Reconstruction(num_reg_images=34, num_cameras=34, num_points3D=18351, num_observations=60761), 3: Reconstruction(num_reg_images=8, num_cameras=8, num_points3D=4260, num_observations=14729), 4: Reconstruction(num_reg_images=15, num_cameras=15, num_points3D=7041, num_observations=22221), 5: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=5840, num_observations=18665), 6: Reconstruction(num_reg_images=13, num_cameras=13, num_points3D=8431, num_observations=28377)}\n",
      "map_index = 0 vineyard_split_3_frame_0070.png vineyard_split_3_frame_0075.png vineyard_split_3_frame_0080.png vineyard_split_3_frame_0085.png vineyard_split_3_frame_0090.png vineyard_split_3_frame_0095.png vineyard_split_3_frame_0100.png vineyard_split_3_frame_0105.png vineyard_split_3_frame_0110.png vineyard_split_3_frame_0115.png vineyard_split_3_frame_0120.png vineyard_split_3_frame_0125.png vineyard_split_3_frame_0130.png vineyard_split_3_frame_0135.png vineyard_split_3_frame_0140.png vineyard_split_3_frame_0150.png vineyard_split_3_frame_0155.png vineyard_split_3_frame_0160.png vineyard_split_3_frame_0165.png vineyard_split_3_frame_0170.png vineyard_split_3_frame_0175.png vineyard_split_3_frame_0180.png vineyard_split_3_frame_0185.png vineyard_split_3_frame_0190.png vineyard_split_3_frame_0195.png vineyard_split_3_frame_0200.png vineyard_split_3_frame_0205.png vineyard_split_3_frame_0210.png vineyard_split_3_frame_0215.png vineyard_split_3_frame_0220.png vineyard_split_3_frame_0225.png vineyard_split_3_frame_0230.png vineyard_split_3_frame_0240.png vineyard_split_3_frame_0245.png vineyard_split_3_frame_0255.png vineyard_split_3_frame_0260.png vineyard_split_3_frame_0265.png vineyard_split_3_frame_0270.png vineyard_split_3_frame_0275.png vineyard_split_3_frame_0280.png vineyard_split_3_frame_0285.png vineyard_split_3_frame_0295.png\n",
      "map_index = 1 vineyard_split_1_frame_1000.png vineyard_split_1_frame_1010.png vineyard_split_1_frame_1015.png vineyard_split_1_frame_1020.png vineyard_split_1_frame_1025.png vineyard_split_1_frame_1030.png vineyard_split_1_frame_1035.png vineyard_split_1_frame_1040.png vineyard_split_1_frame_1045.png vineyard_split_1_frame_1050.png vineyard_split_1_frame_1055.png vineyard_split_1_frame_1065.png vineyard_split_1_frame_1070.png vineyard_split_1_frame_1075.png vineyard_split_1_frame_1080.png vineyard_split_1_frame_1095.png vineyard_split_1_frame_1100.png vineyard_split_1_frame_1105.png vineyard_split_1_frame_1110.png vineyard_split_3_frame_1460.png vineyard_split_3_frame_1465.png vineyard_split_3_frame_1470.png vineyard_split_3_frame_1475.png vineyard_split_3_frame_1480.png vineyard_split_3_frame_1485.png vineyard_split_3_frame_1490.png vineyard_split_3_frame_1495.png vineyard_split_3_frame_1500.png vineyard_split_3_frame_1505.png\n",
      "map_index = 2 vineyard_split_2_frame_1150.png vineyard_split_2_frame_1155.png vineyard_split_2_frame_1160.png vineyard_split_2_frame_1165.png vineyard_split_2_frame_1170.png vineyard_split_2_frame_1175.png vineyard_split_2_frame_1180.png vineyard_split_2_frame_1185.png vineyard_split_2_frame_1195.png vineyard_split_2_frame_1200.png vineyard_split_2_frame_1205.png vineyard_split_2_frame_1210.png vineyard_split_2_frame_1215.png vineyard_split_2_frame_1220.png vineyard_split_2_frame_1225.png vineyard_split_2_frame_1230.png vineyard_split_2_frame_1235.png vineyard_split_2_frame_1240.png vineyard_split_2_frame_1245.png vineyard_split_2_frame_1250.png vineyard_split_2_frame_1255.png vineyard_split_2_frame_1260.png vineyard_split_2_frame_1265.png vineyard_split_2_frame_1270.png vineyard_split_2_frame_1275.png vineyard_split_2_frame_1280.png vineyard_split_2_frame_1285.png vineyard_split_2_frame_1290.png vineyard_split_2_frame_1295.png vineyard_split_2_frame_1300.png vineyard_split_2_frame_1305.png vineyard_split_2_frame_1310.png vineyard_split_2_frame_1315.png vineyard_split_2_frame_1320.png\n",
      "map_index = 3 vineyard_split_1_frame_0940.png vineyard_split_1_frame_0960.png vineyard_split_1_frame_0970.png vineyard_split_1_frame_0975.png vineyard_split_1_frame_0980.png vineyard_split_1_frame_0985.png vineyard_split_1_frame_0990.png vineyard_split_1_frame_1000.png\n",
      "map_index = 4 vineyard_split_1_frame_0900.png vineyard_split_1_frame_0905.png vineyard_split_1_frame_0910.png vineyard_split_1_frame_0915.png vineyard_split_1_frame_0920.png vineyard_split_1_frame_0925.png vineyard_split_1_frame_0930.png vineyard_split_1_frame_0935.png vineyard_split_1_frame_0940.png vineyard_split_1_frame_0945.png vineyard_split_1_frame_0950.png vineyard_split_1_frame_0955.png vineyard_split_1_frame_0960.png vineyard_split_1_frame_0965.png vineyard_split_1_frame_0970.png\n",
      "map_index = 5 vineyard_split_3_frame_1510.png vineyard_split_3_frame_1515.png vineyard_split_3_frame_1520.png vineyard_split_3_frame_1525.png vineyard_split_3_frame_1530.png vineyard_split_3_frame_1535.png vineyard_split_3_frame_1540.png vineyard_split_3_frame_1545.png vineyard_split_3_frame_1550.png vineyard_split_3_frame_1555.png vineyard_split_3_frame_1560.png vineyard_split_3_frame_1565.png vineyard_split_3_frame_1570.png\n",
      "map_index = 6 vineyard_split_3_frame_1390.png vineyard_split_3_frame_1395.png vineyard_split_3_frame_1400.png vineyard_split_3_frame_1405.png vineyard_split_3_frame_1410.png vineyard_split_3_frame_1415.png vineyard_split_3_frame_1420.png vineyard_split_3_frame_1425.png vineyard_split_3_frame_1430.png vineyard_split_3_frame_1435.png vineyard_split_3_frame_1440.png vineyard_split_3_frame_1445.png vineyard_split_3_frame_1450.png\n",
      "Dataset \"fbk_vineyard\" -> Registered 154 / 163 images with 7 clusters\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:01<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "30%:  0.2647\n",
      "USED 50%:  0.2804\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 150. Done in 1.6889 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 3.3262 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:10<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 11.1071 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 92.56it/s]\n",
      " 31%|███       | 65/210 [00:00<00:00, 4507.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 1.1235 sec\n",
      "Reconstruction done in  10.2792 sec\n",
      "{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=4194, num_observations=20137), 1: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=1548, num_observations=8635)}\n",
      "map_index = 0 et_et000.png et_et001.png et_et002.png et_et003.png et_et004.png et_et005.png et_et006.png et_et007.png et_et008.png\n",
      "map_index = 1 another_et_another_et001.png another_et_another_et002.png another_et_another_et003.png another_et_another_et004.png another_et_another_et005.png another_et_another_et006.png another_et_another_et007.png another_et_another_et008.png another_et_another_et009.png another_et_another_et010.png outliers_out_et001.png\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images with 2 clusters\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "30%:  0.2557\n",
      "USED 50%:  0.2767\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 647. Done in 10.0757 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 10.0531 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [01:10<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in 70.7356 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 21.92it/s]\n",
      " 11%|█         | 127/1176 [00:00<00:00, 3709.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran RANSAC in 1.7120 sec\n",
      "Reconstruction done in  18.8388 sec\n",
      "{0: Reconstruction(num_reg_images=8, num_cameras=8, num_points3D=829, num_observations=2039), 1: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=551, num_observations=1690), 2: Reconstruction(num_reg_images=7, num_cameras=7, num_points3D=269, num_observations=708)}\n",
      "map_index = 0 stairs_split_2_1710453805788.png stairs_split_2_1710453871430.png stairs_split_2_1710453720741.png stairs_split_2_1710453739354.png stairs_split_2_1710453740954.png stairs_split_2_1710453759963.png stairs_split_2_1710453783374.png stairs_split_2_1710453786375.png\n",
      "map_index = 1 stairs_split_2_1710453862225.png stairs_split_1_1710453675921.png stairs_split_1_1710453704934.png stairs_split_1_1710453901046.png stairs_split_1_1710453947066.png stairs_split_1_1710453990286.png stairs_split_2_1710453745156.png stairs_split_2_1710453790978.png stairs_split_2_1710453793579.png stairs_split_2_1710453798181.png\n",
      "map_index = 2 stairs_split_1_1710453601885.png stairs_split_1_1710453606287.png stairs_split_1_1710453659313.png stairs_split_1_1710453693529.png stairs_split_1_1710453947066.png stairs_split_1_1710453985484.png stairs_split_1_1710453990286.png\n",
      "Dataset \"stairs\" -> Registered 25 / 51 images with 3 clusters\n",
      "\n",
      "Results\n",
      "Dataset \"imc2023_haiper\" -> Registered 54 / 54 images with 2 clusters\n",
      "Dataset \"amy_gardens\" -> Registered 182 / 200 images with 11 clusters\n",
      "Dataset \"fbk_vineyard\" -> Registered 154 / 163 images with 7 clusters\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images with 2 clusters\n",
      "Dataset \"stairs\" -> Registered 25 / 51 images with 3 clusters\n",
      "\n",
      "Timings\n",
      "shortlisting -> total=70.75 sec.\n",
      "feature_detection -> total=83.01 sec.\n",
      "feature_matching -> total=2481.39 sec.\n",
      "RANSAC -> total=24.19 sec.\n",
      "Reconstruction -> total=712.91 sec.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs', \n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "print (f\"Extracting on device {device}\")\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "    try:\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.5, # should be strict\n",
    "            min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "    \n",
    "        t = time()\n",
    "\n",
    "        detect_aliked(images, feature_dir, 8192, device=device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "        \n",
    "        t = time()\n",
    "        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        # match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'Ran RANSAC in {time() - t:.4f} sec')\n",
    "        \n",
    "        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "        # Lower it to 3.\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = 5\n",
    "        mapper_options.max_num_models = 25\n",
    "        mapper_options.mapper.filter_max_reproj_error\t = 4.0\n",
    "        # mapper_options.min_num_matches\t = 60\n",
    "        # mapper_options.ba_local_max_num_iterations = 100\n",
    "        # mapper_options.ba_local_num_images = 10\n",
    "        # mapper_options.ba_global_images_freq = 5\n",
    "        \n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path, \n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options)\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction done in  {time() - t:.4f} sec')\n",
    "        print(maps)\n",
    "\n",
    "        # clear_output(wait=False)\n",
    "    \n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            img_list =[]\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                img_list.append(image.name)\n",
    "                registered += 1\n",
    "            img_list_str = ' '.join(img_list) \n",
    "            print(f\"map_index = {map_index}\", img_list_str)\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a53c27e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:33:43.747846Z",
     "iopub.status.busy": "2025-05-15T09:33:43.747524Z",
     "iopub.status.idle": "2025-05-15T09:33:43.936342Z",
     "shell.execute_reply": "2025-05-15T09:33:43.935431Z"
    },
    "papermill": {
     "duration": 0.636091,
     "end_time": "2025-05-15T09:33:43.937739",
     "exception": false,
     "start_time": "2025-05-15T09:33:43.301648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,cluster0,fountain_image_116.png,0.869023880;0.232841889;-0.436557156;-0.403481111;0.844182526;-0.352928968;0.286357275;0.482846268;0.827562078,0.358364191;-0.844410890;1.845444607\r\n",
      "imc2023_haiper,cluster0,fountain_image_108.png,0.933614539;-0.136145884;0.331403366;0.282408099;0.848857654;-0.446862786;-0.220475755;0.510788588;0.830954547,0.330336685;-0.730621170;1.630238752\r\n",
      "imc2023_haiper,cluster0,fountain_image_101.png,0.638938069;-0.278825593;0.716947998;0.592685600;0.772565811;-0.227740747;-0.490389563;0.570436987;0.658877621,-0.091686642;-1.002567971;1.791908998\r\n",
      "imc2023_haiper,cluster0,fountain_image_082.png,-0.981602687;-0.123205778;0.145864668;0.029795983;0.655745312;0.754393985;-0.188595770;0.744861344;-0.640010323,0.099329972;-1.709317572;4.182944527\r\n",
      "imc2023_haiper,cluster0,fountain_image_071.png,-0.805770873;0.188780928;-0.561333289;-0.314883031;0.666190203;0.676046811;0.501579482;0.721493156;-0.477352751,0.178164947;-1.875478213;3.967966213\r\n",
      "imc2023_haiper,cluster0,fountain_image_025.png,0.877232338;-0.221619621;0.425849936;0.222677399;0.973708380;0.048028812;-0.425297779;0.052694729;0.903518160,-0.413264919;-1.793991516;3.396185687\r\n",
      "imc2023_haiper,cluster0,fountain_image_000.png,-0.685732717;-0.310558811;0.658273397;0.278042477;0.724045460;0.631229398;-0.672653716;0.615882616;-0.410153120,-0.505903036;-2.120420050;5.301694530\r\n",
      "imc2023_haiper,cluster0,fountain_image_007.png,-0.245556026;-0.485824438;0.838854489;0.454864670;0.706422642;0.542277773;-0.856037599;0.514724845;0.047518036,-0.646401060;-2.482136665;4.036453728\r\n",
      "imc2023_haiper,cluster0,fountain_image_012.png,0.073401079;-0.474424107;0.877230898;0.447147109;0.801893383;0.396265649;-0.891443630;0.363164933;0.270997208,-0.497462816;-1.973233768;3.455952334\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05f838d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:33:44.810581Z",
     "iopub.status.busy": "2025-05-15T09:33:44.810230Z",
     "iopub.status.idle": "2025-05-15T09:34:47.128071Z",
     "shell.execute_reply": "2025-05-15T09:34:47.127025Z"
    },
    "papermill": {
     "duration": 62.777187,
     "end_time": "2025-05-15T09:34:47.129470",
     "exception": false,
     "start_time": "2025-05-15T09:33:44.352283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=65.84% (mAA=68.33%, clusterness=63.53%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "amy_gardens: score=24.50% (mAA=13.96%, clusterness=100.00%)\n",
      "fbk_vineyard: score=43.04% (mAA=28.25%, clusterness=90.38%)\n",
      "ETs: score=44.25% (mAA=28.85%, clusterness=95.00%)\n",
      "stairs: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "Average over all datasets: score=13.66% (mAA=10.72%, clusterness=34.53%)\n",
      "Computed metric in: 62.31 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04334225",
   "metadata": {
    "papermill": {
     "duration": 0.452964,
     "end_time": "2025-05-15T09:34:47.989894",
     "exception": false,
     "start_time": "2025-05-15T09:34:47.536930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11217117,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3498.835074,
   "end_time": "2025-05-15T09:34:52.266695",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-15T08:36:33.431621",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
