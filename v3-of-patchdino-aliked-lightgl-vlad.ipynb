{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29f4e4a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006658,
     "end_time": "2025-05-28T20:38:57.587488",
     "exception": false,
     "start_time": "2025-05-28T20:38:57.580830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd88193c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:38:57.600170Z",
     "iopub.status.busy": "2025-05-28T20:38:57.599826Z",
     "iopub.status.idle": "2025-05-28T20:39:04.107474Z",
     "shell.execute_reply": "2025-05-28T20:39:04.106314Z"
    },
    "papermill": {
     "duration": 6.515846,
     "end_time": "2025-05-28T20:39:04.109232",
     "exception": false,
     "start_time": "2025-05-28T20:38:57.593386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c886fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:04.123672Z",
     "iopub.status.busy": "2025-05-28T20:39:04.123344Z",
     "iopub.status.idle": "2025-05-28T20:39:05.182127Z",
     "shell.execute_reply": "2025-05-28T20:39:05.180968Z"
    },
    "papermill": {
     "duration": 1.067479,
     "end_time": "2025-05-28T20:39:05.183644",
     "exception": false,
     "start_time": "2025-05-28T20:39:04.116165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/depth-save.pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a594ebd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:05.197982Z",
     "iopub.status.busy": "2025-05-28T20:39:05.197714Z",
     "iopub.status.idle": "2025-05-28T20:39:05.201766Z",
     "shell.execute_reply": "2025-05-28T20:39:05.201158Z"
    },
    "papermill": {
     "duration": 0.012224,
     "end_time": "2025-05-28T20:39:05.202950",
     "exception": false,
     "start_time": "2025-05-28T20:39:05.190726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/root/.cache/torch/hub/checkpoints/depth-save.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bb1a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:05.216359Z",
     "iopub.status.busy": "2025-05-28T20:39:05.216110Z",
     "iopub.status.idle": "2025-05-28T20:39:40.343913Z",
     "shell.execute_reply": "2025-05-28T20:39:40.343237Z"
    },
    "papermill": {
     "duration": 35.13612,
     "end_time": "2025-05-28T20:39:40.345336",
     "exception": false,
     "start_time": "2025-05-28T20:39:05.209216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# from lightglue import DISK\n",
    "from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\n",
    "from scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint,DISK, DoGHardNet, LightGlue, SIFT\n",
    "from fastprogress import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131e268c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.360686Z",
     "iopub.status.busy": "2025-05-28T20:39:40.360214Z",
     "iopub.status.idle": "2025-05-28T20:39:40.363661Z",
     "shell.execute_reply": "2025-05-28T20:39:40.362905Z"
    },
    "papermill": {
     "duration": 0.012516,
     "end_time": "2025-05-28T20:39:40.364829",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.352313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d0c446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.378654Z",
     "iopub.status.busy": "2025-05-28T20:39:40.378423Z",
     "iopub.status.idle": "2025-05-28T20:39:40.497441Z",
     "shell.execute_reply": "2025-05-28T20:39:40.496505Z"
    },
    "papermill": {
     "duration": 0.127405,
     "end_time": "2025-05-28T20:39:40.498786",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.371381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6158d901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.513094Z",
     "iopub.status.busy": "2025-05-28T20:39:40.512816Z",
     "iopub.status.idle": "2025-05-28T20:39:40.516707Z",
     "shell.execute_reply": "2025-05-28T20:39:40.515935Z"
    },
    "papermill": {
     "duration": 0.012207,
     "end_time": "2025-05-28T20:39:40.517992",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.505785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c9648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.532088Z",
     "iopub.status.busy": "2025-05-28T20:39:40.531882Z",
     "iopub.status.idle": "2025-05-28T20:39:40.534998Z",
     "shell.execute_reply": "2025-05-28T20:39:40.534204Z"
    },
    "papermill": {
     "duration": 0.011481,
     "end_time": "2025-05-28T20:39:40.536130",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.524649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb5c771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.549720Z",
     "iopub.status.busy": "2025-05-28T20:39:40.549483Z",
     "iopub.status.idle": "2025-05-28T20:39:40.557455Z",
     "shell.execute_reply": "2025-05-28T20:39:40.556620Z"
    },
    "papermill": {
     "duration": 0.016104,
     "end_time": "2025-05-28T20:39:40.558578",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.542474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : True,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 3,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = True\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":1.0\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.2,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":1.0\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf7e804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.574904Z",
     "iopub.status.busy": "2025-05-28T20:39:40.574642Z",
     "iopub.status.idle": "2025-05-28T20:39:40.595281Z",
     "shell.execute_reply": "2025-05-28T20:39:40.594688Z"
    },
    "papermill": {
     "duration": 0.031276,
     "end_time": "2025-05-28T20:39:40.596495",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.565219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume these are available from your environment or previous code\n",
    "# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n",
    "# from kornia.feature import ALIKED # Already in your detect_aliked\n",
    "# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n",
    "# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n",
    "# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n",
    "\n",
    "# --- Helper function for image loading (if not already defined) ---\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "def get_dino_patch_features_for_keypoints(img_path, keypoints_xy, dino_processor, dino_model, patch_size=16, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Extracts DINO patch features corresponding to given ALIKED keypoint locations.\n",
    "    It correctly infers the DINO patch grid dimensions from the processed input.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        keypoints_xy (torch.Tensor): Nx2 tensor of (x, y) keypoint coordinates in image pixel space.\n",
    "                                     These keypoints are assumed to be in the original image's coordinate system.\n",
    "        dino_processor: HuggingFace AutoImageProcessor for DINO.\n",
    "        dino_model: HuggingFace AutoModel for DINO.\n",
    "        patch_size (int): The patch size used by the DINO model (e.g., 14 or 16).\n",
    "        device (torch.device): Device to run the models on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: NxD_dino tensor of DINO patch features for each keypoint.\n",
    "                      Returns None if no keypoints or image loading fails.\n",
    "    \"\"\"\n",
    "    if len(keypoints_xy) == 0:\n",
    "        dino_feature_dim = dino_model.config.hidden_size # Get actual DINO hidden size\n",
    "        return torch.empty((0, dino_feature_dim), device=device)\n",
    "\n",
    "    # 1. Load the original image (ALIKED processed this size)\n",
    "    original_img = load_torch_image(img_path, device=device)\n",
    "    original_h, original_w = original_img.shape[-2], original_img.shape[-1]\n",
    "\n",
    "\n",
    "    # 2. Process the image with DINO's processor\n",
    "    #    This step performs resizing, padding, etc., as needed by the DINO model\n",
    "    with torch.inference_mode():\n",
    "        # dino_processor returns a BatchFeature object which includes pixel_values\n",
    "        # and potentially other information like `pixel_mask`\n",
    "        inputs = dino_processor(images=original_img, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "        # Get the actual dimensions of the image as processed by the DINO model\n",
    "        # This is the crucial part: the actual H and W that produced `patch_tokens`\n",
    "        # We can infer this from the `pixel_values` shape\n",
    "        processed_h = inputs['pixel_values'].shape[-2]\n",
    "        processed_w = inputs['pixel_values'].shape[-1]\n",
    "\n",
    "        # Extract patch tokens (excluding the CLS token)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:].squeeze(0) # Shape: (num_patches, hidden_size)\n",
    "\n",
    "        # Calculate the actual grid dimensions based on the *processed* image size\n",
    "        # and the model's patch size.\n",
    "        # This should perfectly match the number of patch_tokens if the model is well-behaved.\n",
    "        num_patches_h = processed_h // patch_size\n",
    "        num_patches_w = processed_w // patch_size\n",
    "\n",
    "        # Safety check: ensure calculated grid matches actual token count\n",
    "        expected_token_count = num_patches_h * num_patches_w\n",
    "        if patch_tokens.shape[0] != expected_token_count:\n",
    "            # This indicates a deeper issue with how the model's output tokens\n",
    "            # map to the spatial grid, or an unexpected patch size/model behavior.\n",
    "            # Some models might have slightly different patch token arrangements.\n",
    "            # DINOv2 typically aligns well.\n",
    "            raise ValueError(\n",
    "                f\"DINO patch token count ({patch_tokens.shape[0]}) does not match \"\n",
    "                f\"expected grid dimensions ({num_patches_h}x{num_patches_w} = {expected_token_count}) \"\n",
    "                f\"for processed image size {processed_w}x{processed_h} with patch size {patch_size}. \"\n",
    "                f\"Please verify DINO model and processor configuration.\"\n",
    "            )\n",
    "\n",
    "        # Reshape patch tokens into a 2D grid\n",
    "        patch_features_grid = patch_tokens.reshape(num_patches_h, num_patches_w, -1)\n",
    "        dino_feature_dim = patch_features_grid.shape[-1] # Actual feature dimension\n",
    "\n",
    "\n",
    "    dino_features_for_kpts = torch.zeros((len(keypoints_xy), dino_feature_dim), device=device)\n",
    "\n",
    "    # 3. Rescale ALIKED keypoints to the DINO *processed* image dimensions\n",
    "    #    ALIKED keypoints are in original_w x original_h coordinates.\n",
    "    #    DINO patches correspond to processed_w x processed_h coordinates.\n",
    "    scale_x = processed_w / original_w\n",
    "    scale_y = processed_h / original_h\n",
    "\n",
    "    scaled_keypoints_xy = keypoints_xy.clone()\n",
    "    scaled_keypoints_xy[:, 0] *= scale_x\n",
    "    scaled_keypoints_xy[:, 1] *= scale_y\n",
    "\n",
    "    # 4. Map scaled keypoints to DINO patch grid indices\n",
    "    keypoint_cols = (scaled_keypoints_xy[:, 0] / patch_size).long()\n",
    "    keypoint_rows = (scaled_keypoints_xy[:, 1] / patch_size).long()\n",
    "\n",
    "    # Clip indices to ensure they are within bounds of the patch grid\n",
    "    keypoint_rows = torch.clamp(keypoint_rows, 0, num_patches_h - 1)\n",
    "    keypoint_cols = torch.clamp(keypoint_cols, 0, num_patches_w - 1)\n",
    "\n",
    "    # Gather DINO features for each keypoint's corresponding patch\n",
    "    dino_features_for_kpts = patch_features_grid[keypoint_rows, keypoint_cols]\n",
    "\n",
    "    return dino_features_for_kpts\n",
    "\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,\n",
    "                  match_confidence_threshold = 0.0,\n",
    "                  verbose=VERBOSE\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    # extractor = extractor_class(max_num_keypoints=num_features, detection_threshold=detection_threshold, \n",
    "    #                             resize=resize_to).eval().to(device, dtype)\n",
    "    # if model_name == 'disk':\n",
    "    #     extractor = DISK(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device).eval()\n",
    "    #     checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    #     extractor.load_state_dict(checkpoint['model'])\n",
    "    # else:\n",
    "    #     extractor_class = dict_model[model_name]\n",
    "    #     extractor = extractor_class(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device, dtype).eval()\n",
    "\n",
    "    extractor_class = dict_model[model_name]\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=detection_threshold,\n",
    "        resize=resize_to\n",
    "    ).to(device, dtype).eval()\n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            len1 = len(idxs)\n",
    "            n_matches = len1\n",
    "            if len(idxs) >= min_matches:                \n",
    "                conf = dists.cpu().numpy()  # lower is better\n",
    "                if conf.ndim == 2:\n",
    "                    conf = conf[:, 0]  # force (N,)\n",
    "                conf_mask = conf <= match_confidence_threshold\n",
    "                if not np.any(conf_mask):\n",
    "                    continue\n",
    "                idxs = idxs[conf_mask]\n",
    "                conf = conf[conf_mask]\n",
    "                n_matches = len(idxs)\n",
    "                if verbose:\n",
    "                    print(f\"match after conf threshold: {key1}-{key2}: {len1}->{n_matches}\")\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                pass\n",
    "                # if verbose:\n",
    "                #     print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "    match_confidence_threshold = 0.0\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "        match_confidence_threshold = match_confidence_threshold\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5232cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.610462Z",
     "iopub.status.busy": "2025-05-28T20:39:40.610266Z",
     "iopub.status.idle": "2025-05-28T20:39:40.636059Z",
     "shell.execute_reply": "2025-05-28T20:39:40.635403Z"
    },
    "papermill": {
     "duration": 0.034106,
     "end_time": "2025-05-28T20:39:40.637310",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.603204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    verbose = VERBOSE\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                # if verbose:\n",
    "                #     print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    if verbose:\n",
    "                        print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "                # print(f\"KKKKKKK KKKKKK {k1} - {k2}: {len(match)} matches\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55dff0a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.651167Z",
     "iopub.status.busy": "2025-05-28T20:39:40.650966Z",
     "iopub.status.idle": "2025-05-28T20:39:40.653459Z",
     "shell.execute_reply": "2025-05-28T20:39:40.652914Z"
    },
    "papermill": {
     "duration": 0.010837,
     "end_time": "2025-05-28T20:39:40.654657",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.643820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daef10b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.668539Z",
     "iopub.status.busy": "2025-05-28T20:39:40.668316Z",
     "iopub.status.idle": "2025-05-28T20:39:40.675139Z",
     "shell.execute_reply": "2025-05-28T20:39:40.674356Z"
    },
    "papermill": {
     "duration": 0.015019,
     "end_time": "2025-05-28T20:39:40.676286",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.661267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: Detect ALIKED and Combine with DINO Patch Features ---\n",
    "def detect_aliked_and_combine_with_dino(img_fnames,\n",
    "                                        feature_dir='.featureout',\n",
    "                                        num_features=4096,\n",
    "                                        resize_to=1024,\n",
    "                                        dino_processor=None,\n",
    "                                        dino_model=None,\n",
    "                                        dino_patch_size=16, # Typically 14 or 16 for DINO\n",
    "                                        device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    aliked_extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    aliked_extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='w') as f_desc_aliked, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='w') as f_desc_combined: # New HDF5 for combined features\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = aliked_extractor.extract(image0)\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy() # ALIKED keypoints (x,y)\n",
    "                descs_aliked = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy() # ALIKED descriptors\n",
    "\n",
    "                # Get DINO patch features for these keypoints\n",
    "                kpts_torch = torch.from_numpy(kpts).to(device)\n",
    "                descs_dino_patch = get_dino_patch_features_for_keypoints(\n",
    "                    img_path, kpts_torch, dino_processor, dino_model, dino_patch_size, device\n",
    "                ).detach().cpu().numpy()\n",
    "\n",
    "                # Concatenate ALIKED and DINO features\n",
    "                if len(descs_aliked) > 0 and len(descs_dino_patch) > 0:\n",
    "                    combined_descs = np.concatenate((descs_aliked, descs_dino_patch), axis=1)\n",
    "                elif len(descs_aliked) > 0: # Only ALIKED if no DINO features (shouldn't happen often)\n",
    "                    combined_descs = descs_aliked\n",
    "                else: # No features found\n",
    "                    combined_descs = np.array([]) # Empty array\n",
    "\n",
    "                f_kp[key] = kpts\n",
    "                f_desc_aliked[key] = descs_aliked # Keep ALIKED descriptors for debugging or other uses\n",
    "                f_desc_combined[key] = combined_descs # Store the new combined descriptors\n",
    "    print(f\"Combined features saved to {feature_dir}/descriptors_combined.h5\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbbe3703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:40.690327Z",
     "iopub.status.busy": "2025-05-28T20:39:40.690130Z",
     "iopub.status.idle": "2025-05-28T20:39:41.234776Z",
     "shell.execute_reply": "2025-05-28T20:39:41.233916Z"
    },
    "papermill": {
     "duration": 0.553634,
     "end_time": "2025-05-28T20:39:41.236310",
     "exception": false,
     "start_time": "2025-05-28T20:39:40.682676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans # MiniBatchKMeans is faster for large datasets\n",
    "\n",
    "# --- VLAD Aggregation Function ---\n",
    "def vlad_encode(descriptors, centroids):\n",
    "    \"\"\"\n",
    "    Performs VLAD encoding.\n",
    "\n",
    "    Args:\n",
    "        descriptors (np.ndarray): NxM array of local descriptors.\n",
    "        centroids (np.ndarray): KxM array of K-Means cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1x(K*M) VLAD descriptor.\n",
    "    \"\"\"\n",
    "    if descriptors.shape[0] == 0:\n",
    "        return np.zeros(centroids.shape[0] * centroids.shape[1], dtype=np.float32)\n",
    "\n",
    "    num_descriptors, desc_dim = descriptors.shape\n",
    "    num_centroids, _ = centroids.shape\n",
    "\n",
    "    # Assign each descriptor to its nearest centroid\n",
    "    # Using cdist for efficiency\n",
    "    distances = np.sqrt(np.sum((descriptors[:, None, :] - centroids[None, :, :])**2, axis=2))\n",
    "    # distances = cdist(descriptors, centroids, 'sqeuclidean') # Could use cdist for sqeuclidean\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Initialize VLAD accumulator\n",
    "    vlad_accumulator = np.zeros((num_centroids, desc_dim), dtype=np.float32)\n",
    "\n",
    "    # Accumulate residuals\n",
    "    for i in range(num_descriptors):\n",
    "        cluster_idx = cluster_assignments[i]\n",
    "        residual = descriptors[i] - centroids[cluster_idx]\n",
    "        vlad_accumulator[cluster_idx] += residual\n",
    "\n",
    "    # Flatten and L2 normalize\n",
    "    vlad_descriptor = vlad_accumulator.flatten()\n",
    "    vlad_descriptor = F.normalize(torch.from_numpy(vlad_descriptor).unsqueeze(0), dim=1, p=2).squeeze(0).numpy()\n",
    "\n",
    "    return vlad_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e30adf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.252535Z",
     "iopub.status.busy": "2025-05-28T20:39:41.252279Z",
     "iopub.status.idle": "2025-05-28T20:39:41.260233Z",
     "shell.execute_reply": "2025-05-28T20:39:41.259427Z"
    },
    "papermill": {
     "duration": 0.017772,
     "end_time": "2025-05-28T20:39:41.261432",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.243660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- NEW: Get Global Descriptors using K-Means + VLAD ---\n",
    "def get_global_desc_vlad(fnames, feature_dir='.featureout', num_clusters=64, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Generates global descriptors for images using K-Means + VLAD on combined ALIKED+DINO features.\n",
    "\n",
    "    Args:\n",
    "        fnames (list): List of image file paths.\n",
    "        feature_dir (str): Directory where combined descriptors are stored.\n",
    "        num_clusters (int): Number of clusters for K-Means (K in VLAD).\n",
    "        device (torch.device): Not directly used for VLAD computation, but passed for consistency.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Nx(K*M) tensor of global VLAD descriptors.\n",
    "    \"\"\"\n",
    "    all_local_descs = []\n",
    "    keys_order = [] # To maintain order of descriptors with respect to fnames\n",
    "\n",
    "    # 1. Load all combined local descriptors\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Loading combined local descriptors for K-Means\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                if descs.shape[0] > 0:\n",
    "                    all_local_descs.append(descs)\n",
    "                    keys_order.append(key)\n",
    "\n",
    "    if not all_local_descs:\n",
    "        print(\"No combined local descriptors found. Cannot train K-Means or compute VLAD.\")\n",
    "        return torch.empty((0, num_clusters * 0), dtype=torch.float32) # Return empty tensor\n",
    "\n",
    "    # Concatenate all descriptors for K-Means training\n",
    "    all_local_descs_flat = np.concatenate(all_local_descs, axis=0)\n",
    "\n",
    "    # 2. Train K-Means on a subset of descriptors if the dataset is too large\n",
    "    # Or directly on all_local_descs_flat if memory permits\n",
    "    print(f\"Training K-Means with {num_clusters} clusters on {all_local_descs_flat.shape[0]} descriptors...\")\n",
    "    # Use MiniBatchKMeans for efficiency\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, n_init='auto', batch_size=256).fit(all_local_descs_flat)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    print(\"K-Means training complete.\")\n",
    "\n",
    "    # 3. Compute VLAD descriptor for each image\n",
    "    global_descs_vlad = []\n",
    "    # Re-iterate through original fnames to match the output order\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Computing VLAD descriptors\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                vlad_desc = vlad_encode(descs, centroids)\n",
    "                global_descs_vlad.append(torch.from_numpy(vlad_desc).unsqueeze(0))\n",
    "            else:\n",
    "                # Handle cases where an image might not have any combined descriptors\n",
    "                # (e.g., no ALIKED keypoints detected). Append a zero vector of correct size.\n",
    "                print(f\"Warning: No combined descriptors for {key}. Appending zero VLAD descriptor.\")\n",
    "                # Determine descriptor dimension from centroids\n",
    "                desc_dim_per_cluster = centroids.shape[1] if centroids.shape[1] > 0 else 0 # Should not be 0 normally\n",
    "                zero_vlad = np.zeros(num_clusters * desc_dim_per_cluster, dtype=np.float32)\n",
    "                global_descs_vlad.append(torch.from_numpy(zero_vlad).unsqueeze(0))\n",
    "\n",
    "\n",
    "    if not global_descs_vlad:\n",
    "        return torch.empty((0, num_clusters * centroids.shape[1] if centroids.shape[1] > 0 else 0), dtype=torch.float32)\n",
    "\n",
    "    global_descs_vlad = torch.cat(global_descs_vlad, dim=0)\n",
    "    return global_descs_vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4816ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.275463Z",
     "iopub.status.busy": "2025-05-28T20:39:41.275239Z",
     "iopub.status.idle": "2025-05-28T20:39:41.282670Z",
     "shell.execute_reply": "2025-05-28T20:39:41.281856Z"
    },
    "papermill": {
     "duration": 0.01598,
     "end_time": "2025-05-28T20:39:41.284033",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.268053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- RE-DEFINED: get_image_pairs_shortlist to use the new VLAD global descriptor ---\n",
    "def get_image_pairs_shortlist_vlad(fnames,\n",
    "                                   sim_th=0.6, # should be strict\n",
    "                                   min_pairs=30,\n",
    "                                   exhaustive_if_less=20,\n",
    "                                   feature_dir='.featureout', # Pass feature_dir\n",
    "                                   num_clusters_vlad=64, # New parameter for VLAD\n",
    "                                   device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames) # You need to define get_img_pairs_exhaustive if not done.\n",
    "\n",
    "    # Use the new VLAD-based global descriptor\n",
    "    descs = get_global_desc_vlad(fnames, feature_dir=feature_dir, num_clusters=num_clusters_vlad, device=device)\n",
    "\n",
    "    if descs.shape[0] == 0:\n",
    "        print(\"No global descriptors generated. Returning empty matching list.\")\n",
    "        return []\n",
    "\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # \n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 60)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = set() # Use a set for faster lookup of already added pairs\n",
    "\n",
    "    for st_idx in range(num_imgs - 1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold: # Ensure distance is not effectively infinite\n",
    "                pair = tuple(sorted((st_idx, idx.item())))\n",
    "                if pair not in already_there_set:\n",
    "                    matching_list.append(pair)\n",
    "                    already_there_set.add(pair)\n",
    "                    total += 1\n",
    "    matching_list = sorted(list(matching_list)) # Sort the list of tuples\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1003a3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.298044Z",
     "iopub.status.busy": "2025-05-28T20:39:41.297727Z",
     "iopub.status.idle": "2025-05-28T20:39:41.301026Z",
     "shell.execute_reply": "2025-05-28T20:39:41.300431Z"
    },
    "papermill": {
     "duration": 0.011701,
     "end_time": "2025-05-28T20:39:41.302229",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.290528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c0ca9fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.316154Z",
     "iopub.status.busy": "2025-05-28T20:39:41.315945Z",
     "iopub.status.idle": "2025-05-28T20:39:41.325473Z",
     "shell.execute_reply": "2025-05-28T20:39:41.324896Z"
    },
    "papermill": {
     "duration": 0.017721,
     "end_time": "2025-05-28T20:39:41.326501",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.308780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th=0.6,\n",
    "                              min_pairs=30,\n",
    "                              max_pairs=100,  #  max_pairs \n",
    "                              exhaustive_if_less=20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "\n",
    "    threshold = np.percentile(dm_flat, 50) + np.sqrt(3) * dm_flat.std()\n",
    "    mask = dm <= np.percentile(dm_flat, 30)\n",
    "\n",
    "    ar = np.arange(num_imgs)\n",
    "    matching_set = set()\n",
    "\n",
    "    for st_idx in range(num_imgs):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "\n",
    "        #  min_pairs \n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        #  max_pairs\n",
    "        sorted_matches = sorted(\n",
    "            [(idx, dm[st_idx, idx]) for idx in to_match if idx != st_idx and dm[st_idx, idx] < threshold],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        for idx, _ in sorted_matches[:max_pairs]:\n",
    "            pair = tuple(sorted((st_idx, idx)))\n",
    "            matching_set.add(pair)\n",
    "\n",
    "    matching_list = sorted(list(matching_set))\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc120309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.340260Z",
     "iopub.status.busy": "2025-05-28T20:39:41.340055Z",
     "iopub.status.idle": "2025-05-28T20:39:41.358501Z",
     "shell.execute_reply": "2025-05-28T20:39:41.357951Z"
    },
    "papermill": {
     "duration": 0.026663,
     "end_time": "2025-05-28T20:39:41.359655",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.332992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_aliked_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_disk_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f86e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.373438Z",
     "iopub.status.busy": "2025-05-28T20:39:41.373195Z",
     "iopub.status.idle": "2025-05-28T20:39:41.379619Z",
     "shell.execute_reply": "2025-05-28T20:39:41.378913Z"
    },
    "papermill": {
     "duration": 0.014624,
     "end_time": "2025-05-28T20:39:41.380711",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.366087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(feature_dir, img_dir):\n",
    "    result = {}\n",
    "    local_timings = {'RANSAC': [], 'Reconstruction': []}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(\"colmap database\")\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # options = pycolmap.SiftMatchingOptions()\n",
    "    # options.confidence = 0.9999\n",
    "    # options.max_num_trials = 20000\n",
    "    # pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    print(\"matching done!!!!\")\n",
    "    local_timings['RANSAC'].append(time() - t)\n",
    "    print(f'RANSAC in {local_timings[\"RANSAC\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # mapper_options.mapper.filter_max_reproj_error\t = 1.0\n",
    "    # mapper_options.mapper.init_max_error = 2.0\n",
    "    mapper_options.min_model_size = 5\n",
    "    mapper_options.max_num_models = 25\n",
    "    mapper_options.ba_global_images_freq = 5\n",
    "    mapper_options.ba_local_num_images = 8\n",
    "    mapper_options.mapper.abs_pose_min_inlier_ratio = 0.4\n",
    "    mapper_options.ba_global_max_num_iterations = 100\n",
    "    # mapper_options.mapper.filter_max_reproj_error = 6.0\n",
    "    mapper_options.mapper.max_reg_trials = 10\n",
    "    # mapper_options.mapper.init_min_num_inliers = 50\n",
    "    # mapper_options.mapper.abs_pose_min_num_inliers = 15\n",
    "    \n",
    "\n",
    "    \n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, \n",
    "                                        output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    for map_index, rec in maps.items():\n",
    "        result[map_index] = {}\n",
    "        for img_id, image in rec.images.items():\n",
    "            result[map_index][image.name] = {\n",
    "                'R': image.cam_from_world.rotation.matrix().tolist(),\n",
    "                't': image.cam_from_world.translation.tolist()\n",
    "            }\n",
    "    # clear_output(wait=False)\n",
    "    local_timings['Reconstruction'].append(time() - t)\n",
    "    print(f'Reconstruction done in {local_timings[\"Reconstruction\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    return result, local_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75e3975d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.394532Z",
     "iopub.status.busy": "2025-05-28T20:39:41.394310Z",
     "iopub.status.idle": "2025-05-28T20:39:41.546950Z",
     "shell.execute_reply": "2025-05-28T20:39:41.546301Z"
    },
    "papermill": {
     "duration": 0.160814,
     "end_time": "2025-05-28T20:39:41.548169",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.387355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e33e133e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.562367Z",
     "iopub.status.busy": "2025-05-28T20:39:41.562152Z",
     "iopub.status.idle": "2025-05-28T20:39:41.576444Z",
     "shell.execute_reply": "2025-05-28T20:39:41.575815Z"
    },
    "papermill": {
     "duration": 0.022495,
     "end_time": "2025-05-28T20:39:41.577604",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.555109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_keypoints_and_matches(images_input, unified_kp_path, remapped_matches_path, feature_dir='visualization_output'):\n",
    "    output_dir = os.path.join(feature_dir, 'visualization_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load images and determine image_keys for HDF5 lookup\n",
    "    if isinstance(images_input[0], str):\n",
    "        loaded_images = [cv2.imread(img_path) for img_path in images_input]\n",
    "        image_keys = [os.path.basename(img_path) for img_path in images_input]\n",
    "    else:\n",
    "        loaded_images = images_input\n",
    "        # If images_input are already arrays, you need to provide the corresponding keys\n",
    "        # This part is crucial: image_keys MUST align with the HDF5 keys\n",
    "        image_keys = image_keys_in_h5 # Use the predefined list for the dummy case\n",
    "\n",
    "    # Load unified keypoints\n",
    "    keypoints_data = {}\n",
    "    with h5py.File(unified_kp_path, 'r') as f_kp:\n",
    "        for img_name_raw in f_kp.keys():\n",
    "            img_name = img_name_raw.decode('utf-8') if isinstance(img_name_raw, bytes) else img_name_raw\n",
    "            keypoints_data[img_name] = f_kp[img_name_raw][()] # Access with raw key if bytes\n",
    "\n",
    "    # Load remapped matches - CORRECTED LOGIC\n",
    "    # Store (img1_key, img2_key) directly with matches for robust iteration\n",
    "    matches_data_pairs = [] # Will store (img1_key, img2_key, matches_array)\n",
    "    with h5py.File(remapped_matches_path, 'r') as f_matches:\n",
    "        print(\"\\n--- Loading remapped matches from HDF5 ---\")\n",
    "        for img1_group_key_candidate in tqdm(f_matches.keys(), desc=\"Loading matches\"):\n",
    "            img1_key = img1_group_key_candidate.decode('utf-8') if isinstance(img1_group_key_candidate, bytes) else img1_group_key_candidate\n",
    "\n",
    "            img1_group = f_matches[img1_group_key_candidate] # Access with raw key\n",
    "\n",
    "            if isinstance(img1_group, h5py.Group):\n",
    "                for img2_dataset_key_candidate in img1_group.keys():\n",
    "                    img2_key = img2_dataset_key_candidate.decode('utf-8') if isinstance(img2_dataset_key_candidate, bytes) else img2_dataset_key_candidate\n",
    "\n",
    "                    try:\n",
    "                        matches_array = img1_group[img2_dataset_key_candidate][()]\n",
    "                        matches_data_pairs.append((img1_key, img2_key, matches_array))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading matches for pair ({img1_key}, {img2_key}): {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Expected '{img1_key}' to be a group, but found {type(img1_group)}. Skipping its contents.\")\n",
    "\n",
    "\n",
    "    # --- Drawing Keypoints ---\n",
    "    print(\"\\n--- Drawing Keypoints ---\")\n",
    "    for i, img_key in enumerate(image_keys):\n",
    "        if img_key in keypoints_data:\n",
    "            img = loaded_images[i].copy()\n",
    "            kpts = keypoints_data[img_key]\n",
    "\n",
    "            for kp in kpts:\n",
    "                x, y = int(kp[0]), int(kp[1])\n",
    "                cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Green circle for keypoint\n",
    "\n",
    "            output_kp_path = os.path.join(output_dir, f\"keypoints_{img_key}\")\n",
    "            if len(img.shape) == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(output_kp_path, img)\n",
    "            print(f\"Keypoints drawn on {img_key}, saved to {output_kp_path}\")\n",
    "        else:\n",
    "            print(f\"No keypoints found for {img_key} in unified keypoints file.\")\n",
    "\n",
    "    # --- Drawing Matches ---\n",
    "    print(\"\\n--- Drawing Matches ---\")\n",
    "    # Iterate through the (img1_key, img2_key, matches) tuples directly\n",
    "    for img_name1, img_name2, matches in matches_data_pairs:\n",
    "        # We no longer need to split img_pair_key, as we have img_name1 and img_name2 directly\n",
    "\n",
    "        # Find the actual image objects and their keypoints using image_keys list\n",
    "        try:\n",
    "            img1_idx = image_keys.index(img_name1)\n",
    "            img2_idx = image_keys.index(img_name2)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: One or both image names not found in the provided 'images' list/keys.\")\n",
    "            continue\n",
    "\n",
    "        img1 = loaded_images[img1_idx].copy()\n",
    "        img2 = loaded_images[img2_idx].copy()\n",
    "\n",
    "        kpts1 = keypoints_data.get(img_name1)\n",
    "        kpts2 = keypoints_data.get(img_name2)\n",
    "\n",
    "        if kpts1 is None or kpts2 is None:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: keypoints not found for one or both images in unified keypoints.\")\n",
    "            continue\n",
    "        if len(matches) == 0:\n",
    "            print(f\"No matches to draw for {img_name1}-{img_name2}.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure images are 3 channels for drawing lines\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "        if len(img2.shape) == 2:\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Create a concatenated image for drawing matches\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        max_h = max(h1, h2)\n",
    "        matched_img = np.zeros((max_h, w1 + w2, 3), dtype=np.uint8)\n",
    "        matched_img[0:h1, 0:w1] = img1\n",
    "        matched_img[0:h2, w1:w1+w2] = img2\n",
    "\n",
    "        num_matches_to_draw = min(len(matches), 200) # Draw up to 200 matches to avoid clutter, adjust as needed\n",
    "\n",
    "        for i in range(num_matches_to_draw):\n",
    "            match = matches[i]\n",
    "            kp1_idx, kp2_idx = int(match[0]), int(match[1])\n",
    "\n",
    "            # Bounds check for keypoint indices\n",
    "            if kp1_idx >= len(kpts1) or kp2_idx >= len(kpts2):\n",
    "                # print(f\"Warning: Match index out of bounds for {img_name1}-{img_name2}. Skipping match {kp1_idx}-{kp2_idx}.\")\n",
    "                continue\n",
    "\n",
    "            pt1 = tuple(map(int, kpts1[kp1_idx][:2]))\n",
    "            pt2 = tuple(map(int, kpts2[kp2_idx][:2]))\n",
    "\n",
    "            # Draw circles on the concatenated image\n",
    "            cv2.circle(matched_img, pt1, 5, (0, 0, 255), 2) # Red circle on img1 side\n",
    "            cv2.circle(matched_img, (pt2[0] + w1, pt2[1]), 5, (255, 0, 0), 2) # Blue circle on img2 side\n",
    "\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            cv2.line(matched_img, pt1, (pt2[0] + w1, pt2[1]), color, 1)\n",
    "\n",
    "        output_match_path = os.path.join(output_dir, f\"matches_{img_name1}_{img_name2}.png\")\n",
    "        cv2.imwrite(output_match_path, matched_img)\n",
    "        print(f\"Matches drawn between {img_name1} and {img_name2}, saved to {output_match_path}\")\n",
    "\n",
    "\n",
    "# Example call (replace with your actual 'images' list)\n",
    "# If your 'images' are file paths:\n",
    "# images_file_paths = ['path/to/your/image1.jpg', 'path/to/your/image2.jpg', ...]\n",
    "# draw_keypoints_and_matches(images_file_paths, unified_kp_path, remapped_matches_path)\n",
    "\n",
    "# If your 'images' are loaded numpy arrays (as in the dummy example above):\n",
    "# draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d054e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:39:41.591224Z",
     "iopub.status.busy": "2025-05-28T20:39:41.591015Z",
     "iopub.status.idle": "2025-05-28T20:43:32.253893Z",
     "shell.execute_reply": "2025-05-28T20:43:32.252643Z"
    },
    "papermill": {
     "duration": 230.671369,
     "end_time": "2025-05-28T20:43:32.255427",
     "exception": false,
     "start_time": "2025-05-28T20:39:41.584058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model for patch feature extraction...\n",
      "DINOv2 model loaded.\n",
      "Skipping \"imc2023_haiper\"\n",
      "Skipping \"imc2023_heritage\"\n",
      "Skipping \"imc2023_theather_imc2024_church\"\n",
      "Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Skipping \"imc2024_lizard_pond\"\n",
      "Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Skipping \"pt_stpeters_stpauls\"\n",
      "Skipping \"amy_gardens\"\n",
      "Skipping \"fbk_vineyard\"\n",
      "Skipping \"ETs\"\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n",
      "rotation_detection for 51 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:11<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "60%:  0.2868\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 620. Done in 11.7301 sec\n",
      "Generated 620 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 620. Done in 12.0283 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([632, 2]), descs.shape=torch.Size([632, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([970, 2]), descs.shape=torch.Size([970, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([440, 2]), descs.shape=torch.Size([440, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([976, 2]), descs.shape=torch.Size([976, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1257, 2]), descs.shape=torch.Size([1257, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2208, 2]), descs.shape=torch.Size([2208, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2303, 2]), descs.shape=torch.Size([2303, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1486, 2]), descs.shape=torch.Size([1486, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([806, 2]), descs.shape=torch.Size([806, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([851, 2]), descs.shape=torch.Size([851, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1128, 2]), descs.shape=torch.Size([1128, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([544, 2]), descs.shape=torch.Size([544, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1933, 2]), descs.shape=torch.Size([1933, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1467, 2]), descs.shape=torch.Size([1467, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1514, 2]), descs.shape=torch.Size([1514, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3646, 2]), descs.shape=torch.Size([3646, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1017, 2]), descs.shape=torch.Size([1017, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1209, 2]), descs.shape=torch.Size([1209, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([392, 2]), descs.shape=torch.Size([392, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([885, 2]), descs.shape=torch.Size([885, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1430, 2]), descs.shape=torch.Size([1430, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1327, 2]), descs.shape=torch.Size([1327, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2183, 2]), descs.shape=torch.Size([2183, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1172, 2]), descs.shape=torch.Size([1172, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([597, 2]), descs.shape=torch.Size([597, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1025, 2]), descs.shape=torch.Size([1025, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1702, 2]), descs.shape=torch.Size([1702, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([238, 2]), descs.shape=torch.Size([238, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1450, 2]), descs.shape=torch.Size([1450, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2413, 2]), descs.shape=torch.Size([2413, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2404, 2]), descs.shape=torch.Size([2404, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1319, 2]), descs.shape=torch.Size([1319, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1729, 2]), descs.shape=torch.Size([1729, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3269, 2]), descs.shape=torch.Size([3269, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2769, 2]), descs.shape=torch.Size([2769, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1897, 2]), descs.shape=torch.Size([1897, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2382, 2]), descs.shape=torch.Size([2382, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1054, 2]), descs.shape=torch.Size([1054, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2995, 2]), descs.shape=torch.Size([2995, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2149, 2]), descs.shape=torch.Size([2149, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2114, 2]), descs.shape=torch.Size([2114, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2111, 2]), descs.shape=torch.Size([2111, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2257, 2]), descs.shape=torch.Size([2257, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1541, 2]), descs.shape=torch.Size([1541, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2563, 2]), descs.shape=torch.Size([2563, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1133, 2]), descs.shape=torch.Size([1133, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3735, 2]), descs.shape=torch.Size([3735, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3395, 2]), descs.shape=torch.Size([3395, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1181, 2]), descs.shape=torch.Size([1181, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1078, 2]), descs.shape=torch.Size([1078, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 19/620 [00:00<00:19, 30.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453963274.png-stairs_split_1_1710453643106.png: 133->133\n",
      "aliked> stairs_split_1_1710453963274.png-stairs_split_1_1710453643106.png: 133 matches @ 1th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 32/620 [00:01<00:16, 35.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 224->224\n",
      "aliked> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 224 matches @ 2th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 44/620 [00:01<00:16, 35.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453659313.png: 218->218\n",
      "aliked> stairs_split_1_1710453947066.png-stairs_split_1_1710453659313.png: 218 matches @ 3th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 52/620 [00:01<00:17, 32.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_2_1710453725143.png: 103->103\n",
      "aliked> stairs_split_1_1710453947066.png-stairs_split_2_1710453725143.png: 103 matches @ 4th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453985484.png-stairs_split_1_1710453606287.png: 143->143\n",
      "aliked> stairs_split_1_1710453985484.png-stairs_split_1_1710453606287.png: 143 matches @ 5th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 68/620 [00:02<00:15, 34.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453985484.png-stairs_split_1_1710453612890.png: 154->154\n",
      "aliked> stairs_split_1_1710453985484.png-stairs_split_1_1710453612890.png: 154 matches @ 6th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 80/620 [00:02<00:14, 36.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_1_1710453668718.png: 137->137\n",
      "aliked> stairs_split_1_1710453930259.png-stairs_split_1_1710453668718.png: 137 matches @ 7th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_1_1710453651110.png: 308->308\n",
      "aliked> stairs_split_1_1710453930259.png-stairs_split_1_1710453651110.png: 308 matches @ 8th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 88/620 [00:02<00:15, 33.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 163->163\n",
      "aliked> stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 163 matches @ 9th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_1_1710453704934.png: 313->313\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_1_1710453704934.png: 313 matches @ 10th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 113/620 [00:03<00:13, 37.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 224->224\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 224 matches @ 11th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453798181.png: 133->133\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453798181.png: 133 matches @ 12th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 174->174\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 174 matches @ 13th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453790978.png: 151->151\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453790978.png: 151 matches @ 14th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 121/620 [00:03<00:13, 35.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 162->162\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 162 matches @ 15th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 129/620 [00:03<00:14, 33.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_1_1710453601885.png: 124->124\n",
      "aliked> stairs_split_1_1710453693529.png-stairs_split_1_1710453601885.png: 124 matches @ 16th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 155/620 [00:05<00:18, 24.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 203->203\n",
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 203 matches @ 17th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 167/620 [00:05<00:20, 22.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453689727.png-stairs_split_2_1710453736752.png: 197->197\n",
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453736752.png: 197 matches @ 18th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 179/620 [00:05<00:14, 30.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453798181.png: 122->122\n",
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453798181.png: 122 matches @ 19th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 333->333\n",
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 333 matches @ 20th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 187/620 [00:06<00:14, 30.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 435->435\n",
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 435 matches @ 21th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 216/620 [00:07<00:10, 36.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 173->173\n",
      "aliked> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 173 matches @ 22th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453606287.png-stairs_split_1_1710453659313.png: 119->119\n",
      "aliked> stairs_split_1_1710453606287.png-stairs_split_1_1710453659313.png: 119 matches @ 23th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|     | 256/620 [00:08<00:09, 39.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453663515.png-stairs_split_1_1710453612890.png: 103->103\n",
      "aliked> stairs_split_1_1710453663515.png-stairs_split_1_1710453612890.png: 103 matches @ 24th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 284/620 [00:08<00:10, 32.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453668718.png-stairs_split_1_1710453651110.png: 264->264\n",
      "aliked> stairs_split_1_1710453668718.png-stairs_split_1_1710453651110.png: 264 matches @ 25th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 292/620 [00:09<00:11, 29.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453668718.png-stairs_split_2_1710453740954.png: 245->245\n",
      "aliked> stairs_split_1_1710453668718.png-stairs_split_2_1710453740954.png: 245 matches @ 26th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 299/620 [00:09<00:10, 31.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_1_1710453576271.png: 303->303\n",
      "aliked> stairs_split_1_1710453601885.png-stairs_split_1_1710453576271.png: 303 matches @ 27th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 311/620 [00:09<00:10, 28.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_2_1710453736752.png: 137->137\n",
      "aliked> stairs_split_1_1710453601885.png-stairs_split_2_1710453736752.png: 137 matches @ 28th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 317/620 [00:10<00:12, 25.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_1_1710453651110.png: 267->267\n",
      "aliked> stairs_split_1_1710453955270.png-stairs_split_1_1710453651110.png: 267 matches @ 29th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 320/620 [00:10<00:12, 23.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 138->138\n",
      "aliked> stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 138 matches @ 30th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_2_1710453786375.png: 127->127\n",
      "aliked> stairs_split_1_1710453955270.png-stairs_split_2_1710453786375.png: 127 matches @ 31th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 328/620 [00:10<00:16, 17.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_2_1710453720741.png: 118->118\n",
      "aliked> stairs_split_1_1710453955270.png-stairs_split_2_1710453720741.png: 118 matches @ 32th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 334/620 [00:11<00:16, 17.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_2_1710453736752.png: 105->105\n",
      "aliked> stairs_split_1_1710453955270.png-stairs_split_2_1710453736752.png: 105 matches @ 33th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 342/620 [00:11<00:10, 26.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453616892.png-stairs_split_1_1710453612890.png: 136->136\n",
      "aliked> stairs_split_1_1710453616892.png-stairs_split_1_1710453612890.png: 136 matches @ 34th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 447/620 [00:14<00:05, 32.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 143->143\n",
      "aliked> stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 143 matches @ 35th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 487/620 [00:15<00:04, 31.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453801783.png: 104->104\n",
      "aliked> stairs_split_2_1710453871430.png-stairs_split_2_1710453801783.png: 104 matches @ 36th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453783374.png: 266->266\n",
      "aliked> stairs_split_2_1710453871430.png-stairs_split_2_1710453783374.png: 266 matches @ 37th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 494/620 [00:16<00:04, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453720741.png: 102->102\n",
      "aliked> stairs_split_2_1710453871430.png-stairs_split_2_1710453720741.png: 102 matches @ 38th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453739354.png: 653->653\n",
      "aliked> stairs_split_2_1710453871430.png-stairs_split_2_1710453739354.png: 653 matches @ 39th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453756762.png: 129->129\n",
      "aliked> stairs_split_2_1710453871430.png-stairs_split_2_1710453756762.png: 129 matches @ 40th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 497/620 [00:16<00:05, 22.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453736752.png: 439->439\n",
      "aliked> stairs_split_2_1710453871430.png-stairs_split_2_1710453736752.png: 439 matches @ 41th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 523/620 [00:17<00:03, 29.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453798181.png-stairs_split_2_1710453753160.png: 126->126\n",
      "aliked> stairs_split_2_1710453798181.png-stairs_split_2_1710453753160.png: 126 matches @ 42th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453793579.png-stairs_split_2_1710453790978.png: 151->151\n",
      "aliked> stairs_split_2_1710453793579.png-stairs_split_2_1710453790978.png: 151 matches @ 43th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 538/620 [00:17<00:03, 23.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453783374.png: 828->828\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453783374.png: 828 matches @ 44th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453720741.png: 1170->1170\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453720741.png: 1170 matches @ 45th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453740954.png: 922->922\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453740954.png: 922 matches @ 46th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 541/620 [00:18<00:03, 21.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453739354.png: 182->182\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453739354.png: 182 matches @ 47th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 544/620 [00:18<00:03, 19.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453759963.png: 883->883\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453759963.png: 883 matches @ 48th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 375->375\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 375 matches @ 49th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 550/620 [00:18<00:03, 19.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453740954.png: 187->187\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453740954.png: 187 matches @ 50th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453739354.png: 196->196\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453739354.png: 196 matches @ 51th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 556/620 [00:18<00:03, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 481->481\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 481 matches @ 52th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 562/620 [00:19<00:02, 22.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453790978.png-stairs_split_2_1710453745156.png: 495->495\n",
      "aliked> stairs_split_2_1710453790978.png-stairs_split_2_1710453745156.png: 495 matches @ 53th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 574/620 [00:19<00:01, 23.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 440->440\n",
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 440 matches @ 54th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 580/620 [00:20<00:02, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 937->937\n",
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 937 matches @ 55th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 139->139\n",
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 139 matches @ 56th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453728949.png-stairs_split_2_1710453725143.png: 110->110\n",
      "aliked> stairs_split_2_1710453728949.png-stairs_split_2_1710453725143.png: 110 matches @ 57th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 587/620 [00:20<00:01, 21.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453728949.png-stairs_split_2_1710453774370.png: 110->110\n",
      "aliked> stairs_split_2_1710453728949.png-stairs_split_2_1710453774370.png: 110 matches @ 58th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453740954.png-stairs_split_2_1710453739354.png: 833->833\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453739354.png: 833 matches @ 59th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 593/620 [00:20<00:01, 20.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 727->727\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 727 matches @ 60th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 134->134\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 134 matches @ 61th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 605/620 [00:21<00:00, 22.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 154->154\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 154 matches @ 62th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 614/620 [00:21<00:00, 23.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453756762.png-stairs_split_2_1710453736752.png: 140->140\n",
      "aliked> stairs_split_2_1710453756762.png-stairs_split_2_1710453736752.png: 140 matches @ 63th pair(aliked+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 193->193\n",
      "aliked> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 193 matches @ 64th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 620/620 [00:21<00:00, 28.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 295->295\n",
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 295 matches @ 65th pair(aliked+lightglue)\n",
      "Features matched in  27.5128 sec (aliked+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 10/620 [00:01<01:01,  9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453963274.png-stairs_split_1_1710453626698.png: 185->185\n",
      "disk> stairs_split_1_1710453963274.png-stairs_split_1_1710453626698.png: 185 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 14/620 [00:01<01:00,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453963274.png-stairs_split_1_1710453643106.png: 665->665\n",
      "disk> stairs_split_1_1710453963274.png-stairs_split_1_1710453643106.png: 665 matches @ 2th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 23/620 [00:02<01:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453930259.png: 110->110\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453930259.png: 110 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 25/620 [00:02<01:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453704934.png: 133->133\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453704934.png: 133 matches @ 4th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453675921.png: 297->297\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453675921.png: 297 matches @ 5th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 28/620 [00:02<00:59,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 220->220\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 220 matches @ 6th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 31/620 [00:03<00:59,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453601885.png: 143->143\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453601885.png: 143 matches @ 7th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 37/620 [00:03<00:59,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453651110.png: 436->436\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453651110.png: 436 matches @ 8th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_1_1710453659313.png: 162->162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 41/620 [00:04<00:58,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453659313.png: 162 matches @ 9th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 50/620 [00:05<00:57,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453947066.png-stairs_split_2_1710453725143.png: 119->119\n",
      "disk> stairs_split_1_1710453947066.png-stairs_split_2_1710453725143.png: 119 matches @ 10th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 55/620 [00:05<00:57,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453985484.png-stairs_split_1_1710453606287.png: 504->504\n",
      "disk> stairs_split_1_1710453985484.png-stairs_split_1_1710453606287.png: 504 matches @ 11th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 63/620 [00:06<00:56,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453985484.png-stairs_split_1_1710453612890.png: 797->797\n",
      "disk> stairs_split_1_1710453985484.png-stairs_split_1_1710453612890.png: 797 matches @ 12th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 73/620 [00:07<00:56,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_1_1710453675921.png: 378->378\n",
      "disk> stairs_split_1_1710453930259.png-stairs_split_1_1710453675921.png: 378 matches @ 13th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 75/620 [00:07<00:55,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_1_1710453668718.png: 798->798\n",
      "disk> stairs_split_1_1710453930259.png-stairs_split_1_1710453668718.png: 798 matches @ 14th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_1_1710453601885.png: 288->288\n",
      "disk> stairs_split_1_1710453930259.png-stairs_split_1_1710453601885.png: 288 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 79/620 [00:08<00:55,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_1_1710453651110.png: 668->668\n",
      "disk> stairs_split_1_1710453930259.png-stairs_split_1_1710453651110.png: 668 matches @ 16th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 84/620 [00:08<00:55,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 117->117\n",
      "disk> stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 117 matches @ 17th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 90/620 [00:09<00:54,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_1_1710453704934.png: 922->922\n",
      "disk> stairs_split_1_1710453901046.png-stairs_split_1_1710453704934.png: 922 matches @ 18th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 113/620 [00:11<00:52,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 157->157\n",
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 157 matches @ 19th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453790978.png: 118->118\n",
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453790978.png: 118 matches @ 20th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 119/620 [00:12<00:52,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 274->274\n",
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 274 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 125/620 [00:12<00:51,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_1_1710453704934.png: 126->126\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_1_1710453704934.png: 126 matches @ 22th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 127/620 [00:12<00:51,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_1_1710453601885.png: 103->103\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_1_1710453601885.png: 103 matches @ 23th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|       | 132/620 [00:13<00:50,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_2_1710453871430.png: 134->134\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453871430.png: 134 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 134/620 [00:13<00:50,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_2_1710453786375.png: 179->179\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453786375.png: 179 matches @ 25th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 137/620 [00:14<00:49,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_2_1710453740954.png: 267->267\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453740954.png: 267 matches @ 26th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 314->314\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 314 matches @ 27th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 140/620 [00:14<00:49,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_2_1710453756762.png: 121->121\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453756762.png: 121 matches @ 28th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 142/620 [00:14<00:49,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453693529.png-stairs_split_2_1710453759963.png: 210->210\n",
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453759963.png: 210 matches @ 29th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 152/620 [00:15<00:48,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 361->361\n",
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 361 matches @ 30th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453689727.png-stairs_split_2_1710453801783.png: 181->181\n",
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453801783.png: 181 matches @ 31th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 156/620 [00:15<00:48,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453689727.png-stairs_split_2_1710453783374.png: 147->147\n",
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453783374.png: 147 matches @ 32th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 168/620 [00:17<00:46,  9.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_1_1710453675921.png: 563->563\n",
      "disk> stairs_split_1_1710453704934.png-stairs_split_1_1710453675921.png: 563 matches @ 33th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_1_1710453990286.png: 128->128\n",
      "disk> stairs_split_1_1710453704934.png-stairs_split_1_1710453990286.png: 128 matches @ 34th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 179/620 [00:18<00:45,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 595->595\n",
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 595 matches @ 35th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 181/620 [00:18<00:45,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453720741.png: 135->135\n",
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453720741.png: 135 matches @ 36th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 184/620 [00:18<00:45,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 570->570\n",
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 570 matches @ 37th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 186/620 [00:19<00:45,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453704934.png-stairs_split_2_1710453725143.png: 126->126\n",
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453725143.png: 126 matches @ 38th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 189/620 [00:19<00:44,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453675921.png-stairs_split_1_1710453990286.png: 129->129\n",
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453990286.png: 129 matches @ 39th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 197/620 [00:20<00:44,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453675921.png-stairs_split_2_1710453862225.png: 126->126\n",
      "disk> stairs_split_1_1710453675921.png-stairs_split_2_1710453862225.png: 126 matches @ 40th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 212/620 [00:21<00:43,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 331->331\n",
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 331 matches @ 41th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 229/620 [00:23<00:41,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453990286.png-stairs_split_1_1710453601885.png: 789->789\n",
      "disk> stairs_split_1_1710453990286.png-stairs_split_1_1710453601885.png: 789 matches @ 42th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 234/620 [00:24<00:40,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453990286.png-stairs_split_1_1710453651110.png: 139->139\n",
      "disk> stairs_split_1_1710453990286.png-stairs_split_1_1710453651110.png: 139 matches @ 43th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 243/620 [00:25<00:39,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453990286.png-stairs_split_2_1710453745156.png: 114->114\n",
      "disk> stairs_split_1_1710453990286.png-stairs_split_2_1710453745156.png: 114 matches @ 44th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 283/620 [00:29<00:36,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 687->687\n",
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 687 matches @ 45th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 285/620 [00:29<00:35,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 635->635\n",
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 635 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 287/620 [00:29<00:35,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453668718.png-stairs_split_2_1710453720741.png: 260->260\n",
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453720741.png: 260 matches @ 47th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 296/620 [00:30<00:34,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_1_1710453576271.png: 550->550\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453576271.png: 550 matches @ 48th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 298/620 [00:30<00:34,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_1_1710453651110.png: 151->151\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453651110.png: 151 matches @ 49th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 301/620 [00:31<00:34,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_2_1710453871430.png: 101->101\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453871430.png: 101 matches @ 50th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 303/620 [00:31<00:34,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 332->332\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 332 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 306/620 [00:31<00:33,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 393->393\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 393 matches @ 52th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 338->338\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 338 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 312/620 [00:32<00:32,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453601885.png-stairs_split_2_1710453805788.png: 362->362\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453805788.png: 362 matches @ 54th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 316/620 [00:32<00:32,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_1_1710453651110.png: 119->119\n",
      "disk> stairs_split_1_1710453955270.png-stairs_split_1_1710453651110.png: 119 matches @ 55th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 321/620 [00:33<00:32,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 158->158\n",
      "disk> stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 158 matches @ 56th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 336/620 [00:35<00:30,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453616892.png-stairs_split_1_1710453612890.png: 274->274\n",
      "disk> stairs_split_1_1710453616892.png-stairs_split_1_1710453612890.png: 274 matches @ 57th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 487->487\n",
      "disk> stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 487 matches @ 58th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 366/620 [00:38<00:27,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453626698.png-stairs_split_1_1710453620694.png: 132->132\n",
      "disk> stairs_split_1_1710453626698.png-stairs_split_1_1710453620694.png: 132 matches @ 59th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 385/620 [00:40<00:25,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453576271.png-stairs_split_1_1710453651110.png: 160->160\n",
      "disk> stairs_split_1_1710453576271.png-stairs_split_1_1710453651110.png: 160 matches @ 60th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 389/620 [00:40<00:25,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453576271.png-stairs_split_2_1710453871430.png: 202->202\n",
      "disk> stairs_split_1_1710453576271.png-stairs_split_2_1710453871430.png: 202 matches @ 61th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 421/620 [00:44<00:21,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453697531.png-stairs_split_2_1710453759963.png: 222->222\n",
      "disk> stairs_split_1_1710453697531.png-stairs_split_2_1710453759963.png: 222 matches @ 62th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 423/620 [00:44<00:21,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_1_1710453620694.png: 172->172\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453620694.png: 172 matches @ 63th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 426/620 [00:44<00:21,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453871430.png: 155->155\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453871430.png: 155 matches @ 64th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 429/620 [00:45<00:21,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 590->590\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 590 matches @ 65th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453783374.png: 148->148\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453783374.png: 148 matches @ 66th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 433/620 [00:45<00:20,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453720741.png: 199->199\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453720741.png: 199 matches @ 67th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 250->250\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 250 matches @ 68th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 436/620 [00:45<00:20,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 345->345\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 345 matches @ 69th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 440/620 [00:46<00:19,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 289->289\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 289 matches @ 70th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 386->386\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 386 matches @ 71th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 483/620 [00:51<00:15,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453862225.png-stairs_split_2_1710453745156.png: 142->142\n",
      "disk> stairs_split_2_1710453862225.png-stairs_split_2_1710453745156.png: 142 matches @ 72th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 487/620 [00:51<00:14,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453801783.png: 145->145\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453801783.png: 145 matches @ 73th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453786375.png: 176->176\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453786375.png: 176 matches @ 74th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 489/620 [00:51<00:14,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453783374.png: 1174->1174\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453783374.png: 1174 matches @ 75th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 491/620 [00:52<00:14,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453720741.png: 490->490\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453720741.png: 490 matches @ 76th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453740954.png: 151->151\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453740954.png: 151 matches @ 77th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 493/620 [00:52<00:14,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453739354.png: 1477->1477\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453739354.png: 1477 matches @ 78th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 496/620 [00:52<00:13,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453736752.png: 567->567\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453736752.png: 567 matches @ 79th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453759963.png: 151->151\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453759963.png: 151 matches @ 80th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 498/620 [00:52<00:13,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453871430.png-stairs_split_2_1710453805788.png: 608->608\n",
      "disk> stairs_split_2_1710453871430.png-stairs_split_2_1710453805788.png: 608 matches @ 81th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 502/620 [00:53<00:13,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453801783.png-stairs_split_2_1710453783374.png: 172->172\n",
      "disk> stairs_split_2_1710453801783.png-stairs_split_2_1710453783374.png: 172 matches @ 82th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 510/620 [00:54<00:12,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453801783.png-stairs_split_2_1710453736752.png: 426->426\n",
      "disk> stairs_split_2_1710453801783.png-stairs_split_2_1710453736752.png: 426 matches @ 83th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 525/620 [00:55<00:10,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453793579.png-stairs_split_2_1710453790978.png: 139->139\n",
      "disk> stairs_split_2_1710453793579.png-stairs_split_2_1710453790978.png: 139 matches @ 84th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 536/620 [00:57<00:09,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453783374.png: 909->909\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453783374.png: 909 matches @ 85th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 251->251\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 251 matches @ 86th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 538/620 [00:57<00:09,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453720741.png: 1371->1371\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453720741.png: 1371 matches @ 87th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453740954.png: 1088->1088\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453740954.png: 1088 matches @ 88th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 540/620 [00:57<00:09,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453739354.png: 366->366\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453739354.png: 366 matches @ 89th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453745156.png: 103->103\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453745156.png: 103 matches @ 90th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 542/620 [00:57<00:08,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453756762.png: 120->120\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453756762.png: 120 matches @ 91th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 544/620 [00:58<00:08,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453759963.png: 911->911\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453759963.png: 911 matches @ 92th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 685->685\n",
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 685 matches @ 93th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 549/620 [00:58<00:08,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453739354.png: 424->424\n",
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453739354.png: 424 matches @ 94th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 552/620 [00:58<00:07,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453736752.png: 533->533\n",
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453736752.png: 533 matches @ 95th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453759963.png: 109->109\n",
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453759963.png: 109 matches @ 96th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 554/620 [00:59<00:07,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 1410->1410\n",
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 1410 matches @ 97th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 556/620 [00:59<00:07,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453790978.png-stairs_split_2_1710453720741.png: 130->130\n",
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453720741.png: 130 matches @ 98th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 560/620 [00:59<00:06,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453790978.png-stairs_split_2_1710453745156.png: 1131->1131\n",
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453745156.png: 1131 matches @ 99th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 562/620 [01:00<00:06,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453790978.png-stairs_split_2_1710453725143.png: 189->189\n",
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453725143.png: 189 matches @ 100th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 565/620 [01:00<00:06,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453790978.png-stairs_split_2_1710453805788.png: 111->111\n",
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453805788.png: 111 matches @ 101th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453790978.png-stairs_split_2_1710453765165.png: 137->137\n",
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453765165.png: 137 matches @ 102th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 575/620 [01:01<00:05,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 1233->1233\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 1233 matches @ 103th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 577/620 [01:01<00:04,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453739354.png: 228->228\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453739354.png: 228 matches @ 104th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453745156.png: 108->108\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453745156.png: 108 matches @ 105th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 579/620 [01:02<00:04,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 222->222\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 222 matches @ 106th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 875->875\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 875 matches @ 107th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 581/620 [01:02<00:04,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 731->731\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 731 matches @ 108th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 587/620 [01:02<00:03,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453740954.png-stairs_split_2_1710453739354.png: 1106->1106\n",
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453739354.png: 1106 matches @ 109th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 591/620 [01:03<00:03,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 1383->1383\n",
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 1383 matches @ 110th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 376->376\n",
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 376 matches @ 111th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 599/620 [01:04<00:02,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453739354.png-stairs_split_2_1710453745156.png: 101->101\n",
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453745156.png: 101 matches @ 112th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 602/620 [01:04<00:02,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 887->887\n",
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 887 matches @ 113th pair(disk+lightglue)\n",
      "match after conf threshold: stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 846->846\n",
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 846 matches @ 114th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 605/620 [01:05<00:01,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453745156.png-stairs_split_2_1710453725143.png: 190->190\n",
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453725143.png: 190 matches @ 115th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 608/620 [01:05<00:01,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453745156.png-stairs_split_2_1710453759963.png: 143->143\n",
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453759963.png: 143 matches @ 116th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 611/620 [01:05<00:01,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453756762.png-stairs_split_2_1710453736752.png: 467->467\n",
      "disk> stairs_split_2_1710453756762.png-stairs_split_2_1710453736752.png: 467 matches @ 117th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 615/620 [01:06<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 306->306\n",
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 306 matches @ 118th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 617/620 [01:06<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453736752.png-stairs_split_2_1710453805788.png: 139->139\n",
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453805788.png: 139 matches @ 119th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 620/620 [01:06<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match after conf threshold: stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 414->414\n",
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 414 matches @ 120th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  80.4632 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='620' class='' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [620/620 01:38&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_FundamentalMatrix: 185 matches --> 24 matches\n",
      "stairs_split_1_1710453963274.png-stairs_split_1_1710453626698.png: 185 --> 24 matches\n",
      "filter_FundamentalMatrix: 798 matches --> 106 matches\n",
      "stairs_split_1_1710453963274.png-stairs_split_1_1710453643106.png: 798 --> 106 matches\n",
      "skipped key1=stairs_split_1_1710453947066.png, key2=stairs_split_1_1710453930259.png: mkpts.shape=(14, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453947066.png, key2=stairs_split_1_1710453704934.png: mkpts.shape=(13, 4) after filtered.\n",
      "filter_FundamentalMatrix: 297 matches --> 18 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453675921.png: 297 --> 18 matches\n",
      "filter_FundamentalMatrix: 444 matches --> 39 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 444 --> 39 matches\n",
      "filter_FundamentalMatrix: 143 matches --> 17 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453601885.png: 143 --> 17 matches\n",
      "filter_FundamentalMatrix: 436 matches --> 39 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453651110.png: 436 --> 39 matches\n",
      "filter_FundamentalMatrix: 380 matches --> 58 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453659313.png: 380 --> 58 matches\n",
      "filter_FundamentalMatrix: 222 matches --> 18 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_2_1710453725143.png: 222 --> 18 matches\n",
      "filter_FundamentalMatrix: 647 matches --> 54 matches\n",
      "stairs_split_1_1710453985484.png-stairs_split_1_1710453606287.png: 647 --> 54 matches\n",
      "filter_FundamentalMatrix: 951 matches --> 111 matches\n",
      "stairs_split_1_1710453985484.png-stairs_split_1_1710453612890.png: 951 --> 111 matches\n",
      "filter_FundamentalMatrix: 378 matches --> 19 matches\n",
      "stairs_split_1_1710453930259.png-stairs_split_1_1710453675921.png: 378 --> 19 matches\n",
      "filter_FundamentalMatrix: 935 matches --> 45 matches\n",
      "stairs_split_1_1710453930259.png-stairs_split_1_1710453668718.png: 935 --> 45 matches\n",
      "filter_FundamentalMatrix: 288 matches --> 33 matches\n",
      "stairs_split_1_1710453930259.png-stairs_split_1_1710453601885.png: 288 --> 33 matches\n",
      "filter_FundamentalMatrix: 976 matches --> 63 matches\n",
      "stairs_split_1_1710453930259.png-stairs_split_1_1710453651110.png: 976 --> 63 matches\n",
      "filter_FundamentalMatrix: 280 matches --> 44 matches\n",
      "stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 280 --> 44 matches\n",
      "filter_FundamentalMatrix: 1235 matches --> 91 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_1_1710453704934.png: 1235 --> 91 matches\n",
      "skipped key1=stairs_split_1_1710453901046.png, key2=stairs_split_2_1710453862225.png: mkpts.shape=(9, 4) after filtered.\n",
      "filter_FundamentalMatrix: 133 matches --> 47 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453798181.png: 133 --> 47 matches\n",
      "filter_FundamentalMatrix: 331 matches --> 27 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 331 --> 27 matches\n",
      "skipped key1=stairs_split_1_1710453901046.png, key2=stairs_split_2_1710453790978.png: mkpts.shape=(12, 4) after filtered.\n",
      "filter_FundamentalMatrix: 436 matches --> 20 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 436 --> 20 matches\n",
      "skipped key1=stairs_split_1_1710453693529.png, key2=stairs_split_1_1710453704934.png: mkpts.shape=(7, 4) after filtered.\n",
      "filter_FundamentalMatrix: 227 matches --> 16 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_1_1710453601885.png: 227 --> 16 matches\n",
      "skipped key1=stairs_split_1_1710453693529.png, key2=stairs_split_2_1710453871430.png: mkpts.shape=(5, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453693529.png, key2=stairs_split_2_1710453786375.png: mkpts.shape=(14, 4) after filtered.\n",
      "filter_FundamentalMatrix: 267 matches --> 22 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453740954.png: 267 --> 22 matches\n",
      "filter_FundamentalMatrix: 314 matches --> 18 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 314 --> 18 matches\n",
      "skipped key1=stairs_split_1_1710453693529.png, key2=stairs_split_2_1710453756762.png: mkpts.shape=(13, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453693529.png, key2=stairs_split_2_1710453759963.png: mkpts.shape=(13, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453689727.png, key2=stairs_split_2_1710453871430.png: mkpts.shape=(13, 4) after filtered.\n",
      "filter_FundamentalMatrix: 181 matches --> 21 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453801783.png: 181 --> 21 matches\n",
      "skipped key1=stairs_split_1_1710453689727.png, key2=stairs_split_2_1710453783374.png: mkpts.shape=(8, 4) after filtered.\n",
      "filter_FundamentalMatrix: 197 matches --> 17 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453736752.png: 197 --> 17 matches\n",
      "filter_FundamentalMatrix: 563 matches --> 56 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_1_1710453675921.png: 563 --> 56 matches\n",
      "skipped key1=stairs_split_1_1710453704934.png, key2=stairs_split_1_1710453990286.png: mkpts.shape=(4, 4) after filtered.\n",
      "filter_FundamentalMatrix: 122 matches --> 41 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453798181.png: 122 --> 41 matches\n",
      "filter_FundamentalMatrix: 928 matches --> 80 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 928 --> 80 matches\n",
      "skipped key1=stairs_split_1_1710453704934.png, key2=stairs_split_2_1710453720741.png: mkpts.shape=(11, 4) after filtered.\n",
      "filter_FundamentalMatrix: 1005 matches --> 109 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 1005 --> 109 matches\n",
      "skipped key1=stairs_split_1_1710453704934.png, key2=stairs_split_2_1710453725143.png: mkpts.shape=(7, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453675921.png, key2=stairs_split_1_1710453990286.png: mkpts.shape=(14, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453675921.png, key2=stairs_split_2_1710453862225.png: mkpts.shape=(10, 4) after filtered.\n",
      "filter_FundamentalMatrix: 504 matches --> 121 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 504 --> 121 matches\n",
      "filter_FundamentalMatrix: 119 matches --> 57 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453659313.png: 119 --> 57 matches\n",
      "filter_FundamentalMatrix: 789 matches --> 77 matches\n",
      "stairs_split_1_1710453990286.png-stairs_split_1_1710453601885.png: 789 --> 77 matches\n",
      "filter_FundamentalMatrix: 139 matches --> 18 matches\n",
      "stairs_split_1_1710453990286.png-stairs_split_1_1710453651110.png: 139 --> 18 matches\n",
      "skipped key1=stairs_split_1_1710453990286.png, key2=stairs_split_2_1710453745156.png: mkpts.shape=(8, 4) after filtered.\n",
      "filter_FundamentalMatrix: 103 matches --> 32 matches\n",
      "stairs_split_1_1710453663515.png-stairs_split_1_1710453612890.png: 103 --> 32 matches\n",
      "filter_FundamentalMatrix: 264 matches --> 63 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_1_1710453651110.png: 264 --> 63 matches\n",
      "filter_FundamentalMatrix: 687 matches --> 54 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 687 --> 54 matches\n",
      "filter_FundamentalMatrix: 635 matches --> 40 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 635 --> 40 matches\n",
      "filter_FundamentalMatrix: 260 matches --> 17 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453720741.png: 260 --> 17 matches\n",
      "filter_FundamentalMatrix: 245 matches --> 86 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453740954.png: 245 --> 86 matches\n",
      "filter_FundamentalMatrix: 853 matches --> 83 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453576271.png: 853 --> 83 matches\n",
      "skipped key1=stairs_split_1_1710453601885.png, key2=stairs_split_1_1710453651110.png: mkpts.shape=(7, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453601885.png, key2=stairs_split_2_1710453871430.png: mkpts.shape=(6, 4) after filtered.\n",
      "filter_FundamentalMatrix: 332 matches --> 26 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 332 --> 26 matches\n",
      "filter_FundamentalMatrix: 393 matches --> 41 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 393 --> 41 matches\n",
      "filter_FundamentalMatrix: 338 matches --> 27 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 338 --> 27 matches\n",
      "filter_FundamentalMatrix: 137 matches --> 32 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453736752.png: 137 --> 32 matches\n",
      "filter_FundamentalMatrix: 362 matches --> 35 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453805788.png: 362 --> 35 matches\n",
      "filter_FundamentalMatrix: 386 matches --> 48 matches\n",
      "stairs_split_1_1710453955270.png-stairs_split_1_1710453651110.png: 386 --> 48 matches\n",
      "filter_FundamentalMatrix: 296 matches --> 37 matches\n",
      "stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 296 --> 37 matches\n",
      "skipped key1=stairs_split_1_1710453955270.png, key2=stairs_split_2_1710453786375.png: mkpts.shape=(10, 4) after filtered.\n",
      "filter_FundamentalMatrix: 118 matches --> 17 matches\n",
      "stairs_split_1_1710453955270.png-stairs_split_2_1710453720741.png: 118 --> 17 matches\n",
      "skipped key1=stairs_split_1_1710453955270.png, key2=stairs_split_2_1710453736752.png: mkpts.shape=(9, 4) after filtered.\n",
      "filter_FundamentalMatrix: 410 matches --> 31 matches\n",
      "stairs_split_1_1710453616892.png-stairs_split_1_1710453612890.png: 410 --> 31 matches\n",
      "filter_FundamentalMatrix: 487 matches --> 22 matches\n",
      "stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 487 --> 22 matches\n",
      "skipped key1=stairs_split_1_1710453626698.png, key2=stairs_split_1_1710453620694.png: mkpts.shape=(14, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453576271.png, key2=stairs_split_1_1710453651110.png: mkpts.shape=(12, 4) after filtered.\n",
      "skipped key1=stairs_split_1_1710453576271.png, key2=stairs_split_2_1710453871430.png: mkpts.shape=(13, 4) after filtered.\n",
      "filter_FundamentalMatrix: 222 matches --> 23 matches\n",
      "stairs_split_1_1710453697531.png-stairs_split_2_1710453759963.png: 222 --> 23 matches\n",
      "filter_FundamentalMatrix: 172 matches --> 15 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453620694.png: 172 --> 15 matches\n",
      "skipped key1=stairs_split_1_1710453651110.png, key2=stairs_split_2_1710453871430.png: mkpts.shape=(14, 4) after filtered.\n",
      "filter_FundamentalMatrix: 590 matches --> 43 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 590 --> 43 matches\n",
      "skipped key1=stairs_split_1_1710453651110.png, key2=stairs_split_2_1710453783374.png: mkpts.shape=(13, 4) after filtered.\n",
      "filter_FundamentalMatrix: 199 matches --> 19 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453720741.png: 199 --> 19 matches\n",
      "skipped key1=stairs_split_1_1710453651110.png, key2=stairs_split_2_1710453740954.png: mkpts.shape=(8, 4) after filtered.\n",
      "filter_FundamentalMatrix: 345 matches --> 22 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 345 --> 22 matches\n",
      "filter_FundamentalMatrix: 289 matches --> 21 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 289 --> 21 matches\n",
      "filter_FundamentalMatrix: 529 matches --> 22 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 529 --> 22 matches\n",
      "skipped key1=stairs_split_2_1710453862225.png, key2=stairs_split_2_1710453745156.png: mkpts.shape=(11, 4) after filtered.\n",
      "filter_FundamentalMatrix: 249 matches --> 18 matches\n",
      "stairs_split_2_1710453871430.png-stairs_split_2_1710453801783.png: 249 --> 18 matches\n",
      "skipped key1=stairs_split_2_1710453871430.png, key2=stairs_split_2_1710453786375.png: mkpts.shape=(13, 4) after filtered.\n",
      "filter_FundamentalMatrix: 1440 matches --> 92 matches\n",
      "stairs_split_2_1710453871430.png-stairs_split_2_1710453783374.png: 1440 --> 92 matches\n",
      "filter_FundamentalMatrix: 592 matches --> 41 matches\n",
      "stairs_split_2_1710453871430.png-stairs_split_2_1710453720741.png: 592 --> 41 matches\n",
      "skipped key1=stairs_split_2_1710453871430.png, key2=stairs_split_2_1710453740954.png: mkpts.shape=(9, 4) after filtered.\n",
      "filter_FundamentalMatrix: 2130 matches --> 191 matches\n",
      "stairs_split_2_1710453871430.png-stairs_split_2_1710453739354.png: 2130 --> 191 matches\n",
      "skipped key1=stairs_split_2_1710453871430.png, key2=stairs_split_2_1710453756762.png: mkpts.shape=(5, 4) after filtered.\n",
      "filter_FundamentalMatrix: 1006 matches --> 128 matches\n",
      "stairs_split_2_1710453871430.png-stairs_split_2_1710453736752.png: 1006 --> 128 matches\n",
      "skipped key1=stairs_split_2_1710453871430.png, key2=stairs_split_2_1710453759963.png: mkpts.shape=(10, 4) after filtered.\n",
      "filter_FundamentalMatrix: 608 matches --> 27 matches\n",
      "stairs_split_2_1710453871430.png-stairs_split_2_1710453805788.png: 608 --> 27 matches\n",
      "skipped key1=stairs_split_2_1710453801783.png, key2=stairs_split_2_1710453783374.png: mkpts.shape=(9, 4) after filtered.\n",
      "filter_FundamentalMatrix: 426 matches --> 38 matches\n",
      "stairs_split_2_1710453801783.png-stairs_split_2_1710453736752.png: 426 --> 38 matches\n",
      "filter_FundamentalMatrix: 126 matches --> 17 matches\n",
      "stairs_split_2_1710453798181.png-stairs_split_2_1710453753160.png: 126 --> 17 matches\n",
      "filter_FundamentalMatrix: 290 matches --> 24 matches\n",
      "stairs_split_2_1710453793579.png-stairs_split_2_1710453790978.png: 290 --> 24 matches\n",
      "filter_FundamentalMatrix: 1737 matches --> 268 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453783374.png: 1737 --> 268 matches\n",
      "filter_FundamentalMatrix: 251 matches --> 15 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 251 --> 15 matches\n",
      "filter_FundamentalMatrix: 2541 matches --> 183 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453720741.png: 2541 --> 183 matches\n",
      "filter_FundamentalMatrix: 2010 matches --> 169 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453740954.png: 2010 --> 169 matches\n",
      "filter_FundamentalMatrix: 548 matches --> 49 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453739354.png: 548 --> 49 matches\n",
      "skipped key1=stairs_split_2_1710453786375.png, key2=stairs_split_2_1710453745156.png: mkpts.shape=(13, 4) after filtered.\n",
      "skipped key1=stairs_split_2_1710453786375.png, key2=stairs_split_2_1710453756762.png: mkpts.shape=(7, 4) after filtered.\n",
      "filter_FundamentalMatrix: 1794 matches --> 156 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453759963.png: 1794 --> 156 matches\n",
      "filter_FundamentalMatrix: 1060 matches --> 121 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 1060 --> 121 matches\n",
      "filter_FundamentalMatrix: 187 matches --> 36 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453740954.png: 187 --> 36 matches\n",
      "filter_FundamentalMatrix: 620 matches --> 61 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453739354.png: 620 --> 61 matches\n",
      "filter_FundamentalMatrix: 533 matches --> 35 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453736752.png: 533 --> 35 matches\n",
      "skipped key1=stairs_split_2_1710453783374.png, key2=stairs_split_2_1710453759963.png: mkpts.shape=(8, 4) after filtered.\n",
      "filter_FundamentalMatrix: 1891 matches --> 226 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 1891 --> 226 matches\n",
      "skipped key1=stairs_split_2_1710453790978.png, key2=stairs_split_2_1710453720741.png: mkpts.shape=(14, 4) after filtered.\n",
      "filter_FundamentalMatrix: 1626 matches --> 173 matches\n",
      "stairs_split_2_1710453790978.png-stairs_split_2_1710453745156.png: 1626 --> 173 matches\n",
      "filter_FundamentalMatrix: 189 matches --> 17 matches\n",
      "stairs_split_2_1710453790978.png-stairs_split_2_1710453725143.png: 189 --> 17 matches\n",
      "skipped key1=stairs_split_2_1710453790978.png, key2=stairs_split_2_1710453805788.png: mkpts.shape=(10, 4) after filtered.\n",
      "filter_FundamentalMatrix: 137 matches --> 17 matches\n",
      "stairs_split_2_1710453790978.png-stairs_split_2_1710453765165.png: 137 --> 17 matches\n",
      "filter_FundamentalMatrix: 1673 matches --> 122 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 1673 --> 122 matches\n",
      "skipped key1=stairs_split_2_1710453720741.png, key2=stairs_split_2_1710453739354.png: mkpts.shape=(14, 4) after filtered.\n",
      "skipped key1=stairs_split_2_1710453720741.png, key2=stairs_split_2_1710453745156.png: mkpts.shape=(14, 4) after filtered.\n",
      "filter_FundamentalMatrix: 222 matches --> 21 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 222 --> 21 matches\n",
      "filter_FundamentalMatrix: 1812 matches --> 84 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 1812 --> 84 matches\n",
      "filter_FundamentalMatrix: 870 matches --> 53 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 870 --> 53 matches\n",
      "filter_FundamentalMatrix: 110 matches --> 62 matches\n",
      "stairs_split_2_1710453728949.png-stairs_split_2_1710453725143.png: 110 --> 62 matches\n",
      "filter_FundamentalMatrix: 110 matches --> 27 matches\n",
      "stairs_split_2_1710453728949.png-stairs_split_2_1710453774370.png: 110 --> 27 matches\n",
      "filter_FundamentalMatrix: 1939 matches --> 162 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453739354.png: 1939 --> 162 matches\n",
      "filter_FundamentalMatrix: 2110 matches --> 147 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 2110 --> 147 matches\n",
      "filter_FundamentalMatrix: 510 matches --> 21 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 510 --> 21 matches\n",
      "skipped key1=stairs_split_2_1710453739354.png, key2=stairs_split_2_1710453745156.png: mkpts.shape=(8, 4) after filtered.\n",
      "filter_FundamentalMatrix: 887 matches --> 67 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 887 --> 67 matches\n",
      "filter_FundamentalMatrix: 1000 matches --> 88 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 1000 --> 88 matches\n",
      "filter_FundamentalMatrix: 190 matches --> 15 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453725143.png: 190 --> 15 matches\n",
      "skipped key1=stairs_split_2_1710453745156.png, key2=stairs_split_2_1710453759963.png: mkpts.shape=(8, 4) after filtered.\n",
      "filter_FundamentalMatrix: 607 matches --> 48 matches\n",
      "stairs_split_2_1710453756762.png-stairs_split_2_1710453736752.png: 607 --> 48 matches\n",
      "filter_FundamentalMatrix: 499 matches --> 35 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 499 --> 35 matches\n",
      "filter_FundamentalMatrix: 139 matches --> 15 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453805788.png: 139 --> 15 matches\n",
      "filter_FundamentalMatrix: 709 matches --> 46 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 709 --> 46 matches\n",
      "Ensembled pairs : 95 pairs\n",
      "Local feature extracting and matching. Done in 208.2433 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 44/44 [00:02<00:00, 20.57it/s]\n",
      " 18%|        | 95/528 [00:00<00:00, 4305.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 0.4575 sec\n",
      "{0: Reconstruction(num_reg_images=2, num_cameras=2, num_points3D=118, num_observations=236)}\n",
      "Reconstruction done in 0.5377 sec\n",
      "Dataset  stairs -> Registered 2 / 51 images with 1 clusters\n",
      "\n",
      "Results\n",
      "Dataset  stairs -> Registered 2 / 51 images with 1 clusters\n",
      "\n",
      "Timings\n",
      "rotation_detection -> total=0.00 sec.\n",
      "global feature extraction -> total=0.00 sec.\n",
      "shortlisting -> total=23.76 sec.\n",
      "feature_detection -> total=0.00 sec.\n",
      "feature_matching -> total=107.98 sec.\n",
      "RANSAC -> total=0.46 sec.\n",
      "Reconstruction -> total=0.54 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t# 'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    'rotation_detection':[],\n",
    "    \"global feature extraction\":[],\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "# Load DINOv2 model (for feature extraction, not global descriptor here)\n",
    "print(\"Loading DINOv2 model for patch feature extraction...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = dino_model.eval().to(device)\n",
    "print(\"DINOv2 model loaded.\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    # print (f\"Extracting on device {device}\")\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "        # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "        try:\n",
    "            # --- Pipeline Execution ---\n",
    "            \n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            # if CONFIG.ROTATION_CORRECTION:\n",
    "            #     rots = exec_rotation_detection(images, device)\n",
    "            # else:\n",
    "            #     rots = [ 0 for fname in images ]\n",
    "            rots = [ 0 for fname in images ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print(f'rotation_detection for {len(images)} images : {t:.4f} sec')\n",
    "            # print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            # 1. Detect ALIKED features and combine with DINO patch features\n",
    "            t = time()\n",
    "            index_pairs = get_image_pairs_shortlist(\n",
    "                images,\n",
    "                sim_th = 0.3, # should be strict\n",
    "                min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "                max_pairs = 25,\n",
    "                exhaustive_if_less = 20,\n",
    "                device=device\n",
    "            )\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            # print(\"\\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\")\n",
    "            # detect_aliked_and_combine_with_dino(\n",
    "            #     img_fnames=images,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_features=4096,\n",
    "            #     resize_to=1024,\n",
    "            #     dino_processor=dino_processor,\n",
    "            #     dino_model=dino_model,\n",
    "            #     dino_patch_size=14, # Adjust based on your DINO model's patch size (e.g., 14 for DINOv2 base)\n",
    "            #     device=device\n",
    "            # )\n",
    "            # timings['global feature extraction'].append(time() - t)\n",
    "            # print (f'Gloabl feature extracting. Done in {time() - t:.4f} sec')\n",
    "            # gc.collect()\n",
    "            \n",
    "            # # 2. Get image pairs shortlist using VLAD global descriptors\n",
    "            # print(\"\\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\")\n",
    "            # # Adjust num_clusters_vlad as needed (e.g., 64, 128, 256)\n",
    "            # # Higher clusters mean higher dimensionality for global descriptor.\n",
    "            # index_pairs = get_image_pairs_shortlist_vlad(\n",
    "            #     fnames=images,\n",
    "            #     sim_th=0.5,\n",
    "            #     min_pairs=20,\n",
    "            #     exhaustive_if_less=20,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_clusters_vlad=128, # Example: 128 clusters for VLAD\n",
    "            #     device=device\n",
    "            # )\n",
    "            # index_pairs = get_img_pairs_exhaustive(images)\n",
    "            \n",
    "            print(f\"Generated {len(index_pairs)} image pairs using VLAD global descriptor.\")\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################    \n",
    "            t=time()\n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                images, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "            print (f'Local feature extracting and matching. Done in {time() - t:.4f} sec')\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            future = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                feature_dir, images_dir)\n",
    "            maps, local_timings = future.result()\n",
    "            #  timings\n",
    "            for k in local_timings:\n",
    "                timings[k].extend(local_timings[k])\n",
    "            # clear_output(wait=False)\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():  # cur_map: image_name  {'R': list, 't': list}\n",
    "                for image_name, pose in cur_map.items():\n",
    "                    idx = filename_to_index[image_name]\n",
    "                    pred = predictions[idx]\n",
    "                    pred.cluster_index = map_index\n",
    "                    pred.rotation = np.array(pose['R'])  # convert back to np.ndarray\n",
    "                    pred.translation = np.array(pose['t'])\n",
    "                    registered += 1\n",
    "            mapping_result_str = f\"Dataset  {dataset} -> Registered {registered} / {len(images)} images with {len(maps)} clusters\"\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # raise e\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be989f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:43:32.375175Z",
     "iopub.status.busy": "2025-05-28T20:43:32.374722Z",
     "iopub.status.idle": "2025-05-28T20:43:32.546955Z",
     "shell.execute_reply": "2025-05-28T20:43:32.545859Z"
    },
    "papermill": {
     "duration": 0.232231,
     "end_time": "2025-05-28T20:43:32.548363",
     "exception": false,
     "start_time": "2025-05-28T20:43:32.316132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                #  `rotation` is a list of lists, flatten it\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()  # flatten 3x3 list -> 9 elems\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                #  `translation` is a flat list\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "\n",
    "# Preview the output\n",
    "!head {submission_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93a7ac95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:43:32.708453Z",
     "iopub.status.busy": "2025-05-28T20:43:32.708055Z",
     "iopub.status.idle": "2025-05-28T20:43:32.789431Z",
     "shell.execute_reply": "2025-05-28T20:43:32.788576Z"
    },
    "papermill": {
     "duration": 0.141493,
     "end_time": "2025-05-28T20:43:32.790634",
     "exception": false,
     "start_time": "2025-05-28T20:43:32.649141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "amy_gardens: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "fbk_vineyard: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "ETs: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "stairs: score=0.00% (mAA=0.00%, clusterness=50.00%)\n",
      "Average over all datasets: score=0.00% (mAA=0.00%, clusterness=96.15%)\n",
      "Computed metric in: 0.08 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709aadc",
   "metadata": {
    "papermill": {
     "duration": 0.058769,
     "end_time": "2025-05-28T20:43:32.909501",
     "exception": false,
     "start_time": "2025-05-28T20:43:32.850732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7505602,
     "sourceId": 11938492,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 282.342538,
   "end_time": "2025-05-28T20:43:36.432678",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T20:38:54.090140",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
