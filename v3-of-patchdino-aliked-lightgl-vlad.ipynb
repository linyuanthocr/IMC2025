{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b2fe67",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006808,
     "end_time": "2025-05-29T19:32:53.705746",
     "exception": false,
     "start_time": "2025-05-29T19:32:53.698938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae6fa1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:32:53.718779Z",
     "iopub.status.busy": "2025-05-29T19:32:53.718434Z",
     "iopub.status.idle": "2025-05-29T19:32:59.590885Z",
     "shell.execute_reply": "2025-05-29T19:32:59.589332Z"
    },
    "papermill": {
     "duration": 5.880905,
     "end_time": "2025-05-29T19:32:59.592748",
     "exception": false,
     "start_time": "2025-05-29T19:32:53.711843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0747aef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:32:59.609724Z",
     "iopub.status.busy": "2025-05-29T19:32:59.609362Z",
     "iopub.status.idle": "2025-05-29T19:33:00.921995Z",
     "shell.execute_reply": "2025-05-29T19:33:00.920790Z"
    },
    "papermill": {
     "duration": 1.322964,
     "end_time": "2025-05-29T19:33:00.923650",
     "exception": false,
     "start_time": "2025-05-29T19:32:59.600686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/depth-save.pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d89340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:00.938167Z",
     "iopub.status.busy": "2025-05-29T19:33:00.937903Z",
     "iopub.status.idle": "2025-05-29T19:33:01.992550Z",
     "shell.execute_reply": "2025-05-29T19:33:01.991405Z"
    },
    "papermill": {
     "duration": 1.063275,
     "end_time": "2025-05-29T19:33:01.994080",
     "exception": false,
     "start_time": "2025-05-29T19:33:00.930805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/superpoint-lightglue/superpoint_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/superpoint-lightglue/superpoint_lightglue.pth  /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/superpoint-lightglue/superpoint_v1.pth /root/.cache/torch/hub/checkpoints/superpoint_v1.pth\n",
    "!cp /kaggle/input/superpoint-lightglue/superpoint_v1.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d65a749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:02.008568Z",
     "iopub.status.busy": "2025-05-29T19:33:02.008262Z",
     "iopub.status.idle": "2025-05-29T19:33:02.012867Z",
     "shell.execute_reply": "2025-05-29T19:33:02.011858Z"
    },
    "papermill": {
     "duration": 0.013038,
     "end_time": "2025-05-29T19:33:02.014118",
     "exception": false,
     "start_time": "2025-05-29T19:33:02.001080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/root/.cache/torch/hub/checkpoints/depth-save.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3fcf5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:02.028120Z",
     "iopub.status.busy": "2025-05-29T19:33:02.027890Z",
     "iopub.status.idle": "2025-05-29T19:33:24.581083Z",
     "shell.execute_reply": "2025-05-29T19:33:24.580165Z"
    },
    "papermill": {
     "duration": 22.561929,
     "end_time": "2025-05-29T19:33:24.582774",
     "exception": false,
     "start_time": "2025-05-29T19:33:02.020845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# from lightglue import DISK\n",
    "from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\n",
    "from scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint,DISK, DoGHardNet, LightGlue, SIFT\n",
    "from fastprogress import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bbff0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.598287Z",
     "iopub.status.busy": "2025-05-29T19:33:24.597808Z",
     "iopub.status.idle": "2025-05-29T19:33:24.601185Z",
     "shell.execute_reply": "2025-05-29T19:33:24.600596Z"
    },
    "papermill": {
     "duration": 0.012207,
     "end_time": "2025-05-29T19:33:24.602301",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.590094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d018bf8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.616382Z",
     "iopub.status.busy": "2025-05-29T19:33:24.616153Z",
     "iopub.status.idle": "2025-05-29T19:33:24.726801Z",
     "shell.execute_reply": "2025-05-29T19:33:24.725800Z"
    },
    "papermill": {
     "duration": 0.119106,
     "end_time": "2025-05-29T19:33:24.728095",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.608989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93273b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.742813Z",
     "iopub.status.busy": "2025-05-29T19:33:24.742528Z",
     "iopub.status.idle": "2025-05-29T19:33:24.746674Z",
     "shell.execute_reply": "2025-05-29T19:33:24.745855Z"
    },
    "papermill": {
     "duration": 0.012878,
     "end_time": "2025-05-29T19:33:24.747992",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.735114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f822f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.762227Z",
     "iopub.status.busy": "2025-05-29T19:33:24.762020Z",
     "iopub.status.idle": "2025-05-29T19:33:24.764946Z",
     "shell.execute_reply": "2025-05-29T19:33:24.764346Z"
    },
    "papermill": {
     "duration": 0.011424,
     "end_time": "2025-05-29T19:33:24.766041",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.754617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078852c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.780338Z",
     "iopub.status.busy": "2025-05-29T19:33:24.780129Z",
     "iopub.status.idle": "2025-05-29T19:33:24.787816Z",
     "shell.execute_reply": "2025-05-29T19:33:24.787142Z"
    },
    "papermill": {
     "duration": 0.016194,
     "end_time": "2025-05-29T19:33:24.788989",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.772795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : True,\n",
    "        \"filter_iterations\" : 5,\n",
    "        \"filter_threshold\" : 3,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = True\n",
    "    use_disk_lightglue = True\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":0.2\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 50,\n",
    "        \"resize_to\" : 1024,\n",
    "        \"match_confidence_threshold\":0.2\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":0.2\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba56aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.803168Z",
     "iopub.status.busy": "2025-05-29T19:33:24.802957Z",
     "iopub.status.idle": "2025-05-29T19:33:24.858722Z",
     "shell.execute_reply": "2025-05-29T19:33:24.858043Z"
    },
    "papermill": {
     "duration": 0.064441,
     "end_time": "2025-05-29T19:33:24.860077",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.795636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume these are available from your environment or previous code\n",
    "# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n",
    "# from kornia.feature import ALIKED # Already in your detect_aliked\n",
    "# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n",
    "# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n",
    "# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n",
    "\n",
    "# --- Helper function for image loading (if not already defined) ---\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "def get_dino_patch_features_for_keypoints(img_path, keypoints_xy, dino_processor, dino_model, patch_size=16, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Extracts DINO patch features corresponding to given ALIKED keypoint locations.\n",
    "    It correctly infers the DINO patch grid dimensions from the processed input.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        keypoints_xy (torch.Tensor): Nx2 tensor of (x, y) keypoint coordinates in image pixel space.\n",
    "                                     These keypoints are assumed to be in the original image's coordinate system.\n",
    "        dino_processor: HuggingFace AutoImageProcessor for DINO.\n",
    "        dino_model: HuggingFace AutoModel for DINO.\n",
    "        patch_size (int): The patch size used by the DINO model (e.g., 14 or 16).\n",
    "        device (torch.device): Device to run the models on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: NxD_dino tensor of DINO patch features for each keypoint.\n",
    "                      Returns None if no keypoints or image loading fails.\n",
    "    \"\"\"\n",
    "    if len(keypoints_xy) == 0:\n",
    "        dino_feature_dim = dino_model.config.hidden_size # Get actual DINO hidden size\n",
    "        return torch.empty((0, dino_feature_dim), device=device)\n",
    "\n",
    "    # 1. Load the original image (ALIKED processed this size)\n",
    "    original_img = load_torch_image(img_path, device=device)\n",
    "    original_h, original_w = original_img.shape[-2], original_img.shape[-1]\n",
    "\n",
    "\n",
    "    # 2. Process the image with DINO's processor\n",
    "    #    This step performs resizing, padding, etc., as needed by the DINO model\n",
    "    with torch.inference_mode():\n",
    "        # dino_processor returns a BatchFeature object which includes pixel_values\n",
    "        # and potentially other information like `pixel_mask`\n",
    "        inputs = dino_processor(images=original_img, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "        # Get the actual dimensions of the image as processed by the DINO model\n",
    "        # This is the crucial part: the actual H and W that produced `patch_tokens`\n",
    "        # We can infer this from the `pixel_values` shape\n",
    "        processed_h = inputs['pixel_values'].shape[-2]\n",
    "        processed_w = inputs['pixel_values'].shape[-1]\n",
    "\n",
    "        # Extract patch tokens (excluding the CLS token)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:].squeeze(0) # Shape: (num_patches, hidden_size)\n",
    "\n",
    "        # Calculate the actual grid dimensions based on the *processed* image size\n",
    "        # and the model's patch size.\n",
    "        # This should perfectly match the number of patch_tokens if the model is well-behaved.\n",
    "        num_patches_h = processed_h // patch_size\n",
    "        num_patches_w = processed_w // patch_size\n",
    "\n",
    "        # Safety check: ensure calculated grid matches actual token count\n",
    "        expected_token_count = num_patches_h * num_patches_w\n",
    "        if patch_tokens.shape[0] != expected_token_count:\n",
    "            # This indicates a deeper issue with how the model's output tokens\n",
    "            # map to the spatial grid, or an unexpected patch size/model behavior.\n",
    "            # Some models might have slightly different patch token arrangements.\n",
    "            # DINOv2 typically aligns well.\n",
    "            raise ValueError(\n",
    "                f\"DINO patch token count ({patch_tokens.shape[0]}) does not match \"\n",
    "                f\"expected grid dimensions ({num_patches_h}x{num_patches_w} = {expected_token_count}) \"\n",
    "                f\"for processed image size {processed_w}x{processed_h} with patch size {patch_size}. \"\n",
    "                f\"Please verify DINO model and processor configuration.\"\n",
    "            )\n",
    "\n",
    "        # Reshape patch tokens into a 2D grid\n",
    "        patch_features_grid = patch_tokens.reshape(num_patches_h, num_patches_w, -1)\n",
    "        dino_feature_dim = patch_features_grid.shape[-1] # Actual feature dimension\n",
    "\n",
    "\n",
    "    dino_features_for_kpts = torch.zeros((len(keypoints_xy), dino_feature_dim), device=device)\n",
    "\n",
    "    # 3. Rescale ALIKED keypoints to the DINO *processed* image dimensions\n",
    "    #    ALIKED keypoints are in original_w x original_h coordinates.\n",
    "    #    DINO patches correspond to processed_w x processed_h coordinates.\n",
    "    scale_x = processed_w / original_w\n",
    "    scale_y = processed_h / original_h\n",
    "\n",
    "    scaled_keypoints_xy = keypoints_xy.clone()\n",
    "    scaled_keypoints_xy[:, 0] *= scale_x\n",
    "    scaled_keypoints_xy[:, 1] *= scale_y\n",
    "\n",
    "    # 4. Map scaled keypoints to DINO patch grid indices\n",
    "    keypoint_cols = (scaled_keypoints_xy[:, 0] / patch_size).long()\n",
    "    keypoint_rows = (scaled_keypoints_xy[:, 1] / patch_size).long()\n",
    "\n",
    "    # Clip indices to ensure they are within bounds of the patch grid\n",
    "    keypoint_rows = torch.clamp(keypoint_rows, 0, num_patches_h - 1)\n",
    "    keypoint_cols = torch.clamp(keypoint_cols, 0, num_patches_w - 1)\n",
    "\n",
    "    # Gather DINO features for each keypoint's corresponding patch\n",
    "    dino_features_for_kpts = patch_features_grid[keypoint_rows, keypoint_cols]\n",
    "\n",
    "    return dino_features_for_kpts\n",
    "\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,\n",
    "                  match_confidence_threshold = 0.0,\n",
    "                  verbose=VERBOSE\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    # extractor = extractor_class(max_num_keypoints=num_features, detection_threshold=detection_threshold, \n",
    "    #                             resize=resize_to).eval().to(device, dtype)\n",
    "    # if model_name == 'disk':\n",
    "    #     extractor = DISK(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device).eval()\n",
    "    #     checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    #     extractor.load_state_dict(checkpoint['model'])\n",
    "    # else:\n",
    "    #     extractor_class = dict_model[model_name]\n",
    "    #     extractor = extractor_class(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device, dtype).eval()\n",
    "\n",
    "    extractor_class = dict_model[model_name]\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=detection_threshold,\n",
    "        resize=resize_to\n",
    "    ).to(device, dtype).eval()\n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                            \"filter_threshold\":match_confidence_threshold,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            len1 = len(idxs)\n",
    "            n_matches = len1\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                pass\n",
    "                # if verbose:\n",
    "                #     print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "    match_confidence_threshold = 0.0\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "        match_confidence_threshold = match_confidence_threshold\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t\n",
    "\n",
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    verbose = VERBOSE\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                # if verbose:\n",
    "                #     print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 3, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    if verbose:\n",
    "                        print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "                # print(f\"KKKKKKK KKKKKK {k1} - {k2}: {len(match)} matches\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "852b0c8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.875766Z",
     "iopub.status.busy": "2025-05-29T19:33:24.875480Z",
     "iopub.status.idle": "2025-05-29T19:33:24.878517Z",
     "shell.execute_reply": "2025-05-29T19:33:24.877824Z"
    },
    "papermill": {
     "duration": 0.011856,
     "end_time": "2025-05-29T19:33:24.879653",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.867797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd2ddfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.894918Z",
     "iopub.status.busy": "2025-05-29T19:33:24.894690Z",
     "iopub.status.idle": "2025-05-29T19:33:24.902281Z",
     "shell.execute_reply": "2025-05-29T19:33:24.901507Z"
    },
    "papermill": {
     "duration": 0.01664,
     "end_time": "2025-05-29T19:33:24.903528",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.886888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: Detect ALIKED and Combine with DINO Patch Features ---\n",
    "def detect_aliked_and_combine_with_dino(img_fnames,\n",
    "                                        feature_dir='.featureout',\n",
    "                                        num_features=4096,\n",
    "                                        resize_to=1024,\n",
    "                                        dino_processor=None,\n",
    "                                        dino_model=None,\n",
    "                                        dino_patch_size=16, # Typically 14 or 16 for DINO\n",
    "                                        device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    aliked_extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    aliked_extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='w') as f_desc_aliked, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='w') as f_desc_combined: # New HDF5 for combined features\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = aliked_extractor.extract(image0)\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy() # ALIKED keypoints (x,y)\n",
    "                descs_aliked = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy() # ALIKED descriptors\n",
    "\n",
    "                # Get DINO patch features for these keypoints\n",
    "                kpts_torch = torch.from_numpy(kpts).to(device)\n",
    "                descs_dino_patch = get_dino_patch_features_for_keypoints(\n",
    "                    img_path, kpts_torch, dino_processor, dino_model, dino_patch_size, device\n",
    "                ).detach().cpu().numpy()\n",
    "\n",
    "                # Concatenate ALIKED and DINO features\n",
    "                if len(descs_aliked) > 0 and len(descs_dino_patch) > 0:\n",
    "                    combined_descs = np.concatenate((descs_aliked, descs_dino_patch), axis=1)\n",
    "                elif len(descs_aliked) > 0: # Only ALIKED if no DINO features (shouldn't happen often)\n",
    "                    combined_descs = descs_aliked\n",
    "                else: # No features found\n",
    "                    combined_descs = np.array([]) # Empty array\n",
    "\n",
    "                f_kp[key] = kpts\n",
    "                f_desc_aliked[key] = descs_aliked # Keep ALIKED descriptors for debugging or other uses\n",
    "                f_desc_combined[key] = combined_descs # Store the new combined descriptors\n",
    "    print(f\"Combined features saved to {feature_dir}/descriptors_combined.h5\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b59522c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:24.919918Z",
     "iopub.status.busy": "2025-05-29T19:33:24.919661Z",
     "iopub.status.idle": "2025-05-29T19:33:25.234264Z",
     "shell.execute_reply": "2025-05-29T19:33:25.233543Z"
    },
    "papermill": {
     "duration": 0.324409,
     "end_time": "2025-05-29T19:33:25.236005",
     "exception": false,
     "start_time": "2025-05-29T19:33:24.911596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans # MiniBatchKMeans is faster for large datasets\n",
    "\n",
    "# --- VLAD Aggregation Function ---\n",
    "def vlad_encode(descriptors, centroids):\n",
    "    \"\"\"\n",
    "    Performs VLAD encoding.\n",
    "\n",
    "    Args:\n",
    "        descriptors (np.ndarray): NxM array of local descriptors.\n",
    "        centroids (np.ndarray): KxM array of K-Means cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1x(K*M) VLAD descriptor.\n",
    "    \"\"\"\n",
    "    if descriptors.shape[0] == 0:\n",
    "        return np.zeros(centroids.shape[0] * centroids.shape[1], dtype=np.float32)\n",
    "\n",
    "    num_descriptors, desc_dim = descriptors.shape\n",
    "    num_centroids, _ = centroids.shape\n",
    "\n",
    "    # Assign each descriptor to its nearest centroid\n",
    "    # Using cdist for efficiency\n",
    "    distances = np.sqrt(np.sum((descriptors[:, None, :] - centroids[None, :, :])**2, axis=2))\n",
    "    # distances = cdist(descriptors, centroids, 'sqeuclidean') # Could use cdist for sqeuclidean\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Initialize VLAD accumulator\n",
    "    vlad_accumulator = np.zeros((num_centroids, desc_dim), dtype=np.float32)\n",
    "\n",
    "    # Accumulate residuals\n",
    "    for i in range(num_descriptors):\n",
    "        cluster_idx = cluster_assignments[i]\n",
    "        residual = descriptors[i] - centroids[cluster_idx]\n",
    "        vlad_accumulator[cluster_idx] += residual\n",
    "\n",
    "    # Flatten and L2 normalize\n",
    "    vlad_descriptor = vlad_accumulator.flatten()\n",
    "    vlad_descriptor = F.normalize(torch.from_numpy(vlad_descriptor).unsqueeze(0), dim=1, p=2).squeeze(0).numpy()\n",
    "\n",
    "    return vlad_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89275038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.252203Z",
     "iopub.status.busy": "2025-05-29T19:33:25.251905Z",
     "iopub.status.idle": "2025-05-29T19:33:25.260088Z",
     "shell.execute_reply": "2025-05-29T19:33:25.259249Z"
    },
    "papermill": {
     "duration": 0.018166,
     "end_time": "2025-05-29T19:33:25.261691",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.243525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- NEW: Get Global Descriptors using K-Means + VLAD ---\n",
    "def get_global_desc_vlad(fnames, feature_dir='.featureout', num_clusters=64, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Generates global descriptors for images using K-Means + VLAD on combined ALIKED+DINO features.\n",
    "\n",
    "    Args:\n",
    "        fnames (list): List of image file paths.\n",
    "        feature_dir (str): Directory where combined descriptors are stored.\n",
    "        num_clusters (int): Number of clusters for K-Means (K in VLAD).\n",
    "        device (torch.device): Not directly used for VLAD computation, but passed for consistency.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Nx(K*M) tensor of global VLAD descriptors.\n",
    "    \"\"\"\n",
    "    all_local_descs = []\n",
    "    keys_order = [] # To maintain order of descriptors with respect to fnames\n",
    "\n",
    "    # 1. Load all combined local descriptors\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Loading combined local descriptors for K-Means\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                if descs.shape[0] > 0:\n",
    "                    all_local_descs.append(descs)\n",
    "                    keys_order.append(key)\n",
    "\n",
    "    if not all_local_descs:\n",
    "        print(\"No combined local descriptors found. Cannot train K-Means or compute VLAD.\")\n",
    "        return torch.empty((0, num_clusters * 0), dtype=torch.float32) # Return empty tensor\n",
    "\n",
    "    # Concatenate all descriptors for K-Means training\n",
    "    all_local_descs_flat = np.concatenate(all_local_descs, axis=0)\n",
    "\n",
    "    # 2. Train K-Means on a subset of descriptors if the dataset is too large\n",
    "    # Or directly on all_local_descs_flat if memory permits\n",
    "    print(f\"Training K-Means with {num_clusters} clusters on {all_local_descs_flat.shape[0]} descriptors...\")\n",
    "    # Use MiniBatchKMeans for efficiency\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, n_init='auto', batch_size=256).fit(all_local_descs_flat)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    print(\"K-Means training complete.\")\n",
    "\n",
    "    # 3. Compute VLAD descriptor for each image\n",
    "    global_descs_vlad = []\n",
    "    # Re-iterate through original fnames to match the output order\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Computing VLAD descriptors\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                vlad_desc = vlad_encode(descs, centroids)\n",
    "                global_descs_vlad.append(torch.from_numpy(vlad_desc).unsqueeze(0))\n",
    "            else:\n",
    "                # Handle cases where an image might not have any combined descriptors\n",
    "                # (e.g., no ALIKED keypoints detected). Append a zero vector of correct size.\n",
    "                print(f\"Warning: No combined descriptors for {key}. Appending zero VLAD descriptor.\")\n",
    "                # Determine descriptor dimension from centroids\n",
    "                desc_dim_per_cluster = centroids.shape[1] if centroids.shape[1] > 0 else 0 # Should not be 0 normally\n",
    "                zero_vlad = np.zeros(num_clusters * desc_dim_per_cluster, dtype=np.float32)\n",
    "                global_descs_vlad.append(torch.from_numpy(zero_vlad).unsqueeze(0))\n",
    "\n",
    "\n",
    "    if not global_descs_vlad:\n",
    "        return torch.empty((0, num_clusters * centroids.shape[1] if centroids.shape[1] > 0 else 0), dtype=torch.float32)\n",
    "\n",
    "    global_descs_vlad = torch.cat(global_descs_vlad, dim=0)\n",
    "    return global_descs_vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28684ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.276894Z",
     "iopub.status.busy": "2025-05-29T19:33:25.276641Z",
     "iopub.status.idle": "2025-05-29T19:33:25.284510Z",
     "shell.execute_reply": "2025-05-29T19:33:25.283662Z"
    },
    "papermill": {
     "duration": 0.016867,
     "end_time": "2025-05-29T19:33:25.285803",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.268936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- RE-DEFINED: get_image_pairs_shortlist to use the new VLAD global descriptor ---\n",
    "def get_image_pairs_shortlist_vlad(fnames,\n",
    "                                   sim_th=0.6, # should be strict\n",
    "                                   min_pairs=30,\n",
    "                                   exhaustive_if_less=20,\n",
    "                                   feature_dir='.featureout', # Pass feature_dir\n",
    "                                   num_clusters_vlad=64, # New parameter for VLAD\n",
    "                                   device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames) # You need to define get_img_pairs_exhaustive if not done.\n",
    "\n",
    "    # Use the new VLAD-based global descriptor\n",
    "    descs = get_global_desc_vlad(fnames, feature_dir=feature_dir, num_clusters=num_clusters_vlad, device=device)\n",
    "\n",
    "    if descs.shape[0] == 0:\n",
    "        print(\"No global descriptors generated. Returning empty matching list.\")\n",
    "        return []\n",
    "\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # \n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 60)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = set() # Use a set for faster lookup of already added pairs\n",
    "\n",
    "    for st_idx in range(num_imgs - 1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold: # Ensure distance is not effectively infinite\n",
    "                pair = tuple(sorted((st_idx, idx.item())))\n",
    "                if pair not in already_there_set:\n",
    "                    matching_list.append(pair)\n",
    "                    already_there_set.add(pair)\n",
    "                    total += 1\n",
    "    matching_list = sorted(list(matching_list)) # Sort the list of tuples\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5bf9e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.299870Z",
     "iopub.status.busy": "2025-05-29T19:33:25.299654Z",
     "iopub.status.idle": "2025-05-29T19:33:25.303090Z",
     "shell.execute_reply": "2025-05-29T19:33:25.302497Z"
    },
    "papermill": {
     "duration": 0.011864,
     "end_time": "2025-05-29T19:33:25.304335",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.292471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "009ba895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.318906Z",
     "iopub.status.busy": "2025-05-29T19:33:25.318694Z",
     "iopub.status.idle": "2025-05-29T19:33:25.328477Z",
     "shell.execute_reply": "2025-05-29T19:33:25.327760Z"
    },
    "papermill": {
     "duration": 0.018604,
     "end_time": "2025-05-29T19:33:25.329764",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.311160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th=0.6,\n",
    "                              min_pairs=30,\n",
    "                              max_pairs=100,  #  max_pairs \n",
    "                              exhaustive_if_less=20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "    mask = dm <= np.percentile(dm_flat, 50)\n",
    "\n",
    "    ar = np.arange(num_imgs)\n",
    "    matching_set = set()\n",
    "\n",
    "    for st_idx in range(num_imgs):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "\n",
    "        #  min_pairs \n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        #  max_pairs\n",
    "        sorted_matches = sorted(\n",
    "            [(idx, dm[st_idx, idx]) for idx in to_match if idx != st_idx and dm[st_idx, idx] < threshold],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        for idx, _ in sorted_matches[:max_pairs]:\n",
    "            pair = tuple(sorted((st_idx, idx)))\n",
    "            matching_set.add(pair)\n",
    "\n",
    "    matching_list = sorted(list(matching_set))\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b569a79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.344593Z",
     "iopub.status.busy": "2025-05-29T19:33:25.344333Z",
     "iopub.status.idle": "2025-05-29T19:33:25.363933Z",
     "shell.execute_reply": "2025-05-29T19:33:25.363107Z"
    },
    "papermill": {
     "duration": 0.028545,
     "end_time": "2025-05-29T19:33:25.365357",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.336812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_aliked_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_superpoint_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_disk_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf5fb548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.379698Z",
     "iopub.status.busy": "2025-05-29T19:33:25.379455Z",
     "iopub.status.idle": "2025-05-29T19:33:25.385634Z",
     "shell.execute_reply": "2025-05-29T19:33:25.384974Z"
    },
    "papermill": {
     "duration": 0.014578,
     "end_time": "2025-05-29T19:33:25.386836",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.372258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(feature_dir, img_dir):\n",
    "    result = {}\n",
    "    local_timings = {'RANSAC': [], 'Reconstruction': []}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(\"colmap database\")\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # options = pycolmap.SiftMatchingOptions()\n",
    "    # options.confidence = 0.9999\n",
    "    # options.max_num_trials = 20000\n",
    "    # pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    print(\"matching done!!!!\")\n",
    "    local_timings['RANSAC'].append(time() - t)\n",
    "    print(f'RANSAC in {local_timings[\"RANSAC\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # mapper_options.mapper.filter_max_reproj_error\t = 1.0\n",
    "    # mapper_options.mapper.init_max_error = 2.0\n",
    "    mapper_options.min_model_size = 5\n",
    "    mapper_options.max_num_models = 25\n",
    "    # mapper_options.ba_global_images_freq = 5\n",
    "    # # mapper_options.ba_local_num_images = 8\n",
    "    # # mapper_options.mapper.abs_pose_min_inlier_ratio = 0.4\n",
    "    # mapper_options.ba_global_max_num_iterations = 100\n",
    "    # # mapper_options.mapper.filter_max_reproj_error = 6.0\n",
    "    # mapper_options.mapper.max_reg_trials = 10\n",
    "    # mapper_options.mapper.init_min_num_inliers = 50\n",
    "    # mapper_options.mapper.abs_pose_min_num_inliers = 60\n",
    "    \n",
    "\n",
    "    \n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, \n",
    "                                        output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    for map_index, rec in maps.items():\n",
    "        result[map_index] = {}\n",
    "        for img_id, image in rec.images.items():\n",
    "            result[map_index][image.name] = {\n",
    "                'R': image.cam_from_world.rotation.matrix().tolist(),\n",
    "                't': image.cam_from_world.translation.tolist()\n",
    "            }\n",
    "    # clear_output(wait=False)\n",
    "    local_timings['Reconstruction'].append(time() - t)\n",
    "    print(f'Reconstruction done in {local_timings[\"Reconstruction\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    return result, local_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec220eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.400856Z",
     "iopub.status.busy": "2025-05-29T19:33:25.400648Z",
     "iopub.status.idle": "2025-05-29T19:33:25.566280Z",
     "shell.execute_reply": "2025-05-29T19:33:25.565242Z"
    },
    "papermill": {
     "duration": 0.174256,
     "end_time": "2025-05-29T19:33:25.567708",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.393452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "534a967e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.582985Z",
     "iopub.status.busy": "2025-05-29T19:33:25.582730Z",
     "iopub.status.idle": "2025-05-29T19:33:25.597975Z",
     "shell.execute_reply": "2025-05-29T19:33:25.597329Z"
    },
    "papermill": {
     "duration": 0.024161,
     "end_time": "2025-05-29T19:33:25.599112",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.574951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_keypoints_and_matches(images_input, unified_kp_path, remapped_matches_path, feature_dir='visualization_output'):\n",
    "    output_dir = os.path.join(feature_dir, 'visualization_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load images and determine image_keys for HDF5 lookup\n",
    "    if isinstance(images_input[0], str):\n",
    "        loaded_images = [cv2.imread(img_path) for img_path in images_input]\n",
    "        image_keys = [os.path.basename(img_path) for img_path in images_input]\n",
    "    else:\n",
    "        loaded_images = images_input\n",
    "        # If images_input are already arrays, you need to provide the corresponding keys\n",
    "        # This part is crucial: image_keys MUST align with the HDF5 keys\n",
    "        image_keys = image_keys_in_h5 # Use the predefined list for the dummy case\n",
    "\n",
    "    # Load unified keypoints\n",
    "    keypoints_data = {}\n",
    "    with h5py.File(unified_kp_path, 'r') as f_kp:\n",
    "        for img_name_raw in f_kp.keys():\n",
    "            img_name = img_name_raw.decode('utf-8') if isinstance(img_name_raw, bytes) else img_name_raw\n",
    "            keypoints_data[img_name] = f_kp[img_name_raw][()] # Access with raw key if bytes\n",
    "\n",
    "    # Load remapped matches - CORRECTED LOGIC\n",
    "    # Store (img1_key, img2_key) directly with matches for robust iteration\n",
    "    matches_data_pairs = [] # Will store (img1_key, img2_key, matches_array)\n",
    "    with h5py.File(remapped_matches_path, 'r') as f_matches:\n",
    "        print(\"\\n--- Loading remapped matches from HDF5 ---\")\n",
    "        for img1_group_key_candidate in tqdm(f_matches.keys(), desc=\"Loading matches\"):\n",
    "            img1_key = img1_group_key_candidate.decode('utf-8') if isinstance(img1_group_key_candidate, bytes) else img1_group_key_candidate\n",
    "\n",
    "            img1_group = f_matches[img1_group_key_candidate] # Access with raw key\n",
    "\n",
    "            if isinstance(img1_group, h5py.Group):\n",
    "                for img2_dataset_key_candidate in img1_group.keys():\n",
    "                    img2_key = img2_dataset_key_candidate.decode('utf-8') if isinstance(img2_dataset_key_candidate, bytes) else img2_dataset_key_candidate\n",
    "\n",
    "                    try:\n",
    "                        matches_array = img1_group[img2_dataset_key_candidate][()]\n",
    "                        matches_data_pairs.append((img1_key, img2_key, matches_array))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading matches for pair ({img1_key}, {img2_key}): {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Expected '{img1_key}' to be a group, but found {type(img1_group)}. Skipping its contents.\")\n",
    "\n",
    "\n",
    "    # --- Drawing Keypoints ---\n",
    "    print(\"\\n--- Drawing Keypoints ---\")\n",
    "    for i, img_key in enumerate(image_keys):\n",
    "        if img_key in keypoints_data:\n",
    "            img = loaded_images[i].copy()\n",
    "            kpts = keypoints_data[img_key]\n",
    "\n",
    "            for kp in kpts:\n",
    "                x, y = int(kp[0]), int(kp[1])\n",
    "                cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Green circle for keypoint\n",
    "\n",
    "            output_kp_path = os.path.join(output_dir, f\"keypoints_{img_key}\")\n",
    "            if len(img.shape) == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(output_kp_path, img)\n",
    "            print(f\"Keypoints drawn on {img_key}, saved to {output_kp_path}\")\n",
    "        else:\n",
    "            print(f\"No keypoints found for {img_key} in unified keypoints file.\")\n",
    "\n",
    "    # --- Drawing Matches ---\n",
    "    print(\"\\n--- Drawing Matches ---\")\n",
    "    # Iterate through the (img1_key, img2_key, matches) tuples directly\n",
    "    for img_name1, img_name2, matches in matches_data_pairs:\n",
    "        # We no longer need to split img_pair_key, as we have img_name1 and img_name2 directly\n",
    "\n",
    "        # Find the actual image objects and their keypoints using image_keys list\n",
    "        try:\n",
    "            img1_idx = image_keys.index(img_name1)\n",
    "            img2_idx = image_keys.index(img_name2)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: One or both image names not found in the provided 'images' list/keys.\")\n",
    "            continue\n",
    "\n",
    "        img1 = loaded_images[img1_idx].copy()\n",
    "        img2 = loaded_images[img2_idx].copy()\n",
    "\n",
    "        kpts1 = keypoints_data.get(img_name1)\n",
    "        kpts2 = keypoints_data.get(img_name2)\n",
    "\n",
    "        if kpts1 is None or kpts2 is None:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: keypoints not found for one or both images in unified keypoints.\")\n",
    "            continue\n",
    "        if len(matches) == 0:\n",
    "            print(f\"No matches to draw for {img_name1}-{img_name2}.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure images are 3 channels for drawing lines\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "        if len(img2.shape) == 2:\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Create a concatenated image for drawing matches\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        max_h = max(h1, h2)\n",
    "        matched_img = np.zeros((max_h, w1 + w2, 3), dtype=np.uint8)\n",
    "        matched_img[0:h1, 0:w1] = img1\n",
    "        matched_img[0:h2, w1:w1+w2] = img2\n",
    "\n",
    "        num_matches_to_draw = min(len(matches), 200) # Draw up to 200 matches to avoid clutter, adjust as needed\n",
    "\n",
    "        for i in range(num_matches_to_draw):\n",
    "            match = matches[i]\n",
    "            kp1_idx, kp2_idx = int(match[0]), int(match[1])\n",
    "\n",
    "            # Bounds check for keypoint indices\n",
    "            if kp1_idx >= len(kpts1) or kp2_idx >= len(kpts2):\n",
    "                # print(f\"Warning: Match index out of bounds for {img_name1}-{img_name2}. Skipping match {kp1_idx}-{kp2_idx}.\")\n",
    "                continue\n",
    "\n",
    "            pt1 = tuple(map(int, kpts1[kp1_idx][:2]))\n",
    "            pt2 = tuple(map(int, kpts2[kp2_idx][:2]))\n",
    "\n",
    "            # Draw circles on the concatenated image\n",
    "            cv2.circle(matched_img, pt1, 5, (0, 0, 255), 2) # Red circle on img1 side\n",
    "            cv2.circle(matched_img, (pt2[0] + w1, pt2[1]), 5, (255, 0, 0), 2) # Blue circle on img2 side\n",
    "\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            cv2.line(matched_img, pt1, (pt2[0] + w1, pt2[1]), color, 1)\n",
    "\n",
    "        output_match_path = os.path.join(output_dir, f\"matches_{img_name1}_{img_name2}.png\")\n",
    "        cv2.imwrite(output_match_path, matched_img)\n",
    "        print(f\"Matches drawn between {img_name1} and {img_name2}, saved to {output_match_path}\")\n",
    "\n",
    "\n",
    "# Example call (replace with your actual 'images' list)\n",
    "# If your 'images' are file paths:\n",
    "# images_file_paths = ['path/to/your/image1.jpg', 'path/to/your/image2.jpg', ...]\n",
    "# draw_keypoints_and_matches(images_file_paths, unified_kp_path, remapped_matches_path)\n",
    "\n",
    "# If your 'images' are loaded numpy arrays (as in the dummy example above):\n",
    "# draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53209d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:33:25.614182Z",
     "iopub.status.busy": "2025-05-29T19:33:25.613936Z",
     "iopub.status.idle": "2025-05-29T19:37:26.186174Z",
     "shell.execute_reply": "2025-05-29T19:37:26.184977Z"
    },
    "papermill": {
     "duration": 240.581473,
     "end_time": "2025-05-29T19:37:26.187574",
     "exception": false,
     "start_time": "2025-05-29T19:33:25.606101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model for patch feature extraction...\n",
      "DINOv2 model loaded.\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n",
      "rotation_detection for 22 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 22/22 [00:02<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "25%:  0.2547\n",
      "60%:  0.2913\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 150. Done in 2.6800 sec\n",
      "Generated 150 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 150. Done in 2.9698 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1656, 2]), descs.shape=torch.Size([1656, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1449, 2]), descs.shape=torch.Size([1449, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1341, 2]), descs.shape=torch.Size([1341, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1395, 2]), descs.shape=torch.Size([1395, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1387, 2]), descs.shape=torch.Size([1387, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1343, 2]), descs.shape=torch.Size([1343, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1422, 2]), descs.shape=torch.Size([1422, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1375, 2]), descs.shape=torch.Size([1375, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1132, 2]), descs.shape=torch.Size([1132, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1022, 2]), descs.shape=torch.Size([1022, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2785, 2]), descs.shape=torch.Size([2785, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2762, 2]), descs.shape=torch.Size([2762, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2473, 2]), descs.shape=torch.Size([2473, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2462, 2]), descs.shape=torch.Size([2462, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2866, 2]), descs.shape=torch.Size([2866, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2248, 2]), descs.shape=torch.Size([2248, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2026, 2]), descs.shape=torch.Size([2026, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2171, 2]), descs.shape=torch.Size([2171, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2480, 2]), descs.shape=torch.Size([2480, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1830, 2]), descs.shape=torch.Size([1830, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2499, 2]), descs.shape=torch.Size([2499, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2361, 2]), descs.shape=torch.Size([2361, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 5/150 [00:00<00:08, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et001.png-another_et_another_et002.png: 962 matches @ 1th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et004.png: 682 matches @ 2th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et005.png: 746 matches @ 3th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et006.png: 342 matches @ 4th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et007.png: 262 matches @ 5th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et008.png: 107 matches @ 6th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 16/150 [00:00<00:04, 28.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et002.png-another_et_another_et003.png: 486 matches @ 7th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et004.png: 707 matches @ 8th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et005.png: 652 matches @ 9th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et006.png: 373 matches @ 10th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et007.png: 296 matches @ 11th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et008.png: 103 matches @ 12th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|       | 32/150 [00:01<00:03, 33.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et003.png-another_et_another_et004.png: 498 matches @ 13th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et005.png: 378 matches @ 14th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et006.png: 259 matches @ 15th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et007.png: 183 matches @ 16th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 44/150 [00:01<00:03, 34.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et004.png-another_et_another_et005.png: 682 matches @ 17th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et004.png-another_et_another_et006.png: 333 matches @ 18th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et004.png-another_et_another_et007.png: 197 matches @ 19th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 52/150 [00:01<00:02, 35.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et005.png-another_et_another_et006.png: 269 matches @ 20th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et005.png-another_et_another_et007.png: 177 matches @ 21th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et006.png-another_et_another_et007.png: 393 matches @ 22th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et006.png-another_et_another_et008.png: 271 matches @ 23th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 72/150 [00:02<00:02, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et007.png-another_et_another_et008.png: 393 matches @ 24th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et007.png-another_et_another_et009.png: 170 matches @ 25th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et007.png-another_et_another_et010.png: 113 matches @ 26th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 84/150 [00:02<00:02, 31.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et008.png-another_et_another_et009.png: 329 matches @ 27th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et008.png-another_et_another_et010.png: 239 matches @ 28th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 96/150 [00:03<00:01, 32.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et009.png-another_et_another_et010.png: 225 matches @ 29th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 104/150 [00:03<00:01, 27.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et000.png-et_et001.png: 1330 matches @ 30th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et002.png: 892 matches @ 31th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et003.png: 1738 matches @ 32th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et004.png: 1003 matches @ 33th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|  | 107/150 [00:03<00:01, 25.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et000.png-et_et005.png: 137 matches @ 34th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et006.png: 135 matches @ 35th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et007.png: 101 matches @ 36th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 113/150 [00:03<00:01, 22.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et001.png-et_et002.png: 1416 matches @ 37th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et003.png: 985 matches @ 38th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et004.png: 1239 matches @ 39th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et005.png: 227 matches @ 40th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et006.png: 270 matches @ 41th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 119/150 [00:04<00:01, 21.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et001.png-et_et007.png: 229 matches @ 42th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et003.png: 680 matches @ 43th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et004.png: 905 matches @ 44th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 122/150 [00:04<00:01, 21.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et002.png-et_et005.png: 328 matches @ 45th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et006.png: 403 matches @ 46th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et007.png: 272 matches @ 47th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et008.png: 204 matches @ 48th pair(aliked+lightglue)\n",
      "aliked> et_et003.png-et_et004.png: 792 matches @ 49th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 134/150 [00:04<00:00, 21.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et004.png-et_et005.png: 208 matches @ 50th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et006.png: 245 matches @ 51th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et007.png: 189 matches @ 52th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et008.png: 141 matches @ 53th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|| 137/150 [00:04<00:00, 22.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et005.png-et_et006.png: 1235 matches @ 54th pair(aliked+lightglue)\n",
      "aliked> et_et005.png-et_et007.png: 1281 matches @ 55th pair(aliked+lightglue)\n",
      "aliked> et_et005.png-et_et008.png: 933 matches @ 56th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 143/150 [00:05<00:00, 23.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et006.png-et_et007.png: 1362 matches @ 57th pair(aliked+lightglue)\n",
      "aliked> et_et006.png-et_et008.png: 690 matches @ 58th pair(aliked+lightglue)\n",
      "aliked> et_et007.png-et_et008.png: 884 matches @ 59th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:05<00:00, 27.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  8.0429 sec (aliked+LightGlue)\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([395, 2]), descs.shape=torch.Size([395, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([218, 2]), descs.shape=torch.Size([218, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([166, 2]), descs.shape=torch.Size([166, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([207, 2]), descs.shape=torch.Size([207, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([262, 2]), descs.shape=torch.Size([262, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([215, 2]), descs.shape=torch.Size([215, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([203, 2]), descs.shape=torch.Size([203, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([163, 2]), descs.shape=torch.Size([163, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([145, 2]), descs.shape=torch.Size([145, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([165, 2]), descs.shape=torch.Size([165, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([324, 2]), descs.shape=torch.Size([324, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([332, 2]), descs.shape=torch.Size([332, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([319, 2]), descs.shape=torch.Size([319, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([239, 2]), descs.shape=torch.Size([239, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([273, 2]), descs.shape=torch.Size([273, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([181, 2]), descs.shape=torch.Size([181, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([206, 2]), descs.shape=torch.Size([206, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([207, 2]), descs.shape=torch.Size([207, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([198, 2]), descs.shape=torch.Size([198, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([88, 2]), descs.shape=torch.Size([88, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([55, 2]), descs.shape=torch.Size([55, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([103, 2]), descs.shape=torch.Size([103, 256])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 4/150 [00:00<00:04, 32.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et001.png-another_et_another_et002.png: 149 matches @ 1th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et001.png-another_et_another_et004.png: 103 matches @ 2th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et001.png-another_et_another_et005.png: 135 matches @ 3th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et001.png-another_et_another_et006.png: 81 matches @ 4th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 18/150 [00:00<00:03, 38.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et002.png-another_et_another_et004.png: 80 matches @ 5th pair(superpoint+lightglue)\n",
      "superpoint> another_et_another_et002.png-another_et_another_et005.png: 93 matches @ 6th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 46/150 [00:01<00:02, 40.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et004.png-another_et_another_et005.png: 98 matches @ 7th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 56/150 [00:01<00:02, 38.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et006.png-another_et_another_et007.png: 52 matches @ 8th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 74/150 [00:01<00:01, 39.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> another_et_another_et007.png-another_et_another_et008.png: 54 matches @ 9th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 109/150 [00:02<00:01, 40.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et000.png-et_et001.png: 163 matches @ 10th pair(superpoint+lightglue)\n",
      "superpoint> et_et000.png-et_et002.png: 122 matches @ 11th pair(superpoint+lightglue)\n",
      "superpoint> et_et000.png-et_et003.png: 184 matches @ 12th pair(superpoint+lightglue)\n",
      "superpoint> et_et000.png-et_et004.png: 111 matches @ 13th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 114/150 [00:02<00:00, 40.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et001.png-et_et002.png: 187 matches @ 14th pair(superpoint+lightglue)\n",
      "superpoint> et_et001.png-et_et003.png: 124 matches @ 15th pair(superpoint+lightglue)\n",
      "superpoint> et_et001.png-et_et004.png: 118 matches @ 16th pair(superpoint+lightglue)\n",
      "superpoint> et_et001.png-et_et006.png: 53 matches @ 17th pair(superpoint+lightglue)\n",
      "superpoint> et_et002.png-et_et003.png: 100 matches @ 18th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 124/150 [00:03<00:00, 40.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et002.png-et_et004.png: 104 matches @ 19th pair(superpoint+lightglue)\n",
      "superpoint> et_et002.png-et_et006.png: 65 matches @ 20th pair(superpoint+lightglue)\n",
      "superpoint> et_et002.png-et_et007.png: 53 matches @ 21th pair(superpoint+lightglue)\n",
      "superpoint> et_et003.png-et_et004.png: 85 matches @ 22th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|| 137/150 [00:03<00:00, 39.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et004.png-et_et006.png: 56 matches @ 23th pair(superpoint+lightglue)\n",
      "superpoint> et_et005.png-et_et006.png: 103 matches @ 24th pair(superpoint+lightglue)\n",
      "superpoint> et_et005.png-et_et007.png: 102 matches @ 25th pair(superpoint+lightglue)\n",
      "superpoint> et_et005.png-et_et008.png: 68 matches @ 26th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 146/150 [00:03<00:00, 39.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> et_et006.png-et_et007.png: 132 matches @ 27th pair(superpoint+lightglue)\n",
      "superpoint> et_et006.png-et_et008.png: 65 matches @ 28th pair(superpoint+lightglue)\n",
      "superpoint> et_et007.png-et_et008.png: 76 matches @ 29th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:03<00:00, 39.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  5.8186 sec (superpoint+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3959, 2]), descs.shape=torch.Size([3959, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3945, 2]), descs.shape=torch.Size([3945, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3789, 2]), descs.shape=torch.Size([3789, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3722, 2]), descs.shape=torch.Size([3722, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3725, 2]), descs.shape=torch.Size([3725, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3742, 2]), descs.shape=torch.Size([3742, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3406, 2]), descs.shape=torch.Size([3406, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3319, 2]), descs.shape=torch.Size([3319, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([2928, 2]), descs.shape=torch.Size([2928, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et002.png: 2565 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 2/150 [00:00<00:13, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et004.png: 1794 matches @ 2th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et005.png: 1929 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 4/150 [00:00<00:13, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et006.png: 1280 matches @ 4th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 6/150 [00:00<00:12, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et007.png: 619 matches @ 5th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et008.png: 255 matches @ 6th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 12/150 [00:01<00:12, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et003.png: 1364 matches @ 7th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et004.png: 1855 matches @ 8th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et005.png: 1798 matches @ 9th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 16/150 [00:01<00:11, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et006.png: 1240 matches @ 10th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et007.png: 675 matches @ 11th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et008.png: 288 matches @ 12th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 28/150 [00:02<00:11, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et003.png-another_et_another_et004.png: 1460 matches @ 13th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et005.png: 1169 matches @ 14th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et006.png: 852 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|       | 32/150 [00:02<00:10, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et003.png-another_et_another_et007.png: 461 matches @ 16th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et008.png: 193 matches @ 17th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 40/150 [00:03<00:09, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et004.png-another_et_another_et005.png: 1791 matches @ 18th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et006.png: 1024 matches @ 19th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et007.png: 464 matches @ 20th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 44/150 [00:03<00:09, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et004.png-another_et_another_et008.png: 240 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 48/150 [00:04<00:08, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et005.png-another_et_another_et006.png: 1185 matches @ 22th pair(disk+lightglue)\n",
      "disk> another_et_another_et005.png-another_et_another_et007.png: 405 matches @ 23th pair(disk+lightglue)\n",
      "disk> another_et_another_et005.png-another_et_another_et008.png: 213 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 54/150 [00:04<00:08, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et006.png-another_et_another_et007.png: 1201 matches @ 25th pair(disk+lightglue)\n",
      "disk> another_et_another_et006.png-another_et_another_et008.png: 787 matches @ 26th pair(disk+lightglue)\n",
      "disk> another_et_another_et006.png-another_et_another_et009.png: 160 matches @ 27th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 68/150 [00:06<00:06, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et007.png-another_et_another_et008.png: 1104 matches @ 28th pair(disk+lightglue)\n",
      "disk> another_et_another_et007.png-another_et_another_et009.png: 555 matches @ 29th pair(disk+lightglue)\n",
      "disk> another_et_another_et007.png-another_et_another_et010.png: 292 matches @ 30th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 80/150 [00:07<00:05, 12.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et008.png-another_et_another_et009.png: 993 matches @ 31th pair(disk+lightglue)\n",
      "disk> another_et_another_et008.png-another_et_another_et010.png: 356 matches @ 32th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|   | 92/150 [00:08<00:04, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et009.png-another_et_another_et010.png: 637 matches @ 33th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 102/150 [00:08<00:04, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et000.png-et_et001.png: 2182 matches @ 34th pair(disk+lightglue)\n",
      "disk> et_et000.png-et_et002.png: 1747 matches @ 35th pair(disk+lightglue)\n",
      "disk> et_et000.png-et_et003.png: 2874 matches @ 36th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 106/150 [00:09<00:04, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et000.png-et_et004.png: 1572 matches @ 37th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 112/150 [00:09<00:03, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et001.png-et_et002.png: 2335 matches @ 38th pair(disk+lightglue)\n",
      "disk> et_et001.png-et_et003.png: 1873 matches @ 39th pair(disk+lightglue)\n",
      "disk> et_et001.png-et_et004.png: 1855 matches @ 40th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 120/150 [00:10<00:02, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et003.png: 1538 matches @ 41th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et004.png: 1610 matches @ 42th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et005.png: 290 matches @ 43th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 122/150 [00:10<00:02, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et006.png: 774 matches @ 44th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et007.png: 187 matches @ 45th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 126/150 [00:11<00:02, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et003.png-et_et004.png: 1410 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 132/150 [00:11<00:01, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et004.png-et_et005.png: 224 matches @ 47th pair(disk+lightglue)\n",
      "disk> et_et004.png-et_et006.png: 349 matches @ 48th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 136/150 [00:12<00:01, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et005.png-et_et006.png: 2465 matches @ 49th pair(disk+lightglue)\n",
      "disk> et_et005.png-et_et007.png: 2452 matches @ 50th pair(disk+lightglue)\n",
      "disk> et_et005.png-et_et008.png: 1848 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 142/150 [00:12<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et006.png-et_et007.png: 2729 matches @ 52th pair(disk+lightglue)\n",
      "disk> et_et006.png-et_et008.png: 1614 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 146/150 [00:13<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et007.png-et_et008.png: 1931 matches @ 54th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:13<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  18.7153 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='150' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [150/150 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_FundamentalMatrix: 3676 matches --> 3672 matches\n",
      "another_et_another_et001.png-another_et_another_et002.png: 3676 --> 3672 matches\n",
      "filter_FundamentalMatrix: 2579 matches --> 2573 matches\n",
      "another_et_another_et001.png-another_et_another_et004.png: 2579 --> 2573 matches\n",
      "filter_FundamentalMatrix: 2810 matches --> 2809 matches\n",
      "another_et_another_et001.png-another_et_another_et005.png: 2810 --> 2809 matches\n",
      "filter_FundamentalMatrix: 1703 matches --> 1653 matches\n",
      "another_et_another_et001.png-another_et_another_et006.png: 1703 --> 1653 matches\n",
      "filter_FundamentalMatrix: 881 matches --> 857 matches\n",
      "another_et_another_et001.png-another_et_another_et007.png: 881 --> 857 matches\n",
      "filter_FundamentalMatrix: 362 matches --> 329 matches\n",
      "another_et_another_et001.png-another_et_another_et008.png: 362 --> 329 matches\n",
      "filter_FundamentalMatrix: 1850 matches --> 1846 matches\n",
      "another_et_another_et002.png-another_et_another_et003.png: 1850 --> 1846 matches\n",
      "filter_FundamentalMatrix: 2642 matches --> 2627 matches\n",
      "another_et_another_et002.png-another_et_another_et004.png: 2642 --> 2627 matches\n",
      "filter_FundamentalMatrix: 2543 matches --> 2527 matches\n",
      "another_et_another_et002.png-another_et_another_et005.png: 2543 --> 2527 matches\n",
      "filter_FundamentalMatrix: 1613 matches --> 1580 matches\n",
      "another_et_another_et002.png-another_et_another_et006.png: 1613 --> 1580 matches\n",
      "filter_FundamentalMatrix: 971 matches --> 943 matches\n",
      "another_et_another_et002.png-another_et_another_et007.png: 971 --> 943 matches\n",
      "filter_FundamentalMatrix: 391 matches --> 373 matches\n",
      "another_et_another_et002.png-another_et_another_et008.png: 391 --> 373 matches\n",
      "filter_FundamentalMatrix: 1958 matches --> 1943 matches\n",
      "another_et_another_et003.png-another_et_another_et004.png: 1958 --> 1943 matches\n",
      "filter_FundamentalMatrix: 1547 matches --> 1536 matches\n",
      "another_et_another_et003.png-another_et_another_et005.png: 1547 --> 1536 matches\n",
      "filter_FundamentalMatrix: 1111 matches --> 1089 matches\n",
      "another_et_another_et003.png-another_et_another_et006.png: 1111 --> 1089 matches\n",
      "filter_FundamentalMatrix: 644 matches --> 613 matches\n",
      "another_et_another_et003.png-another_et_another_et007.png: 644 --> 613 matches\n",
      "filter_FundamentalMatrix: 193 matches --> 186 matches\n",
      "another_et_another_et003.png-another_et_another_et008.png: 193 --> 186 matches\n",
      "filter_FundamentalMatrix: 2571 matches --> 2553 matches\n",
      "another_et_another_et004.png-another_et_another_et005.png: 2571 --> 2553 matches\n",
      "filter_FundamentalMatrix: 1357 matches --> 1335 matches\n",
      "another_et_another_et004.png-another_et_another_et006.png: 1357 --> 1335 matches\n",
      "filter_FundamentalMatrix: 661 matches --> 641 matches\n",
      "another_et_another_et004.png-another_et_another_et007.png: 661 --> 641 matches\n",
      "filter_FundamentalMatrix: 240 matches --> 234 matches\n",
      "another_et_another_et004.png-another_et_another_et008.png: 240 --> 234 matches\n",
      "filter_FundamentalMatrix: 1454 matches --> 1441 matches\n",
      "another_et_another_et005.png-another_et_another_et006.png: 1454 --> 1441 matches\n",
      "filter_FundamentalMatrix: 582 matches --> 557 matches\n",
      "another_et_another_et005.png-another_et_another_et007.png: 582 --> 557 matches\n",
      "filter_FundamentalMatrix: 213 matches --> 193 matches\n",
      "another_et_another_et005.png-another_et_another_et008.png: 213 --> 193 matches\n",
      "filter_FundamentalMatrix: 1646 matches --> 1604 matches\n",
      "another_et_another_et006.png-another_et_another_et007.png: 1646 --> 1604 matches\n",
      "filter_FundamentalMatrix: 1058 matches --> 1043 matches\n",
      "another_et_another_et006.png-another_et_another_et008.png: 1058 --> 1043 matches\n",
      "filter_FundamentalMatrix: 160 matches --> 159 matches\n",
      "another_et_another_et006.png-another_et_another_et009.png: 160 --> 159 matches\n",
      "filter_FundamentalMatrix: 1551 matches --> 1512 matches\n",
      "another_et_another_et007.png-another_et_another_et008.png: 1551 --> 1512 matches\n",
      "filter_FundamentalMatrix: 725 matches --> 712 matches\n",
      "another_et_another_et007.png-another_et_another_et009.png: 725 --> 712 matches\n",
      "filter_FundamentalMatrix: 405 matches --> 403 matches\n",
      "another_et_another_et007.png-another_et_another_et010.png: 405 --> 403 matches\n",
      "filter_FundamentalMatrix: 1322 matches --> 1309 matches\n",
      "another_et_another_et008.png-another_et_another_et009.png: 1322 --> 1309 matches\n",
      "filter_FundamentalMatrix: 595 matches --> 532 matches\n",
      "another_et_another_et008.png-another_et_another_et010.png: 595 --> 532 matches\n",
      "filter_FundamentalMatrix: 862 matches --> 841 matches\n",
      "another_et_another_et009.png-another_et_another_et010.png: 862 --> 841 matches\n",
      "filter_FundamentalMatrix: 3675 matches --> 3674 matches\n",
      "et_et000.png-et_et001.png: 3675 --> 3674 matches\n",
      "filter_FundamentalMatrix: 2761 matches --> 2759 matches\n",
      "et_et000.png-et_et002.png: 2761 --> 2759 matches\n",
      "filter_FundamentalMatrix: 4796 matches --> 4796 matches\n",
      "et_et000.png-et_et003.png: 4796 --> 4796 matches\n",
      "filter_FundamentalMatrix: 2686 matches --> 2676 matches\n",
      "et_et000.png-et_et004.png: 2686 --> 2676 matches\n",
      "filter_FundamentalMatrix: 137 matches --> 134 matches\n",
      "et_et000.png-et_et005.png: 137 --> 134 matches\n",
      "filter_FundamentalMatrix: 135 matches --> 126 matches\n",
      "et_et000.png-et_et006.png: 135 --> 126 matches\n",
      "filter_FundamentalMatrix: 101 matches --> 78 matches\n",
      "et_et000.png-et_et007.png: 101 --> 78 matches\n",
      "filter_FundamentalMatrix: 3938 matches --> 3937 matches\n",
      "et_et001.png-et_et002.png: 3938 --> 3937 matches\n",
      "filter_FundamentalMatrix: 2982 matches --> 2979 matches\n",
      "et_et001.png-et_et003.png: 2982 --> 2979 matches\n",
      "filter_FundamentalMatrix: 3212 matches --> 3202 matches\n",
      "et_et001.png-et_et004.png: 3212 --> 3202 matches\n",
      "filter_FundamentalMatrix: 227 matches --> 224 matches\n",
      "et_et001.png-et_et005.png: 227 --> 224 matches\n",
      "filter_FundamentalMatrix: 323 matches --> 307 matches\n",
      "et_et001.png-et_et006.png: 323 --> 307 matches\n",
      "filter_FundamentalMatrix: 229 matches --> 222 matches\n",
      "et_et001.png-et_et007.png: 229 --> 222 matches\n",
      "filter_FundamentalMatrix: 2318 matches --> 2317 matches\n",
      "et_et002.png-et_et003.png: 2318 --> 2317 matches\n",
      "filter_FundamentalMatrix: 2619 matches --> 2602 matches\n",
      "et_et002.png-et_et004.png: 2619 --> 2602 matches\n",
      "filter_FundamentalMatrix: 618 matches --> 615 matches\n",
      "et_et002.png-et_et005.png: 618 --> 615 matches\n",
      "filter_FundamentalMatrix: 1242 matches --> 1231 matches\n",
      "et_et002.png-et_et006.png: 1242 --> 1231 matches\n",
      "filter_FundamentalMatrix: 512 matches --> 496 matches\n",
      "et_et002.png-et_et007.png: 512 --> 496 matches\n",
      "filter_FundamentalMatrix: 204 matches --> 197 matches\n",
      "et_et002.png-et_et008.png: 204 --> 197 matches\n",
      "filter_FundamentalMatrix: 2287 matches --> 2274 matches\n",
      "et_et003.png-et_et004.png: 2287 --> 2274 matches\n",
      "filter_FundamentalMatrix: 432 matches --> 427 matches\n",
      "et_et004.png-et_et005.png: 432 --> 427 matches\n",
      "filter_FundamentalMatrix: 650 matches --> 616 matches\n",
      "et_et004.png-et_et006.png: 650 --> 616 matches\n",
      "filter_FundamentalMatrix: 189 matches --> 180 matches\n",
      "et_et004.png-et_et007.png: 189 --> 180 matches\n",
      "filter_FundamentalMatrix: 141 matches --> 132 matches\n",
      "et_et004.png-et_et008.png: 141 --> 132 matches\n",
      "filter_FundamentalMatrix: 3803 matches --> 3798 matches\n",
      "et_et005.png-et_et006.png: 3803 --> 3798 matches\n",
      "filter_FundamentalMatrix: 3835 matches --> 3828 matches\n",
      "et_et005.png-et_et007.png: 3835 --> 3828 matches\n",
      "filter_FundamentalMatrix: 2849 matches --> 2845 matches\n",
      "et_et005.png-et_et008.png: 2849 --> 2845 matches\n",
      "filter_FundamentalMatrix: 4223 matches --> 4223 matches\n",
      "et_et006.png-et_et007.png: 4223 --> 4223 matches\n",
      "filter_FundamentalMatrix: 2369 matches --> 2358 matches\n",
      "et_et006.png-et_et008.png: 2369 --> 2358 matches\n",
      "filter_FundamentalMatrix: 2891 matches --> 2887 matches\n",
      "et_et007.png-et_et008.png: 2891 --> 2887 matches\n",
      "Ensembled pairs : 63 pairs\n",
      "Local feature extracting and matching. Done in 36.5362 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19/19 [00:00<00:00, 69.72it/s]\n",
      " 46%|     | 63/136 [00:00<00:00, 3672.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 2.0125 sec\n",
      "{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=7648, num_observations=35042), 1: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=4703, num_observations=23961)}\n",
      "Reconstruction done in 20.8847 sec\n",
      "Dataset  ETs -> Registered 19 / 22 images with 2 clusters\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n",
      "rotation_detection for 200 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/amy_gardens/peach_0000.png\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n",
      "rotation_detection for 163 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/fbk_vineyard/vineyard_split_1_frame_0900.png\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n",
      "rotation_detection for 54 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_haiper/bike_image_004.png\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_heritage\": 209 images\n",
      "rotation_detection for 209 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/209 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_heritage/cyprus_dsc_6480.png\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_theather_imc2024_church\": 76 images\n",
      "rotation_detection for 76 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_theather_imc2024_church/church_00004.png\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_dioscuri_baalshamin\": 138 images\n",
      "rotation_detection for 138 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_dioscuri_baalshamin/baalshamin_182z.png\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_lizard_pond\": 214 images\n",
      "rotation_detection for 214 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/214 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_lizard_pond/lizard_00003.png\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_brandenburg_british_buckingham\": 225 images\n",
      "rotation_detection for 225 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_brandenburg_british_buckingham/brandenburg_gate_01069771_8567470929.png\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_piazzasanmarco_grandplace\": 168 images\n",
      "rotation_detection for 168 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/168 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_piazzasanmarco_grandplace/grand_place_brussels_00460368_4162644685.png\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_sacrecoeur_trevi_tajmahal\": 225 images\n",
      "rotation_detection for 225 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_sacrecoeur_trevi_tajmahal/sacre_coeur_02928139_3448003521.png\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_stpeters_stpauls\": 200 images\n",
      "rotation_detection for 200 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_stpeters_stpauls/st_pauls_cathedral_00162897_2573777698.png\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n",
      "rotation_detection for 51 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:10<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "60%:  0.2868\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 564. Done in 10.8251 sec\n",
      "Generated 564 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 564. Done in 11.1288 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1430, 2]), descs.shape=torch.Size([1430, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1514, 2]), descs.shape=torch.Size([1514, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([851, 2]), descs.shape=torch.Size([851, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([597, 2]), descs.shape=torch.Size([597, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1017, 2]), descs.shape=torch.Size([1017, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1702, 2]), descs.shape=torch.Size([1702, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([392, 2]), descs.shape=torch.Size([392, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1327, 2]), descs.shape=torch.Size([1327, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1172, 2]), descs.shape=torch.Size([1172, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1025, 2]), descs.shape=torch.Size([1025, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([544, 2]), descs.shape=torch.Size([544, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([238, 2]), descs.shape=torch.Size([238, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1467, 2]), descs.shape=torch.Size([1467, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([806, 2]), descs.shape=torch.Size([806, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1209, 2]), descs.shape=torch.Size([1209, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([885, 2]), descs.shape=torch.Size([885, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2303, 2]), descs.shape=torch.Size([2303, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2208, 2]), descs.shape=torch.Size([2208, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2183, 2]), descs.shape=torch.Size([2183, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1486, 2]), descs.shape=torch.Size([1486, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1257, 2]), descs.shape=torch.Size([1257, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1933, 2]), descs.shape=torch.Size([1933, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([976, 2]), descs.shape=torch.Size([976, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([970, 2]), descs.shape=torch.Size([970, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3646, 2]), descs.shape=torch.Size([3646, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([632, 2]), descs.shape=torch.Size([632, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([440, 2]), descs.shape=torch.Size([440, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1128, 2]), descs.shape=torch.Size([1128, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1541, 2]), descs.shape=torch.Size([1541, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1054, 2]), descs.shape=torch.Size([1054, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1133, 2]), descs.shape=torch.Size([1133, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2563, 2]), descs.shape=torch.Size([2563, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2114, 2]), descs.shape=torch.Size([2114, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2995, 2]), descs.shape=torch.Size([2995, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2111, 2]), descs.shape=torch.Size([2111, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2149, 2]), descs.shape=torch.Size([2149, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2257, 2]), descs.shape=torch.Size([2257, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3735, 2]), descs.shape=torch.Size([3735, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1181, 2]), descs.shape=torch.Size([1181, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1078, 2]), descs.shape=torch.Size([1078, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2382, 2]), descs.shape=torch.Size([2382, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2769, 2]), descs.shape=torch.Size([2769, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3269, 2]), descs.shape=torch.Size([3269, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1897, 2]), descs.shape=torch.Size([1897, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1729, 2]), descs.shape=torch.Size([1729, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1319, 2]), descs.shape=torch.Size([1319, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2404, 2]), descs.shape=torch.Size([2404, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([3395, 2]), descs.shape=torch.Size([3395, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1450, 2]), descs.shape=torch.Size([1450, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([2413, 2]), descs.shape=torch.Size([2413, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/564 [00:00<00:14, 37.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 163 matches @ 1th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 60/564 [00:01<00:14, 34.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 144 matches @ 2th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 76/564 [00:02<00:12, 37.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 141 matches @ 3th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 144/564 [00:04<00:11, 36.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453643106.png-stairs_split_1_1710453963274.png: 131 matches @ 4th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 156/564 [00:04<00:11, 36.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453668718.png: 141 matches @ 5th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 164/564 [00:04<00:12, 33.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 165 matches @ 6th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 201 matches @ 7th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 192/564 [00:05<00:10, 36.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 145 matches @ 8th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 307/564 [00:09<00:11, 23.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 113 matches @ 9th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 329/564 [00:10<00:09, 25.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 266 matches @ 10th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 339/564 [00:10<00:08, 27.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 246 matches @ 11th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|   | 346/564 [00:10<00:07, 29.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 164 matches @ 12th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 366/564 [00:11<00:06, 32.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 115 matches @ 13th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 394/564 [00:12<00:04, 35.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 135 matches @ 14th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 439/564 [00:14<00:06, 19.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 646 matches @ 15th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 475/564 [00:15<00:04, 21.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 431 matches @ 16th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 740 matches @ 17th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 481/564 [00:15<00:04, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 116 matches @ 18th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 110 matches @ 19th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 484/564 [00:16<00:03, 20.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 628 matches @ 20th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 372 matches @ 21th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 489/564 [00:16<00:04, 18.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 661 matches @ 22th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 503/564 [00:17<00:02, 21.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 263 matches @ 23th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 524/564 [00:18<00:02, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 430 matches @ 24th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 530/564 [00:18<00:01, 18.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 171 matches @ 25th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 543/564 [00:18<00:00, 22.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 541 matches @ 26th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 257 matches @ 27th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 549/564 [00:19<00:00, 19.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 146 matches @ 28th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 174 matches @ 29th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 564/564 [00:19<00:00, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  24.6857 sec (aliked+LightGlue)\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([177, 2]), descs.shape=torch.Size([177, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([163, 2]), descs.shape=torch.Size([163, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([117, 2]), descs.shape=torch.Size([117, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([70, 2]), descs.shape=torch.Size([70, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([84, 2]), descs.shape=torch.Size([84, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([94, 2]), descs.shape=torch.Size([94, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([87, 2]), descs.shape=torch.Size([87, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([193, 2]), descs.shape=torch.Size([193, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([122, 2]), descs.shape=torch.Size([122, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([129, 2]), descs.shape=torch.Size([129, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([68, 2]), descs.shape=torch.Size([68, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([4, 2]), descs.shape=torch.Size([4, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([69, 2]), descs.shape=torch.Size([69, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([49, 2]), descs.shape=torch.Size([49, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([122, 2]), descs.shape=torch.Size([122, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([157, 2]), descs.shape=torch.Size([157, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([247, 2]), descs.shape=torch.Size([247, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([136, 2]), descs.shape=torch.Size([136, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([296, 2]), descs.shape=torch.Size([296, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([164, 2]), descs.shape=torch.Size([164, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([208, 2]), descs.shape=torch.Size([208, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([218, 2]), descs.shape=torch.Size([218, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([65, 2]), descs.shape=torch.Size([65, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([97, 2]), descs.shape=torch.Size([97, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([176, 2]), descs.shape=torch.Size([176, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([135, 2]), descs.shape=torch.Size([135, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([55, 2]), descs.shape=torch.Size([55, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([102, 2]), descs.shape=torch.Size([102, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([163, 2]), descs.shape=torch.Size([163, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([191, 2]), descs.shape=torch.Size([191, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([181, 2]), descs.shape=torch.Size([181, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([300, 2]), descs.shape=torch.Size([300, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([255, 2]), descs.shape=torch.Size([255, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([103, 2]), descs.shape=torch.Size([103, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([143, 2]), descs.shape=torch.Size([143, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([206, 2]), descs.shape=torch.Size([206, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([318, 2]), descs.shape=torch.Size([318, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([263, 2]), descs.shape=torch.Size([263, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([137, 2]), descs.shape=torch.Size([137, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([116, 2]), descs.shape=torch.Size([116, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([204, 2]), descs.shape=torch.Size([204, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([300, 2]), descs.shape=torch.Size([300, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([130, 2]), descs.shape=torch.Size([130, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([120, 2]), descs.shape=torch.Size([120, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([170, 2]), descs.shape=torch.Size([170, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([179, 2]), descs.shape=torch.Size([179, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([152, 2]), descs.shape=torch.Size([152, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([140, 2]), descs.shape=torch.Size([140, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([119, 2]), descs.shape=torch.Size([119, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([158, 2]), descs.shape=torch.Size([158, 256])\n",
      "superpoint > rot_k=0, kpts.shape=torch.Size([218, 2]), descs.shape=torch.Size([218, 256])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 333/564 [00:08<00:05, 39.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 54 matches @ 1th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 337/564 [00:08<00:05, 39.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 69 matches @ 2th pair(superpoint+lightglue)\n",
      "superpoint> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 60 matches @ 3th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 396/564 [00:10<00:04, 36.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 51 matches @ 4th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 420/564 [00:10<00:03, 37.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 53 matches @ 5th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 449/564 [00:11<00:02, 38.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 50 matches @ 6th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 479/564 [00:12<00:02, 39.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 51 matches @ 7th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 491/564 [00:12<00:01, 39.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 58 matches @ 8th pair(superpoint+lightglue)\n",
      "superpoint> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 64 matches @ 9th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 503/564 [00:12<00:01, 39.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superpoint> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 75 matches @ 10th pair(superpoint+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 564/564 [00:14<00:00, 39.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  18.9689 sec (superpoint+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4096, 2]), descs.shape=torch.Size([4096, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/564 [00:00<00:59,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 421 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 33/564 [00:03<00:57,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453930259.png: 146 matches @ 2th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 36/564 [00:03<00:57,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453990286.png: 647 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 38/564 [00:04<00:56,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 192 matches @ 4th pair(disk+lightglue)\n",
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 219 matches @ 5th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 44/564 [00:04<00:56,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 207 matches @ 6th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 47/564 [00:05<00:56,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453805788.png: 191 matches @ 7th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 55/564 [00:05<00:55,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453985484.png: 414 matches @ 8th pair(disk+lightglue)\n",
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 283 matches @ 9th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 58/564 [00:06<00:55,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453612890.png-stairs_split_1_1710453616892.png: 199 matches @ 10th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 72/564 [00:07<00:53,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 766 matches @ 11th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 79/564 [00:08<00:53,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 368 matches @ 12th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 99/564 [00:10<00:51,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453620694.png-stairs_split_1_1710453651110.png: 107 matches @ 13th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 128/564 [00:13<00:48,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453626698.png-stairs_split_1_1710453963274.png: 132 matches @ 14th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 139/564 [00:15<00:47,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453643106.png-stairs_split_1_1710453963274.png: 559 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 160/564 [00:17<00:44,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 370 matches @ 16th pair(disk+lightglue)\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453947066.png: 382 matches @ 17th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 166/564 [00:18<00:44,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 182 matches @ 18th pair(disk+lightglue)\n",
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 156 matches @ 19th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 171/564 [00:18<00:43,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 132 matches @ 20th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 174/564 [00:19<00:43,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 368 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 178/564 [00:19<00:43,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 180 matches @ 22th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 186/564 [00:20<00:42,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 130 matches @ 23th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 230/564 [00:25<00:37,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_1_1710453930259.png: 586 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 239/564 [00:26<00:36,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 299 matches @ 25th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 243/564 [00:26<00:36,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 413 matches @ 26th pair(disk+lightglue)\n",
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453678922.png: 118 matches @ 27th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 247/564 [00:27<00:35,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453704934.png: 231 matches @ 28th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 249/564 [00:27<00:35,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453930259.png: 289 matches @ 29th pair(disk+lightglue)\n",
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453947066.png: 111 matches @ 30th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 305/564 [00:33<00:29,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 306 matches @ 31th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 308/564 [00:34<00:29,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 106 matches @ 32th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 326/564 [00:36<00:27,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 859 matches @ 33th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 335/564 [00:37<00:26,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 323 matches @ 34th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 342/564 [00:38<00:25,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 292 matches @ 35th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 355/564 [00:39<00:23,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 170 matches @ 36th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 391/564 [00:43<00:19,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 119 matches @ 37th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 435/564 [00:48<00:14,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 187 matches @ 38th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 438/564 [00:49<00:14,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 993 matches @ 39th pair(disk+lightglue)\n",
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 350 matches @ 40th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 440/564 [00:49<00:14,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453871430.png: 215 matches @ 41th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 463/564 [00:52<00:11,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453756762.png: 179 matches @ 42th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 466/564 [00:52<00:11,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453783374.png: 222 matches @ 43th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 471/564 [00:52<00:10,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 187 matches @ 44th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 473/564 [00:53<00:10,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 540 matches @ 45th pair(disk+lightglue)\n",
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 1067 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 477/564 [00:53<00:10,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 537 matches @ 47th pair(disk+lightglue)\n",
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 201 matches @ 48th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 479/564 [00:53<00:09,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 172 matches @ 49th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 482/564 [00:54<00:09,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 365 matches @ 50th pair(disk+lightglue)\n",
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 1422 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 486/564 [00:54<00:09,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 942 matches @ 52th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 488/564 [00:54<00:08,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 625 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 491/564 [00:55<00:08,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 207 matches @ 54th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 500/564 [00:56<00:07,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 688 matches @ 55th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 505/564 [00:56<00:06,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453862225.png: 104 matches @ 56th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 525/564 [00:59<00:04,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 549 matches @ 57th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 528/564 [00:59<00:04,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 163 matches @ 58th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 543/564 [01:01<00:02,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 549 matches @ 59th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 546/564 [01:01<00:02,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 934 matches @ 60th pair(disk+lightglue)\n",
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 844 matches @ 61th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 548/564 [01:02<00:01,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 163 matches @ 62th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 550/564 [01:02<00:01,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 474 matches @ 63th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 552/564 [01:02<00:01,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453793579.png: 104 matches @ 64th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 564/564 [01:03<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453805788.png-stairs_split_2_1710453871430.png: 210 matches @ 65th pair(disk+lightglue)\n",
      "Features matched in  77.9358 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='564' class='' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [564/564 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_FundamentalMatrix: 584 matches --> 521 matches\n",
      "stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 584 --> 521 matches\n",
      "filter_FundamentalMatrix: 146 matches --> 124 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453930259.png: 146 --> 124 matches\n",
      "filter_FundamentalMatrix: 647 matches --> 573 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453990286.png: 647 --> 573 matches\n",
      "filter_FundamentalMatrix: 192 matches --> 159 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 192 --> 159 matches\n",
      "filter_FundamentalMatrix: 219 matches --> 184 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 219 --> 184 matches\n",
      "filter_FundamentalMatrix: 207 matches --> 183 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 207 --> 183 matches\n",
      "filter_FundamentalMatrix: 191 matches --> 169 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453805788.png: 191 --> 169 matches\n",
      "filter_FundamentalMatrix: 414 matches --> 354 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453985484.png: 414 --> 354 matches\n",
      "filter_FundamentalMatrix: 427 matches --> 385 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 427 --> 385 matches\n",
      "filter_FundamentalMatrix: 199 matches --> 164 matches\n",
      "stairs_split_1_1710453612890.png-stairs_split_1_1710453616892.png: 199 --> 164 matches\n",
      "filter_FundamentalMatrix: 907 matches --> 862 matches\n",
      "stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 907 --> 862 matches\n",
      "filter_FundamentalMatrix: 368 matches --> 303 matches\n",
      "stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 368 --> 303 matches\n",
      "filter_FundamentalMatrix: 107 matches --> 78 matches\n",
      "stairs_split_1_1710453620694.png-stairs_split_1_1710453651110.png: 107 --> 78 matches\n",
      "filter_FundamentalMatrix: 132 matches --> 96 matches\n",
      "stairs_split_1_1710453626698.png-stairs_split_1_1710453963274.png: 132 --> 96 matches\n",
      "filter_FundamentalMatrix: 690 matches --> 643 matches\n",
      "stairs_split_1_1710453643106.png-stairs_split_1_1710453963274.png: 690 --> 643 matches\n",
      "filter_FundamentalMatrix: 141 matches --> 133 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453668718.png: 141 --> 133 matches\n",
      "filter_FundamentalMatrix: 535 matches --> 377 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 535 --> 377 matches\n",
      "filter_FundamentalMatrix: 382 matches --> 344 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453947066.png: 382 --> 344 matches\n",
      "filter_FundamentalMatrix: 201 matches --> 172 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 201 --> 172 matches\n",
      "filter_FundamentalMatrix: 182 matches --> 105 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 182 --> 105 matches\n",
      "filter_FundamentalMatrix: 156 matches --> 109 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 156 --> 109 matches\n",
      "filter_FundamentalMatrix: 132 matches --> 88 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 132 --> 88 matches\n",
      "filter_FundamentalMatrix: 368 matches --> 275 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 368 --> 275 matches\n",
      "filter_FundamentalMatrix: 180 matches --> 105 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453805788.png: 180 --> 105 matches\n",
      "filter_FundamentalMatrix: 275 matches --> 242 matches\n",
      "stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 275 --> 242 matches\n",
      "filter_FundamentalMatrix: 586 matches --> 427 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_1_1710453930259.png: 586 --> 427 matches\n",
      "filter_FundamentalMatrix: 299 matches --> 236 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 299 --> 236 matches\n",
      "filter_FundamentalMatrix: 413 matches --> 354 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 413 --> 354 matches\n",
      "filter_FundamentalMatrix: 118 matches --> 109 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453678922.png: 118 --> 109 matches\n",
      "filter_FundamentalMatrix: 231 matches --> 201 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453704934.png: 231 --> 201 matches\n",
      "filter_FundamentalMatrix: 289 matches --> 247 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453930259.png: 289 --> 247 matches\n",
      "filter_FundamentalMatrix: 111 matches --> 86 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453947066.png: 111 --> 86 matches\n",
      "filter_FundamentalMatrix: 419 matches --> 251 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 419 --> 251 matches\n",
      "filter_FundamentalMatrix: 106 matches --> 91 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 106 --> 91 matches\n",
      "filter_FundamentalMatrix: 1179 matches --> 1035 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 1179 --> 1035 matches\n",
      "filter_FundamentalMatrix: 638 matches --> 529 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 638 --> 529 matches\n",
      "filter_FundamentalMatrix: 516 matches --> 403 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 516 --> 403 matches\n",
      "filter_FundamentalMatrix: 170 matches --> 97 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 170 --> 97 matches\n",
      "filter_FundamentalMatrix: 115 matches --> 63 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 115 --> 63 matches\n",
      "filter_FundamentalMatrix: 305 matches --> 246 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 305 --> 246 matches\n",
      "filter_FundamentalMatrix: 53 matches --> 36 matches\n",
      "stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 53 --> 36 matches\n",
      "filter_FundamentalMatrix: 187 matches --> 153 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 187 --> 153 matches\n",
      "filter_FundamentalMatrix: 1639 matches --> 1144 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 1639 --> 1144 matches\n",
      "filter_FundamentalMatrix: 350 matches --> 310 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 350 --> 310 matches\n",
      "filter_FundamentalMatrix: 215 matches --> 171 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453871430.png: 215 --> 171 matches\n",
      "filter_FundamentalMatrix: 50 matches --> 31 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 50 --> 31 matches\n",
      "filter_FundamentalMatrix: 179 matches --> 155 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453756762.png: 179 --> 155 matches\n",
      "filter_FundamentalMatrix: 222 matches --> 196 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453783374.png: 222 --> 196 matches\n",
      "filter_FundamentalMatrix: 187 matches --> 166 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 187 --> 166 matches\n",
      "filter_FundamentalMatrix: 1022 matches --> 892 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 1022 --> 892 matches\n",
      "filter_FundamentalMatrix: 1807 matches --> 1686 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 1807 --> 1686 matches\n",
      "filter_FundamentalMatrix: 537 matches --> 494 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 537 --> 494 matches\n",
      "filter_FundamentalMatrix: 317 matches --> 209 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 317 --> 209 matches\n",
      "filter_FundamentalMatrix: 282 matches --> 191 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 282 --> 191 matches\n",
      "filter_FundamentalMatrix: 365 matches --> 281 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 365 --> 281 matches\n",
      "filter_FundamentalMatrix: 2050 matches --> 1936 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 2050 --> 1936 matches\n",
      "filter_FundamentalMatrix: 1372 matches --> 1035 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 1372 --> 1035 matches\n",
      "filter_FundamentalMatrix: 1350 matches --> 1240 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 1350 --> 1240 matches\n",
      "filter_FundamentalMatrix: 207 matches --> 100 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 207 --> 100 matches\n",
      "filter_FundamentalMatrix: 1026 matches --> 905 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 1026 --> 905 matches\n",
      "filter_FundamentalMatrix: 104 matches --> 82 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453862225.png: 104 --> 82 matches\n",
      "filter_FundamentalMatrix: 979 matches --> 604 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 979 --> 604 matches\n",
      "filter_FundamentalMatrix: 334 matches --> 206 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 334 --> 206 matches\n",
      "filter_FundamentalMatrix: 1090 matches --> 1016 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 1090 --> 1016 matches\n",
      "filter_FundamentalMatrix: 1191 matches --> 1130 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 1191 --> 1130 matches\n",
      "filter_FundamentalMatrix: 990 matches --> 769 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 990 --> 769 matches\n",
      "filter_FundamentalMatrix: 163 matches --> 124 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 163 --> 124 matches\n",
      "filter_FundamentalMatrix: 648 matches --> 490 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 648 --> 490 matches\n",
      "filter_FundamentalMatrix: 104 matches --> 77 matches\n",
      "stairs_split_2_1710453790978.png-stairs_split_2_1710453793579.png: 104 --> 77 matches\n",
      "filter_FundamentalMatrix: 210 matches --> 117 matches\n",
      "stairs_split_2_1710453805788.png-stairs_split_2_1710453871430.png: 210 --> 117 matches\n",
      "Ensembled pairs : 70 pairs\n",
      "Local feature extracting and matching. Done in 124.6738 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:01<00:00, 20.69it/s]\n",
      " 17%|        | 70/406 [00:00<00:00, 3927.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 0.8103 sec\n",
      "{0: Reconstruction(num_reg_images=19, num_cameras=19, num_points3D=1968, num_observations=5454), 1: Reconstruction(num_reg_images=5, num_cameras=5, num_points3D=483, num_observations=1243)}\n",
      "Reconstruction done in 24.6999 sec\n",
      "Dataset  stairs -> Registered 24 / 51 images with 2 clusters\n",
      "\n",
      "Results\n",
      "Dataset  ETs -> Registered 19 / 22 images with 2 clusters\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "Dataset  stairs -> Registered 24 / 51 images with 2 clusters\n",
      "\n",
      "Timings\n",
      "rotation_detection -> total=0.00 sec.\n",
      "global feature extraction -> total=0.00 sec.\n",
      "shortlisting -> total=27.60 sec.\n",
      "feature_detection -> total=0.00 sec.\n",
      "feature_matching -> total=154.17 sec.\n",
      "RANSAC -> total=2.82 sec.\n",
      "Reconstruction -> total=45.58 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    'rotation_detection':[],\n",
    "    \"global feature extraction\":[],\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "# Load DINOv2 model (for feature extraction, not global descriptor here)\n",
    "print(\"Loading DINOv2 model for patch feature extraction...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = dino_model.eval().to(device)\n",
    "print(\"DINOv2 model loaded.\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    # print (f\"Extracting on device {device}\")\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "        # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "        try:\n",
    "            # --- Pipeline Execution ---\n",
    "            \n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            # if CONFIG.ROTATION_CORRECTION:\n",
    "            #     rots = exec_rotation_detection(images, device)\n",
    "            # else:\n",
    "            #     rots = [ 0 for fname in images ]\n",
    "            rots = [ 0 for fname in images ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print(f'rotation_detection for {len(images)} images : {t:.4f} sec')\n",
    "            # print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            # 1. Detect ALIKED features and combine with DINO patch features\n",
    "            t = time()\n",
    "            index_pairs = get_image_pairs_shortlist(\n",
    "                images,\n",
    "                sim_th = 0.3, # should be strict\n",
    "                min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "                max_pairs = 20,\n",
    "                exhaustive_if_less = 20,\n",
    "                device=device\n",
    "            )\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            # print(\"\\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\")\n",
    "            # detect_aliked_and_combine_with_dino(\n",
    "            #     img_fnames=images,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_features=4096,\n",
    "            #     resize_to=1024,\n",
    "            #     dino_processor=dino_processor,\n",
    "            #     dino_model=dino_model,\n",
    "            #     dino_patch_size=14, # Adjust based on your DINO model's patch size (e.g., 14 for DINOv2 base)\n",
    "            #     device=device\n",
    "            # )\n",
    "            # timings['global feature extraction'].append(time() - t)\n",
    "            # print (f'Gloabl feature extracting. Done in {time() - t:.4f} sec')\n",
    "            # gc.collect()\n",
    "            \n",
    "            # # 2. Get image pairs shortlist using VLAD global descriptors\n",
    "            # print(\"\\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\")\n",
    "            # # Adjust num_clusters_vlad as needed (e.g., 64, 128, 256)\n",
    "            # # Higher clusters mean higher dimensionality for global descriptor.\n",
    "            # index_pairs = get_image_pairs_shortlist_vlad(\n",
    "            #     fnames=images,\n",
    "            #     sim_th=0.5,\n",
    "            #     min_pairs=20,\n",
    "            #     exhaustive_if_less=20,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_clusters_vlad=128, # Example: 128 clusters for VLAD\n",
    "            #     device=device\n",
    "            # )\n",
    "            # index_pairs = get_img_pairs_exhaustive(images)\n",
    "            \n",
    "            print(f\"Generated {len(index_pairs)} image pairs using VLAD global descriptor.\")\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################    \n",
    "            t=time()\n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                images, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "            print (f'Local feature extracting and matching. Done in {time() - t:.4f} sec')\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            future = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                feature_dir, images_dir)\n",
    "            maps, local_timings = future.result()\n",
    "            #  timings\n",
    "            for k in local_timings:\n",
    "                timings[k].extend(local_timings[k])\n",
    "            # clear_output(wait=False)\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():  # cur_map: image_name  {'R': list, 't': list}\n",
    "                for image_name, pose in cur_map.items():\n",
    "                    idx = filename_to_index[image_name]\n",
    "                    pred = predictions[idx]\n",
    "                    pred.cluster_index = map_index\n",
    "                    pred.rotation = np.array(pose['R'])  # convert back to np.ndarray\n",
    "                    pred.translation = np.array(pose['t'])\n",
    "                    registered += 1\n",
    "            mapping_result_str = f\"Dataset  {dataset} -> Registered {registered} / {len(images)} images with {len(maps)} clusters\"\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # raise e\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "360930c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:37:26.327778Z",
     "iopub.status.busy": "2025-05-29T19:37:26.327438Z",
     "iopub.status.idle": "2025-05-29T19:37:26.532111Z",
     "shell.execute_reply": "2025-05-29T19:37:26.531102Z"
    },
    "papermill": {
     "duration": 0.275855,
     "end_time": "2025-05-29T19:37:26.533824",
     "exception": false,
     "start_time": "2025-05-29T19:37:26.257969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster1,another_et_another_et001.png,0.999847144;-0.004206955;0.016970287;0.003822927;0.999737307;0.022598728;-0.017060901;-0.022530397;0.999600574,-2.823085985;-1.953759354;2.694931965\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster1,another_et_another_et002.png,0.999995952;-0.001857948;0.002154998;0.001860490;0.999997575;-0.001178317;-0.002152804;0.001182322;0.999996984,-2.668391472;-1.174043189;1.165395657\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster1,another_et_another_et003.png,0.997754091;-0.041273316;0.052756878;0.044374807;0.997269099;-0.059035766;-0.050176203;0.061244253;0.996860818,-2.872838366;0.328530188;-0.581364601\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster1,another_et_another_et004.png,0.999183659;-0.011150151;0.038828988;0.005946753;0.991281160;0.131629399;-0.039958132;-0.131291038;0.990538243,-2.781226305;-1.442236309;-0.134958842\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster1,another_et_another_et005.png,0.995138511;0.002649189;0.098449612;-0.009110886;0.997827805;0.065243110;-0.098062919;-0.065822894;0.993001012,-3.306806181;-2.130780048;1.292139552\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster1,another_et_another_et006.png,0.921666127;0.195454727;-0.335155188;-0.220791379;0.974547562;-0.038835764;0.319034038;0.109792984;0.941362196,-0.600455817;-0.733104902;1.037253059\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster1,another_et_another_et007.png,0.784140526;0.267084225;-0.560169307;-0.312631899;0.949752676;0.015203647;0.536082953;0.163204999;0.828238611,1.094358123;-0.440858032;0.579256480\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster1,another_et_another_et008.png,0.572773155;0.314769759;-0.756869151;-0.395881423;0.914736932;0.080834675;0.717780476;0.253330504;0.648548259,2.788897458;-0.762050074;1.366134147\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster1,another_et_another_et009.png,0.322986054;0.349946252;-0.879327942;-0.492904253;0.855365102;0.159361036;0.807914232;0.381953091;0.448760995,4.446436479;-1.084273437;2.013500027\r\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                #  `rotation` is a list of lists, flatten it\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()  # flatten 3x3 list -> 9 elems\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                #  `translation` is a flat list\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "\n",
    "# Preview the output\n",
    "!head {submission_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7672f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T19:37:26.716061Z",
     "iopub.status.busy": "2025-05-29T19:37:26.715742Z",
     "iopub.status.idle": "2025-05-29T19:37:26.720714Z",
     "shell.execute_reply": "2025-05-29T19:37:26.719737Z"
    },
    "papermill": {
     "duration": 0.077516,
     "end_time": "2025-05-29T19:37:26.722210",
     "exception": false,
     "start_time": "2025-05-29T19:37:26.644694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd223b2",
   "metadata": {
    "papermill": {
     "duration": 0.06866,
     "end_time": "2025-05-29T19:37:26.865108",
     "exception": false,
     "start_time": "2025-05-29T19:37:26.796448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "isSourceIdPinned": false,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7505602,
     "sourceId": 11938492,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7542297,
     "sourceId": 11991336,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 278.933093,
   "end_time": "2025-05-29T19:37:29.951936",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-29T19:32:51.018843",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
