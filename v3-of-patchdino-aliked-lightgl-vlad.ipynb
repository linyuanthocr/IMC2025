{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affcbb02",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006713,
     "end_time": "2025-05-28T16:32:34.618709",
     "exception": false,
     "start_time": "2025-05-28T16:32:34.611996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69dfbe13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:32:34.631597Z",
     "iopub.status.busy": "2025-05-28T16:32:34.631352Z",
     "iopub.status.idle": "2025-05-28T16:32:40.206589Z",
     "shell.execute_reply": "2025-05-28T16:32:40.205337Z"
    },
    "papermill": {
     "duration": 5.583285,
     "end_time": "2025-05-28T16:32:40.208267",
     "exception": false,
     "start_time": "2025-05-28T16:32:34.624982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78176488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:32:40.223068Z",
     "iopub.status.busy": "2025-05-28T16:32:40.222768Z",
     "iopub.status.idle": "2025-05-28T16:32:41.022257Z",
     "shell.execute_reply": "2025-05-28T16:32:41.020923Z"
    },
    "papermill": {
     "duration": 0.808627,
     "end_time": "2025-05-28T16:32:41.023989",
     "exception": false,
     "start_time": "2025-05-28T16:32:40.215362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/depth-save.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a5407",
   "metadata": {
    "papermill": {
     "duration": 0.006499,
     "end_time": "2025-05-28T16:32:41.037433",
     "exception": false,
     "start_time": "2025-05-28T16:32:41.030934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483ad87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:32:41.050876Z",
     "iopub.status.busy": "2025-05-28T16:32:41.050601Z",
     "iopub.status.idle": "2025-05-28T16:32:41.054908Z",
     "shell.execute_reply": "2025-05-28T16:32:41.054205Z"
    },
    "papermill": {
     "duration": 0.012581,
     "end_time": "2025-05-28T16:32:41.056220",
     "exception": false,
     "start_time": "2025-05-28T16:32:41.043639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f6274b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:32:41.070027Z",
     "iopub.status.busy": "2025-05-28T16:32:41.069809Z",
     "iopub.status.idle": "2025-05-28T16:33:06.743873Z",
     "shell.execute_reply": "2025-05-28T16:33:06.743143Z"
    },
    "papermill": {
     "duration": 25.68289,
     "end_time": "2025-05-28T16:33:06.745507",
     "exception": false,
     "start_time": "2025-05-28T16:32:41.062617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# from lightglue import DISK\n",
    "from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\n",
    "from scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint,DISK, DoGHardNet, LightGlue, SIFT\n",
    "from fastprogress import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e67ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:06.760940Z",
     "iopub.status.busy": "2025-05-28T16:33:06.760416Z",
     "iopub.status.idle": "2025-05-28T16:33:06.763955Z",
     "shell.execute_reply": "2025-05-28T16:33:06.763318Z"
    },
    "papermill": {
     "duration": 0.012245,
     "end_time": "2025-05-28T16:33:06.765208",
     "exception": false,
     "start_time": "2025-05-28T16:33:06.752963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e636be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:06.779742Z",
     "iopub.status.busy": "2025-05-28T16:33:06.779517Z",
     "iopub.status.idle": "2025-05-28T16:33:06.899141Z",
     "shell.execute_reply": "2025-05-28T16:33:06.898328Z"
    },
    "papermill": {
     "duration": 0.128174,
     "end_time": "2025-05-28T16:33:06.900437",
     "exception": false,
     "start_time": "2025-05-28T16:33:06.772263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5716bafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:06.915322Z",
     "iopub.status.busy": "2025-05-28T16:33:06.915049Z",
     "iopub.status.idle": "2025-05-28T16:33:06.918881Z",
     "shell.execute_reply": "2025-05-28T16:33:06.917948Z"
    },
    "papermill": {
     "duration": 0.012703,
     "end_time": "2025-05-28T16:33:06.920249",
     "exception": false,
     "start_time": "2025-05-28T16:33:06.907546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dadb73de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:06.934073Z",
     "iopub.status.busy": "2025-05-28T16:33:06.933868Z",
     "iopub.status.idle": "2025-05-28T16:33:06.936849Z",
     "shell.execute_reply": "2025-05-28T16:33:06.936223Z"
    },
    "papermill": {
     "duration": 0.01121,
     "end_time": "2025-05-28T16:33:06.938077",
     "exception": false,
     "start_time": "2025-05-28T16:33:06.926867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5f0a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:06.952344Z",
     "iopub.status.busy": "2025-05-28T16:33:06.952128Z",
     "iopub.status.idle": "2025-05-28T16:33:06.959739Z",
     "shell.execute_reply": "2025-05-28T16:33:06.959131Z"
    },
    "papermill": {
     "duration": 0.015975,
     "end_time": "2025-05-28T16:33:06.960925",
     "exception": false,
     "start_time": "2025-05-28T16:33:06.944950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 5,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = False\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.1,\n",
    "        \"min_matches\" : 30,\n",
    "        \"resize_to\" : 2048,\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.3,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96afd2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:06.975042Z",
     "iopub.status.busy": "2025-05-28T16:33:06.974823Z",
     "iopub.status.idle": "2025-05-28T16:33:06.995030Z",
     "shell.execute_reply": "2025-05-28T16:33:06.994414Z"
    },
    "papermill": {
     "duration": 0.028846,
     "end_time": "2025-05-28T16:33:06.996160",
     "exception": false,
     "start_time": "2025-05-28T16:33:06.967314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume these are available from your environment or previous code\n",
    "# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n",
    "# from kornia.feature import ALIKED # Already in your detect_aliked\n",
    "# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n",
    "# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n",
    "# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n",
    "\n",
    "# --- Helper function for image loading (if not already defined) ---\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "def get_dino_patch_features_for_keypoints(img_path, keypoints_xy, dino_processor, dino_model, patch_size=16, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Extracts DINO patch features corresponding to given ALIKED keypoint locations.\n",
    "    It correctly infers the DINO patch grid dimensions from the processed input.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        keypoints_xy (torch.Tensor): Nx2 tensor of (x, y) keypoint coordinates in image pixel space.\n",
    "                                     These keypoints are assumed to be in the original image's coordinate system.\n",
    "        dino_processor: HuggingFace AutoImageProcessor for DINO.\n",
    "        dino_model: HuggingFace AutoModel for DINO.\n",
    "        patch_size (int): The patch size used by the DINO model (e.g., 14 or 16).\n",
    "        device (torch.device): Device to run the models on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: NxD_dino tensor of DINO patch features for each keypoint.\n",
    "                      Returns None if no keypoints or image loading fails.\n",
    "    \"\"\"\n",
    "    if len(keypoints_xy) == 0:\n",
    "        dino_feature_dim = dino_model.config.hidden_size # Get actual DINO hidden size\n",
    "        return torch.empty((0, dino_feature_dim), device=device)\n",
    "\n",
    "    # 1. Load the original image (ALIKED processed this size)\n",
    "    original_img = load_torch_image(img_path, device=device)\n",
    "    original_h, original_w = original_img.shape[-2], original_img.shape[-1]\n",
    "\n",
    "\n",
    "    # 2. Process the image with DINO's processor\n",
    "    #    This step performs resizing, padding, etc., as needed by the DINO model\n",
    "    with torch.inference_mode():\n",
    "        # dino_processor returns a BatchFeature object which includes pixel_values\n",
    "        # and potentially other information like `pixel_mask`\n",
    "        inputs = dino_processor(images=original_img, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "        # Get the actual dimensions of the image as processed by the DINO model\n",
    "        # This is the crucial part: the actual H and W that produced `patch_tokens`\n",
    "        # We can infer this from the `pixel_values` shape\n",
    "        processed_h = inputs['pixel_values'].shape[-2]\n",
    "        processed_w = inputs['pixel_values'].shape[-1]\n",
    "\n",
    "        # Extract patch tokens (excluding the CLS token)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:].squeeze(0) # Shape: (num_patches, hidden_size)\n",
    "\n",
    "        # Calculate the actual grid dimensions based on the *processed* image size\n",
    "        # and the model's patch size.\n",
    "        # This should perfectly match the number of patch_tokens if the model is well-behaved.\n",
    "        num_patches_h = processed_h // patch_size\n",
    "        num_patches_w = processed_w // patch_size\n",
    "\n",
    "        # Safety check: ensure calculated grid matches actual token count\n",
    "        expected_token_count = num_patches_h * num_patches_w\n",
    "        if patch_tokens.shape[0] != expected_token_count:\n",
    "            # This indicates a deeper issue with how the model's output tokens\n",
    "            # map to the spatial grid, or an unexpected patch size/model behavior.\n",
    "            # Some models might have slightly different patch token arrangements.\n",
    "            # DINOv2 typically aligns well.\n",
    "            raise ValueError(\n",
    "                f\"DINO patch token count ({patch_tokens.shape[0]}) does not match \"\n",
    "                f\"expected grid dimensions ({num_patches_h}x{num_patches_w} = {expected_token_count}) \"\n",
    "                f\"for processed image size {processed_w}x{processed_h} with patch size {patch_size}. \"\n",
    "                f\"Please verify DINO model and processor configuration.\"\n",
    "            )\n",
    "\n",
    "        # Reshape patch tokens into a 2D grid\n",
    "        patch_features_grid = patch_tokens.reshape(num_patches_h, num_patches_w, -1)\n",
    "        dino_feature_dim = patch_features_grid.shape[-1] # Actual feature dimension\n",
    "\n",
    "\n",
    "    dino_features_for_kpts = torch.zeros((len(keypoints_xy), dino_feature_dim), device=device)\n",
    "\n",
    "    # 3. Rescale ALIKED keypoints to the DINO *processed* image dimensions\n",
    "    #    ALIKED keypoints are in original_w x original_h coordinates.\n",
    "    #    DINO patches correspond to processed_w x processed_h coordinates.\n",
    "    scale_x = processed_w / original_w\n",
    "    scale_y = processed_h / original_h\n",
    "\n",
    "    scaled_keypoints_xy = keypoints_xy.clone()\n",
    "    scaled_keypoints_xy[:, 0] *= scale_x\n",
    "    scaled_keypoints_xy[:, 1] *= scale_y\n",
    "\n",
    "    # 4. Map scaled keypoints to DINO patch grid indices\n",
    "    keypoint_cols = (scaled_keypoints_xy[:, 0] / patch_size).long()\n",
    "    keypoint_rows = (scaled_keypoints_xy[:, 1] / patch_size).long()\n",
    "\n",
    "    # Clip indices to ensure they are within bounds of the patch grid\n",
    "    keypoint_rows = torch.clamp(keypoint_rows, 0, num_patches_h - 1)\n",
    "    keypoint_cols = torch.clamp(keypoint_cols, 0, num_patches_w - 1)\n",
    "\n",
    "    # Gather DINO features for each keypoint's corresponding patch\n",
    "    dino_features_for_kpts = patch_features_grid[keypoint_rows, keypoint_cols]\n",
    "\n",
    "    return dino_features_for_kpts\n",
    "\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=VERBOSE\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    # extractor = extractor_class(max_num_keypoints=num_features, detection_threshold=detection_threshold, \n",
    "    #                             resize=resize_to).eval().to(device, dtype)\n",
    "    if model_name == 'disk':\n",
    "        extractor = DISK(\n",
    "            max_num_keypoints=num_features,\n",
    "            detection_threshold=detection_threshold,\n",
    "            resize=resize_to\n",
    "        ).to(device).eval()\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        extractor.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        extractor_class = dict_model[model_name]\n",
    "        extractor = extractor_class(\n",
    "            max_num_keypoints=num_features,\n",
    "            detection_threshold=detection_threshold,\n",
    "            resize=resize_to\n",
    "        ).to(device, dtype).eval()\n",
    "\n",
    "    \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f5c0eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.009850Z",
     "iopub.status.busy": "2025-05-28T16:33:07.009641Z",
     "iopub.status.idle": "2025-05-28T16:33:07.035236Z",
     "shell.execute_reply": "2025-05-28T16:33:07.034627Z"
    },
    "papermill": {
     "duration": 0.033968,
     "end_time": "2025-05-28T16:33:07.036580",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.002612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    verbose = VERBOSE\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                if verbose:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    if verbose:\n",
    "                        print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978b2d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.051503Z",
     "iopub.status.busy": "2025-05-28T16:33:07.051290Z",
     "iopub.status.idle": "2025-05-28T16:33:07.053866Z",
     "shell.execute_reply": "2025-05-28T16:33:07.053327Z"
    },
    "papermill": {
     "duration": 0.011545,
     "end_time": "2025-05-28T16:33:07.054922",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.043377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5871cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.068954Z",
     "iopub.status.busy": "2025-05-28T16:33:07.068756Z",
     "iopub.status.idle": "2025-05-28T16:33:07.075856Z",
     "shell.execute_reply": "2025-05-28T16:33:07.075109Z"
    },
    "papermill": {
     "duration": 0.015325,
     "end_time": "2025-05-28T16:33:07.077005",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.061680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: Detect ALIKED and Combine with DINO Patch Features ---\n",
    "def detect_aliked_and_combine_with_dino(img_fnames,\n",
    "                                        feature_dir='.featureout',\n",
    "                                        num_features=4096,\n",
    "                                        resize_to=1024,\n",
    "                                        dino_processor=None,\n",
    "                                        dino_model=None,\n",
    "                                        dino_patch_size=16, # Typically 14 or 16 for DINO\n",
    "                                        device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    aliked_extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    aliked_extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='w') as f_desc_aliked, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='w') as f_desc_combined: # New HDF5 for combined features\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = aliked_extractor.extract(image0)\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy() # ALIKED keypoints (x,y)\n",
    "                descs_aliked = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy() # ALIKED descriptors\n",
    "\n",
    "                # Get DINO patch features for these keypoints\n",
    "                kpts_torch = torch.from_numpy(kpts).to(device)\n",
    "                descs_dino_patch = get_dino_patch_features_for_keypoints(\n",
    "                    img_path, kpts_torch, dino_processor, dino_model, dino_patch_size, device\n",
    "                ).detach().cpu().numpy()\n",
    "\n",
    "                # Concatenate ALIKED and DINO features\n",
    "                if len(descs_aliked) > 0 and len(descs_dino_patch) > 0:\n",
    "                    combined_descs = np.concatenate((descs_aliked, descs_dino_patch), axis=1)\n",
    "                elif len(descs_aliked) > 0: # Only ALIKED if no DINO features (shouldn't happen often)\n",
    "                    combined_descs = descs_aliked\n",
    "                else: # No features found\n",
    "                    combined_descs = np.array([]) # Empty array\n",
    "\n",
    "                f_kp[key] = kpts\n",
    "                f_desc_aliked[key] = descs_aliked # Keep ALIKED descriptors for debugging or other uses\n",
    "                f_desc_combined[key] = combined_descs # Store the new combined descriptors\n",
    "    print(f\"Combined features saved to {feature_dir}/descriptors_combined.h5\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172b32d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.090754Z",
     "iopub.status.busy": "2025-05-28T16:33:07.090500Z",
     "iopub.status.idle": "2025-05-28T16:33:07.434895Z",
     "shell.execute_reply": "2025-05-28T16:33:07.433956Z"
    },
    "papermill": {
     "duration": 0.353101,
     "end_time": "2025-05-28T16:33:07.436592",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.083491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans # MiniBatchKMeans is faster for large datasets\n",
    "\n",
    "# --- VLAD Aggregation Function ---\n",
    "def vlad_encode(descriptors, centroids):\n",
    "    \"\"\"\n",
    "    Performs VLAD encoding.\n",
    "\n",
    "    Args:\n",
    "        descriptors (np.ndarray): NxM array of local descriptors.\n",
    "        centroids (np.ndarray): KxM array of K-Means cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1x(K*M) VLAD descriptor.\n",
    "    \"\"\"\n",
    "    if descriptors.shape[0] == 0:\n",
    "        return np.zeros(centroids.shape[0] * centroids.shape[1], dtype=np.float32)\n",
    "\n",
    "    num_descriptors, desc_dim = descriptors.shape\n",
    "    num_centroids, _ = centroids.shape\n",
    "\n",
    "    # Assign each descriptor to its nearest centroid\n",
    "    # Using cdist for efficiency\n",
    "    distances = np.sqrt(np.sum((descriptors[:, None, :] - centroids[None, :, :])**2, axis=2))\n",
    "    # distances = cdist(descriptors, centroids, 'sqeuclidean') # Could use cdist for sqeuclidean\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Initialize VLAD accumulator\n",
    "    vlad_accumulator = np.zeros((num_centroids, desc_dim), dtype=np.float32)\n",
    "\n",
    "    # Accumulate residuals\n",
    "    for i in range(num_descriptors):\n",
    "        cluster_idx = cluster_assignments[i]\n",
    "        residual = descriptors[i] - centroids[cluster_idx]\n",
    "        vlad_accumulator[cluster_idx] += residual\n",
    "\n",
    "    # Flatten and L2 normalize\n",
    "    vlad_descriptor = vlad_accumulator.flatten()\n",
    "    vlad_descriptor = F.normalize(torch.from_numpy(vlad_descriptor).unsqueeze(0), dim=1, p=2).squeeze(0).numpy()\n",
    "\n",
    "    return vlad_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0561a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.451323Z",
     "iopub.status.busy": "2025-05-28T16:33:07.451034Z",
     "iopub.status.idle": "2025-05-28T16:33:07.458805Z",
     "shell.execute_reply": "2025-05-28T16:33:07.458045Z"
    },
    "papermill": {
     "duration": 0.01623,
     "end_time": "2025-05-28T16:33:07.459924",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.443694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- NEW: Get Global Descriptors using K-Means + VLAD ---\n",
    "def get_global_desc_vlad(fnames, feature_dir='.featureout', num_clusters=64, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Generates global descriptors for images using K-Means + VLAD on combined ALIKED+DINO features.\n",
    "\n",
    "    Args:\n",
    "        fnames (list): List of image file paths.\n",
    "        feature_dir (str): Directory where combined descriptors are stored.\n",
    "        num_clusters (int): Number of clusters for K-Means (K in VLAD).\n",
    "        device (torch.device): Not directly used for VLAD computation, but passed for consistency.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Nx(K*M) tensor of global VLAD descriptors.\n",
    "    \"\"\"\n",
    "    all_local_descs = []\n",
    "    keys_order = [] # To maintain order of descriptors with respect to fnames\n",
    "\n",
    "    # 1. Load all combined local descriptors\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Loading combined local descriptors for K-Means\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                if descs.shape[0] > 0:\n",
    "                    all_local_descs.append(descs)\n",
    "                    keys_order.append(key)\n",
    "\n",
    "    if not all_local_descs:\n",
    "        print(\"No combined local descriptors found. Cannot train K-Means or compute VLAD.\")\n",
    "        return torch.empty((0, num_clusters * 0), dtype=torch.float32) # Return empty tensor\n",
    "\n",
    "    # Concatenate all descriptors for K-Means training\n",
    "    all_local_descs_flat = np.concatenate(all_local_descs, axis=0)\n",
    "\n",
    "    # 2. Train K-Means on a subset of descriptors if the dataset is too large\n",
    "    # Or directly on all_local_descs_flat if memory permits\n",
    "    print(f\"Training K-Means with {num_clusters} clusters on {all_local_descs_flat.shape[0]} descriptors...\")\n",
    "    # Use MiniBatchKMeans for efficiency\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, n_init='auto', batch_size=256).fit(all_local_descs_flat)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    print(\"K-Means training complete.\")\n",
    "\n",
    "    # 3. Compute VLAD descriptor for each image\n",
    "    global_descs_vlad = []\n",
    "    # Re-iterate through original fnames to match the output order\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Computing VLAD descriptors\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                vlad_desc = vlad_encode(descs, centroids)\n",
    "                global_descs_vlad.append(torch.from_numpy(vlad_desc).unsqueeze(0))\n",
    "            else:\n",
    "                # Handle cases where an image might not have any combined descriptors\n",
    "                # (e.g., no ALIKED keypoints detected). Append a zero vector of correct size.\n",
    "                print(f\"Warning: No combined descriptors for {key}. Appending zero VLAD descriptor.\")\n",
    "                # Determine descriptor dimension from centroids\n",
    "                desc_dim_per_cluster = centroids.shape[1] if centroids.shape[1] > 0 else 0 # Should not be 0 normally\n",
    "                zero_vlad = np.zeros(num_clusters * desc_dim_per_cluster, dtype=np.float32)\n",
    "                global_descs_vlad.append(torch.from_numpy(zero_vlad).unsqueeze(0))\n",
    "\n",
    "\n",
    "    if not global_descs_vlad:\n",
    "        return torch.empty((0, num_clusters * centroids.shape[1] if centroids.shape[1] > 0 else 0), dtype=torch.float32)\n",
    "\n",
    "    global_descs_vlad = torch.cat(global_descs_vlad, dim=0)\n",
    "    return global_descs_vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfff06c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.474055Z",
     "iopub.status.busy": "2025-05-28T16:33:07.473819Z",
     "iopub.status.idle": "2025-05-28T16:33:07.481401Z",
     "shell.execute_reply": "2025-05-28T16:33:07.480644Z"
    },
    "papermill": {
     "duration": 0.016005,
     "end_time": "2025-05-28T16:33:07.482618",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.466613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- RE-DEFINED: get_image_pairs_shortlist to use the new VLAD global descriptor ---\n",
    "def get_image_pairs_shortlist_vlad(fnames,\n",
    "                                   sim_th=0.6, # should be strict\n",
    "                                   min_pairs=30,\n",
    "                                   exhaustive_if_less=20,\n",
    "                                   feature_dir='.featureout', # Pass feature_dir\n",
    "                                   num_clusters_vlad=64, # New parameter for VLAD\n",
    "                                   device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames) # You need to define get_img_pairs_exhaustive if not done.\n",
    "\n",
    "    # Use the new VLAD-based global descriptor\n",
    "    descs = get_global_desc_vlad(fnames, feature_dir=feature_dir, num_clusters=num_clusters_vlad, device=device)\n",
    "\n",
    "    if descs.shape[0] == 0:\n",
    "        print(\"No global descriptors generated. Returning empty matching list.\")\n",
    "        return []\n",
    "\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 60)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = set() # Use a set for faster lookup of already added pairs\n",
    "\n",
    "    for st_idx in range(num_imgs - 1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold: # Ensure distance is not effectively infinite\n",
    "                pair = tuple(sorted((st_idx, idx.item())))\n",
    "                if pair not in already_there_set:\n",
    "                    matching_list.append(pair)\n",
    "                    already_there_set.add(pair)\n",
    "                    total += 1\n",
    "    matching_list = sorted(list(matching_list)) # Sort the list of tuples\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17669863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.496519Z",
     "iopub.status.busy": "2025-05-28T16:33:07.496317Z",
     "iopub.status.idle": "2025-05-28T16:33:07.499574Z",
     "shell.execute_reply": "2025-05-28T16:33:07.498976Z"
    },
    "papermill": {
     "duration": 0.011453,
     "end_time": "2025-05-28T16:33:07.500710",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.489257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e68237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.514717Z",
     "iopub.status.busy": "2025-05-28T16:33:07.514506Z",
     "iopub.status.idle": "2025-05-28T16:33:07.524045Z",
     "shell.execute_reply": "2025-05-28T16:33:07.523410Z"
    },
    "papermill": {
     "duration": 0.017896,
     "end_time": "2025-05-28T16:33:07.525422",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.507526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th=0.6,\n",
    "                              min_pairs=30,\n",
    "                              max_pairs=100,  # 每张图像最多匹配 max_pairs 个\n",
    "                              exhaustive_if_less=20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # 上三角分析（排除重复）\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "\n",
    "    threshold = np.percentile(dm_flat, 50) + np.sqrt(3) * dm_flat.std()\n",
    "    mask = dm <= np.percentile(dm_flat, 30)\n",
    "\n",
    "    ar = np.arange(num_imgs)\n",
    "    matching_set = set()\n",
    "\n",
    "    for st_idx in range(num_imgs):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "\n",
    "        # 保证每张图像至少有 min_pairs 个\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        # 按距离排序，选出前 max_pairs\n",
    "        sorted_matches = sorted(\n",
    "            [(idx, dm[st_idx, idx]) for idx in to_match if idx != st_idx and dm[st_idx, idx] < threshold],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        for idx, _ in sorted_matches[:max_pairs]:\n",
    "            pair = tuple(sorted((st_idx, idx)))\n",
    "            matching_set.add(pair)\n",
    "\n",
    "    matching_list = sorted(list(matching_set))\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "394f8597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.539954Z",
     "iopub.status.busy": "2025-05-28T16:33:07.539708Z",
     "iopub.status.idle": "2025-05-28T16:33:07.559476Z",
     "shell.execute_reply": "2025-05-28T16:33:07.558685Z"
    },
    "papermill": {
     "duration": 0.028207,
     "end_time": "2025-05-28T16:33:07.560633",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.532426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f1156bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.574859Z",
     "iopub.status.busy": "2025-05-28T16:33:07.574609Z",
     "iopub.status.idle": "2025-05-28T16:33:07.581236Z",
     "shell.execute_reply": "2025-05-28T16:33:07.580482Z"
    },
    "papermill": {
     "duration": 0.014999,
     "end_time": "2025-05-28T16:33:07.582488",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.567489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(feature_dir, img_dir):\n",
    "    result = {}\n",
    "    local_timings = {'RANSAC': [], 'Reconstruction': []}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(\"colmap database\")\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # options = pycolmap.SiftMatchingOptions()\n",
    "    # options.confidence = 0.9999\n",
    "    # options.max_num_trials = 20000\n",
    "    # pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    print(\"matching done!!!!\")\n",
    "    local_timings['RANSAC'].append(time() - t)\n",
    "    print(f'RANSAC in {local_timings[\"RANSAC\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # mapper_options.mapper.filter_max_reproj_error\t = 1.0\n",
    "    # mapper_options.mapper.init_max_error = 2.0\n",
    "    mapper_options.min_model_size = 5\n",
    "    mapper_options.max_num_models = 25\n",
    "    mapper_options.ba_global_images_freq = 5\n",
    "    # mapper_options.ba_local_num_images = 8\n",
    "    mapper_options.mapper.abs_pose_min_inlier_ratio = 0.4\n",
    "    mapper_options.ba_global_max_num_iterations = 100\n",
    "    mapper_options.mapper.filter_max_reproj_error = 6.0\n",
    "    mapper_options.mapper.max_reg_trials = 10\n",
    "\n",
    "    \n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, \n",
    "                                        output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    for map_index, rec in maps.items():\n",
    "        result[map_index] = {}\n",
    "        for img_id, image in rec.images.items():\n",
    "            result[map_index][image.name] = {\n",
    "                'R': image.cam_from_world.rotation.matrix().tolist(),\n",
    "                't': image.cam_from_world.translation.tolist()\n",
    "            }\n",
    "    # clear_output(wait=False)\n",
    "    local_timings['Reconstruction'].append(time() - t)\n",
    "    print(f'Reconstruction done in {local_timings[\"Reconstruction\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    return result, local_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3781070b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.596797Z",
     "iopub.status.busy": "2025-05-28T16:33:07.596572Z",
     "iopub.status.idle": "2025-05-28T16:33:07.754602Z",
     "shell.execute_reply": "2025-05-28T16:33:07.753676Z"
    },
    "papermill": {
     "duration": 0.166475,
     "end_time": "2025-05-28T16:33:07.755979",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.589504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06c158ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.770512Z",
     "iopub.status.busy": "2025-05-28T16:33:07.770242Z",
     "iopub.status.idle": "2025-05-28T16:33:07.785383Z",
     "shell.execute_reply": "2025-05-28T16:33:07.784605Z"
    },
    "papermill": {
     "duration": 0.024018,
     "end_time": "2025-05-28T16:33:07.786907",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.762889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_keypoints_and_matches(images_input, unified_kp_path, remapped_matches_path, feature_dir='visualization_output'):\n",
    "    output_dir = os.path.join(feature_dir, 'visualization_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load images and determine image_keys for HDF5 lookup\n",
    "    if isinstance(images_input[0], str):\n",
    "        loaded_images = [cv2.imread(img_path) for img_path in images_input]\n",
    "        image_keys = [os.path.basename(img_path) for img_path in images_input]\n",
    "    else:\n",
    "        loaded_images = images_input\n",
    "        # If images_input are already arrays, you need to provide the corresponding keys\n",
    "        # This part is crucial: image_keys MUST align with the HDF5 keys\n",
    "        image_keys = image_keys_in_h5 # Use the predefined list for the dummy case\n",
    "\n",
    "    # Load unified keypoints\n",
    "    keypoints_data = {}\n",
    "    with h5py.File(unified_kp_path, 'r') as f_kp:\n",
    "        for img_name_raw in f_kp.keys():\n",
    "            img_name = img_name_raw.decode('utf-8') if isinstance(img_name_raw, bytes) else img_name_raw\n",
    "            keypoints_data[img_name] = f_kp[img_name_raw][()] # Access with raw key if bytes\n",
    "\n",
    "    # Load remapped matches - CORRECTED LOGIC\n",
    "    # Store (img1_key, img2_key) directly with matches for robust iteration\n",
    "    matches_data_pairs = [] # Will store (img1_key, img2_key, matches_array)\n",
    "    with h5py.File(remapped_matches_path, 'r') as f_matches:\n",
    "        print(\"\\n--- Loading remapped matches from HDF5 ---\")\n",
    "        for img1_group_key_candidate in tqdm(f_matches.keys(), desc=\"Loading matches\"):\n",
    "            img1_key = img1_group_key_candidate.decode('utf-8') if isinstance(img1_group_key_candidate, bytes) else img1_group_key_candidate\n",
    "\n",
    "            img1_group = f_matches[img1_group_key_candidate] # Access with raw key\n",
    "\n",
    "            if isinstance(img1_group, h5py.Group):\n",
    "                for img2_dataset_key_candidate in img1_group.keys():\n",
    "                    img2_key = img2_dataset_key_candidate.decode('utf-8') if isinstance(img2_dataset_key_candidate, bytes) else img2_dataset_key_candidate\n",
    "\n",
    "                    try:\n",
    "                        matches_array = img1_group[img2_dataset_key_candidate][()]\n",
    "                        matches_data_pairs.append((img1_key, img2_key, matches_array))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading matches for pair ({img1_key}, {img2_key}): {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Expected '{img1_key}' to be a group, but found {type(img1_group)}. Skipping its contents.\")\n",
    "\n",
    "\n",
    "    # --- Drawing Keypoints ---\n",
    "    print(\"\\n--- Drawing Keypoints ---\")\n",
    "    for i, img_key in enumerate(image_keys):\n",
    "        if img_key in keypoints_data:\n",
    "            img = loaded_images[i].copy()\n",
    "            kpts = keypoints_data[img_key]\n",
    "\n",
    "            for kp in kpts:\n",
    "                x, y = int(kp[0]), int(kp[1])\n",
    "                cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Green circle for keypoint\n",
    "\n",
    "            output_kp_path = os.path.join(output_dir, f\"keypoints_{img_key}\")\n",
    "            if len(img.shape) == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(output_kp_path, img)\n",
    "            print(f\"Keypoints drawn on {img_key}, saved to {output_kp_path}\")\n",
    "        else:\n",
    "            print(f\"No keypoints found for {img_key} in unified keypoints file.\")\n",
    "\n",
    "    # --- Drawing Matches ---\n",
    "    print(\"\\n--- Drawing Matches ---\")\n",
    "    # Iterate through the (img1_key, img2_key, matches) tuples directly\n",
    "    for img_name1, img_name2, matches in matches_data_pairs:\n",
    "        # We no longer need to split img_pair_key, as we have img_name1 and img_name2 directly\n",
    "\n",
    "        # Find the actual image objects and their keypoints using image_keys list\n",
    "        try:\n",
    "            img1_idx = image_keys.index(img_name1)\n",
    "            img2_idx = image_keys.index(img_name2)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: One or both image names not found in the provided 'images' list/keys.\")\n",
    "            continue\n",
    "\n",
    "        img1 = loaded_images[img1_idx].copy()\n",
    "        img2 = loaded_images[img2_idx].copy()\n",
    "\n",
    "        kpts1 = keypoints_data.get(img_name1)\n",
    "        kpts2 = keypoints_data.get(img_name2)\n",
    "\n",
    "        if kpts1 is None or kpts2 is None:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: keypoints not found for one or both images in unified keypoints.\")\n",
    "            continue\n",
    "        if len(matches) == 0:\n",
    "            print(f\"No matches to draw for {img_name1}-{img_name2}.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure images are 3 channels for drawing lines\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "        if len(img2.shape) == 2:\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Create a concatenated image for drawing matches\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        max_h = max(h1, h2)\n",
    "        matched_img = np.zeros((max_h, w1 + w2, 3), dtype=np.uint8)\n",
    "        matched_img[0:h1, 0:w1] = img1\n",
    "        matched_img[0:h2, w1:w1+w2] = img2\n",
    "\n",
    "        num_matches_to_draw = min(len(matches), 200) # Draw up to 200 matches to avoid clutter, adjust as needed\n",
    "\n",
    "        for i in range(num_matches_to_draw):\n",
    "            match = matches[i]\n",
    "            kp1_idx, kp2_idx = int(match[0]), int(match[1])\n",
    "\n",
    "            # Bounds check for keypoint indices\n",
    "            if kp1_idx >= len(kpts1) or kp2_idx >= len(kpts2):\n",
    "                # print(f\"Warning: Match index out of bounds for {img_name1}-{img_name2}. Skipping match {kp1_idx}-{kp2_idx}.\")\n",
    "                continue\n",
    "\n",
    "            pt1 = tuple(map(int, kpts1[kp1_idx][:2]))\n",
    "            pt2 = tuple(map(int, kpts2[kp2_idx][:2]))\n",
    "\n",
    "            # Draw circles on the concatenated image\n",
    "            cv2.circle(matched_img, pt1, 5, (0, 0, 255), 2) # Red circle on img1 side\n",
    "            cv2.circle(matched_img, (pt2[0] + w1, pt2[1]), 5, (255, 0, 0), 2) # Blue circle on img2 side\n",
    "\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            cv2.line(matched_img, pt1, (pt2[0] + w1, pt2[1]), color, 1)\n",
    "\n",
    "        output_match_path = os.path.join(output_dir, f\"matches_{img_name1}_{img_name2}.png\")\n",
    "        cv2.imwrite(output_match_path, matched_img)\n",
    "        print(f\"Matches drawn between {img_name1} and {img_name2}, saved to {output_match_path}\")\n",
    "\n",
    "\n",
    "# Example call (replace with your actual 'images' list)\n",
    "# If your 'images' are file paths:\n",
    "# images_file_paths = ['path/to/your/image1.jpg', 'path/to/your/image2.jpg', ...]\n",
    "# draw_keypoints_and_matches(images_file_paths, unified_kp_path, remapped_matches_path)\n",
    "\n",
    "# If your 'images' are loaded numpy arrays (as in the dummy example above):\n",
    "# draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7d1af06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T16:33:07.801043Z",
     "iopub.status.busy": "2025-05-28T16:33:07.800834Z",
     "iopub.status.idle": "2025-05-28T17:31:27.788780Z",
     "shell.execute_reply": "2025-05-28T17:31:27.787666Z"
    },
    "papermill": {
     "duration": 3499.996731,
     "end_time": "2025-05-28T17:31:27.790284",
     "exception": false,
     "start_time": "2025-05-28T16:33:07.793553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model for patch feature extraction...\n",
      "DINOv2 model loaded.\n",
      "Skipping \"imc2023_haiper\"\n",
      "Skipping \"imc2023_heritage\"\n",
      "Skipping \"imc2023_theather_imc2024_church\"\n",
      "Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Skipping \"imc2024_lizard_pond\"\n",
      "Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Skipping \"pt_stpeters_stpauls\"\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n",
      "rotation_detection for 200 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:27<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1448\n",
      "Max:  0.4239\n",
      "Mean: 0.2737\n",
      "Std:  0.0388\n",
      "20%:  0.2375\n",
      "25%:  0.2439\n",
      "60%:  0.2863\n",
      "75%:  0.3017\n",
      "Shortlisting. Number of pairs to match: 3256. Done in 27.5055 sec\n",
      "Generated 3256 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 3256. Done in 27.8075 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256/3256 [09:48<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  601.9272 sec (aliked+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='3256' class='' max='3256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3256/3256 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled pairs : 783 pairs\n",
      "Local feature extracting and matching. Done in 612.4980 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:04<00:00, 39.16it/s]\n",
      "  6%|▌         | 783/13203 [00:00<00:03, 4002.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 17.8851 sec\n",
      "{0: Reconstruction(num_reg_images=2, num_cameras=2, num_points3D=151, num_observations=302), 1: Reconstruction(num_reg_images=71, num_cameras=71, num_points3D=32451, num_observations=156718), 2: Reconstruction(num_reg_images=74, num_cameras=74, num_points3D=21532, num_observations=74357), 3: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=3607, num_observations=11086), 4: Reconstruction(num_reg_images=10, num_cameras=10, num_points3D=4694, num_observations=16282), 5: Reconstruction(num_reg_images=7, num_cameras=7, num_points3D=1197, num_observations=3098)}\n",
      "Reconstruction done in 567.4742 sec\n",
      "Dataset  amy_gardens -> Registered 175 / 200 images with 6 clusters\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n",
      "rotation_detection for 163 images : 0.0001 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:10<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1510\n",
      "Max:  0.3338\n",
      "Mean: 0.2232\n",
      "Std:  0.0303\n",
      "20%:  0.1980\n",
      "25%:  0.2014\n",
      "60%:  0.2244\n",
      "75%:  0.2396\n",
      "Shortlisting. Number of pairs to match: 2704. Done in 10.6527 sec\n",
      "Generated 2704 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 2704. Done in 10.9486 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2704/2704 [06:55<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  423.3112 sec (aliked+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2704' class='' max='2704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2704/2704 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled pairs : 1265 pairs\n",
      "Local feature extracting and matching. Done in 432.2835 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:01<00:00, 92.53it/s]\n",
      " 10%|█         | 1265/12090 [00:00<00:02, 4735.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 11.9120 sec\n",
      "{0: Reconstruction(num_reg_images=162, num_cameras=162, num_points3D=61720, num_observations=196341)}\n",
      "Reconstruction done in 1657.7254 sec\n",
      "Dataset  fbk_vineyard -> Registered 162 / 163 images with 1 clusters\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n",
      "rotation_detection for 22 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:01<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "25%:  0.2547\n",
      "60%:  0.2913\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 224. Done in 1.7189 sec\n",
      "Generated 224 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 224. Done in 2.0199 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:07<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  9.7227 sec (aliked+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='224' class='' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [224/224 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled pairs : 139 pairs\n",
      "Local feature extracting and matching. Done in 11.3457 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 87.13it/s]\n",
      " 66%|██████▌   | 139/210 [00:00<00:00, 5038.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 3.6739 sec\n",
      "{0: Reconstruction(num_reg_images=22, num_cameras=22, num_points3D=4416, num_observations=20378)}\n",
      "Reconstruction done in 32.4562 sec\n",
      "Dataset  ETs -> Registered 22 / 22 images with 1 clusters\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n",
      "rotation_detection for 51 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:10<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "60%:  0.2868\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 620. Done in 10.4727 sec\n",
      "Generated 620 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 620. Done in 10.7696 sec\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:20<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  25.6588 sec (aliked+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='620' class='' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [620/620 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled pairs : 219 pairs\n",
      "Local feature extracting and matching. Done in 27.3003 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:02<00:00, 21.88it/s]\n",
      " 21%|██        | 219/1035 [00:00<00:00, 4993.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 4.7156 sec\n",
      "{0: Reconstruction(num_reg_images=14, num_cameras=14, num_points3D=915, num_observations=2338), 1: Reconstruction(num_reg_images=20, num_cameras=20, num_points3D=800, num_observations=2603), 2: Reconstruction(num_reg_images=6, num_cameras=6, num_points3D=314, num_observations=743)}\n",
      "Reconstruction done in 49.0652 sec\n",
      "Dataset  stairs -> Registered 40 / 51 images with 3 clusters\n",
      "\n",
      "Results\n",
      "Dataset  amy_gardens -> Registered 175 / 200 images with 6 clusters\n",
      "Dataset  fbk_vineyard -> Registered 162 / 163 images with 1 clusters\n",
      "Dataset  ETs -> Registered 22 / 22 images with 1 clusters\n",
      "Dataset  stairs -> Registered 40 / 51 images with 3 clusters\n",
      "\n",
      "Timings\n",
      "rotation_detection -> total=0.00 sec.\n",
      "global feature extraction -> total=0.00 sec.\n",
      "shortlisting -> total=101.90 sec.\n",
      "feature_detection -> total=0.00 sec.\n",
      "feature_matching -> total=1060.62 sec.\n",
      "RANSAC -> total=38.19 sec.\n",
      "Reconstruction -> total=2306.72 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    'rotation_detection':[],\n",
    "    \"global feature extraction\":[],\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "# Load DINOv2 model (for feature extraction, not global descriptor here)\n",
    "print(\"Loading DINOv2 model for patch feature extraction...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = dino_model.eval().to(device)\n",
    "print(\"DINOv2 model loaded.\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    # print (f\"Extracting on device {device}\")\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "        # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "        try:\n",
    "            # --- Pipeline Execution ---\n",
    "            \n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            # if CONFIG.ROTATION_CORRECTION:\n",
    "            #     rots = exec_rotation_detection(images, device)\n",
    "            # else:\n",
    "            #     rots = [ 0 for fname in images ]\n",
    "            rots = [ 0 for fname in images ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print(f'rotation_detection for {len(images)} images : {t:.4f} sec')\n",
    "            # print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            # 1. Detect ALIKED features and combine with DINO patch features\n",
    "            t = time()\n",
    "            index_pairs = get_image_pairs_shortlist(\n",
    "                images,\n",
    "                sim_th = 0.3, # should be strict\n",
    "                min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "                max_pairs = 25,\n",
    "                exhaustive_if_less = 20,\n",
    "                device=device\n",
    "            )\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            # print(\"\\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\")\n",
    "            # detect_aliked_and_combine_with_dino(\n",
    "            #     img_fnames=images,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_features=4096,\n",
    "            #     resize_to=1024,\n",
    "            #     dino_processor=dino_processor,\n",
    "            #     dino_model=dino_model,\n",
    "            #     dino_patch_size=14, # Adjust based on your DINO model's patch size (e.g., 14 for DINOv2 base)\n",
    "            #     device=device\n",
    "            # )\n",
    "            # timings['global feature extraction'].append(time() - t)\n",
    "            # print (f'Gloabl feature extracting. Done in {time() - t:.4f} sec')\n",
    "            # gc.collect()\n",
    "            \n",
    "            # # 2. Get image pairs shortlist using VLAD global descriptors\n",
    "            # print(\"\\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\")\n",
    "            # # Adjust num_clusters_vlad as needed (e.g., 64, 128, 256)\n",
    "            # # Higher clusters mean higher dimensionality for global descriptor.\n",
    "            # index_pairs = get_image_pairs_shortlist_vlad(\n",
    "            #     fnames=images,\n",
    "            #     sim_th=0.5,\n",
    "            #     min_pairs=20,\n",
    "            #     exhaustive_if_less=20,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_clusters_vlad=128, # Example: 128 clusters for VLAD\n",
    "            #     device=device\n",
    "            # )\n",
    "            # index_pairs = get_img_pairs_exhaustive(images)\n",
    "            \n",
    "            print(f\"Generated {len(index_pairs)} image pairs using VLAD global descriptor.\")\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################    \n",
    "            t=time()\n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                images, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "            print (f'Local feature extracting and matching. Done in {time() - t:.4f} sec')\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            future = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                feature_dir, images_dir)\n",
    "            maps, local_timings = future.result()\n",
    "            # 合并 timings（主进程里）\n",
    "            for k in local_timings:\n",
    "                timings[k].extend(local_timings[k])\n",
    "            # clear_output(wait=False)\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():  # cur_map: image_name → {'R': list, 't': list}\n",
    "                for image_name, pose in cur_map.items():\n",
    "                    idx = filename_to_index[image_name]\n",
    "                    pred = predictions[idx]\n",
    "                    pred.cluster_index = map_index\n",
    "                    pred.rotation = np.array(pose['R'])  # convert back to np.ndarray\n",
    "                    pred.translation = np.array(pose['t'])\n",
    "                    registered += 1\n",
    "            mapping_result_str = f\"Dataset  {dataset} -> Registered {registered} / {len(images)} images with {len(maps)} clusters\"\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # raise e\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4875527a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T17:31:28.472409Z",
     "iopub.status.busy": "2025-05-28T17:31:28.472074Z",
     "iopub.status.idle": "2025-05-28T17:31:28.652267Z",
     "shell.execute_reply": "2025-05-28T17:31:28.651206Z"
    },
    "papermill": {
     "duration": 0.484621,
     "end_time": "2025-05-28T17:31:28.653762",
     "exception": false,
     "start_time": "2025-05-28T17:31:28.169141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                # ✅ `rotation` is a list of lists, flatten it\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()  # flatten 3x3 list -> 9 elems\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                # ✅ `translation` is a flat list\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "\n",
    "# Preview the output\n",
    "!head {submission_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d8eae67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T17:31:29.276000Z",
     "iopub.status.busy": "2025-05-28T17:31:29.275678Z",
     "iopub.status.idle": "2025-05-28T17:33:18.028741Z",
     "shell.execute_reply": "2025-05-28T17:33:18.027790Z"
    },
    "papermill": {
     "duration": 109.279209,
     "end_time": "2025-05-28T17:33:18.252073",
     "exception": false,
     "start_time": "2025-05-28T17:31:28.972864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=100.00%)\n",
      "amy_gardens: score=13.71% (mAA=7.36%, clusterness=100.00%)\n",
      "fbk_vineyard: score=17.66% (mAA=12.01%, clusterness=33.33%)\n",
      "ETs: score=28.40% (mAA=21.15%, clusterness=43.18%)\n",
      "stairs: score=0.00% (mAA=0.00%, clusterness=68.75%)\n",
      "Average over all datasets: score=4.60% (mAA=3.12%, clusterness=88.10%)\n",
      "Computed metric in: 108.75 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362423f7",
   "metadata": {
    "papermill": {
     "duration": 0.303769,
     "end_time": "2025-05-28T17:33:18.858768",
     "exception": false,
     "start_time": "2025-05-28T17:33:18.554999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7505602,
     "sourceId": 11938492,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3650.907423,
   "end_time": "2025-05-28T17:33:22.617911",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T16:32:31.710488",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
