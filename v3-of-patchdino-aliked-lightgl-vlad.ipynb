{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630af33b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006761,
     "end_time": "2025-05-28T23:58:00.850634",
     "exception": false,
     "start_time": "2025-05-28T23:58:00.843873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c17c4bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:00.864453Z",
     "iopub.status.busy": "2025-05-28T23:58:00.864192Z",
     "iopub.status.idle": "2025-05-28T23:58:07.195390Z",
     "shell.execute_reply": "2025-05-28T23:58:07.194151Z"
    },
    "papermill": {
     "duration": 6.339293,
     "end_time": "2025-05-28T23:58:07.197123",
     "exception": false,
     "start_time": "2025-05-28T23:58:00.857830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b8e700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:07.211804Z",
     "iopub.status.busy": "2025-05-28T23:58:07.211524Z",
     "iopub.status.idle": "2025-05-28T23:58:08.638233Z",
     "shell.execute_reply": "2025-05-28T23:58:08.637076Z"
    },
    "papermill": {
     "duration": 1.435598,
     "end_time": "2025-05-28T23:58:08.640015",
     "exception": false,
     "start_time": "2025-05-28T23:58:07.204417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/disk-depth/disk_lightglue.pth /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/depth-save.pth\n",
    "!cp /kaggle/input/disk-depth/depth-save.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2aff244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:08.654743Z",
     "iopub.status.busy": "2025-05-28T23:58:08.654415Z",
     "iopub.status.idle": "2025-05-28T23:58:08.658792Z",
     "shell.execute_reply": "2025-05-28T23:58:08.658012Z"
    },
    "papermill": {
     "duration": 0.012882,
     "end_time": "2025-05-28T23:58:08.659997",
     "exception": false,
     "start_time": "2025-05-28T23:58:08.647115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/root/.cache/torch/hub/checkpoints/depth-save.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1af4aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:08.673593Z",
     "iopub.status.busy": "2025-05-28T23:58:08.673396Z",
     "iopub.status.idle": "2025-05-28T23:58:31.614632Z",
     "shell.execute_reply": "2025-05-28T23:58:31.613773Z"
    },
    "papermill": {
     "duration": 22.949807,
     "end_time": "2025-05-28T23:58:31.616404",
     "exception": false,
     "start_time": "2025-05-28T23:58:08.666597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# from lightglue import DISK\n",
    "from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher\n",
    "from scipy.spatial import cKDTree # For efficient nearest neighbor search to remove duplicate keypoints\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint,DISK, DoGHardNet, LightGlue, SIFT\n",
    "from fastprogress import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585bdcc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.632779Z",
     "iopub.status.busy": "2025-05-28T23:58:31.632255Z",
     "iopub.status.idle": "2025-05-28T23:58:31.635807Z",
     "shell.execute_reply": "2025-05-28T23:58:31.635091Z"
    },
    "papermill": {
     "duration": 0.01211,
     "end_time": "2025-05-28T23:58:31.637098",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.624988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e11d48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.651444Z",
     "iopub.status.busy": "2025-05-28T23:58:31.651128Z",
     "iopub.status.idle": "2025-05-28T23:58:31.766400Z",
     "shell.execute_reply": "2025-05-28T23:58:31.765376Z"
    },
    "papermill": {
     "duration": 0.124222,
     "end_time": "2025-05-28T23:58:31.767999",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.643777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae9c20b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.787354Z",
     "iopub.status.busy": "2025-05-28T23:58:31.787106Z",
     "iopub.status.idle": "2025-05-28T23:58:31.791403Z",
     "shell.execute_reply": "2025-05-28T23:58:31.790531Z"
    },
    "papermill": {
     "duration": 0.013147,
     "end_time": "2025-05-28T23:58:31.792913",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.779766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65ea9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.814357Z",
     "iopub.status.busy": "2025-05-28T23:58:31.814088Z",
     "iopub.status.idle": "2025-05-28T23:58:31.817488Z",
     "shell.execute_reply": "2025-05-28T23:58:31.816636Z"
    },
    "papermill": {
     "duration": 0.013712,
     "end_time": "2025-05-28T23:58:31.819053",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.805341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22185dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.840938Z",
     "iopub.status.busy": "2025-05-28T23:58:31.840599Z",
     "iopub.status.idle": "2025-05-28T23:58:31.853385Z",
     "shell.execute_reply": "2025-05-28T23:58:31.852436Z"
    },
    "papermill": {
     "duration": 0.02339,
     "end_time": "2025-05-28T23:58:31.854912",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.831522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 5,\n",
    "        \"filter_threshold\" : 2,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = True\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.3,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":1.0\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.3,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 2048,\n",
    "        \"match_confidence_threshold\":1.0\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f600c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.870254Z",
     "iopub.status.busy": "2025-05-28T23:58:31.870032Z",
     "iopub.status.idle": "2025-05-28T23:58:31.892512Z",
     "shell.execute_reply": "2025-05-28T23:58:31.891890Z"
    },
    "papermill": {
     "duration": 0.031513,
     "end_time": "2025-05-28T23:58:31.893779",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.862266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume these are available from your environment or previous code\n",
    "# from .utils import load_torch_image # Assuming load_torch_image is defined elsewhere\n",
    "# from kornia.feature import ALIKED # Already in your detect_aliked\n",
    "# from kornia.feature import LightGlueMatcher as KF_LightGlueMatcher # Already in your match_with_lightglue\n",
    "# from kornia.geometry import laf_from_center_scale_ori # Already in your match_with_lightglue\n",
    "# from colmap_database import COLMAPDatabase, add_keypoints, add_matches # Already in your colmap_import\n",
    "\n",
    "# --- Helper function for image loading (if not already defined) ---\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "def get_dino_patch_features_for_keypoints(img_path, keypoints_xy, dino_processor, dino_model, patch_size=16, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Extracts DINO patch features corresponding to given ALIKED keypoint locations.\n",
    "    It correctly infers the DINO patch grid dimensions from the processed input.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        keypoints_xy (torch.Tensor): Nx2 tensor of (x, y) keypoint coordinates in image pixel space.\n",
    "                                     These keypoints are assumed to be in the original image's coordinate system.\n",
    "        dino_processor: HuggingFace AutoImageProcessor for DINO.\n",
    "        dino_model: HuggingFace AutoModel for DINO.\n",
    "        patch_size (int): The patch size used by the DINO model (e.g., 14 or 16).\n",
    "        device (torch.device): Device to run the models on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: NxD_dino tensor of DINO patch features for each keypoint.\n",
    "                      Returns None if no keypoints or image loading fails.\n",
    "    \"\"\"\n",
    "    if len(keypoints_xy) == 0:\n",
    "        dino_feature_dim = dino_model.config.hidden_size # Get actual DINO hidden size\n",
    "        return torch.empty((0, dino_feature_dim), device=device)\n",
    "\n",
    "    # 1. Load the original image (ALIKED processed this size)\n",
    "    original_img = load_torch_image(img_path, device=device)\n",
    "    original_h, original_w = original_img.shape[-2], original_img.shape[-1]\n",
    "\n",
    "\n",
    "    # 2. Process the image with DINO's processor\n",
    "    #    This step performs resizing, padding, etc., as needed by the DINO model\n",
    "    with torch.inference_mode():\n",
    "        # dino_processor returns a BatchFeature object which includes pixel_values\n",
    "        # and potentially other information like `pixel_mask`\n",
    "        inputs = dino_processor(images=original_img, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "        # Get the actual dimensions of the image as processed by the DINO model\n",
    "        # This is the crucial part: the actual H and W that produced `patch_tokens`\n",
    "        # We can infer this from the `pixel_values` shape\n",
    "        processed_h = inputs['pixel_values'].shape[-2]\n",
    "        processed_w = inputs['pixel_values'].shape[-1]\n",
    "\n",
    "        # Extract patch tokens (excluding the CLS token)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:].squeeze(0) # Shape: (num_patches, hidden_size)\n",
    "\n",
    "        # Calculate the actual grid dimensions based on the *processed* image size\n",
    "        # and the model's patch size.\n",
    "        # This should perfectly match the number of patch_tokens if the model is well-behaved.\n",
    "        num_patches_h = processed_h // patch_size\n",
    "        num_patches_w = processed_w // patch_size\n",
    "\n",
    "        # Safety check: ensure calculated grid matches actual token count\n",
    "        expected_token_count = num_patches_h * num_patches_w\n",
    "        if patch_tokens.shape[0] != expected_token_count:\n",
    "            # This indicates a deeper issue with how the model's output tokens\n",
    "            # map to the spatial grid, or an unexpected patch size/model behavior.\n",
    "            # Some models might have slightly different patch token arrangements.\n",
    "            # DINOv2 typically aligns well.\n",
    "            raise ValueError(\n",
    "                f\"DINO patch token count ({patch_tokens.shape[0]}) does not match \"\n",
    "                f\"expected grid dimensions ({num_patches_h}x{num_patches_w} = {expected_token_count}) \"\n",
    "                f\"for processed image size {processed_w}x{processed_h} with patch size {patch_size}. \"\n",
    "                f\"Please verify DINO model and processor configuration.\"\n",
    "            )\n",
    "\n",
    "        # Reshape patch tokens into a 2D grid\n",
    "        patch_features_grid = patch_tokens.reshape(num_patches_h, num_patches_w, -1)\n",
    "        dino_feature_dim = patch_features_grid.shape[-1] # Actual feature dimension\n",
    "\n",
    "\n",
    "    dino_features_for_kpts = torch.zeros((len(keypoints_xy), dino_feature_dim), device=device)\n",
    "\n",
    "    # 3. Rescale ALIKED keypoints to the DINO *processed* image dimensions\n",
    "    #    ALIKED keypoints are in original_w x original_h coordinates.\n",
    "    #    DINO patches correspond to processed_w x processed_h coordinates.\n",
    "    scale_x = processed_w / original_w\n",
    "    scale_y = processed_h / original_h\n",
    "\n",
    "    scaled_keypoints_xy = keypoints_xy.clone()\n",
    "    scaled_keypoints_xy[:, 0] *= scale_x\n",
    "    scaled_keypoints_xy[:, 1] *= scale_y\n",
    "\n",
    "    # 4. Map scaled keypoints to DINO patch grid indices\n",
    "    keypoint_cols = (scaled_keypoints_xy[:, 0] / patch_size).long()\n",
    "    keypoint_rows = (scaled_keypoints_xy[:, 1] / patch_size).long()\n",
    "\n",
    "    # Clip indices to ensure they are within bounds of the patch grid\n",
    "    keypoint_rows = torch.clamp(keypoint_rows, 0, num_patches_h - 1)\n",
    "    keypoint_cols = torch.clamp(keypoint_cols, 0, num_patches_w - 1)\n",
    "\n",
    "    # Gather DINO features for each keypoint's corresponding patch\n",
    "    dino_features_for_kpts = patch_features_grid[keypoint_rows, keypoint_cols]\n",
    "\n",
    "    return dino_features_for_kpts\n",
    "\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,\n",
    "                  match_confidence_threshold = 0.0,\n",
    "                  verbose=VERBOSE\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    # extractor = extractor_class(max_num_keypoints=num_features, detection_threshold=detection_threshold, \n",
    "    #                             resize=resize_to).eval().to(device, dtype)\n",
    "    # if model_name == 'disk':\n",
    "    #     extractor = DISK(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device).eval()\n",
    "    #     checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    #     extractor.load_state_dict(checkpoint['model'])\n",
    "    # else:\n",
    "    #     extractor_class = dict_model[model_name]\n",
    "    #     extractor = extractor_class(\n",
    "    #         max_num_keypoints=num_features,\n",
    "    #         detection_threshold=detection_threshold,\n",
    "    #         resize=resize_to\n",
    "    #     ).to(device, dtype).eval()\n",
    "\n",
    "    extractor_class = dict_model[model_name]\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features,\n",
    "        detection_threshold=detection_threshold,\n",
    "        resize=resize_to\n",
    "    ).to(device, dtype).eval()\n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            if verbose:\n",
    "                print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            len1 = len(idxs)\n",
    "            n_matches = len1\n",
    "            # if len(idxs) >= min_matches:                \n",
    "            #     conf = dists.cpu().numpy()  # lower is better\n",
    "            #     if conf.ndim == 2:\n",
    "            #         conf = conf[:, 0]  # force (N,)\n",
    "            #     conf_mask = conf <= match_confidence_threshold\n",
    "            #     if not np.any(conf_mask):\n",
    "            #         continue\n",
    "            #     idxs = idxs[conf_mask]\n",
    "            #     conf = conf[conf_mask]\n",
    "            #     n_matches = len(idxs)\n",
    "            #     if verbose:\n",
    "            #         print(f\"match after conf threshold: {key1}-{key2}: {len1}->{n_matches}\")\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                if verbose:\n",
    "                    print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                pass\n",
    "                # if verbose:\n",
    "                #     print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "    match_confidence_threshold = 0.0\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "        match_confidence_threshold = match_confidence_threshold\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24dd8d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.908320Z",
     "iopub.status.busy": "2025-05-28T23:58:31.908075Z",
     "iopub.status.idle": "2025-05-28T23:58:31.937184Z",
     "shell.execute_reply": "2025-05-28T23:58:31.935929Z"
    },
    "papermill": {
     "duration": 0.038288,
     "end_time": "2025-05-28T23:58:31.938917",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.900629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "    verbose = VERBOSE\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                # if verbose:\n",
    "                #     print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    if verbose:\n",
    "                        print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "                # print(f\"KKKKKKK KKKKKK {k1} - {k2}: {len(match)} matches\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10694c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.954403Z",
     "iopub.status.busy": "2025-05-28T23:58:31.954138Z",
     "iopub.status.idle": "2025-05-28T23:58:31.957270Z",
     "shell.execute_reply": "2025-05-28T23:58:31.956439Z"
    },
    "papermill": {
     "duration": 0.012349,
     "end_time": "2025-05-28T23:58:31.958596",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.946247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57623d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.973410Z",
     "iopub.status.busy": "2025-05-28T23:58:31.973193Z",
     "iopub.status.idle": "2025-05-28T23:58:31.980458Z",
     "shell.execute_reply": "2025-05-28T23:58:31.979613Z"
    },
    "papermill": {
     "duration": 0.015885,
     "end_time": "2025-05-28T23:58:31.981653",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.965768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: Detect ALIKED and Combine with DINO Patch Features ---\n",
    "def detect_aliked_and_combine_with_dino(img_fnames,\n",
    "                                        feature_dir='.featureout',\n",
    "                                        num_features=4096,\n",
    "                                        resize_to=1024,\n",
    "                                        dino_processor=None,\n",
    "                                        dino_model=None,\n",
    "                                        dino_patch_size=16, # Typically 14 or 16 for DINO\n",
    "                                        device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    aliked_extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    aliked_extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_aliked.h5', mode='w') as f_desc_aliked, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='w') as f_desc_combined: # New HDF5 for combined features\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = aliked_extractor.extract(image0)\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy() # ALIKED keypoints (x,y)\n",
    "                descs_aliked = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy() # ALIKED descriptors\n",
    "\n",
    "                # Get DINO patch features for these keypoints\n",
    "                kpts_torch = torch.from_numpy(kpts).to(device)\n",
    "                descs_dino_patch = get_dino_patch_features_for_keypoints(\n",
    "                    img_path, kpts_torch, dino_processor, dino_model, dino_patch_size, device\n",
    "                ).detach().cpu().numpy()\n",
    "\n",
    "                # Concatenate ALIKED and DINO features\n",
    "                if len(descs_aliked) > 0 and len(descs_dino_patch) > 0:\n",
    "                    combined_descs = np.concatenate((descs_aliked, descs_dino_patch), axis=1)\n",
    "                elif len(descs_aliked) > 0: # Only ALIKED if no DINO features (shouldn't happen often)\n",
    "                    combined_descs = descs_aliked\n",
    "                else: # No features found\n",
    "                    combined_descs = np.array([]) # Empty array\n",
    "\n",
    "                f_kp[key] = kpts\n",
    "                f_desc_aliked[key] = descs_aliked # Keep ALIKED descriptors for debugging or other uses\n",
    "                f_desc_combined[key] = combined_descs # Store the new combined descriptors\n",
    "    print(f\"Combined features saved to {feature_dir}/descriptors_combined.h5\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73bd5c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:31.995551Z",
     "iopub.status.busy": "2025-05-28T23:58:31.995317Z",
     "iopub.status.idle": "2025-05-28T23:58:32.517555Z",
     "shell.execute_reply": "2025-05-28T23:58:32.516597Z"
    },
    "papermill": {
     "duration": 0.530849,
     "end_time": "2025-05-28T23:58:32.519133",
     "exception": false,
     "start_time": "2025-05-28T23:58:31.988284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans # MiniBatchKMeans is faster for large datasets\n",
    "\n",
    "# --- VLAD Aggregation Function ---\n",
    "def vlad_encode(descriptors, centroids):\n",
    "    \"\"\"\n",
    "    Performs VLAD encoding.\n",
    "\n",
    "    Args:\n",
    "        descriptors (np.ndarray): NxM array of local descriptors.\n",
    "        centroids (np.ndarray): KxM array of K-Means cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1x(K*M) VLAD descriptor.\n",
    "    \"\"\"\n",
    "    if descriptors.shape[0] == 0:\n",
    "        return np.zeros(centroids.shape[0] * centroids.shape[1], dtype=np.float32)\n",
    "\n",
    "    num_descriptors, desc_dim = descriptors.shape\n",
    "    num_centroids, _ = centroids.shape\n",
    "\n",
    "    # Assign each descriptor to its nearest centroid\n",
    "    # Using cdist for efficiency\n",
    "    distances = np.sqrt(np.sum((descriptors[:, None, :] - centroids[None, :, :])**2, axis=2))\n",
    "    # distances = cdist(descriptors, centroids, 'sqeuclidean') # Could use cdist for sqeuclidean\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Initialize VLAD accumulator\n",
    "    vlad_accumulator = np.zeros((num_centroids, desc_dim), dtype=np.float32)\n",
    "\n",
    "    # Accumulate residuals\n",
    "    for i in range(num_descriptors):\n",
    "        cluster_idx = cluster_assignments[i]\n",
    "        residual = descriptors[i] - centroids[cluster_idx]\n",
    "        vlad_accumulator[cluster_idx] += residual\n",
    "\n",
    "    # Flatten and L2 normalize\n",
    "    vlad_descriptor = vlad_accumulator.flatten()\n",
    "    vlad_descriptor = F.normalize(torch.from_numpy(vlad_descriptor).unsqueeze(0), dim=1, p=2).squeeze(0).numpy()\n",
    "\n",
    "    return vlad_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ea38f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.533766Z",
     "iopub.status.busy": "2025-05-28T23:58:32.533483Z",
     "iopub.status.idle": "2025-05-28T23:58:32.541514Z",
     "shell.execute_reply": "2025-05-28T23:58:32.540722Z"
    },
    "papermill": {
     "duration": 0.016625,
     "end_time": "2025-05-28T23:58:32.542863",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.526238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- NEW: Get Global Descriptors using K-Means + VLAD ---\n",
    "def get_global_desc_vlad(fnames, feature_dir='.featureout', num_clusters=64, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Generates global descriptors for images using K-Means + VLAD on combined ALIKED+DINO features.\n",
    "\n",
    "    Args:\n",
    "        fnames (list): List of image file paths.\n",
    "        feature_dir (str): Directory where combined descriptors are stored.\n",
    "        num_clusters (int): Number of clusters for K-Means (K in VLAD).\n",
    "        device (torch.device): Not directly used for VLAD computation, but passed for consistency.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Nx(K*M) tensor of global VLAD descriptors.\n",
    "    \"\"\"\n",
    "    all_local_descs = []\n",
    "    keys_order = [] # To maintain order of descriptors with respect to fnames\n",
    "\n",
    "    # 1. Load all combined local descriptors\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Loading combined local descriptors for K-Means\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                if descs.shape[0] > 0:\n",
    "                    all_local_descs.append(descs)\n",
    "                    keys_order.append(key)\n",
    "\n",
    "    if not all_local_descs:\n",
    "        print(\"No combined local descriptors found. Cannot train K-Means or compute VLAD.\")\n",
    "        return torch.empty((0, num_clusters * 0), dtype=torch.float32) # Return empty tensor\n",
    "\n",
    "    # Concatenate all descriptors for K-Means training\n",
    "    all_local_descs_flat = np.concatenate(all_local_descs, axis=0)\n",
    "\n",
    "    # 2. Train K-Means on a subset of descriptors if the dataset is too large\n",
    "    # Or directly on all_local_descs_flat if memory permits\n",
    "    print(f\"Training K-Means with {num_clusters} clusters on {all_local_descs_flat.shape[0]} descriptors...\")\n",
    "    # Use MiniBatchKMeans for efficiency\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, n_init='auto', batch_size=256).fit(all_local_descs_flat)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    print(\"K-Means training complete.\")\n",
    "\n",
    "    # 3. Compute VLAD descriptor for each image\n",
    "    global_descs_vlad = []\n",
    "    # Re-iterate through original fnames to match the output order\n",
    "    with h5py.File(f'{feature_dir}/descriptors_combined.h5', mode='r') as f_desc_combined:\n",
    "        for img_path in tqdm(fnames, desc=\"Computing VLAD descriptors\"):\n",
    "            key = img_path.split('/')[-1]\n",
    "            if key in f_desc_combined:\n",
    "                descs = f_desc_combined[key][...]\n",
    "                vlad_desc = vlad_encode(descs, centroids)\n",
    "                global_descs_vlad.append(torch.from_numpy(vlad_desc).unsqueeze(0))\n",
    "            else:\n",
    "                # Handle cases where an image might not have any combined descriptors\n",
    "                # (e.g., no ALIKED keypoints detected). Append a zero vector of correct size.\n",
    "                print(f\"Warning: No combined descriptors for {key}. Appending zero VLAD descriptor.\")\n",
    "                # Determine descriptor dimension from centroids\n",
    "                desc_dim_per_cluster = centroids.shape[1] if centroids.shape[1] > 0 else 0 # Should not be 0 normally\n",
    "                zero_vlad = np.zeros(num_clusters * desc_dim_per_cluster, dtype=np.float32)\n",
    "                global_descs_vlad.append(torch.from_numpy(zero_vlad).unsqueeze(0))\n",
    "\n",
    "\n",
    "    if not global_descs_vlad:\n",
    "        return torch.empty((0, num_clusters * centroids.shape[1] if centroids.shape[1] > 0 else 0), dtype=torch.float32)\n",
    "\n",
    "    global_descs_vlad = torch.cat(global_descs_vlad, dim=0)\n",
    "    return global_descs_vlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ac91c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.558480Z",
     "iopub.status.busy": "2025-05-28T23:58:32.558241Z",
     "iopub.status.idle": "2025-05-28T23:58:32.565957Z",
     "shell.execute_reply": "2025-05-28T23:58:32.565156Z"
    },
    "papermill": {
     "duration": 0.01733,
     "end_time": "2025-05-28T23:58:32.567181",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.549851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- RE-DEFINED: get_image_pairs_shortlist to use the new VLAD global descriptor ---\n",
    "def get_image_pairs_shortlist_vlad(fnames,\n",
    "                                   sim_th=0.6, # should be strict\n",
    "                                   min_pairs=30,\n",
    "                                   exhaustive_if_less=20,\n",
    "                                   feature_dir='.featureout', # Pass feature_dir\n",
    "                                   num_clusters_vlad=64, # New parameter for VLAD\n",
    "                                   device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames) # You need to define get_img_pairs_exhaustive if not done.\n",
    "\n",
    "    # Use the new VLAD-based global descriptor\n",
    "    descs = get_global_desc_vlad(fnames, feature_dir=feature_dir, num_clusters=num_clusters_vlad, device=device)\n",
    "\n",
    "    if descs.shape[0] == 0:\n",
    "        print(\"No global descriptors generated. Returning empty matching list.\")\n",
    "        return []\n",
    "\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # \n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = np.percentile(dm_flat, 60) + np.sqrt(3) * dm_flat.std()\n",
    "\n",
    "    # removing half\n",
    "    mask = dm <= np.percentile(dm_flat, 60)\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = set() # Use a set for faster lookup of already added pairs\n",
    "\n",
    "    for st_idx in range(num_imgs - 1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold: # Ensure distance is not effectively infinite\n",
    "                pair = tuple(sorted((st_idx, idx.item())))\n",
    "                if pair not in already_there_set:\n",
    "                    matching_list.append(pair)\n",
    "                    already_there_set.add(pair)\n",
    "                    total += 1\n",
    "    matching_list = sorted(list(matching_list)) # Sort the list of tuples\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57db9d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.581130Z",
     "iopub.status.busy": "2025-05-28T23:58:32.580899Z",
     "iopub.status.idle": "2025-05-28T23:58:32.584471Z",
     "shell.execute_reply": "2025-05-28T23:58:32.583681Z"
    },
    "papermill": {
     "duration": 0.012062,
     "end_time": "2025-05-28T23:58:32.585849",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.573787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7297baab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.600722Z",
     "iopub.status.busy": "2025-05-28T23:58:32.600465Z",
     "iopub.status.idle": "2025-05-28T23:58:32.611564Z",
     "shell.execute_reply": "2025-05-28T23:58:32.610693Z"
    },
    "papermill": {
     "duration": 0.019741,
     "end_time": "2025-05-28T23:58:32.612892",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.593151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th=0.6,\n",
    "                              min_pairs=30,\n",
    "                              max_pairs=100,  #  max_pairs \n",
    "                              exhaustive_if_less=20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # \n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"60%:  {np.percentile(dm_flat, 60):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "\n",
    "    threshold = np.percentile(dm_flat, 50) + np.sqrt(3) * dm_flat.std()\n",
    "    mask = dm <= np.percentile(dm_flat, 30)\n",
    "\n",
    "    ar = np.arange(num_imgs)\n",
    "    matching_set = set()\n",
    "\n",
    "    for st_idx in range(num_imgs):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "\n",
    "        #  min_pairs \n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]\n",
    "\n",
    "        #  max_pairs\n",
    "        sorted_matches = sorted(\n",
    "            [(idx, dm[st_idx, idx]) for idx in to_match if idx != st_idx and dm[st_idx, idx] < threshold],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        for idx, _ in sorted_matches[:max_pairs]:\n",
    "            pair = tuple(sorted((st_idx, idx)))\n",
    "            matching_set.add(pair)\n",
    "\n",
    "    matching_list = sorted(list(matching_set))\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "083aaba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.628156Z",
     "iopub.status.busy": "2025-05-28T23:58:32.627931Z",
     "iopub.status.idle": "2025-05-28T23:58:32.647135Z",
     "shell.execute_reply": "2025-05-28T23:58:32.646527Z"
    },
    "papermill": {
     "duration": 0.028178,
     "end_time": "2025-05-28T23:58:32.648239",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.620061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_aliked_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "            match_confidence_threshold=CONFIG.params_disk_lightglue[\"match_confidence_threshold\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bab736f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.662336Z",
     "iopub.status.busy": "2025-05-28T23:58:32.662125Z",
     "iopub.status.idle": "2025-05-28T23:58:32.668628Z",
     "shell.execute_reply": "2025-05-28T23:58:32.668009Z"
    },
    "papermill": {
     "duration": 0.014721,
     "end_time": "2025-05-28T23:58:32.669805",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.655084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(feature_dir, img_dir):\n",
    "    result = {}\n",
    "    local_timings = {'RANSAC': [], 'Reconstruction': []}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(\"colmap database\")\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # options = pycolmap.SiftMatchingOptions()\n",
    "    # options.confidence = 0.9999\n",
    "    # options.max_num_trials = 20000\n",
    "    # pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    pycolmap.match_exhaustive(database_path)\n",
    "    print(\"matching done!!!!\")\n",
    "    local_timings['RANSAC'].append(time() - t)\n",
    "    print(f'RANSAC in {local_timings[\"RANSAC\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # mapper_options.mapper.filter_max_reproj_error\t = 1.0\n",
    "    # mapper_options.mapper.init_max_error = 2.0\n",
    "    mapper_options.min_model_size = 5\n",
    "    mapper_options.max_num_models = 25\n",
    "    mapper_options.ba_global_images_freq = 5\n",
    "    mapper_options.ba_local_num_images = 8\n",
    "    mapper_options.mapper.abs_pose_min_inlier_ratio = 0.4\n",
    "    mapper_options.ba_global_max_num_iterations = 100\n",
    "    # mapper_options.mapper.filter_max_reproj_error = 6.0\n",
    "    mapper_options.mapper.max_reg_trials = 10\n",
    "    # mapper_options.mapper.init_min_num_inliers = 50\n",
    "    # mapper_options.mapper.abs_pose_min_num_inliers = 15\n",
    "    \n",
    "\n",
    "    \n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, \n",
    "                                        output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    for map_index, rec in maps.items():\n",
    "        result[map_index] = {}\n",
    "        for img_id, image in rec.images.items():\n",
    "            result[map_index][image.name] = {\n",
    "                'R': image.cam_from_world.rotation.matrix().tolist(),\n",
    "                't': image.cam_from_world.translation.tolist()\n",
    "            }\n",
    "    # clear_output(wait=False)\n",
    "    local_timings['Reconstruction'].append(time() - t)\n",
    "    print(f'Reconstruction done in {local_timings[\"Reconstruction\"][-1]:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    return result, local_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22725779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.683782Z",
     "iopub.status.busy": "2025-05-28T23:58:32.683546Z",
     "iopub.status.idle": "2025-05-28T23:58:32.838978Z",
     "shell.execute_reply": "2025-05-28T23:58:32.838208Z"
    },
    "papermill": {
     "duration": 0.163801,
     "end_time": "2025-05-28T23:58:32.840299",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.676498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47840c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.854867Z",
     "iopub.status.busy": "2025-05-28T23:58:32.854599Z",
     "iopub.status.idle": "2025-05-28T23:58:32.868829Z",
     "shell.execute_reply": "2025-05-28T23:58:32.868216Z"
    },
    "papermill": {
     "duration": 0.023069,
     "end_time": "2025-05-28T23:58:32.870012",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.846943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_keypoints_and_matches(images_input, unified_kp_path, remapped_matches_path, feature_dir='visualization_output'):\n",
    "    output_dir = os.path.join(feature_dir, 'visualization_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load images and determine image_keys for HDF5 lookup\n",
    "    if isinstance(images_input[0], str):\n",
    "        loaded_images = [cv2.imread(img_path) for img_path in images_input]\n",
    "        image_keys = [os.path.basename(img_path) for img_path in images_input]\n",
    "    else:\n",
    "        loaded_images = images_input\n",
    "        # If images_input are already arrays, you need to provide the corresponding keys\n",
    "        # This part is crucial: image_keys MUST align with the HDF5 keys\n",
    "        image_keys = image_keys_in_h5 # Use the predefined list for the dummy case\n",
    "\n",
    "    # Load unified keypoints\n",
    "    keypoints_data = {}\n",
    "    with h5py.File(unified_kp_path, 'r') as f_kp:\n",
    "        for img_name_raw in f_kp.keys():\n",
    "            img_name = img_name_raw.decode('utf-8') if isinstance(img_name_raw, bytes) else img_name_raw\n",
    "            keypoints_data[img_name] = f_kp[img_name_raw][()] # Access with raw key if bytes\n",
    "\n",
    "    # Load remapped matches - CORRECTED LOGIC\n",
    "    # Store (img1_key, img2_key) directly with matches for robust iteration\n",
    "    matches_data_pairs = [] # Will store (img1_key, img2_key, matches_array)\n",
    "    with h5py.File(remapped_matches_path, 'r') as f_matches:\n",
    "        print(\"\\n--- Loading remapped matches from HDF5 ---\")\n",
    "        for img1_group_key_candidate in tqdm(f_matches.keys(), desc=\"Loading matches\"):\n",
    "            img1_key = img1_group_key_candidate.decode('utf-8') if isinstance(img1_group_key_candidate, bytes) else img1_group_key_candidate\n",
    "\n",
    "            img1_group = f_matches[img1_group_key_candidate] # Access with raw key\n",
    "\n",
    "            if isinstance(img1_group, h5py.Group):\n",
    "                for img2_dataset_key_candidate in img1_group.keys():\n",
    "                    img2_key = img2_dataset_key_candidate.decode('utf-8') if isinstance(img2_dataset_key_candidate, bytes) else img2_dataset_key_candidate\n",
    "\n",
    "                    try:\n",
    "                        matches_array = img1_group[img2_dataset_key_candidate][()]\n",
    "                        matches_data_pairs.append((img1_key, img2_key, matches_array))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading matches for pair ({img1_key}, {img2_key}): {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Expected '{img1_key}' to be a group, but found {type(img1_group)}. Skipping its contents.\")\n",
    "\n",
    "\n",
    "    # --- Drawing Keypoints ---\n",
    "    print(\"\\n--- Drawing Keypoints ---\")\n",
    "    for i, img_key in enumerate(image_keys):\n",
    "        if img_key in keypoints_data:\n",
    "            img = loaded_images[i].copy()\n",
    "            kpts = keypoints_data[img_key]\n",
    "\n",
    "            for kp in kpts:\n",
    "                x, y = int(kp[0]), int(kp[1])\n",
    "                cv2.circle(img, (x, y), 3, (0, 255, 0), -1) # Green circle for keypoint\n",
    "\n",
    "            output_kp_path = os.path.join(output_dir, f\"keypoints_{img_key}\")\n",
    "            if len(img.shape) == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(output_kp_path, img)\n",
    "            print(f\"Keypoints drawn on {img_key}, saved to {output_kp_path}\")\n",
    "        else:\n",
    "            print(f\"No keypoints found for {img_key} in unified keypoints file.\")\n",
    "\n",
    "    # --- Drawing Matches ---\n",
    "    print(\"\\n--- Drawing Matches ---\")\n",
    "    # Iterate through the (img1_key, img2_key, matches) tuples directly\n",
    "    for img_name1, img_name2, matches in matches_data_pairs:\n",
    "        # We no longer need to split img_pair_key, as we have img_name1 and img_name2 directly\n",
    "\n",
    "        # Find the actual image objects and their keypoints using image_keys list\n",
    "        try:\n",
    "            img1_idx = image_keys.index(img_name1)\n",
    "            img2_idx = image_keys.index(img_name2)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: One or both image names not found in the provided 'images' list/keys.\")\n",
    "            continue\n",
    "\n",
    "        img1 = loaded_images[img1_idx].copy()\n",
    "        img2 = loaded_images[img2_idx].copy()\n",
    "\n",
    "        kpts1 = keypoints_data.get(img_name1)\n",
    "        kpts2 = keypoints_data.get(img_name2)\n",
    "\n",
    "        if kpts1 is None or kpts2 is None:\n",
    "            print(f\"Skipping matches for {img_name1}-{img_name2}: keypoints not found for one or both images in unified keypoints.\")\n",
    "            continue\n",
    "        if len(matches) == 0:\n",
    "            print(f\"No matches to draw for {img_name1}-{img_name2}.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure images are 3 channels for drawing lines\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "        if len(img2.shape) == 2:\n",
    "            img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Create a concatenated image for drawing matches\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        max_h = max(h1, h2)\n",
    "        matched_img = np.zeros((max_h, w1 + w2, 3), dtype=np.uint8)\n",
    "        matched_img[0:h1, 0:w1] = img1\n",
    "        matched_img[0:h2, w1:w1+w2] = img2\n",
    "\n",
    "        num_matches_to_draw = min(len(matches), 200) # Draw up to 200 matches to avoid clutter, adjust as needed\n",
    "\n",
    "        for i in range(num_matches_to_draw):\n",
    "            match = matches[i]\n",
    "            kp1_idx, kp2_idx = int(match[0]), int(match[1])\n",
    "\n",
    "            # Bounds check for keypoint indices\n",
    "            if kp1_idx >= len(kpts1) or kp2_idx >= len(kpts2):\n",
    "                # print(f\"Warning: Match index out of bounds for {img_name1}-{img_name2}. Skipping match {kp1_idx}-{kp2_idx}.\")\n",
    "                continue\n",
    "\n",
    "            pt1 = tuple(map(int, kpts1[kp1_idx][:2]))\n",
    "            pt2 = tuple(map(int, kpts2[kp2_idx][:2]))\n",
    "\n",
    "            # Draw circles on the concatenated image\n",
    "            cv2.circle(matched_img, pt1, 5, (0, 0, 255), 2) # Red circle on img1 side\n",
    "            cv2.circle(matched_img, (pt2[0] + w1, pt2[1]), 5, (255, 0, 0), 2) # Blue circle on img2 side\n",
    "\n",
    "            # Draw a line connecting the matched keypoints\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            cv2.line(matched_img, pt1, (pt2[0] + w1, pt2[1]), color, 1)\n",
    "\n",
    "        output_match_path = os.path.join(output_dir, f\"matches_{img_name1}_{img_name2}.png\")\n",
    "        cv2.imwrite(output_match_path, matched_img)\n",
    "        print(f\"Matches drawn between {img_name1} and {img_name2}, saved to {output_match_path}\")\n",
    "\n",
    "\n",
    "# Example call (replace with your actual 'images' list)\n",
    "# If your 'images' are file paths:\n",
    "# images_file_paths = ['path/to/your/image1.jpg', 'path/to/your/image2.jpg', ...]\n",
    "# draw_keypoints_and_matches(images_file_paths, unified_kp_path, remapped_matches_path)\n",
    "\n",
    "# If your 'images' are loaded numpy arrays (as in the dummy example above):\n",
    "# draw_keypoints_and_matches(images, unified_kp_path, remapped_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b791d369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T23:58:32.884173Z",
     "iopub.status.busy": "2025-05-28T23:58:32.883964Z",
     "iopub.status.idle": "2025-05-29T00:07:16.942380Z",
     "shell.execute_reply": "2025-05-29T00:07:16.941313Z"
    },
    "papermill": {
     "duration": 524.067428,
     "end_time": "2025-05-29T00:07:16.944070",
     "exception": false,
     "start_time": "2025-05-28T23:58:32.876642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 model for patch feature extraction...\n",
      "DINOv2 model loaded.\n",
      "\n",
      "Processing dataset \"ETs\": 22 images\n",
      "rotation_detection for 22 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 22/22 [00:02<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1504\n",
      "Max:  0.4104\n",
      "Mean: 0.2817\n",
      "Std:  0.0495\n",
      "20%:  0.2356\n",
      "25%:  0.2547\n",
      "60%:  0.2913\n",
      "75%:  0.3260\n",
      "Shortlisting. Number of pairs to match: 224. Done in 2.6577 sec\n",
      "Generated 224 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 224. Done in 2.9500 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([979, 2]), descs.shape=torch.Size([979, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([880, 2]), descs.shape=torch.Size([880, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([725, 2]), descs.shape=torch.Size([725, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([779, 2]), descs.shape=torch.Size([779, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([860, 2]), descs.shape=torch.Size([860, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([832, 2]), descs.shape=torch.Size([832, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([882, 2]), descs.shape=torch.Size([882, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([809, 2]), descs.shape=torch.Size([809, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([659, 2]), descs.shape=torch.Size([659, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([614, 2]), descs.shape=torch.Size([614, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1768, 2]), descs.shape=torch.Size([1768, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1687, 2]), descs.shape=torch.Size([1687, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1409, 2]), descs.shape=torch.Size([1409, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1500, 2]), descs.shape=torch.Size([1500, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1746, 2]), descs.shape=torch.Size([1746, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1282, 2]), descs.shape=torch.Size([1282, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1177, 2]), descs.shape=torch.Size([1177, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1307, 2]), descs.shape=torch.Size([1307, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1463, 2]), descs.shape=torch.Size([1463, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([954, 2]), descs.shape=torch.Size([954, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1737, 2]), descs.shape=torch.Size([1737, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1499, 2]), descs.shape=torch.Size([1499, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 5/224 [00:00<00:14, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et001.png-another_et_another_et002.png: 597 matches @ 1th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et003.png: 288 matches @ 2th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et004.png: 400 matches @ 3th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et005.png: 474 matches @ 4th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et006.png: 232 matches @ 5th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et001.png-another_et_another_et007.png: 189 matches @ 6th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 25/224 [00:00<00:05, 34.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et002.png-another_et_another_et003.png: 291 matches @ 7th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et004.png: 402 matches @ 8th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et005.png: 413 matches @ 9th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et006.png: 242 matches @ 10th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et002.png-another_et_another_et007.png: 215 matches @ 11th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 47/224 [00:01<00:04, 38.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et003.png-another_et_another_et004.png: 301 matches @ 12th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et005.png: 264 matches @ 13th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et003.png-another_et_another_et006.png: 167 matches @ 14th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 65/224 [00:01<00:04, 39.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et004.png-another_et_another_et005.png: 418 matches @ 15th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et004.png-another_et_another_et006.png: 242 matches @ 16th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et004.png-another_et_another_et007.png: 151 matches @ 17th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 79/224 [00:02<00:03, 39.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et005.png-another_et_another_et006.png: 204 matches @ 18th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et005.png-another_et_another_et007.png: 152 matches @ 19th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 97/224 [00:02<00:03, 39.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et006.png-another_et_another_et007.png: 266 matches @ 20th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et006.png-another_et_another_et008.png: 180 matches @ 21th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 114/224 [00:03<00:02, 39.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et007.png-another_et_another_et008.png: 262 matches @ 22th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et007.png-another_et_another_et009.png: 138 matches @ 23th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 127/224 [00:03<00:02, 39.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et008.png-another_et_another_et009.png: 216 matches @ 24th pair(aliked+lightglue)\n",
      "aliked> another_et_another_et008.png-another_et_another_et010.png: 173 matches @ 25th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 142/224 [00:03<00:02, 40.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> another_et_another_et009.png-another_et_another_et010.png: 147 matches @ 26th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 166/224 [00:04<00:01, 39.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et000.png-et_et001.png: 889 matches @ 27th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et002.png: 622 matches @ 28th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et003.png: 1126 matches @ 29th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et004.png: 660 matches @ 30th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et005.png: 131 matches @ 31th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et006.png: 130 matches @ 32th pair(aliked+lightglue)\n",
      "aliked> et_et000.png-et_et007.png: 114 matches @ 33th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 178/224 [00:04<00:01, 38.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et001.png-et_et002.png: 901 matches @ 34th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et003.png: 638 matches @ 35th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et004.png: 797 matches @ 36th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et005.png: 181 matches @ 37th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et006.png: 209 matches @ 38th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et007.png: 198 matches @ 39th pair(aliked+lightglue)\n",
      "aliked> et_et001.png-et_et008.png: 148 matches @ 40th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 190/224 [00:05<00:00, 37.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et002.png-et_et003.png: 457 matches @ 41th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et004.png: 603 matches @ 42th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et005.png: 225 matches @ 43th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et006.png: 253 matches @ 44th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et007.png: 237 matches @ 45th pair(aliked+lightglue)\n",
      "aliked> et_et002.png-et_et008.png: 181 matches @ 46th pair(aliked+lightglue)\n",
      "aliked> et_et003.png-et_et004.png: 526 matches @ 47th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 198/224 [00:05<00:00, 38.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et003.png-et_et005.png: 100 matches @ 48th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et005.png: 164 matches @ 49th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 206/224 [00:05<00:00, 38.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et004.png-et_et006.png: 188 matches @ 50th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et007.png: 148 matches @ 51th pair(aliked+lightglue)\n",
      "aliked> et_et004.png-et_et008.png: 152 matches @ 52th pair(aliked+lightglue)\n",
      "aliked> et_et005.png-et_et006.png: 761 matches @ 53th pair(aliked+lightglue)\n",
      "aliked> et_et005.png-et_et007.png: 771 matches @ 54th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 214/224 [00:05<00:00, 38.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et005.png-et_et008.png: 563 matches @ 55th pair(aliked+lightglue)\n",
      "aliked> et_et006.png-et_et007.png: 833 matches @ 56th pair(aliked+lightglue)\n",
      "aliked> et_et006.png-et_et008.png: 427 matches @ 57th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 222/224 [00:05<00:00, 38.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> et_et007.png-et_et008.png: 554 matches @ 58th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 224/224 [00:06<00:00, 37.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  8.5461 sec (aliked+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4461, 2]), descs.shape=torch.Size([4461, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3945, 2]), descs.shape=torch.Size([3945, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3924, 2]), descs.shape=torch.Size([3924, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3770, 2]), descs.shape=torch.Size([3770, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3711, 2]), descs.shape=torch.Size([3711, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3705, 2]), descs.shape=torch.Size([3705, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3721, 2]), descs.shape=torch.Size([3721, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3389, 2]), descs.shape=torch.Size([3389, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([3303, 2]), descs.shape=torch.Size([3303, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([2907, 2]), descs.shape=torch.Size([2907, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5994, 2]), descs.shape=torch.Size([5994, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([6094, 2]), descs.shape=torch.Size([6094, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5982, 2]), descs.shape=torch.Size([5982, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5290, 2]), descs.shape=torch.Size([5290, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5628, 2]), descs.shape=torch.Size([5628, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5465, 2]), descs.shape=torch.Size([5465, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5533, 2]), descs.shape=torch.Size([5533, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5250, 2]), descs.shape=torch.Size([5250, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5550, 2]), descs.shape=torch.Size([5550, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4841, 2]), descs.shape=torch.Size([4841, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([4382, 2]), descs.shape=torch.Size([4382, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5032, 2]), descs.shape=torch.Size([5032, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et002.png: 2717 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/224 [00:00<00:20, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et003.png: 1414 matches @ 2th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et004.png: 1947 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 4/224 [00:00<00:20, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et005.png: 2070 matches @ 4th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/224 [00:00<00:19, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et006.png: 1416 matches @ 5th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et007.png: 737 matches @ 6th pair(disk+lightglue)\n",
      "disk> another_et_another_et001.png-another_et_another_et008.png: 314 matches @ 7th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 8/224 [00:00<00:19, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et001.png-another_et_another_et009.png: 122 matches @ 8th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 21/224 [00:02<00:21,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et003.png: 1432 matches @ 9th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et004.png: 1904 matches @ 10th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et005.png: 1860 matches @ 11th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 25/224 [00:02<00:18, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et006.png: 1305 matches @ 12th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et007.png: 765 matches @ 13th pair(disk+lightglue)\n",
      "disk> another_et_another_et002.png-another_et_another_et008.png: 336 matches @ 14th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 27/224 [00:02<00:16, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-another_et_another_et009.png: 112 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 39/224 [00:04<00:19,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et002.png-outliers_out_et001.png: 106 matches @ 16th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 41/224 [00:04<00:17, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et003.png-another_et_another_et004.png: 1524 matches @ 17th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et005.png: 1231 matches @ 18th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et006.png: 928 matches @ 19th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 45/224 [00:04<00:15, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et003.png-another_et_another_et007.png: 533 matches @ 20th pair(disk+lightglue)\n",
      "disk> another_et_another_et003.png-another_et_another_et008.png: 250 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 60/224 [00:06<00:15, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et004.png-another_et_another_et005.png: 1851 matches @ 22th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et006.png: 1110 matches @ 23th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et007.png: 518 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 64/224 [00:06<00:13, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et004.png-another_et_another_et008.png: 269 matches @ 25th pair(disk+lightglue)\n",
      "disk> another_et_another_et004.png-another_et_another_et009.png: 115 matches @ 26th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 76/224 [00:07<00:13, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et005.png-another_et_another_et006.png: 1272 matches @ 27th pair(disk+lightglue)\n",
      "disk> another_et_another_et005.png-another_et_another_et007.png: 489 matches @ 28th pair(disk+lightglue)\n",
      "disk> another_et_another_et005.png-another_et_another_et008.png: 252 matches @ 29th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 90/224 [00:09<00:13,  9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et005.png-outliers_out_et001.png: 127 matches @ 30th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 94/224 [00:09<00:11, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et006.png-another_et_another_et007.png: 1286 matches @ 31th pair(disk+lightglue)\n",
      "disk> another_et_another_et006.png-another_et_another_et008.png: 860 matches @ 32th pair(disk+lightglue)\n",
      "disk> another_et_another_et006.png-another_et_another_et009.png: 197 matches @ 33th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 110/224 [00:10<00:09, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et007.png-another_et_another_et008.png: 1191 matches @ 34th pair(disk+lightglue)\n",
      "disk> another_et_another_et007.png-another_et_another_et009.png: 651 matches @ 35th pair(disk+lightglue)\n",
      "disk> another_et_another_et007.png-another_et_another_et010.png: 320 matches @ 36th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 125/224 [00:12<00:09, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et008.png-another_et_another_et009.png: 1068 matches @ 37th pair(disk+lightglue)\n",
      "disk> another_et_another_et008.png-another_et_another_et010.png: 409 matches @ 38th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 137/224 [00:13<00:07, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> another_et_another_et009.png-another_et_another_et010.png: 730 matches @ 39th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 163/224 [00:16<00:07,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et000.png-et_et001.png: 3216 matches @ 40th pair(disk+lightglue)\n",
      "disk> et_et000.png-et_et002.png: 2586 matches @ 41th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 165/224 [00:16<00:07,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et000.png-et_et003.png: 3934 matches @ 42th pair(disk+lightglue)\n",
      "disk> et_et000.png-et_et004.png: 2343 matches @ 43th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 174/224 [00:18<00:07,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et001.png-et_et002.png: 3452 matches @ 44th pair(disk+lightglue)\n",
      "disk> et_et001.png-et_et003.png: 2559 matches @ 45th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 176/224 [00:18<00:07,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et001.png-et_et004.png: 2608 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 178/224 [00:18<00:07,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et001.png-et_et006.png: 112 matches @ 47th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 184/224 [00:19<00:06,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et003.png: 2087 matches @ 48th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et004.png: 2323 matches @ 49th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 186/224 [00:20<00:06,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et005.png: 394 matches @ 50th pair(disk+lightglue)\n",
      "disk> et_et002.png-et_et006.png: 959 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 188/224 [00:20<00:05,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et002.png-et_et007.png: 321 matches @ 52th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 191/224 [00:20<00:04,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et003.png-et_et004.png: 1939 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 199/224 [00:21<00:03,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et004.png-et_et005.png: 254 matches @ 54th pair(disk+lightglue)\n",
      "disk> et_et004.png-et_et006.png: 518 matches @ 55th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 206/224 [00:22<00:02,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et005.png-et_et006.png: 3341 matches @ 56th pair(disk+lightglue)\n",
      "disk> et_et005.png-et_et007.png: 3224 matches @ 57th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 208/224 [00:23<00:02,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et005.png-et_et008.png: 2515 matches @ 58th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 212/224 [00:23<00:01,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et006.png-et_et007.png: 3625 matches @ 59th pair(disk+lightglue)\n",
      "disk> et_et006.png-et_et008.png: 2231 matches @ 60th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 217/224 [00:24<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> et_et007.png-et_et008.png: 2533 matches @ 61th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 224/224 [00:25<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  30.0969 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='224' class='' max='224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [224/224 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another_et_another_et001.png-another_et_another_et002.png: 3314 --> 3314 matches\n",
      "another_et_another_et001.png-another_et_another_et003.png: 1702 --> 1702 matches\n",
      "another_et_another_et001.png-another_et_another_et004.png: 2347 --> 2347 matches\n",
      "another_et_another_et001.png-another_et_another_et005.png: 2544 --> 2544 matches\n",
      "another_et_another_et001.png-another_et_another_et006.png: 1648 --> 1648 matches\n",
      "another_et_another_et001.png-another_et_another_et007.png: 926 --> 926 matches\n",
      "another_et_another_et001.png-another_et_another_et008.png: 314 --> 314 matches\n",
      "another_et_another_et001.png-another_et_another_et009.png: 122 --> 122 matches\n",
      "another_et_another_et002.png-another_et_another_et003.png: 1723 --> 1723 matches\n",
      "another_et_another_et002.png-another_et_another_et004.png: 2306 --> 2306 matches\n",
      "another_et_another_et002.png-another_et_another_et005.png: 2273 --> 2273 matches\n",
      "another_et_another_et002.png-another_et_another_et006.png: 1547 --> 1547 matches\n",
      "another_et_another_et002.png-another_et_another_et007.png: 980 --> 980 matches\n",
      "another_et_another_et002.png-another_et_another_et008.png: 336 --> 336 matches\n",
      "another_et_another_et002.png-another_et_another_et009.png: 112 --> 112 matches\n",
      "another_et_another_et002.png-outliers_out_et001.png: 106 --> 106 matches\n",
      "another_et_another_et003.png-another_et_another_et004.png: 1825 --> 1825 matches\n",
      "another_et_another_et003.png-another_et_another_et005.png: 1495 --> 1495 matches\n",
      "another_et_another_et003.png-another_et_another_et006.png: 1095 --> 1095 matches\n",
      "another_et_another_et003.png-another_et_another_et007.png: 533 --> 533 matches\n",
      "another_et_another_et003.png-another_et_another_et008.png: 250 --> 250 matches\n",
      "another_et_another_et004.png-another_et_another_et005.png: 2269 --> 2269 matches\n",
      "another_et_another_et004.png-another_et_another_et006.png: 1352 --> 1352 matches\n",
      "another_et_another_et004.png-another_et_another_et007.png: 669 --> 669 matches\n",
      "another_et_another_et004.png-another_et_another_et008.png: 269 --> 269 matches\n",
      "another_et_another_et004.png-another_et_another_et009.png: 115 --> 115 matches\n",
      "another_et_another_et005.png-another_et_another_et006.png: 1476 --> 1476 matches\n",
      "another_et_another_et005.png-another_et_another_et007.png: 641 --> 641 matches\n",
      "another_et_another_et005.png-another_et_another_et008.png: 252 --> 252 matches\n",
      "another_et_another_et005.png-outliers_out_et001.png: 127 --> 127 matches\n",
      "another_et_another_et006.png-another_et_another_et007.png: 1552 --> 1552 matches\n",
      "another_et_another_et006.png-another_et_another_et008.png: 1040 --> 1040 matches\n",
      "another_et_another_et006.png-another_et_another_et009.png: 197 --> 197 matches\n",
      "another_et_another_et007.png-another_et_another_et008.png: 1453 --> 1453 matches\n",
      "another_et_another_et007.png-another_et_another_et009.png: 789 --> 789 matches\n",
      "another_et_another_et007.png-another_et_another_et010.png: 320 --> 320 matches\n",
      "another_et_another_et008.png-another_et_another_et009.png: 1284 --> 1284 matches\n",
      "another_et_another_et008.png-another_et_another_et010.png: 582 --> 582 matches\n",
      "another_et_another_et009.png-another_et_another_et010.png: 877 --> 877 matches\n",
      "et_et000.png-et_et001.png: 4105 --> 4105 matches\n",
      "et_et000.png-et_et002.png: 3208 --> 3208 matches\n",
      "et_et000.png-et_et003.png: 5060 --> 5060 matches\n",
      "et_et000.png-et_et004.png: 3003 --> 3003 matches\n",
      "et_et000.png-et_et005.png: 131 --> 131 matches\n",
      "et_et000.png-et_et006.png: 130 --> 130 matches\n",
      "et_et000.png-et_et007.png: 114 --> 114 matches\n",
      "et_et001.png-et_et002.png: 4353 --> 4353 matches\n",
      "et_et001.png-et_et003.png: 3197 --> 3197 matches\n",
      "et_et001.png-et_et004.png: 3405 --> 3405 matches\n",
      "et_et001.png-et_et005.png: 181 --> 181 matches\n",
      "et_et001.png-et_et006.png: 321 --> 321 matches\n",
      "et_et001.png-et_et007.png: 198 --> 198 matches\n",
      "et_et001.png-et_et008.png: 148 --> 148 matches\n",
      "et_et002.png-et_et003.png: 2544 --> 2544 matches\n",
      "et_et002.png-et_et004.png: 2926 --> 2926 matches\n",
      "et_et002.png-et_et005.png: 619 --> 619 matches\n",
      "et_et002.png-et_et006.png: 1212 --> 1212 matches\n",
      "et_et002.png-et_et007.png: 558 --> 558 matches\n",
      "et_et002.png-et_et008.png: 181 --> 181 matches\n",
      "et_et003.png-et_et004.png: 2465 --> 2465 matches\n",
      "et_et003.png-et_et005.png: 100 --> 100 matches\n",
      "et_et004.png-et_et005.png: 418 --> 418 matches\n",
      "et_et004.png-et_et006.png: 706 --> 706 matches\n",
      "et_et004.png-et_et007.png: 148 --> 148 matches\n",
      "et_et004.png-et_et008.png: 152 --> 152 matches\n",
      "et_et005.png-et_et006.png: 4102 --> 4102 matches\n",
      "et_et005.png-et_et007.png: 3995 --> 3995 matches\n",
      "et_et005.png-et_et008.png: 3078 --> 3078 matches\n",
      "et_et006.png-et_et007.png: 4458 --> 4458 matches\n",
      "et_et006.png-et_et008.png: 2658 --> 2658 matches\n",
      "et_et007.png-et_et008.png: 3087 --> 3087 matches\n",
      "Ensembled pairs : 71 pairs\n",
      "Local feature extracting and matching. Done in 41.4428 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:00<00:00, 71.95it/s]\n",
      " 52%|    | 71/136 [00:00<00:00, 3843.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 2.5225 sec\n",
      "{0: Reconstruction(num_reg_images=9, num_cameras=9, num_points3D=8560, num_observations=38617), 1: Reconstruction(num_reg_images=11, num_cameras=11, num_points3D=4510, num_observations=22699)}\n",
      "Reconstruction done in 26.0369 sec\n",
      "Dataset  ETs -> Registered 20 / 22 images with 2 clusters\n",
      "\n",
      "Processing dataset \"amy_gardens\": 200 images\n",
      "rotation_detection for 200 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/amy_gardens/peach_0000.png\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "\n",
      "Processing dataset \"fbk_vineyard\": 163 images\n",
      "rotation_detection for 163 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/fbk_vineyard/vineyard_split_1_frame_0900.png\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_haiper\": 54 images\n",
      "rotation_detection for 54 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_haiper/bike_image_004.png\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_heritage\": 209 images\n",
      "rotation_detection for 209 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/209 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_heritage/cyprus_dsc_6480.png\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2023_theather_imc2024_church\": 76 images\n",
      "rotation_detection for 76 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2023_theather_imc2024_church/church_00004.png\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_dioscuri_baalshamin\": 138 images\n",
      "rotation_detection for 138 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_dioscuri_baalshamin/baalshamin_182z.png\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "\n",
      "Processing dataset \"imc2024_lizard_pond\": 214 images\n",
      "rotation_detection for 214 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/214 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/imc2024_lizard_pond/lizard_00003.png\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_brandenburg_british_buckingham\": 225 images\n",
      "rotation_detection for 225 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_brandenburg_british_buckingham/brandenburg_gate_01069771_8567470929.png\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_piazzasanmarco_grandplace\": 168 images\n",
      "rotation_detection for 168 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/168 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_piazzasanmarco_grandplace/grand_place_brussels_00460368_4162644685.png\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_sacrecoeur_trevi_tajmahal\": 225 images\n",
      "rotation_detection for 225 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_sacrecoeur_trevi_tajmahal/sacre_coeur_02928139_3448003521.png\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "\n",
      "Processing dataset \"pt_stpeters_stpauls\": 200 images\n",
      "rotation_detection for 200 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: /kaggle/input/image-matching-challenge-2025/test/pt_stpeters_stpauls/st_pauls_cathedral_00162897_2573777698.png\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n",
      "rotation_detection for 51 images : 0.0000 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:10<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0451\n",
      "20%:  0.2433\n",
      "25%:  0.2499\n",
      "60%:  0.2868\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 620. Done in 10.4035 sec\n",
      "Generated 620 image pairs using VLAD global descriptor.\n",
      "Shortlisting. Number of pairs to match: 620. Done in 10.7103 sec\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([679, 2]), descs.shape=torch.Size([679, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([631, 2]), descs.shape=torch.Size([631, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([358, 2]), descs.shape=torch.Size([358, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([241, 2]), descs.shape=torch.Size([241, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([430, 2]), descs.shape=torch.Size([430, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([661, 2]), descs.shape=torch.Size([661, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([116, 2]), descs.shape=torch.Size([116, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([482, 2]), descs.shape=torch.Size([482, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([442, 2]), descs.shape=torch.Size([442, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([413, 2]), descs.shape=torch.Size([413, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([195, 2]), descs.shape=torch.Size([195, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([46, 2]), descs.shape=torch.Size([46, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([570, 2]), descs.shape=torch.Size([570, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([282, 2]), descs.shape=torch.Size([282, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([410, 2]), descs.shape=torch.Size([410, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([361, 2]), descs.shape=torch.Size([361, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1047, 2]), descs.shape=torch.Size([1047, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([918, 2]), descs.shape=torch.Size([918, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1053, 2]), descs.shape=torch.Size([1053, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([744, 2]), descs.shape=torch.Size([744, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([582, 2]), descs.shape=torch.Size([582, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([776, 2]), descs.shape=torch.Size([776, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([455, 2]), descs.shape=torch.Size([455, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([416, 2]), descs.shape=torch.Size([416, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1160, 2]), descs.shape=torch.Size([1160, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([210, 2]), descs.shape=torch.Size([210, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([116, 2]), descs.shape=torch.Size([116, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([519, 2]), descs.shape=torch.Size([519, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1509, 2]), descs.shape=torch.Size([1509, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([722, 2]), descs.shape=torch.Size([722, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([405, 2]), descs.shape=torch.Size([405, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([459, 2]), descs.shape=torch.Size([459, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([975, 2]), descs.shape=torch.Size([975, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([967, 2]), descs.shape=torch.Size([967, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1417, 2]), descs.shape=torch.Size([1417, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1102, 2]), descs.shape=torch.Size([1102, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([855, 2]), descs.shape=torch.Size([855, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([978, 2]), descs.shape=torch.Size([978, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1442, 2]), descs.shape=torch.Size([1442, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([531, 2]), descs.shape=torch.Size([531, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([404, 2]), descs.shape=torch.Size([404, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([947, 2]), descs.shape=torch.Size([947, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1360, 2]), descs.shape=torch.Size([1360, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1100, 2]), descs.shape=torch.Size([1100, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([927, 2]), descs.shape=torch.Size([927, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([639, 2]), descs.shape=torch.Size([639, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([539, 2]), descs.shape=torch.Size([539, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([761, 2]), descs.shape=torch.Size([761, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1340, 2]), descs.shape=torch.Size([1340, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([533, 2]), descs.shape=torch.Size([533, 128])\n",
      "aliked > rot_k=0, kpts.shape=torch.Size([1131, 2]), descs.shape=torch.Size([1131, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 179/620 [00:04<00:11, 39.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 147 matches @ 1th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 119 matches @ 2th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 100 matches @ 3th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 192/620 [00:04<00:10, 39.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 126 matches @ 4th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 207/620 [00:05<00:10, 40.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 100 matches @ 5th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 319/620 [00:08<00:07, 40.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453720741.png: 127 matches @ 6th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453736752.png: 185 matches @ 7th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 334/620 [00:08<00:07, 40.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 182 matches @ 8th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 354/620 [00:08<00:06, 40.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453697531.png-stairs_split_2_1710453759963.png: 153 matches @ 9th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 160 matches @ 10th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 369/620 [00:09<00:06, 39.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 298 matches @ 11th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 176 matches @ 12th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 393/620 [00:09<00:05, 40.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 138 matches @ 13th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 131 matches @ 14th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 398/620 [00:10<00:05, 40.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 147 matches @ 15th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 427/620 [00:10<00:04, 40.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 165 matches @ 16th pair(aliked+lightglue)\n",
      "aliked> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 209 matches @ 17th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 482/620 [00:12<00:03, 40.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 218 matches @ 18th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 377 matches @ 19th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 579 matches @ 20th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 500/620 [00:12<00:03, 39.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 103 matches @ 21th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 525/620 [00:13<00:02, 40.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453736752.png-stairs_split_2_1710453756762.png: 161 matches @ 22th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453736752.png-stairs_split_2_1710453759963.png: 136 matches @ 23th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 150 matches @ 24th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 530/620 [00:13<00:02, 40.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 195 matches @ 25th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 435 matches @ 26th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 203 matches @ 27th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 117 matches @ 28th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 540/620 [00:13<00:01, 40.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 131 matches @ 29th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 372 matches @ 30th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 343 matches @ 31th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453783374.png: 213 matches @ 32th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 524 matches @ 33th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 559/620 [00:14<00:01, 37.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 307 matches @ 34th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 578/620 [00:14<00:01, 39.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453756762.png-stairs_split_2_1710453783374.png: 119 matches @ 35th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453756762.png-stairs_split_2_1710453871430.png: 181 matches @ 36th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 298 matches @ 37th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 586/620 [00:14<00:00, 39.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 126 matches @ 38th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 600/620 [00:15<00:00, 39.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453779372.png-stairs_split_2_1710453801783.png: 115 matches @ 39th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 340 matches @ 40th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 225 matches @ 41th pair(aliked+lightglue)\n",
      "aliked> stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 193 matches @ 42th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 609/620 [00:15<00:00, 39.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliked> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 111 matches @ 43th pair(aliked+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 620/620 [00:15<00:00, 39.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  20.2656 sec (aliked+LightGlue)\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([5869, 2]), descs.shape=torch.Size([5869, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7945, 2]), descs.shape=torch.Size([7945, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7390, 2]), descs.shape=torch.Size([7390, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8057, 2]), descs.shape=torch.Size([8057, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8110, 2]), descs.shape=torch.Size([8110, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([8192, 2]), descs.shape=torch.Size([8192, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7936, 2]), descs.shape=torch.Size([7936, 128])\n",
      "disk > rot_k=0, kpts.shape=torch.Size([7656, 2]), descs.shape=torch.Size([7656, 128])\n",
      "Loaded LightGlue model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/620 [00:00<02:25,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 539 matches @ 1th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 24/620 [00:07<02:58,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453576271.png-stairs_split_2_1710453871430.png: 164 matches @ 2th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 31/620 [00:08<02:23,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453693529.png: 126 matches @ 3th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 35/620 [00:09<02:17,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453930259.png: 154 matches @ 4th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 38/620 [00:10<02:18,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_1_1710453990286.png: 1136 matches @ 5th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 40/620 [00:10<02:18,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 159 matches @ 6th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 41/620 [00:11<02:18,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 372 matches @ 7th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 46/620 [00:12<02:17,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 169 matches @ 8th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 61/620 [00:16<02:51,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453985484.png: 454 matches @ 9th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 62/620 [00:17<02:51,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 1089 matches @ 10th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 83/620 [00:23<02:46,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 931 matches @ 11th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 90/620 [00:25<02:43,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 672 matches @ 12th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 108/620 [00:31<02:35,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453620694.png-stairs_split_1_1710453626698.png: 315 matches @ 13th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 137/620 [00:40<02:26,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453626698.png-stairs_split_1_1710453963274.png: 143 matches @ 14th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 166/620 [00:48<02:17,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453668718.png: 227 matches @ 15th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 169/620 [00:49<02:16,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453693529.png: 198 matches @ 16th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 174/620 [00:51<02:13,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 344 matches @ 17th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 175/620 [00:51<02:13,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453947066.png: 567 matches @ 18th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 176/620 [00:51<02:13,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 406 matches @ 19th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 180/620 [00:53<02:12,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 573 matches @ 20th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 181/620 [00:53<02:12,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 262 matches @ 21th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 185/620 [00:54<02:11,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 472 matches @ 22th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 188/620 [00:55<02:10,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 118 matches @ 23th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|      | 194/620 [00:57<02:06,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453651110.png-stairs_split_2_1710453871430.png: 397 matches @ 24th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 200/620 [00:59<02:05,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 431 matches @ 25th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 245/620 [01:12<01:51,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_1_1710453689727.png: 176 matches @ 26th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 247/620 [01:13<01:52,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_1_1710453930259.png: 868 matches @ 27th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 249/620 [01:13<01:51,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453720741.png: 250 matches @ 28th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 251/620 [01:14<01:51,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453739354.png: 441 matches @ 29th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 255/620 [01:15<01:50,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453759963.png: 371 matches @ 30th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|     | 256/620 [01:15<01:50,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453783374.png: 140 matches @ 31th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|     | 257/620 [01:16<01:49,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 329 matches @ 32th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 259/620 [01:16<01:47,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 1203 matches @ 33th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 261/620 [01:17<01:46,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453704934.png: 495 matches @ 34th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 263/620 [01:17<01:43,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453930259.png: 169 matches @ 35th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 264/620 [01:18<01:43,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453675921.png-stairs_split_1_1710453947066.png: 210 matches @ 36th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 312/620 [01:32<01:32,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453736752.png: 110 matches @ 37th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 326/620 [01:36<01:27,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 445 matches @ 38th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 327/620 [01:37<01:27,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_1_1710453697531.png: 328 matches @ 39th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 330/620 [01:37<01:27,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 263 matches @ 40th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 331/620 [01:38<01:26,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453740954.png: 241 matches @ 41th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 334/620 [01:39<01:26,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453759963.png: 536 matches @ 42th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 340/620 [01:40<01:23,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453693529.png-stairs_split_2_1710453871430.png: 244 matches @ 43th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 356/620 [01:45<01:17,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 1653 matches @ 44th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 365/620 [01:48<01:17,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 1215 matches @ 45th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 370/620 [01:50<01:15,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 1190 matches @ 46th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 386/620 [01:54<01:05,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 295 matches @ 47th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 391/620 [01:55<01:03,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453790978.png: 183 matches @ 48th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 392/620 [01:56<01:03,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 308 matches @ 49th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 424/620 [02:05<00:59,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 144 matches @ 50th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 452/620 [02:14<00:49,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 164 matches @ 51th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 469/620 [02:19<00:45,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_1_1710453990286.png-stairs_split_2_1710453745156.png: 115 matches @ 52th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 476/620 [02:21<00:43,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 565 matches @ 53th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 478/620 [02:22<00:42,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 1678 matches @ 54th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 481/620 [02:23<00:42,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 735 matches @ 55th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 482/620 [02:23<00:41,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453783374.png: 155 matches @ 56th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 483/620 [02:23<00:41,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 1935 matches @ 57th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 486/620 [02:24<00:40,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 520 matches @ 58th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 487/620 [02:24<00:39,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453720741.png-stairs_split_2_1710453871430.png: 262 matches @ 59th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 488/620 [02:25<00:39,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453728949.png: 185 matches @ 60th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 490/620 [02:25<00:39,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453745156.png: 180 matches @ 61th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 492/620 [02:26<00:38,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 386 matches @ 62th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 494/620 [02:27<00:38,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453725143.png-stairs_split_2_1710453790978.png: 256 matches @ 63th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 512/620 [02:32<00:32,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453733751.png-stairs_split_2_1710453798181.png: 114 matches @ 64th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 517/620 [02:33<00:31,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453756762.png: 221 matches @ 65th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 520/620 [02:34<00:30,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453783374.png: 144 matches @ 66th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 525/620 [02:36<00:28,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 484 matches @ 67th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 526/620 [02:36<00:28,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453805788.png: 173 matches @ 68th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 527/620 [02:36<00:27,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 1640 matches @ 69th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 528/620 [02:37<00:27,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 2569 matches @ 70th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 531/620 [02:38<00:26,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 879 matches @ 71th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 532/620 [02:38<00:26,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 910 matches @ 72th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 533/620 [02:38<00:26,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 261 matches @ 73th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 536/620 [02:39<00:25,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 767 matches @ 74th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 537/620 [02:39<00:24,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 3083 matches @ 75th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 540/620 [02:40<00:24,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 1808 matches @ 76th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 542/620 [02:41<00:23,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 1599 matches @ 77th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 545/620 [02:42<00:22,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 580 matches @ 78th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 546/620 [02:42<00:22,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453740954.png-stairs_split_2_1710453871430.png: 143 matches @ 79th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 554/620 [02:45<00:19,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 1657 matches @ 80th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 559/620 [02:46<00:18,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453745156.png-stairs_split_2_1710453862225.png: 201 matches @ 81th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 571/620 [02:50<00:14,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453756762.png-stairs_split_2_1710453783374.png: 184 matches @ 82th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 576/620 [02:51<00:13,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453756762.png-stairs_split_2_1710453805788.png: 103 matches @ 83th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 577/620 [02:52<00:12,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453756762.png-stairs_split_2_1710453871430.png: 116 matches @ 84th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 579/620 [02:52<00:12,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 1249 matches @ 85th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 582/620 [02:53<00:11,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 707 matches @ 86th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 583/620 [02:53<00:10,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453759963.png-stairs_split_2_1710453871430.png: 176 matches @ 87th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 586/620 [02:54<00:10,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453765165.png-stairs_split_2_1710453790978.png: 245 matches @ 88th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 597/620 [02:58<00:06,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 526 matches @ 89th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 600/620 [02:58<00:06,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 2062 matches @ 90th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 601/620 [02:59<00:05,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 2259 matches @ 91th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 602/620 [02:59<00:05,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 575 matches @ 92th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 604/620 [03:00<00:04,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 1143 matches @ 93th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 605/620 [03:00<00:04,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453786375.png-stairs_split_2_1710453871430.png: 396 matches @ 94th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 606/620 [03:00<00:04,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453790978.png-stairs_split_2_1710453793579.png: 320 matches @ 95th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 620/620 [03:04<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk> stairs_split_2_1710453805788.png-stairs_split_2_1710453871430.png: 486 matches @ 96th pair(disk+lightglue)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matched in  198.0653 sec (disk+LightGlue)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='620' class='' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [620/620 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stairs_split_1_1710453576271.png-stairs_split_1_1710453601885.png: 539 --> 539 matches\n",
      "stairs_split_1_1710453576271.png-stairs_split_2_1710453871430.png: 164 --> 164 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453693529.png: 126 --> 126 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453930259.png: 154 --> 154 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_1_1710453990286.png: 1136 --> 1136 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453739354.png: 159 --> 159 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453740954.png: 372 --> 372 matches\n",
      "stairs_split_1_1710453601885.png-stairs_split_2_1710453786375.png: 169 --> 169 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453985484.png: 454 --> 454 matches\n",
      "stairs_split_1_1710453606287.png-stairs_split_1_1710453990286.png: 1089 --> 1089 matches\n",
      "stairs_split_1_1710453612890.png-stairs_split_1_1710453985484.png: 931 --> 931 matches\n",
      "stairs_split_1_1710453616892.png-stairs_split_1_1710453620694.png: 672 --> 672 matches\n",
      "stairs_split_1_1710453620694.png-stairs_split_1_1710453626698.png: 315 --> 315 matches\n",
      "stairs_split_1_1710453626698.png-stairs_split_1_1710453963274.png: 143 --> 143 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453668718.png: 227 --> 227 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453693529.png: 198 --> 198 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453930259.png: 491 --> 491 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453947066.png: 567 --> 567 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_1_1710453955270.png: 525 --> 525 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453739354.png: 673 --> 673 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453740954.png: 262 --> 262 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453759963.png: 472 --> 472 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453786375.png: 244 --> 244 matches\n",
      "stairs_split_1_1710453651110.png-stairs_split_2_1710453871430.png: 397 --> 397 matches\n",
      "stairs_split_1_1710453659313.png-stairs_split_1_1710453947066.png: 531 --> 531 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_1_1710453689727.png: 176 --> 176 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_1_1710453930259.png: 868 --> 868 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453720741.png: 250 --> 250 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453739354.png: 441 --> 441 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453759963.png: 371 --> 371 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453783374.png: 140 --> 140 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453786375.png: 329 --> 329 matches\n",
      "stairs_split_1_1710453668718.png-stairs_split_2_1710453871430.png: 1203 --> 1203 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453704934.png: 495 --> 495 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453930259.png: 169 --> 169 matches\n",
      "stairs_split_1_1710453675921.png-stairs_split_1_1710453947066.png: 210 --> 210 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453720741.png: 127 --> 127 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453736752.png: 295 --> 295 matches\n",
      "stairs_split_1_1710453689727.png-stairs_split_2_1710453871430.png: 627 --> 627 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_1_1710453697531.png: 328 --> 328 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453739354.png: 263 --> 263 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453740954.png: 241 --> 241 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453759963.png: 536 --> 536 matches\n",
      "stairs_split_1_1710453693529.png-stairs_split_2_1710453871430.png: 244 --> 244 matches\n",
      "stairs_split_1_1710453697531.png-stairs_split_2_1710453759963.png: 153 --> 153 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_1_1710453901046.png: 1813 --> 1813 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453745156.png: 1513 --> 1513 matches\n",
      "stairs_split_1_1710453704934.png-stairs_split_2_1710453790978.png: 1366 --> 1366 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453745156.png: 433 --> 433 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453790978.png: 183 --> 183 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453793579.png: 439 --> 439 matches\n",
      "stairs_split_1_1710453901046.png-stairs_split_2_1710453862225.png: 147 --> 147 matches\n",
      "stairs_split_1_1710453930259.png-stairs_split_2_1710453871430.png: 165 --> 165 matches\n",
      "stairs_split_1_1710453947066.png-stairs_split_1_1710453990286.png: 353 --> 353 matches\n",
      "stairs_split_1_1710453955270.png-stairs_split_2_1710453871430.png: 164 --> 164 matches\n",
      "stairs_split_1_1710453990286.png-stairs_split_2_1710453745156.png: 115 --> 115 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453725143.png: 565 --> 565 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453740954.png: 1896 --> 1896 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453759963.png: 1112 --> 1112 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453783374.png: 155 --> 155 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453786375.png: 2514 --> 2514 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453805788.png: 520 --> 520 matches\n",
      "stairs_split_2_1710453720741.png-stairs_split_2_1710453871430.png: 262 --> 262 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453728949.png: 185 --> 185 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453745156.png: 180 --> 180 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453765165.png: 489 --> 489 matches\n",
      "stairs_split_2_1710453725143.png-stairs_split_2_1710453790978.png: 256 --> 256 matches\n",
      "stairs_split_2_1710453733751.png-stairs_split_2_1710453798181.png: 114 --> 114 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453756762.png: 382 --> 382 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453759963.png: 136 --> 136 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453783374.png: 144 --> 144 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453801783.png: 634 --> 634 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453805788.png: 173 --> 173 matches\n",
      "stairs_split_2_1710453736752.png-stairs_split_2_1710453871430.png: 1835 --> 1835 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453740954.png: 3004 --> 3004 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453759963.png: 879 --> 879 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453783374.png: 1113 --> 1113 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453786375.png: 378 --> 378 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453805788.png: 898 --> 898 matches\n",
      "stairs_split_2_1710453739354.png-stairs_split_2_1710453871430.png: 3455 --> 3455 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453759963.png: 2151 --> 2151 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453783374.png: 213 --> 213 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453786375.png: 2123 --> 2123 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453805788.png: 580 --> 580 matches\n",
      "stairs_split_2_1710453740954.png-stairs_split_2_1710453871430.png: 143 --> 143 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453790978.png: 1964 --> 1964 matches\n",
      "stairs_split_2_1710453745156.png-stairs_split_2_1710453862225.png: 201 --> 201 matches\n",
      "stairs_split_2_1710453756762.png-stairs_split_2_1710453783374.png: 303 --> 303 matches\n",
      "stairs_split_2_1710453756762.png-stairs_split_2_1710453805788.png: 103 --> 103 matches\n",
      "stairs_split_2_1710453756762.png-stairs_split_2_1710453871430.png: 297 --> 297 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453786375.png: 1547 --> 1547 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453805788.png: 833 --> 833 matches\n",
      "stairs_split_2_1710453759963.png-stairs_split_2_1710453871430.png: 176 --> 176 matches\n",
      "stairs_split_2_1710453765165.png-stairs_split_2_1710453790978.png: 245 --> 245 matches\n",
      "stairs_split_2_1710453779372.png-stairs_split_2_1710453801783.png: 115 --> 115 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453786375.png: 866 --> 866 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453805788.png: 2287 --> 2287 matches\n",
      "stairs_split_2_1710453783374.png-stairs_split_2_1710453871430.png: 2452 --> 2452 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453790978.png: 575 --> 575 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453805788.png: 1254 --> 1254 matches\n",
      "stairs_split_2_1710453786375.png-stairs_split_2_1710453871430.png: 396 --> 396 matches\n",
      "stairs_split_2_1710453790978.png-stairs_split_2_1710453793579.png: 320 --> 320 matches\n",
      "stairs_split_2_1710453805788.png-stairs_split_2_1710453871430.png: 486 --> 486 matches\n",
      "Ensembled pairs : 103 pairs\n",
      "Local feature extracting and matching. Done in 220.6464 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 43/43 [00:01<00:00, 21.89it/s]\n",
      " 17%|        | 103/595 [00:00<00:00, 3988.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colmap database\n",
      "matching done!!!!\n",
      "RANSAC in 2.9470 sec\n",
      "{0: Reconstruction(num_reg_images=4, num_cameras=4, num_points3D=336, num_observations=716), 1: Reconstruction(num_reg_images=21, num_cameras=21, num_points3D=5488, num_observations=15259), 2: Reconstruction(num_reg_images=29, num_cameras=29, num_points3D=6442, num_observations=17652)}\n",
      "Reconstruction done in 200.6386 sec\n",
      "Dataset  stairs -> Registered 54 / 51 images with 3 clusters\n",
      "\n",
      "Results\n",
      "Dataset  ETs -> Registered 20 / 22 images with 2 clusters\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "Dataset  stairs -> Registered 54 / 51 images with 3 clusters\n",
      "\n",
      "Timings\n",
      "rotation_detection -> total=0.00 sec.\n",
      "global feature extraction -> total=0.00 sec.\n",
      "shortlisting -> total=26.72 sec.\n",
      "feature_detection -> total=0.00 sec.\n",
      "feature_matching -> total=256.97 sec.\n",
      "RANSAC -> total=5.47 sec.\n",
      "Reconstruction -> total=226.68 sec.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t# 'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t# 'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    'rotation_detection':[],\n",
    "    \"global feature extraction\":[],\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "# Load DINOv2 model (for feature extraction, not global descriptor here)\n",
    "print(\"Loading DINOv2 model for patch feature extraction...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "dino_model = dino_model.eval().to(device)\n",
    "print(\"DINOv2 model loaded.\")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    # print (f\"Extracting on device {device}\")\n",
    "    for dataset, predictions in samples.items():\n",
    "        if datasets_to_process and dataset not in datasets_to_process:\n",
    "            print(f'Skipping \"{dataset}\"')\n",
    "            continue\n",
    "        \n",
    "        images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "        images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "        if max_images is not None:\n",
    "            images = images[:max_images]\n",
    "    \n",
    "        print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "    \n",
    "        filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "        feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "        # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "        try:\n",
    "            # --- Pipeline Execution ---\n",
    "            \n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            # if CONFIG.ROTATION_CORRECTION:\n",
    "            #     rots = exec_rotation_detection(images, device)\n",
    "            # else:\n",
    "            #     rots = [ 0 for fname in images ]\n",
    "            rots = [ 0 for fname in images ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print(f'rotation_detection for {len(images)} images : {t:.4f} sec')\n",
    "            # print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            # 1. Detect ALIKED features and combine with DINO patch features\n",
    "            t = time()\n",
    "            index_pairs = get_image_pairs_shortlist(\n",
    "                images,\n",
    "                sim_th = 0.3, # should be strict\n",
    "                min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "                max_pairs = 25,\n",
    "                exhaustive_if_less = 20,\n",
    "                device=device\n",
    "            )\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            # print(\"\\n--- Step 1: Detecting ALIKED and Combining with DINO Patch Features ---\")\n",
    "            # detect_aliked_and_combine_with_dino(\n",
    "            #     img_fnames=images,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_features=4096,\n",
    "            #     resize_to=1024,\n",
    "            #     dino_processor=dino_processor,\n",
    "            #     dino_model=dino_model,\n",
    "            #     dino_patch_size=14, # Adjust based on your DINO model's patch size (e.g., 14 for DINOv2 base)\n",
    "            #     device=device\n",
    "            # )\n",
    "            # timings['global feature extraction'].append(time() - t)\n",
    "            # print (f'Gloabl feature extracting. Done in {time() - t:.4f} sec')\n",
    "            # gc.collect()\n",
    "            \n",
    "            # # 2. Get image pairs shortlist using VLAD global descriptors\n",
    "            # print(\"\\n--- Step 2: Generating Image Pair Shortlist using VLAD ---\")\n",
    "            # # Adjust num_clusters_vlad as needed (e.g., 64, 128, 256)\n",
    "            # # Higher clusters mean higher dimensionality for global descriptor.\n",
    "            # index_pairs = get_image_pairs_shortlist_vlad(\n",
    "            #     fnames=images,\n",
    "            #     sim_th=0.5,\n",
    "            #     min_pairs=20,\n",
    "            #     exhaustive_if_less=20,\n",
    "            #     feature_dir=feature_dir,\n",
    "            #     num_clusters_vlad=128, # Example: 128 clusters for VLAD\n",
    "            #     device=device\n",
    "            # )\n",
    "            # index_pairs = get_img_pairs_exhaustive(images)\n",
    "            \n",
    "            print(f\"Generated {len(index_pairs)} image pairs using VLAD global descriptor.\")\n",
    "            timings['shortlisting'].append(time() - t)\n",
    "            print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "            gc.collect()\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################    \n",
    "            t=time()\n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                images, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "            print (f'Local feature extracting and matching. Done in {time() - t:.4f} sec')\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            future = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                feature_dir, images_dir)\n",
    "            maps, local_timings = future.result()\n",
    "            #  timings\n",
    "            for k in local_timings:\n",
    "                timings[k].extend(local_timings[k])\n",
    "            # clear_output(wait=False)\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():  # cur_map: image_name  {'R': list, 't': list}\n",
    "                for image_name, pose in cur_map.items():\n",
    "                    idx = filename_to_index[image_name]\n",
    "                    pred = predictions[idx]\n",
    "                    pred.cluster_index = map_index\n",
    "                    pred.rotation = np.array(pose['R'])  # convert back to np.ndarray\n",
    "                    pred.translation = np.array(pose['t'])\n",
    "                    registered += 1\n",
    "            mapping_result_str = f\"Dataset  {dataset} -> Registered {registered} / {len(images)} images with {len(maps)} clusters\"\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # raise e\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "342a019a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T00:07:17.082960Z",
     "iopub.status.busy": "2025-05-29T00:07:17.082629Z",
     "iopub.status.idle": "2025-05-29T00:07:17.254390Z",
     "shell.execute_reply": "2025-05-29T00:07:17.253295Z"
    },
    "papermill": {
     "duration": 0.240298,
     "end_time": "2025-05-29T00:07:17.256031",
     "exception": false,
     "start_time": "2025-05-29T00:07:17.015733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster1,another_et_another_et001.png,0.999775980;-0.004417434;0.020699680;0.004333895;0.999982290;0.004078885;-0.020717332;-0.003988261;0.999777418,-2.142267782;-1.532801818;2.795184675\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster1,another_et_another_et002.png,0.999980251;-0.002372890;0.005819479;0.002487879;0.999800224;-0.019832370;-0.005771256;0.019846457;0.999786383,-1.979244121;-0.719173444;1.215936005\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster1,another_et_another_et003.png,0.997547576;-0.040328345;0.057205404;0.044728113;0.995961936;-0.077840972;-0.053835208;0.080208763;0.995323226,-2.215171652;0.887766789;-0.623781087\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster1,another_et_another_et004.png,0.999027603;-0.010829988;0.042738277;0.005938325;0.993581241;0.112964833;-0.043687358;-0.112601192;0.992679397,-2.103519779;-1.001561920;-0.246923012\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster1,another_et_another_et005.png,0.994755864;0.004095863;0.102195862;-0.008919504;0.998865030;0.046787756;-0.101888236;-0.047453931;0.993663379,-2.650362760;-1.730766683;1.236413013\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster1,another_et_another_et006.png,0.922982520;0.189001299;-0.335233913;-0.220354282;0.973710471;-0.057722698;0.315511106;0.127147270;0.940364990,0.223372209;-0.451929320;1.402229795\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster1,another_et_another_et007.png,0.786121579;0.256410975;-0.562375563;-0.312526911;0.949901433;-0.003767938;0.533235213;0.178719555;0.826873345,1.924645602;-0.233388984;1.132872481\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster1,another_et_another_et008.png,0.575524346;0.300699393;-0.760494314;-0.396282294;0.916012973;0.062294268;0.715354507;0.265518564;0.646349613,3.551837287;-0.655160265;2.140012701\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster1,another_et_another_et009.png,0.327589385;0.332698011;-0.884306072;-0.493085651;0.858583805;0.140358077;0.805947725;0.390058820;0.445311556,5.098007429;-1.097143213;2.967843647\r\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                #  `rotation` is a list of lists, flatten it\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()  # flatten 3x3 list -> 9 elems\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                #  `translation` is a flat list\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset, predictions in samples.items():\n",
    "            for prediction in predictions:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "\n",
    "                if prediction.rotation is None:\n",
    "                    rotation_str = none_to_str(9)\n",
    "                else:\n",
    "                    rotation_flat =  prediction.rotation.flatten()\n",
    "                    rotation_str = array_to_str(rotation_flat)\n",
    "\n",
    "                if prediction.translation is None:\n",
    "                    translation_str = none_to_str(3)\n",
    "                else:\n",
    "                    translation_str = array_to_str(prediction.translation)\n",
    "\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation_str},{translation_str}\\n')\n",
    "\n",
    "# Preview the output\n",
    "!head {submission_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "498526a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T00:07:17.396873Z",
     "iopub.status.busy": "2025-05-29T00:07:17.396555Z",
     "iopub.status.idle": "2025-05-29T00:07:17.400904Z",
     "shell.execute_reply": "2025-05-29T00:07:17.400228Z"
    },
    "papermill": {
     "duration": 0.072793,
     "end_time": "2025-05-29T00:07:17.402105",
     "exception": false,
     "start_time": "2025-05-29T00:07:17.329312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf4838",
   "metadata": {
    "papermill": {
     "duration": 0.068738,
     "end_time": "2025-05-29T00:07:17.537620",
     "exception": false,
     "start_time": "2025-05-29T00:07:17.468882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11924468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7505602,
     "sourceId": 11938492,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 563.050386,
   "end_time": "2025-05-29T00:07:21.199703",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T23:57:58.149317",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
