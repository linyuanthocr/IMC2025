{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11217117,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":4535,"sourceType":"modelInstanceVersion","modelInstanceId":3327,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Example submission\n\nImage Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n\nThis notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n\nRemember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# IMPORTANT \n#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:05.774855Z","iopub.execute_input":"2025-05-13T19:19:05.775246Z","iopub.status.idle":"2025-05-13T19:19:12.628656Z","shell.execute_reply.started":"2025-05-13T19:19:05.775215Z","shell.execute_reply":"2025-05-13T19:19:12.627422Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nInstalling collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\n  Attempting uninstall: kornia-rs\n    Found existing installation: kornia_rs 0.1.8\n    Uninstalling kornia_rs-0.1.8:\n      Successfully uninstalled kornia_rs-0.1.8\n  Attempting uninstall: kornia\n    Found existing installation: kornia 0.8.0\n    Uninstalling kornia-0.8.0:\n      Successfully uninstalled kornia-0.8.0\nSuccessfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\n\n# IMPORTANT Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric\nfrom sklearn.cluster import DBSCAN\n# ... other imports","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:12.630311Z","iopub.execute_input":"2025-05-13T19:19:12.630644Z","iopub.status.idle":"2025-05-13T19:19:44.939682Z","shell.execute_reply.started":"2025-05-13T19:19:12.630611Z","shell.execute_reply":"2025-05-13T19:19:44.939005Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nprint(\"PyTorch version:\", torch.__version__)\nimport sys\nprint(\"Python version:\", sys.version)\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"Device count:\", torch.cuda.device_count())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:44.941405Z","iopub.execute_input":"2025-05-13T19:19:44.941992Z","iopub.status.idle":"2025-05-13T19:19:45.078556Z","shell.execute_reply.started":"2025-05-13T19:19:44.941965Z","shell.execute_reply":"2025-05-13T19:19:45.077652Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu121\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nCUDA available: True\nCUDA version: 12.1\nDevice count: 2\nCurrent device: 0\nDevice name: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Do not forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.079737Z","iopub.execute_input":"2025-05-13T19:19:45.080006Z","iopub.status.idle":"2025-05-13T19:19:45.084559Z","shell.execute_reply.started":"2025-05-13T19:19:45.079983Z","shell.execute_reply":"2025-05-13T19:19:45.083687Z"}},"outputs":[{"name":"stdout","text":"device=device(type='cuda', index=0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def load_pil_image(fname):\n    \"\"\"Loads an image using PIL.\"\"\"\n    return Image.open(fname).convert('RGB')\n\ndef get_image_size(fname):\n    \"\"\"Gets image size (width, height) using PIL.\"\"\"\n    with Image.open(fname) as img:\n        return img.size # (width, height)\n\ndef get_original_coords(kp_coords, img_orig_size, variation_info):\n    \"\"\"\n    Transforms keypoint coordinates from variation space back to original image space.\n\n    Args:\n        kp_coords (np.ndarray): Keypoint coordinates [N, 2] in the variation space.\n        img_orig_size (tuple): Original image size (width, height).\n        variation_info (dict): Dictionary containing 'type' ('orig' or 'crop'),\n                               'scale_factor' (scale used for resize),\n                               'crop_box' ([x, y, w, h] in original coords, None if type is 'orig').\n\n    Returns:\n        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n    \"\"\"\n    if len(kp_coords) == 0:\n        return np.empty((0, 2))\n\n    coords = kp_coords.copy() # Work on a copy\n\n    # 1. Reverse scaling\n    scale_factor = variation_info['scale_factor']\n    coords /= scale_factor # Now coords are in the space of the original/cropped image (before resize)\n\n    # 2. Reverse cropping offset\n    if variation_info['type'] == 'crop' and variation_info['crop_box'] is not None:\n        x_crop, y_crop, _, _ = variation_info['crop_box']\n        coords[:, 0] += x_crop\n        coords[:, 1] += y_crop\n\n    # Ensure points are within original image bounds (optional, but good practice)\n    # coords[:, 0] = np.clip(coords[:, 0], 0, img_orig_size[0] - 1)\n    # coords[:, 1] = np.clip(coords[:, 1], 0, img_orig_size[1] - 1)\n\n    return coords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.085424Z","iopub.execute_input":"2025-05-13T19:19:45.085725Z","iopub.status.idle":"2025-05-13T19:19:45.106286Z","shell.execute_reply.started":"2025-05-13T19:19:45.085696Z","shell.execute_reply":"2025-05-13T19:19:45.105646Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- Configuration ---\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nGLOBAL_DESC_MODEL = '/kaggle/input/dinov2/pytorch/base/1' # Path to your DINOv2 model\nDATA_DIR = '.' # Base directory for outputs\n# FEATURE_DIR = os.path.join(DATA_DIR, 'features_combined')\n# MATCH_DIR = os.path.join(DATA_DIR, 'matches_global')\n\n# Initial detection parameters for cropping data collection\nINITIAL_DETECTION_RESIZE = 1280\nINITIAL_DETECTION_NUM_FEATURES = 4096\n\n# Parameters for TTA detection and combination\nTTA_SCALES = [2048]\nTTA_NUM_FEATURES = 2048\nUSE_CROPPED_IMAGES = True\n\n# Parameters for New Cropping Method\nMIN_PAIRS_FOR_CROPPING = 3\nDBSCAN_EPS = 20\nDBSCAN_MIN_SAMPLES = 5\nCROP_PADDING = 50\n\n# Coordinate precision for deduplication (rounding float coordinates)\nCOORD_PRECISION = 1 # Number of decimal places to round coordinates for uniqueness check\n\n# Matching parameters\nMIN_MATCHES_PER_VARIATION = 5 # Lowered this threshold slightly, as combining might filter some\nMIN_TOTAL_MATCHES_PER_PAIR = 20 # Minimum unique matches for a pair to be saved in global list\n\n# Output file names (within FEATURE_DIR and MATCH_DIR)\nKEYPOINTS_SUBDIR = 'keypoints'\nDESCRIPTORS_H5 = 'descriptors.h5'\nMATCHES_PT = 'matches.pt'\nCROP_DATA = 'crop_data.h5'\nCROP_INFO = 'crop_info.h5'\n\n# Parameters for graph building and clustering thresholds\n# These are the internal names used in the function; map external arguments to these if needed\nMIN_MATCHES_FOR_GRAPH_EDGE = 20  # Min matches for adding edge to graph\nMIN_MATCHES_FOR_FILTERED_GRAPH = 100 # Min matches for filtering graph (your aliked_dis_min)\nMIN_IMAGES_PER_CLUSTER = 2 # Min images in a final cluster\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.107035Z","iopub.execute_input":"2025-05-13T19:19:45.107291Z","iopub.status.idle":"2025-05-13T19:19:45.129283Z","shell.execute_reply.started":"2025-05-13T19:19:45.107270Z","shell.execute_reply":"2025-05-13T19:19:45.128651Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\n\n# # Must Use efficientnet global descriptor to get matching shortlists.\n# def get_global_desc(fnames, device = torch.device('cpu')):\n#     processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n#     model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n#     model = model.eval()\n#     model = model.to(device)\n#     global_descs_dinov2 = []\n#     for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n#         key = os.path.splitext(os.path.basename(img_fname_full))[0]\n#         timg = load_torch_image(img_fname_full)\n#         with torch.inference_mode():\n#             inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n#             outputs = model(**inputs)\n#             dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n#         global_descs_dinov2.append(dino_mac.detach().cpu())\n#     global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n#     return global_descs_dinov2\n\n\ndef get_global_desc(fnames, device=torch.device('cpu')):\n    \"\"\"Computes global descriptors for images.\"\"\"\n    print(f\"Computing global descriptors with DINOv2 on {device}...\")\n    processor = AutoImageProcessor.from_pretrained(GLOBAL_DESC_MODEL)\n    model = AutoModel.from_pretrained(GLOBAL_DESC_MODEL)\n    model = model.eval().to(device)\n\n    global_descs_dinov2 = []\n    for img_fname_full in tqdm(fnames, desc=\"DINOv2 Descriptors\"):\n        # Need error handling here for potentially problematic images\n        try:\n            timg = load_torch_image(img_fname_full, device=device)\n            with torch.inference_mode():\n                inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n                outputs = model(**inputs)\n                # Using CLS token or pooling as descriptor\n                # descriptor = outputs.last_hidden_state[:, 0].mean(dim=1) # CLS token\n                descriptor = F.normalize(outputs.last_hidden_state[:, 1:].max(dim=1)[0], dim=1, p=2) # Pool spatial tokens\n                global_descs_dinov2.append(descriptor.detach().cpu())\n        except Exception as e:\n            print(f\"Error processing {img_fname_full} for global descriptor: {e}\")\n            # Append a zero vector or handle missing descriptor later\n            global_descs_dinov2.append(torch.zeros(1, model.config.hidden_size))\n\n\n    # Pad or handle missing descriptors if errors occurred\n    max_dim = max(d.shape[1] for d in global_descs_dinov2)\n    global_descs_dinov2 = [\n        F.pad(d, (0, max_dim - d.shape[1])) for d in global_descs_dinov2\n    ]\n    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n\n    return global_descs_dinov2\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 30,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc(fnames, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n\n    # 只分析上三角（去掉对角线），避免重复\n    triu_indices = np.triu_indices_from(dm, k=1)\n    dm_flat = dm[triu_indices]\n    \n    # 打印统计信息\n    print(\"Distance Matrix Statistics:\")\n    print(f\"Min:  {dm_flat.min():.4f}\")\n    print(f\"Max:  {dm_flat.max():.4f}\")\n    print(f\"Mean: {dm_flat.mean():.4f}\")\n    print(f\"Std:  {dm_flat.std():.4f}\")\n    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n    # removing half\n    # thr = min(np.percentile(dm_flat, 50), sim_th)\n    mask = dm <= np.percentile(dm_flat, 50)\n    # print(\"thr :\", thr)\n    # mask = dm<=threshold\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < threshold:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\n# def detect_aliked(img_fnames,\n#                   feature_dir = '.featureout',\n#                   num_features = 4096,\n#                   resize_to = 1024,\n#                   device=torch.device('cpu')):\n#     dtype = torch.float32 # ALIKED has issues with float16\n#     extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n#     extractor.preprocess_conf[\"resize\"] = resize_to\n#     if not os.path.isdir(feature_dir):\n#         os.makedirs(feature_dir)\n#     with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n#          h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n#         for img_path in tqdm(img_fnames):\n#             img_fname = img_path.split('/')[-1]\n#             key = img_fname\n#             with torch.inference_mode():\n#                 image0 = load_torch_image(img_path, device=device).to(dtype)\n#                 feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n#                 kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n#                 descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n#                 f_kp[key] = kpts\n#                 f_desc[key] = descs\n#     return\n\n# def match_with_lightglue(img_fnames,\n#                    index_pairs,\n#                    feature_dir = '.featureout',\n#                    device=torch.device('cpu'),\n#                    min_matches=20,verbose=True):\n#     lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n#                                                 \"depth_confidence\": -1,\n#                                                  \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n#     with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n#         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n#         h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n#         for pair_idx in tqdm(index_pairs):\n#             idx1, idx2 = pair_idx\n#             fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n#             key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n#             kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n#             kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n#             desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n#             desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n#             with torch.inference_mode():\n#                 dists, idxs = lg_matcher(desc1,\n#                                          desc2,\n#                                          KF.laf_from_center_scale_ori(kp1[None]),\n#                                          KF.laf_from_center_scale_ori(kp2[None]))\n#             if len(idxs)  == 0:\n#                 continue\n#             n_matches = len(idxs)\n#             if verbose:\n#                 print (f'{key1}-{key2}: {n_matches} matches')\n#             group  = f_match.require_group(key1)\n#             if n_matches >= min_matches:\n#                  group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n#     return\n\n\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.129973Z","iopub.execute_input":"2025-05-13T19:19:45.130239Z","iopub.status.idle":"2025-05-13T19:19:45.154656Z","shell.execute_reply.started":"2025-05-13T19:19:45.130219Z","shell.execute_reply":"2025-05-13T19:19:45.154071Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def calculate_kornia_resize_scale(original_size_hw, target_resize):\n    \"\"\"\n    Calculates the scale factor Kornia's default resize applies.\n    Assumes aspect ratio is maintained and longer side is scaled to target_resize,\n    only if longer side > target_resize.\n\n    Args:\n        original_size_hw (tuple): Original image size (H, W).\n        target_resize (int): The target size for the longer side.\n\n    Returns:\n        float: The scale factor applied (processed_size / original_size).\n    \"\"\"\n    h_orig, w_orig = original_size_hw\n    max_orig_dim = max(h_orig, w_orig)\n\n    if target_resize is None or target_resize <= 0 or max_orig_dim <= target_resize:\n        # No resizing or scaling up is needed based on default logic\n        return 1.0\n    else:\n        # Scale down the longer side to target_resize\n        return target_resize / max_orig_dim\n\ndef transform_points_from_processed(kp_processed, processed_scale_factor, crop_box=None):\n    \"\"\"\n    Transforms keypoint coordinates from the 'processed' scale space\n    back to the original image space, accounting for scaling and cropping.\n\n    Args:\n        kp_processed (np.ndarray): Keypoint coordinates [N, 2] in the processed space (after scaling by ALIKED).\n        processed_scale_factor (float): The calculated scale factor applied by ALIKED (processed_size / original_or_cropped_size).\n        crop_box (list): [x, y, w, h] of the crop in original image coords, or None.\n\n    Returns:\n        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n    \"\"\"\n    if len(kp_processed) == 0:\n        return np.empty((0, 2), dtype=np.float32)\n\n    coords = kp_processed.copy().astype(np.float32) # Ensure float32\n\n    # 1. Reverse scaling (from processed scale back to original or cropped scale)\n    if processed_scale_factor > 0:\n        coords /= processed_scale_factor\n    # else: scale_factor is 1.0, no change needed here\n\n    # 2. Add cropping offset (from cropped coordinates back to original coordinates)\n    if crop_box is not None and len(crop_box) == 4 and crop_box[2] > 0 and crop_box[3] > 0:\n        x_crop, y_crop, _, _ = crop_box\n        coords[:, 0] += x_crop\n        coords[:, 1] += y_crop\n\n    # Note: We don't clip to original bounds here, as that might discard valid points near edges.\n    # Downstream steps should handle points outside bounds if necessary.\n\n    return coords\n\n# Remove the old get_keypoint_original_coords function entirely","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.156912Z","iopub.execute_input":"2025-05-13T19:19:45.157108Z","iopub.status.idle":"2025-05-13T19:19:45.179781Z","shell.execute_reply.started":"2025-05-13T19:19:45.157091Z","shell.execute_reply":"2025-05-13T19:19:45.179002Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ... (imports, configs, utility functions like load_torch_image, load_pil_image, get_image_size)\n# Add the new calculate_kornia_resize_scale and transform_points_from_processed functions here\n\ndef perform_initial_detection_and_matching(img_fnames, index_pairs, data_dir, device=DEVICE):\n    \"\"\"\n    Performs detection (ALIKED) and matching (LightGlue) on original images\n    at a base resolution (e.g., 1024) to collect data for the cropping step.\n    Stores keypoints (at detection scale) and matches in temporary HDF5.\n    Analyzes matches to create crop data file.\n    \"\"\"\n    temp_feature_dir = os.path.join(data_dir, '.temp_crop_features')\n    os.makedirs(temp_feature_dir, exist_ok=True)\n\n    initial_feature_file = os.path.join(temp_feature_dir, f'initial_features_{INITIAL_DETECTION_RESIZE}.h5')\n    initial_match_file = os.path.join(temp_feature_dir, f'initial_matches_{INITIAL_DETECTION_RESIZE}.h5')\n    crop_data_file = os.path.join(data_dir, CROP_DATA)\n\n    if os.path.exists(crop_data_file):\n         print(f\"Initial detection and matching data for cropping exists: {crop_data_file}. deleting.\")\n         os.remove\n         # return crop_data_file\n\n    print(f\"Performing initial ALIKED detection ({INITIAL_DETECTION_RESIZE}) and LightGlue matching for cropping data...\")\n\n    # 1. Initial Detection\n    print(\"Running initial ALIKED detection...\")\n    try:\n        extractor = ALIKED(max_num_keypoints=INITIAL_DETECTION_NUM_FEATURES, detection_threshold=0.1).eval().to(device, dtype=torch.float32)\n        # Set resize parameter here\n        extractor.preprocess_conf[\"resize\"] = INITIAL_DETECTION_RESIZE\n        # Ensure ALIKED is on the correct device/dtype\n        # extractor.to(device, dtype=torch.float32)\n\n    except Exception as e:\n        print(f\"Error loading ALIKED extractor: {e}\")\n        return None\n\n    with h5py.File(initial_feature_file, mode='w') as f_kp_desc:\n        for img_path in tqdm(img_fnames, desc=\"Initial ALIKED Detection\"):\n            img_key = os.path.basename(img_path)\n            kp = None # Initialize features to None\n            desc = None\n            calculated_scale = None # Will store the calculated scale factor\n            original_pil_size = None\n\n            try:\n                # Load original PIL image to get its size\n                img_orig_pil = load_pil_image(img_path)\n                if img_orig_pil is None:\n                     print(f\"Skipping ALIKED for {img_path}: PIL image loading failed.\")\n                     continue\n                original_pil_size = img_orig_pil.size # (W, H)\n\n                # Calculate the expected scale factor ALIKED will apply\n                # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n                # Input image tensor size will be (H, W) after Kornia loading/conversion\n                input_tensor_size_hw = (original_pil_size[1], original_pil_size[0]) # Convert (W, H) to (H, W)\n                calculated_scale = calculate_kornia_resize_scale(input_tensor_size_hw, INITIAL_DETECTION_RESIZE)\n\n\n                # Load image as Kornia tensor for ALIKED\n                timg = K.image_to_tensor(np.array(img_orig_pil), keepdim=True).to(device, torch.float32) / 255.0 # Normalize\n                if timg.ndim == 3: timg = timg[None, ...] # Ensure BxCxHxW\n\n\n                with torch.inference_mode():\n                    # Pass resize to extractor, but we don't expect processed_size back\n                    # The scale is calculated based on preprocess_conf[\"resize\"]\n                    feats = extractor.extract(timg)\n\n\n                    # --- Add Error Handling for accessing feats dictionary ---\n                    try:\n                        # Check for expected keys ('keypoints', 'descriptors')\n                        if 'keypoints' in feats and 'descriptors' in feats and \\\n                           len(feats.get('keypoints', [])) > 0 and len(feats.get('descriptors', [])) > 0: # Use .get with default for safety\n\n                            kp = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                            desc = feats['descriptors'].reshape(len(kp), -1).detach().cpu().numpy()\n\n                            # Check length after reshaping just in case\n                            if len(kp) == 0 or len(desc) == 0:\n                                 print(f\"Warning: Extracted features are empty for {img_path} after reshape.\")\n                                 kp = None; desc = None # Invalidate data\n\n\n                        else:\n                             missing_keys = [k for k in ['keypoints', 'descriptors'] if k not in feats]\n                             empty_data_keys = [k for k in ['keypoints', 'descriptors'] if k in feats and len(feats[k]) == 0]\n\n                             if missing_keys or empty_data_keys:\n                                 print(f\"Warning: Missing or empty required data in ALIKED output for {img_path}. (Missing Keys: {missing_keys}, Empty Data Keys: {empty_data_keys})\")\n\n                             kp = None; desc = None # Ensure invalid data is cleared\n\n\n                    except KeyError as ke:\n                         # This catches if the keys 'keypoints' or 'descriptors' are unexpectedly missing entirely\n                         print(f\"Error: Expected key '{ke}' not found in ALIKED features for {img_path}.\")\n                         kp = None; desc = None\n                    except Exception as e:\n                         print(f\"Unexpected error processing ALIKED features result for {img_path}: {e}\")\n                         kp = None; desc = None\n\n\n            except Exception as e:\n                 print(f\"Error during initial ALIKED feature extraction for {img_path}: {e}\")\n\n\n            # --- Check if features were successfully obtained and save ---\n            if kp is not None and desc is not None and calculated_scale is not None and original_pil_size is not None:\n                try:\n                    img_group = f_kp_desc.create_group(img_key)\n                    # Save keypoints AS IS (in the processed scale space)\n                    img_group.create_dataset('keypoints', data=kp.astype(np.float32))\n                    img_group.create_dataset('descriptors', data=desc.astype(np.float32))\n                    # Store the calculated scale factor and original size for later transformation\n                    img_group.attrs['calculated_scale_factor'] = float(calculated_scale)\n                    img_group.attrs['original_pil_size'] = original_pil_size # (W, H) tuple\n                    img_group.attrs['original_path'] = img_path # Store original path\n                except Exception as e:\n                     print(f\"Error saving initial features for {img_path} to HDF5: {e}\")\n            else:\n                print(f\"Skipping saving initial features for {img_path} due to extraction failure or empty results.\")\n\n\n    # ... (rest of the perform_initial_detection_and_matching function: Initial Matching, Analyze Matches, Save crop_data.h5)\n    # Ensure the rest of the function correctly handles cases where some images might not have initial features saved in f_kp_desc\n\n    # 2. Initial Matching (Keep this part as it's needed for the current cropping method)\n    print(\"Running initial LightGlue matching...\")\n    try:\n        lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                    \"depth_confidence\": -1,\n                                                    \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n        if device == torch.device('cpu'):\n             lg_matcher.to('cpu')\n\n    except Exception as e:\n         print(f\"Error loading LightGlue matcher: {e}\")\n         pass # Allow to continue to analysis if possible\n\n\n    # Ensure initial_match_file is created even if empty, if matching failed entirely\n    # This prevents subsequent H5 read errors\n    if not os.path.exists(initial_match_file): # Corrected typo here\n        try:\n            with h5py.File(initial_match_file, mode='w') as f:\n                pass # Create an empty file\n        except Exception as e:\n            print(f\"Error creating empty initial match file {initial_match_file}: {e}\")\n\n\n    # Match only images that had features successfully extracted\n    # Read keys from the initial_feature_file HDF5\n    extracted_image_keys = []\n    if os.path.exists(initial_feature_file): # Corrected typo here\n        try:\n            with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read:\n                 extracted_image_keys = list(f_kp_desc_read.keys())\n        except Exception as e:\n            print(f\"Error reading keys from initial feature file {initial_feature_file}: {e}\")\n\n\n    # Create a mapping from image key to its original index\n    # key_to_idx = {os.path.basename(fname): i for i, fname in enumerate(img_fnames)} # Not used in this block directly\n\n    # Filter index_pairs to include only pairs where both images had features extracted\n    filtered_index_pairs = []\n    for idx1, idx2 in index_pairs:\n         key1, key2 = os.path.basename(img_fnames[idx1]), os.path.basename(img_fnames[idx2])\n         if key1 in extracted_image_keys and key2 in extracted_image_keys:\n              filtered_index_pairs.append((idx1, idx2))\n         # else: print(f\"Skipping initial match for {key1}-{key2}: features not extracted for one or both.\")\n\n\n    if not filtered_index_pairs:\n         print(\"No image pairs with extracted features to perform initial matching.\")\n         # Proceed to analysis, crop_data.h5 might be empty\n\n    else:\n        with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read, \\\n             h5py.File(initial_match_file, mode='a') as f_match: # Use append mode if file might exist but be incomplete\n\n            for idx1, idx2 in tqdm(filtered_index_pairs, desc=\"Initial LightGlue Matching\"):\n                fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n                key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n\n                # Check if pair already matched (useful if appending)\n                if key1 in f_match and key2 in f_match[key1]:\n                     continue # Skip if already matched\n\n                try:\n                    # Load keypoints and descriptors from the initial detection file\n                    # These KPs are at the processed scale\n                    # Check for dataset existence within the group\n                    if 'keypoints' not in f_kp_desc_read[key1] or 'descriptors' not in f_kp_desc_read[key1] or \\\n                       'keypoints' not in f_kp_desc_read[key2] or 'descriptors' not in f_kp_desc_read[key2]:\n                         print(f\"Warning: Missing keypoint/descriptor datasets for {key1}-{key2} in initial feature file. Skipping match.\")\n                         continue\n\n                    kp1 = torch.from_numpy(f_kp_desc_read[key1]['keypoints'][...]).to(device)\n                    kp2 = torch.from_numpy(f_kp_desc_read[key2]['keypoints'][...]).to(device)\n                    desc1 = torch.from_numpy(f_kp_desc_read[key1]['descriptors'][...]).to(device)\n                    desc2 = torch.from_numpy(f_kp_desc_read[key2]['descriptors'][...]).to(device)\n\n                    if len(kp1) == 0 or len(kp2) == 0:\n                         continue\n\n                    # Create LAFs based on processed scale keypoints\n                    # Assuming scale 1.0 relative to processed size is appropriate here\n                    laf1 = KF.laf_from_center_scale_ori(kp1[None])\n                    laf2 = KF.laf_from_center_scale_ori(kp2[None])\n\n                    with torch.inference_mode():\n                        dists, idxs = lg_matcher(desc1, desc2, laf1, laf2)\n                    if len(idxs) > 0:\n                        group = f_match.require_group(key1)\n                        group.create_dataset(key2, data=idxs.detach().cpu().numpy().astype(np.int32))\n\n                except Exception as e:\n                    print(f\"Error during initial LightGlue matching for {key1}-{key2}: {e}\")\n\n\n    # --- (rest of the Analyze Initial Matches part) ---\n    print(\"Analyzing initial matches for cropping data...\")\n\n    kp_match_pairs = {}\n\n    if os.path.exists(initial_match_file):\n        try:\n            with h5py.File(initial_match_file, mode='r') as f_match:\n                for img_key1 in f_match.keys():\n                    for img_key2 in f_match[img_key1].keys():\n                        try:\n                            matches = f_match[img_key1][img_key2][...] # Indices (kp1_idx, kp2_idx)\n\n                            for kp1_idx, kp2_idx in matches:\n                                if img_key1 not in kp_match_pairs: kp_match_pairs[img_key1] = {}\n                                if kp1_idx not in kp_match_pairs[img_key1]: kp_match_pairs[img_key1][kp1_idx] = set()\n                                kp_match_pairs[img_key1][kp1_idx].add(img_key2)\n\n                                if img_key2 not in kp_match_pairs: kp_match_pairs[img_key2] = {}\n                                if kp2_idx not in kp_match_pairs[img_key2]: kp_match_pairs[img_key2][kp2_idx] = set()\n                                kp_match_pairs[img_key2][kp2_idx].add(img_key1)\n\n                        except Exception as e:\n                            print(f\"Error processing initial match data for {img_key1}-{img_key2} in analysis: {e}\")\n        except Exception as e:\n             print(f\"Error reading initial match file {initial_match_file} for analysis: {e}\")\n\n\n    frequent_kp_data = {}\n\n    if os.path.exists(initial_feature_file):\n        try:\n            with h5py.File(initial_feature_file, mode='r') as f_kp_desc:\n                 for img_key in kp_match_pairs.keys():\n                     if img_key not in f_kp_desc:\n                          print(f\"Warning: Initial features not found for {img_key} (present in matches but not in detection file). Skipping crop data processing for this image.\")\n                          continue\n\n                     try:\n                         # Check for needed data/attrs\n                         if 'keypoints' not in f_kp_desc[img_key] or 'calculated_scale_factor' not in f_kp_desc[img_key].attrs or 'original_pil_size' not in f_kp_desc[img_key].attrs:\n                              print(f\"Warning: Incomplete initial feature data for {img_key}. Skipping crop data processing.\")\n                              continue\n\n                         all_kp_coords_processed = f_kp_desc[img_key]['keypoints'][...] # KPs at processed scale\n                         calculated_scale_factor = f_kp_desc[img_key].attrs['calculated_scale_factor']\n                         original_pil_size = f_kp_desc[img_key].attrs['original_pil_size'] # (W, H)\n\n                         kp_data_dict = kp_match_pairs[img_key]\n\n                         frequent_indices = [kp_idx for kp_idx, matched_pairs in kp_data_dict.items()\n                                             if len(matched_pairs) >= MIN_PAIRS_FOR_CROPPING and kp_idx < len(all_kp_coords_processed)] # Bounds check\n\n\n                         if frequent_indices:\n                             # Get coordinates at the processed scale\n                             frequent_coords_processed = all_kp_coords_processed[frequent_indices]\n\n                             # Transform coordinates back to original image space using the calculated scale\n                             # We pass None for crop_box as these points are from the original (non-cropped) image\n                             frequent_coords_orig_scale = transform_points_from_processed(\n                                 frequent_coords_processed, calculated_scale_factor, crop_box=None\n                             )\n\n                             # Check for valid transformed coordinates (non-negative)\n                             valid_frequent_coords_orig = frequent_coords_orig_scale[~np.any(frequent_coords_orig_scale < 0, axis=1)]\n\n\n                             if len(valid_frequent_coords_orig) > 0:\n                                 frequent_kp_data[img_key] = {\n                                     'kp_coords_original_scale': valid_frequent_coords_orig\n                                 }\n                             else:\n                                  print(f\"No valid frequent keypoints in original scale for {img_key}.\")\n\n                         else:\n                              print(f\"No frequent keypoints found for {img_key} (threshold={MIN_PAIRS_FOR_CROPPING} pairs).\")\n\n                     except Exception as e:\n                         print(f\"Error processing frequent keypoints for {img_key}: {e}\")\n\n        except Exception as e:\n             print(f\"Error reading initial feature file {initial_feature_file} for analysis: {e}\")\n\n\n    # ... (rest of the saving frequent_kp_data to crop_data_file) ...\n    with h5py.File(crop_data_file, mode='w') as f_crop_data:\n        if frequent_kp_data:\n            for img_key, data in frequent_kp_data.items():\n                 group = f_crop_data.create_group(img_key)\n                 group.create_dataset('kp_coords_original_scale', data=data['kp_coords_original_scale'])\n\n\n    print(f\"Initial detection and matching complete. Cropping data saved to {crop_data_file}\")\n\n    return crop_data_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.181225Z","iopub.execute_input":"2025-05-13T19:19:45.181519Z","iopub.status.idle":"2025-05-13T19:19:45.214736Z","shell.execute_reply.started":"2025-05-13T19:19:45.181497Z","shell.execute_reply":"2025-05-13T19:19:45.214077Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- New Cropping Method ---\n\ndef calculate_crop_boxes(img_fnames, crop_data_file, feature_dir, data_dir, verbose = False):\n    \"\"\"\n    Calculates crop bounding boxes for each image based on clustered frequent keypoints.\n    Stores crop boxes in a dedicated HDF5 file.\n    \"\"\"\n    print(\"Calculating crop boxes using DBSCAN...\")\n    # Store crop info in a separate file for clarity\n    crop_info_file = os.path.join(data_dir, CROP_INFO)\n    if verbose:\n        if os.path.exists(crop_data_file):\n            print(f\"Crop file : {crop_data_file} found\")\n        else:\n            print(f\"Crop file : {crop_data_file} not found\")\n    os.makedirs(data_dir, exist_ok=True)\n\n\n    with h5py.File(crop_data_file, mode='r') as f_crop_data, \\\n         h5py.File(crop_info_file, mode='w') as f_crop_info: # Use write mode to start fresh\n\n        for img_path in tqdm(img_fnames, desc=\"Calculating Crops\"):\n            img_key = os.path.basename(img_path)\n\n            if img_key not in f_crop_data:\n                 print(f\"No cropping data for {img_key}. No crop will be used.\")\n                 img_group = f_crop_info.create_group(img_key)\n                 img_group.attrs['has_crop'] = False\n                 img_group.attrs['crop_box'] = [0, 0, 0, 0] # Store a dummy box\n                 continue\n\n            try:\n                # Load frequent keypoints (at original scale)\n                frequent_coords_orig_scale = f_crop_data[img_key]['kp_coords_original_scale'][...]\n\n                img_orig_w, img_orig_h = get_image_size(img_path)\n                if img_orig_w is None or img_orig_h is None:\n                    print(f\"Could not get original size for {img_key}. Skipping crop calculation.\")\n                    img_group = f_crop_info.create_group(img_key)\n                    img_group.attrs['has_crop'] = False\n                    img_group.attrs['crop_box'] = [0, 0, 0, 0]\n                    continue\n\n\n                if len(frequent_coords_orig_scale) > 0:\n                    # Apply DBSCAN on original scale coordinates\n                    # Optional: Scale coordinates for DBSCAN if ranges vary greatly\n                    # scaler = StandardScaler()\n                    # frequent_coords_scaled = scaler.fit_transform(frequent_coords_orig_scale)\n                    # db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(frequent_coords_scaled) # EPS might need scaling too\n                    # Using raw coordinates is often simpler if EPS is tuned for pixel values\n                    db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(frequent_coords_orig_scale)\n                    labels = db.labels_\n\n                    # Find the bounding box of all non-noise clustered points\n                    clustered_points = frequent_coords_orig_scale[labels != -1] # Exclude noise points (-1)\n\n                    if len(clustered_points) > 0:\n                        min_x, min_y = np.min(clustered_points, axis=0)\n                        max_x, max_y = np.max(clustered_points, axis=0)\n\n                        # Add padding and ensure bounds are within the original image\n                        min_x = max(0.0, min_x - CROP_PADDING)\n                        min_y = max(0.0, min_y - CROP_PADDING)\n                        max_x = min(float(img_orig_w - 1), max_x + CROP_PADDING)\n                        max_y = min(float(img_orig_h - 1), max_y + CROP_PADDING)\n\n                        # Ensure crop has positive dimensions\n                        crop_w = max_x - min_x + 1\n                        crop_h = max_y - min_y + 1\n\n                        if crop_w > 0 and crop_h > 0:\n                            # Crop box format: [x, y, w, h] (integers for simplicity)\n                            crop_box = [int(min_x), int(min_y), int(crop_w), int(crop_h)]\n                            if verbose:\n                                print(f\"Calculated crop box for {img_key}: {crop_box}\")\n\n                            # Store crop box\n                            img_group = f_crop_info.create_group(img_key)\n                            img_group.attrs['has_crop'] = True\n                            img_group.attrs['crop_box'] = crop_box\n                        else:\n                            print(f\"Calculated crop box for {img_key} has zero dimensions. No crop will be used.\")\n                            img_group = f_crop_info.create_group(img_key)\n                            img_group.attrs['has_crop'] = False\n                            img_group.attrs['crop_box'] = [0, 0, 0, 0]\n\n\n                    else:\n                        print(f\"DBSCAN found no significant clusters for {img_key}. No crop will be used.\")\n                        img_group = f_crop_info.create_group(img_key)\n                        img_group.attrs['has_crop'] = False\n                        img_group.attrs['crop_box'] = [0, 0, 0, 0]\n\n                else:\n                     print(f\"No frequent keypoints found for {img_key}. No crop will be used.\")\n                     img_group = f_crop_info.create_group(img_key)\n                     img_group.attrs['has_crop'] = False\n                     img_group.attrs['crop_box'] = [0, 0, 0, 0]\n\n\n            except Exception as e:\n                print(f\"Error calculating crop box for {img_key}: {e}\")\n                img_group = f_crop_info.create_group(img_key)\n                img_group.attrs['has_crop'] = False\n                img_group.attrs['crop_box'] = [0, 0, 0, 0]\n\n\n    print(\"Crop box calculation complete.\")\n    return crop_info_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.215511Z","iopub.execute_input":"2025-05-13T19:19:45.215814Z","iopub.status.idle":"2025-05-13T19:19:45.238403Z","shell.execute_reply.started":"2025-05-13T19:19:45.215782Z","shell.execute_reply":"2025-05-13T19:19:45.237785Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ... (imports, configs, utility functions like calculate_kornia_resize_scale, transform_points_from_processed)\n# Note: load_torch_image and load_pil_image are also needed\n\ndef detect_and_combine_features(img_fnames, crop_info_file, feature_dir, device=DEVICE, verbose=False):\n    \"\"\"\n    Detects ALIKED features for multiple scales and original/cropped images,\n    combines unique features per image (deduplicating based on original coords),\n    and saves combined features to .pt and .h5 files per image.\n    \"\"\"\n    print(\"Running multi-variation ALIKED detection and combining features...\")\n    os.makedirs(feature_dir, exist_ok=True)\n    keypoints_subdir_path = os.path.join(feature_dir, KEYPOINTS_SUBDIR)\n    os.makedirs(keypoints_subdir_path, exist_ok=True)\n    descriptors_h5_path = os.path.join(feature_dir, DESCRIPTORS_H5)\n\n    with h5py.File(descriptors_h5_path, mode='w') as f_descriptors, \\\n         h5py.File(crop_info_file, mode='r') as f_crop_info:\n\n        extractor = None # Initialize extractor outside the loop\n\n        for img_path in tqdm(img_fnames, desc=\"Detecting & Combining Features\"):\n            img_key = os.path.basename(img_path)\n            kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt')\n\n            # Skip if combined features already exist for this image\n            if os.path.exists(kp_pt_path) and img_key in f_descriptors:\n                 print(f\"Combined features for {img_key} already exist. Skipping detection.\")\n                 continue\n\n            img_orig_pil = load_pil_image(img_path)\n            if img_orig_pil is None:\n                print(f\"Could not load original image {img_path}. Skipping.\")\n                # Create empty files/datasets to indicate processing happened (and failed)\n                try:\n                    torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n                    f_descriptors.create_group(img_key)\n                    if verbose:\n                        print(f\"Created empty combined feature files for {img_key}.\")\n                except Exception as e:\n                     print(f\"Error creating empty combined feature files for {img_key}: {e}\")\n                continue\n\n            img_orig_w, img_orig_h = img_orig_pil.size\n            img_orig_size_hw = (img_orig_h, img_orig_w) # (H, W)\n\n            # Get crop info\n            has_crop = False\n            # Default dummy crop box, will be [0,0,0,0] if no crop or invalid\n            crop_box = [0, 0, 0, 0]\n            if img_key in f_crop_info:\n                 img_crop_group = f_crop_info[img_key]\n                 has_crop = img_crop_group.attrs.get('has_crop', False)\n                 crop_box = img_crop_group.attrs.get('crop_box', [0, 0, 0, 0])\n\n            # Ensure crop_box is valid if has_crop is true\n            if has_crop and (len(crop_box) != 4 or crop_box[2] <= 0 or crop_box[3] <= 0):\n                 print(f\"Warning: Invalid crop box {crop_box} for {img_key} despite has_crop=True. Ignoring crop.\")\n                 has_crop = False\n                 crop_box = [0, 0, 0, 0]\n\n\n            variations_to_process = []\n            for scale in TTA_SCALES:\n                # Original image variations\n                variations_to_process.append({'type': 'orig', 'scale_target': scale, 'crop_box': None, 'pil_img': img_orig_pil})\n                # Cropped image variations (if enabled and crop exists and is valid)\n                if USE_CROPPED_IMAGES and has_crop:\n                    x, y, w, h = crop_box\n                    try:\n                         img_cropped_pil = img_orig_pil.crop((x, y, x + w, y + h))\n                         variations_to_process.append({'type': 'crop', 'scale_target': scale, 'crop_box': crop_box, 'pil_img': img_cropped_pil})\n                    except Exception as e:\n                         print(f\"Error cropping image {img_key} with box {crop_box}: {e}. Skipping cropped variation.\")\n\n\n            all_kps_orig_coords = [] # List to collect all transformed keypoints (in original image space)\n            all_descriptors = []     # List to collect corresponding descriptors\n\n            if extractor is None:\n                 try:\n                    extractor = ALIKED(max_num_keypoints=TTA_NUM_FEATURES, detection_threshold=0.1).eval().to(DEVICE, dtype=torch.float32)\n                    if DEVICE == torch.device('cpu'):\n                         extractor.to('cpu', torch.float32)\n                 except Exception as e:\n                    print(f\"Error loading ALIKED extractor: {e}\")\n                    # Create empty files/datasets on extractor failure as well\n                    try:\n                        torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n                        f_descriptors.create_group(img_key)\n                        if verbose:\n                            print(f\"Created empty combined feature files for {img_key} due to extractor failure.\")\n                    except Exception as e_save:\n                         print(f\"Error creating empty combined feature files for {img_key} on extractor failure: {e_save}\")\n                    continue # Move to next image if extractor fails\n\n            for var_info in variations_to_process:\n                 var_type = var_info['type']\n                 var_scale_target = var_info['scale_target']\n                 var_pil_img = var_info['pil_img']\n                 var_crop_box = var_info['crop_box']\n\n                 try:\n                     # Get the size of the PIL image *for this variation*\n                     var_pil_size_wh = var_pil_img.size # (W, H)\n                     var_pil_size_hw = (var_pil_size_wh[1], var_pil_size_wh[0]) # (H, W)\n\n                     # Calculate the expected scale factor ALIKED applies to *this variation's* PIL image size\n                     calculated_scale_variation = calculate_kornia_resize_scale(var_pil_size_hw, var_scale_target)\n\n\n                     # Convert PIL image to Kornia tensor\n                     timg = K.image_to_tensor(np.array(var_pil_img), keepdim=True).to(device, torch.float32) / 255.0\n                     if timg.ndim == 3: timg = timg[None, ...] # Ensure BxCxHxW\n\n                     with torch.inference_mode():\n                         # Pass resize to extractor. Keypoints will be in space scaled by calculated_scale_variation\n                         feats = extractor.extract(timg, resize=var_scale_target, return_processed_size=False) # Do not request processed_size\n\n\n                     # --- Add Error Handling for accessing feats dictionary ---\n                     try:\n                         if 'keypoints' in feats and 'descriptors' in feats and \\\n                            len(feats.get('keypoints', [])) > 0 and len(feats.get('descriptors', [])) > 0:\n\n                             kp_variation = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                             desc_variation = feats['descriptors'].reshape(len(kp_variation), -1).detach().cpu().numpy()\n\n                             if len(kp_variation) > 0 and calculated_scale_variation is not None: # Check scale is valid too\n                                 # Transform keypoints from the variation's processed scale back to original image coordinates\n                                 kp_orig_coords = transform_points_from_processed(\n                                     kp_variation, calculated_scale_variation, var_crop_box\n                                 )\n\n                                 # Filter out any points that ended up outside original bounds or invalid (optional but good)\n                                 # This check should ideally be within transform_points_from_processed or after\n                                 # Let's trust transform_points_from_processed returns valid numpy array\n                                 # kp_orig_coords are already in original coordinates space.\n\n                                 all_kps_orig_coords.append(kp_orig_coords)\n                                 all_descriptors.append(desc_variation)\n                             else:\n                                 print(f\"Warning: Extracted features empty or scale invalid for {img_key} ({var_type}, {var_scale_target}).\")\n\n\n                         else:\n                             missing_keys = [k for k in ['keypoints', 'descriptors'] if k not in feats]\n                             empty_data_keys = [k for k in ['keypoints', 'descriptors'] if k in feats and len(feats[k]) == 0]\n\n                             if missing_keys or empty_data_keys:\n                                  print(f\"Warning: Missing or empty required data in ALIKED output for {img_key} ({var_type}, {var_scale_target}). (Missing Keys: {missing_keys}, Empty Data Keys: {empty_data_keys})\")\n\n\n                     except KeyError as ke:\n                          print(f\"Error: Expected key '{ke}' not found in ALIKED features for {img_key} ({var_type}, {var_scale_target}).\")\n                     except Exception as e:\n                          print(f\"Unexpected error processing ALIKED features result for {img_key} ({var_type}, {var_scale_target}): {e}\")\n\n                 except Exception as e:\n                      print(f\"Error during ALIKED feature extraction for {img_key} ({var_type}, {var_scale_target}): {e}\")\n\n\n            # Combine all detected points (now all in original coords) and deduplicate\n            if not all_kps_orig_coords:\n                 print(f\"No valid keypoints detected for any variation of {img_key}.\")\n                 # Create empty files/datasets to indicate processing happened (and failed)\n                 try:\n                    torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n                    f_descriptors.create_group(img_key) # Create group even if no descriptors\n                    if verbose:\n                        print(f\"Created empty combined feature files for {img_key}.\")\n                 except Exception as e:\n                     print(f\"Error creating empty combined feature files for {img_key}: {e}\")\n\n                 continue # Move to the next image\n\n            combined_kps_orig = np.concatenate(all_kps_orig_coords, axis=0)\n            combined_descriptors = np.concatenate(all_descriptors, axis=0)\n\n            # Deduplicate based on rounded coordinates in original image space\n            seen_coords = {}\n            unique_kps_orig = []\n            unique_descriptors = []\n\n            # Using enumerate to get original index for descriptor selection\n            # Use a small tolerance for rounding coordinates\n            try:\n                for i, (kp_coord, descriptor) in enumerate(zip(combined_kps_orig, combined_descriptors)):\n                    # Ensure coordinate is a tuple of floats for dictionary key\n                    rounded_coord = tuple(np.round(kp_coord, COORD_PRECISION).astype(float)) # Use float for tuple key\n\n                    if rounded_coord not in seen_coords:\n                        seen_coords[rounded_coord] = len(unique_kps_orig) # Store index in unique list\n                        unique_kps_orig.append(kp_coord)\n                        unique_descriptors.append(descriptor) # Keep the descriptor from the first occurrence\n                if verbose:\n                    print(f\"Combined features for {img_key}: Detected {len(combined_kps_orig)}, Unique {len(unique_kps_orig)}\")\n                # Convert lists to numpy arrays\n                unique_kps_orig_np = np.array(unique_kps_orig, dtype=np.float32)\n                unique_descriptors_np = np.array(unique_descriptors, dtype=np.float32)\n\n                # Save unique keypoints to .pt\n                try:\n                    torch.save(torch.from_numpy(unique_kps_orig_np), kp_pt_path)\n                    # print(f\"Saved unique keypoints for {img_key} to {kp_pt_path}\") # Optional verbose\n                except Exception as e:\n                     print(f\"Error saving keypoints .pt for {img_key}: {e}\")\n\n                # Save unique descriptors to descriptors.h5\n                try:\n                    img_desc_group = f_descriptors.create_group(img_key)\n                    img_desc_group.create_dataset('data', data=unique_descriptors_np, compression=\"gzip\") # Use compression\n                    # print(f\"Saved unique descriptors for {img_key} to {descriptors_h5_path}/{img_key}\") # Optional verbose\n                except Exception as e:\n                     print(f\"Error saving descriptors .h5 for {img_key}: {e}\")\n\n            except Exception as e:\n                 print(f\"Error during feature combination and deduplication for {img_key}: {e}\")\n                 # Create empty files/datasets if combination/saving fails\n                 try:\n                    torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n                    if img_key not in f_descriptors: # Only create group if it failed before\n                         f_descriptors.create_group(img_key)\n                    if verbose:\n                        print(f\"Created empty combined feature files for {img_key} after combination error.\")\n                 except Exception as e_save:\n                     print(f\"Error creating empty combined feature files for {img_key} after combination error: {e_save}\")\n\n\n    print(\"Multi-variation detection and feature combination complete.\")\n\n# ... (rest of the code: match_and_cluster_images, process_images_combined_output, main block)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.239239Z","iopub.execute_input":"2025-05-13T19:19:45.239518Z","iopub.status.idle":"2025-05-13T19:19:45.267506Z","shell.execute_reply.started":"2025-05-13T19:19:45.239488Z","shell.execute_reply":"2025-05-13T19:19:45.266824Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def load_image_names_from_json(cluster_path):\n    with open(os.path.join(cluster_path, 'images.json'), 'r') as f:\n        full_paths = json.load(f)  # 可能是 ['/path/to/images/img001.jpg', ...]\n        image_names = [os.path.basename(p) for p in full_paths]  # 提取 'img001.jpg'\n    return image_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.268279Z","iopub.execute_input":"2025-05-13T19:19:45.268481Z","iopub.status.idle":"2025-05-13T19:19:45.290433Z","shell.execute_reply.started":"2025-05-13T19:19:45.268463Z","shell.execute_reply":"2025-05-13T19:19:45.289894Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def import_into_colmap_cluster(\n    img_dir,\n    cluster_path='.featureout/cluster_0',\n    database_path = '.featureout/cluster_0/colmap.db',\n    image_names = None\n):\n    \"\"\"\n    Import keypoints and matches into COLMAP database using helper functions.\n\n    Args:\n        img_dir (str): Directory containing image files\n        cluster_path (str): Path with matches.h5\n        database_path (str): Output database location\n        image_names (list[str]): Optional subset of image names to include\n    \"\"\"\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    # Add keypoints and images\n    fname_to_id = add_keypoints(\n        db=db,\n        h5_path=cluster_path,\n        image_path=img_dir,\n        img_ext='',\n        camera_model='simple-pinhole',\n        single_camera=single_camera\n    )\n    # Filter fname_to_id to only use the selected subset (if provided)\n    if image_names is not None:\n        fname_to_id = {k: v for k, v in fname_to_id.items() if k in image_names}\n\n    # Add matches between selected image pairs\n    add_matches(\n        db=db,\n        h5_path=cluster_path,\n        fname_to_id=fname_to_id\n    )\n    db.commit()\n    db.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.291101Z","iopub.execute_input":"2025-05-13T19:19:45.291312Z","iopub.status.idle":"2025-05-13T19:19:45.308835Z","shell.execute_reply.started":"2025-05-13T19:19:45.291293Z","shell.execute_reply":"2025-05-13T19:19:45.308236Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import kornia as K\nimport kornia.feature as KF\nfrom tqdm import tqdm\nimport networkx as nx # Needed for graph clustering\nimport json # Needed for saving images.json (per cluster, or globally)\nimport warnings # To manage potential warnings\n\ndef match_and_cluster_images(\n    img_fnames,\n    index_pairs,\n    data_dir='.', # Use data_dir to find feature files and save cluster outputs\n    device=torch.device('cpu'),\n    min_matches=MIN_MATCHES_FOR_GRAPH_EDGE, # Use external arg names that match your snippet\n    aliked_dis_min=MIN_MATCHES_FOR_FILTERED_GRAPH, # Use external arg names that match your snippet\n    verbose=False\n):\n    \"\"\"\n    Performs LightGlue matching on combined features for image pairs,\n    builds a match graph, clusters images, and saves per-cluster files\n    (images.json, keypoints.h5, matches.h5).\n\n    Args:\n        img_fnames (list): List of full paths to image files.\n        index_pairs (list): List of (idx1, idx2) tuples for image pairs to match.\n        data_dir (str): Base directory where 'features_combined' is located and\n                        where per-cluster outputs will be created.\n        device (torch.device): Device to use for matching.\n        min_matches (int): Min matches for adding edge to graph.\n        aliked_dis_min (int): Min matches for filtering graph.\n        verbose (bool): Whether to print detailed match info.\n\n    Returns:\n        list: A list of lists, where each inner list contains the global indices\n              of images belonging to a cluster.\n    \"\"\"\n    # Define paths based on data_dir and configuration\n    feature_dir_combined = os.path.join(data_dir, 'features_combined')\n    keypoints_subdir_path = os.path.join(feature_dir_combined, KEYPOINTS_SUBDIR) # Directory holding combined per-image .pt files\n    descriptors_h5_path = os.path.join(feature_dir_combined, DESCRIPTORS_H5) # HDF5 holding combined per-image descriptors\n    # Global match dir is still used for global summaries, but per-cluster goes into feature_dir_combined/clusters\n    match_dir_global = os.path.join(data_dir, 'matches_global')\n\n    # Directory where per-cluster subfolders will be created\n    cluster_output_base_dir = os.path.join(feature_dir_combined, 'clusters')\n    os.makedirs(cluster_output_base_dir, exist_ok=True) # Ensure base cluster output dir exists\n\n    try:\n        lg_matcher = KF.LightGlueMatcher(\n            \"aliked\", {\n                \"width_confidence\": -1,\n                \"depth_confidence\": -1,\n                \"mp\": 'cuda' in str(device)\n            }\n        ).eval().to(device)\n    except Exception as e:\n        print(f\"Error loading LightGlue matcher: {e}\")\n        return []\n\n    match_graph = nx.Graph()\n    match_graph.add_nodes_from(range(len(img_fnames)))\n    # Store match indices (relative to combined per-image features)\n    # This is needed to save matches.h5 per cluster later\n    all_matches = {}\n\n    # Open combined descriptors file once\n    try:\n        f_descriptors = h5py.File(descriptors_h5_path, mode='r')\n    except Exception as e:\n        print(f\"Error opening combined descriptors file {descriptors_h5_path}: {e}\")\n        return []\n\n    print(\"Performing LightGlue matching on combined features and building graph...\")\n\n    # Iterate through shortlisted pairs\n    for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching & Graph Building\"):\n        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n        key1 = os.path.basename(fname1)\n        key2 = os.path.basename(fname2)\n\n        kp1_pt_path = os.path.join(keypoints_subdir_path, f'{key1}.pt')\n        kp2_pt_path = os.path.join(keypoints_subdir_path, f'{key2}.pt')\n\n        # Check if combined features exist for both images\n        if not os.path.exists(kp1_pt_path) or key1 not in f_descriptors or \\\n           not os.path.exists(kp2_pt_path) or key2 not in f_descriptors:\n            continue\n\n        try:\n            # Load combined keypoints (original coordinates)\n            kp1_combined_orig = torch.load(kp1_pt_path).to(device)\n            kp2_combined_orig = torch.load(kp2_pt_path).to(device)\n\n            # Load combined descriptors\n            desc1_combined = torch.from_numpy(f_descriptors[key1]['data'][...]).to(device)\n            desc2_combined = torch.from_numpy(f_descriptors[key2]['data'][...]).to(device)\n\n            if len(kp1_combined_orig) == 0 or len(kp2_combined_orig) == 0 or \\\n               len(desc1_combined) == 0 or len(desc2_combined) == 0:\n                continue\n\n            # Create dummy LAFs centered at keypoints (using original coordinates)\n            # Ensure KPs are float tensors for LAF creation\n            kp1_tensor = kp1_combined_orig.float()[None] # Add batch dim\n            kp2_tensor = kp2_combined_orig.float()[None] # Add batch dim\n            laf1 = KF.laf_from_center_scale_ori(kp1_tensor) # Use batch size 1, scale 1.0\n            laf2 = KF.laf_from_center_scale_ori(kp2_tensor)\n\n\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1_combined, desc2_combined, laf1, laf2)\n\n            n_matches = len(idxs)\n\n            if verbose:\n                 tqdm.write(f'{key1}-{key2}: {n_matches} matches')\n\n            # Add edge to graph if enough matches are found\n            if n_matches >= min_matches: # Use the passed min_matches\n                 # Store number of matches as edge weight\n                 match_graph.add_edge(idx1, idx2, weight=n_matches)\n                 # Store match indices (relative to combined KPs)\n                 all_matches.setdefault(key1, {})[key2] = idxs.cpu().detach().numpy().astype('int16')\n\n\n        except Exception as e:\n            print(f\"Error during matching combined features for {key1}-{key2}: {e}\")\n\n    # Close the descriptors file\n    f_descriptors.close()\n\n    print(\"Graph building complete. Performing clustering...\")\n\n    # --- Graph Clustering Logic (Same as before) ---\n\n    raw_clusters = list(nx.connected_components(match_graph))\n    final_clusters = []\n    outliers = set()\n\n    for cluster_indices in raw_clusters:\n        subgraph = match_graph.subgraph(cluster_indices)\n        filtered_subgraph = nx.Graph()\n        for u, v, d in subgraph.edges(data=True):\n            # Use the passed aliked_dis_min for filtering\n            if d['weight'] >= aliked_dis_min:\n                filtered_subgraph.add_edge(u, v)\n\n        for sub_cluster_indices in nx.connected_components(filtered_subgraph):\n            # Use the configured MIN_IMAGES_PER_CLUSTER (assuming 2 is desired based on original)\n            if len(sub_cluster_indices) >= MIN_IMAGES_PER_CLUSTER:\n                final_clusters.append(list(sub_cluster_indices))\n            else:\n                outliers.update(sub_cluster_indices)\n\n    print(f\"Clustering complete. Found {len(final_clusters)} clusters.\")\n\n\n    # --- Save Per-Cluster Files (Updated to match the style from your snippet, using combined data) ---\n\n    print(\"Saving per-cluster files...\")\n\n    for i, cluster_indices in enumerate(final_clusters):\n        # Create the cluster subfolder\n        cluster_dir = os.path.join(cluster_output_base_dir, f'cluster_{i}')\n        os.makedirs(cluster_dir, exist_ok=True)\n\n        cluster_fnames = [img_fnames[j] for j in cluster_indices]\n        cluster_keys = [os.path.basename(f) for f in cluster_fnames]\n\n        # 1. Save images.json (list of full filenames)\n        images_json_path = os.path.join(cluster_dir, 'images.json')\n        try:\n            with open(images_json_path, 'w') as f_json:\n                json.dump(cluster_fnames, f_json, indent=2)\n            # print(f\"Saved {images_json_path}\") # Optional verbose\n        except Exception as e:\n             print(f\"Error saving {images_json_path}: {e}\")\n\n\n        # 2. Save keypoints.h5 for the cluster (using combined keypoints per image)\n        # This replicates the saving style from your snippet but uses the combined KPs\n        keypoints_h5_path = os.path.join(cluster_dir, 'keypoints.h5')\n        try:\n            with h5py.File(keypoints_h5_path, 'w') as f_out_kp:\n                 for img_idx in cluster_indices:\n                     img_key = os.path.basename(img_fnames[img_idx])\n                     kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt') # Path to combined KPs for this image\n                     try:\n                         if os.path.exists(kp_pt_path):\n                             # Load combined KPs for this image (original coords)\n                             kp_combined_np = torch.load(kp_pt_path).cpu().numpy() # Load and convert to numpy\n                             f_out_kp.create_dataset(img_key, data=kp_combined_np.astype(np.float32))\n                             # print(f\"Saved combined KPs for {img_key} to {keypoints_h5_path}\") # Optional verbose\n                         else:\n                              print(f\"Warning: Combined keypoints not found for {img_key} at {kp_pt_path}. Skipping saving to cluster H5.\")\n\n                     except Exception as e:\n                          print(f\"Error loading/saving combined keypoints for {img_key} to cluster H5: {e}\")\n\n        except Exception as e:\n             print(f\"Error creating or saving to {keypoints_h5_path}: {e}\")\n\n\n        # 3. Save matches.h5 (matches between images *within this cluster*)\n        # This replicates the saving style from your snippet\n        # The indices in all_matches are already relative to the combined per-image keypoints\n        # We just need to filter all_matches to include only pairs within this cluster\n        cluster_matches_filtered = {}\n        # Create a set of keys in this cluster for quick lookup\n        cluster_keys_set = set(cluster_keys)\n\n        for key1 in cluster_keys:\n            # Only process if key1 had any matches stored AND key1 is in this cluster\n            if key1 in all_matches and key1 in cluster_keys_set:\n                cluster_matches_filtered[key1] = {}\n                for key2, match_data in all_matches[key1].items():\n                    # Only include matches where key2 is also in this cluster\n                    if key2 in cluster_keys_set:\n                        cluster_matches_filtered[key1][key2] = match_data # Store the original match indices\n\n        matches_h5_path = os.path.join(cluster_dir, 'matches.h5')\n        if cluster_matches_filtered:\n            try:\n                with h5py.File(matches_h5_path, 'w') as f_match:\n                    for key1, matches_dict in cluster_matches_filtered.items():\n                         if matches_dict: # Ensure key1 actually has matches within the cluster_matches_filtered dict\n                            group = f_match.create_group(key1) # Use create_group as this is a new file\n                            for key2, match in matches_dict.items():\n                                group.create_dataset(key2, data=match, dtype='int16') # Save as int16\n                # print(f\"Saved {matches_h5_path}\") # Optional verbose\n            except Exception as e:\n                 print(f\"Error saving {matches_h5_path}: {e}\")\n        # else:\n             # print(f\"No matches to save for cluster {i} at {matches_h5_path}\") # Optional verbose\n\n\n        # 4. Skip saving descriptors.h5 per cluster (as per the commented code in your snippet)\n\n\n    # --- Save Global Cluster Summaries ---\n    # This part saves summaries in the 'matches_global' directory\n    os.makedirs(match_dir_global, exist_ok=True) # Ensure global match dir exists\n\n    # Save clusters as a list of image filenames in a JSON file\n    clusters_json_path = os.path.join(match_dir_global, 'clusters.json')\n    clusters_filename_list = []\n    for cluster_indices in final_clusters:\n        cluster_fnames = [img_fnames[idx] for idx in cluster_indices]\n        clusters_filename_list.append(cluster_fnames)\n\n    try:\n        with open(clusters_json_path, 'w') as f_json:\n            json.dump(clusters_filename_list, f_json, indent=2)\n        print(f\"Saved global cluster filenames summary to {clusters_json_path}\")\n    except Exception as e:\n        print(f\"Error saving global clusters.json: {e}\")\n\n\n    # Save cluster summary in a text file\n    clusters_txt_path = os.path.join(match_dir_global, 'clusters.txt')\n    try:\n        with open(clusters_txt_path, 'w') as f:\n            f.write(f\"Found {len(final_clusters)} clusters:\\n\\n\")\n\n            # Loop through the final clusters and write their images\n            for i, cluster_indices in enumerate(final_clusters):\n                cluster_fnames = [img_fnames[j] for j in cluster_indices]\n                cluster_keys = sorted([os.path.basename(f) for f in cluster_fnames])\n                f.write(f'Cluster {i} (size={len(cluster_keys)}, indices={sorted(cluster_indices)}):\\n')\n                for name in cluster_keys: # Write the image names belonging to THIS cluster\n                     f.write(f'  {name}\\n')\n                f.write('\\n') # Add a newline after listing images for each cluster\n\n\n            # --- Correct placement for writing outliers ---\n            # Generate outlier filenames and keys AFTER clustering is complete\n            if outliers: # Check if the outliers set is not empty\n                 outlier_fnames = [img_fnames[idx] for idx in sorted(list(outliers))]\n                 outlier_keys = sorted([os.path.basename(f) for f in outlier_fnames])\n                 f.write(f'Outliers ({len(outlier_keys)} images, indices={sorted(list(outliers))}):\\n')\n                 for name in outlier_keys: # <-- This loop is now correctly placed\n                     f.write(f'  {name}\\n')\n                 f.write('\\n') # Add a final newline after the outlier list\n\n        print(f\"Saved global cluster summary to {clusters_txt_path}\")\n    except Exception as e:\n         print(f\"Error saving global clusters.txt: {e}\")\n\n\n\n    # Return the list of lists of image indices\n    return final_clusters\n\n# Note: This function assumes that detect_and_combine_features\n# has already been run and created the combined features in\n# data_dir/features_combined/keypoints/ and data_dir/features_combined/descriptors.h5.\n# It replicates the per-cluster saving structure from your older snippet\n# but populates the files using the *new* combined features.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.309596Z","iopub.execute_input":"2025-05-13T19:19:45.309910Z","iopub.status.idle":"2025-05-13T19:19:45.506124Z","shell.execute_reply.started":"2025-05-13T19:19:45.309878Z","shell.execute_reply":"2025-05-13T19:19:45.505504Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import json\n\n\ndef run_colmap_clusters(\n    feature_dir,\n    images_dir,\n    timings\n):\n    \"\"\"\n    Run COLMAP reconstruction for each cluster folder in feature_dir.\n\n    Parameters:\n    - feature_dir: Directory containing cluster_* folders with images.json, keypoints.h5, matches.h5\n    - images_dir: Path to raw image files\n    - timings: dict to record durations\n    - min_model_size: minimum model size for COLMAP mapping\n    - max_num_models: maximum number of maps to attempt\n    \"\"\"\n    cluster_folders = sorted([f for f in os.listdir(feature_dir) if f.startswith('cluster_')])\n    all_maps = []\n    for i, folder in enumerate(cluster_folders):\n        cluster_path = os.path.join(feature_dir, folder)\n        database_path = os.path.join(cluster_path, 'colmap.db')\n        image_list_path = os.path.join(cluster_path, 'images.json')\n\n        if not os.path.isfile(image_list_path):\n            print(f\"[Cluster {i}] Missing images.json, skipping.\")\n            continue\n\n        with open(image_list_path, 'r') as f:\n            image_names = [os.path.basename(x) for x in json.load(f)]\n\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n\n        gc.collect()\n        sleep(1)\n\n        # Step 1: import keypoints, matches into COLMAP db\n        import_into_colmap_cluster(\n            img_dir=images_dir,\n            cluster_path=cluster_path,\n            database_path=database_path,\n            image_names=image_names\n        )\n\n        # Step 2: RANSAC\n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        t_ransac = time() - t\n        timings['RANSAC'].append(t_ransac)\n        print(f'[Cluster {i}] Ran RANSAC in {t_ransac:.4f} sec')\n\n        # Step 3: Incremental mapping\n        output_path = os.path.join(cluster_path, 'colmap_rec_aliked')\n        os.makedirs(output_path, exist_ok=True)\n\n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 8\n        mapper_options.max_num_models = 25\n        mapper_options.mapper.filter_max_reproj_error\t = 10.0\n\n        t = time()\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path,\n            image_path=images_dir,\n            output_path=output_path,\n            options=mapper_options\n        )\n        t_rec = time() - t\n        timings['Reconstruction'].append(t_rec)\n        print(f'[Cluster {i}] Reconstruction done in {t_rec:.4f} sec')\n        all_maps.append(maps)\n    \n    return timings, all_maps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.506831Z","iopub.execute_input":"2025-05-13T19:19:45.507353Z","iopub.status.idle":"2025-05-13T19:19:45.515239Z","shell.execute_reply.started":"2025-05-13T19:19:45.507329Z","shell.execute_reply":"2025-05-13T19:19:45.514267Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Collect vital info from the dataset\n\n@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None\n\n# Set is_train=True to run the notebook on the training data.\n# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\nis_train = False\ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.516173Z","iopub.execute_input":"2025-05-13T19:19:45.516484Z","iopub.status.idle":"2025-05-13T19:19:45.711562Z","shell.execute_reply.started":"2025-05-13T19:19:45.516450Z","shell.execute_reply":"2025-05-13T19:19:45.710941Z"}},"outputs":[{"name":"stdout","text":"Dataset \"imc2023_haiper\" -> num_images=54\nDataset \"imc2023_heritage\" -> num_images=209\nDataset \"imc2023_theather_imc2024_church\" -> num_images=76\nDataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\nDataset \"imc2024_lizard_pond\" -> num_images=214\nDataset \"pt_brandenburg_british_buckingham\" -> num_images=225\nDataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\nDataset \"pt_stpeters_stpauls\" -> num_images=200\nDataset \"amy_gardens\" -> num_images=200\nDataset \"fbk_vineyard\" -> num_images=163\nDataset \"ETs\" -> num_images=22\nDataset \"stairs\" -> num_images=51\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport shutil\n\ndef delete_cluster_folders(base_dir):\n    for name in os.listdir(base_dir):\n        path = os.path.join(base_dir, name)\n        if os.path.isdir(path) and name.startswith(\"cluster\"):\n            print(f\"Deleting: {path}\")\n            shutil.rmtree(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.712303Z","iopub.execute_input":"2025-05-13T19:19:45.712537Z","iopub.status.idle":"2025-05-13T19:19:45.716881Z","shell.execute_reply.started":"2025-05-13T19:19:45.712515Z","shell.execute_reply":"2025-05-13T19:19:45.716013Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"gc.collect()\n\nmax_images = None  # Used For debugging only. Set to None to disable.\ndatasets_to_process = None  # Not the best convention, but None means all datasets.\n\nif is_train:\n    # max_images = 5\n\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n    \t# New data.\n    \t'amy_gardens',\n    \t# 'ETs',\n    \t# 'fbk_vineyard',\n    \t# 'stairs',\n    \t# Data from IMC 2023 and 2024.\n    \t# 'imc2024_dioscuri_baalshamin',\n    \t# 'imc2023_theather_imc2024_church',\n    \t# 'imc2023_heritage',\n    \t# 'imc2023_haiper',\n    \t# 'imc2024_lizard_pond',\n    \t# Crowdsourced PhotoTourism data.\n    \t# 'pt_stpeters_stpauls',\n    \t# 'pt_brandenburg_british_buckingham',\n    \t# 'pt_piazzasanmarco_grandplace',\n    \t# 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_augmentation\":[],\n    \"feature_merge\":[],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n\nprint (f\"Extracting on device {device}\")\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n    try:\n        t = time()\n        index_pairs = get_image_pairs_shortlist(\n            images,\n            sim_th = 0.5, # should be strict\n            min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n            exhaustive_if_less = 20,\n            device=device\n        )\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n    \n        t = time()\n\n        # detect_aliked(images, feature_dir, 4096, device=device)\n        # gc.collect()\n        # timings['feature_detection'].append(time() - t)\n        # print(f'Features detected in {time() - t:.4f} sec')\n        \n        # delete_cluster_folders(feature_dir)\n        # t = time()\n        # # match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        # clusternum = match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, aliked_dis_min=80, device=device, verbose=False)\n        \n        # 2. Perform initial detection and matching for cropping data\n        # This step will skip if the crop data file already exists\n        DATA_DIR = images_dir\n        FEATURE_DIR = os.path.join(feature_dir, 'features_combined')\n        MATCH_DIR = os.path.join(feature_dir, 'matches_global')\n        CLUSTER_DIR = os.path.join(FEATURE_DIR, 'clusters')\n\n        crop_data_file = perform_initial_detection_and_matching(images, index_pairs, data_dir = feature_dir, device=DEVICE)\n        gc.collect()\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n    \n        # 3. Calculate crop boxes based on initial match analysis\n        # This step will skip if the crop info file already exists from a previous run\n        # However, the logic for skipping is currently inside calculate_crop_boxes itself (mode='w')\n        # Let's ensure it writes fresh crop_info based on potentially existing crop_data\n        t = time()\n        crop_info_file = calculate_crop_boxes(images, crop_data_file, FEATURE_DIR, data_dir = feature_dir)\n        gc.collect()\n        timings['feature_augmentation'].append(time() - t)\n        print(f'Features augmentation in {time() - t:.4f} sec')    \n    \n        # 4. Perform multi-variation ALIKED detection, combine features per image, save to .pt/.h5\n        # This step skips images whose combined features already exist\n        t = time()\n        detect_and_combine_features(images, crop_info_file, FEATURE_DIR, device=DEVICE)\n        gc.collect()\n        timings['feature_merge'].append(time() - t)\n        print(f'Features merge in {time() - t:.4f} sec')  \n\n        # 5. Load combined features and perform LightGlue matching, save global matches to .pt\n        # This step runs matching using the combined features generated in step 4\n        t = time()\n        clusters = match_and_cluster_images(images, index_pairs, data_dir = feature_dir, device=DEVICE)\n        gc.collect()\n        print(\"Processing complete. Combined features saved to\", FEATURE_DIR, \"Global matches saved to\", MATCH_DIR)\n        print(\"generate cluster : \", clusters)\n        print(\"cluster num:\", len(clusters))\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n        \n        \n        timings, all_maps = run_colmap_clusters(CLUSTER_DIR, images_dir, timings)\n        gc.collect()\n        timings['Reconstruction'].append(time() - t)\n        print(f'Reconstruction in {time() - t:.4f} sec') \n        # clear_output(wait=False)\n\n        registered = 0\n        cluster_global_index = 0  # 全局 cluster 索引\n        for maps in all_maps:  # 每个 maps 是 Dict[int, Reconstruction]\n            for map_index, cur_map in maps.items():\n                for _, image in cur_map.images.items():\n                    prediction_index = filename_to_index[image.name]\n                    predictions[prediction_index].cluster_index = cluster_global_index\n                    predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                    predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                    registered += 1\n                cluster_global_index += 1\n        \n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images across {cluster_global_index} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:19:45.717894Z","iopub.execute_input":"2025-05-13T19:19:45.718155Z","iopub.status.idle":"2025-05-13T19:59:23.668593Z","shell.execute_reply.started":"2025-05-13T19:19:45.718121Z","shell.execute_reply":"2025-05-13T19:59:23.667863Z"}},"outputs":[{"name":"stdout","text":"Extracting on device cuda:0\nSkipping \"imc2023_haiper\"\nSkipping \"imc2023_heritage\"\nSkipping \"imc2023_theather_imc2024_church\"\nSkipping \"imc2024_dioscuri_baalshamin\"\nSkipping \"imc2024_lizard_pond\"\nSkipping \"pt_brandenburg_british_buckingham\"\nSkipping \"pt_piazzasanmarco_grandplace\"\nSkipping \"pt_sacrecoeur_trevi_tajmahal\"\nSkipping \"pt_stpeters_stpauls\"\n\nProcessing dataset \"amy_gardens\": 200 images\nComputing global descriptors with DINOv2 on cuda:0...\n","output_type":"stream"},{"name":"stderr","text":"DINOv2 Descriptors: 100%|██████████| 200/200 [00:32<00:00,  6.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Distance Matrix Statistics:\nMin:  0.1448\nMax:  0.4239\nMean: 0.2737\nStd:  0.0388\n20%:  0.2375\n25%:  0.2439\nUSED 50%:  0.2754\n75%:  0.3017\nShortlisting. Number of pairs to match: 9954. Done in 38.3573 sec\nPerforming initial ALIKED detection (1280) and LightGlue matching for cropping data...\nRunning initial ALIKED detection...\n","output_type":"stream"},{"name":"stderr","text":"Initial ALIKED Detection: 100%|██████████| 200/200 [00:19<00:00, 10.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Running initial LightGlue matching...\nLoaded LightGlue model\n","output_type":"stream"},{"name":"stderr","text":"Initial LightGlue Matching: 100%|██████████| 9954/9954 [16:57<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Analyzing initial matches for cropping data...\nInitial detection and matching complete. Cropping data saved to /kaggle/working/result/featureout/amy_gardens/crop_data.h5\nFeatures detected in 1041.6149 sec\nCalculating crop boxes using DBSCAN...\n","output_type":"stream"},{"name":"stderr","text":"Calculating Crops:   4%|▍         | 9/200 [00:00<00:02, 86.68it/s]","output_type":"stream"},{"name":"stdout","text":"DBSCAN found no significant clusters for peach_0126.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0140.png. No crop will be used.\n","output_type":"stream"},{"name":"stderr","text":"Calculating Crops:  22%|██▏       | 43/200 [00:00<00:01, 139.22it/s]","output_type":"stream"},{"name":"stdout","text":"DBSCAN found no significant clusters for peach_0170.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0120.png. No crop will be used.\n","output_type":"stream"},{"name":"stderr","text":"Calculating Crops:  50%|█████     | 101/200 [00:00<00:00, 119.94it/s]","output_type":"stream"},{"name":"stdout","text":"DBSCAN found no significant clusters for peach_0070.png. No crop will be used.\n","output_type":"stream"},{"name":"stderr","text":"Calculating Crops:  64%|██████▍   | 128/200 [00:01<00:00, 125.13it/s]","output_type":"stream"},{"name":"stdout","text":"DBSCAN found no significant clusters for peach_0049.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0193.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0059.png. No crop will be used.\n","output_type":"stream"},{"name":"stderr","text":"Calculating Crops:  92%|█████████▎| 185/200 [00:01<00:00, 132.53it/s]","output_type":"stream"},{"name":"stdout","text":"DBSCAN found no significant clusters for peach_0153.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0105.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0051.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0198.png. No crop will be used.\nDBSCAN found no significant clusters for peach_0190.png. No crop will be used.\n","output_type":"stream"},{"name":"stderr","text":"Calculating Crops: 100%|██████████| 200/200 [00:01<00:00, 126.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Crop box calculation complete.\nFeatures augmentation in 1.8920 sec\nRunning multi-variation ALIKED detection and combining features...\n","output_type":"stream"},{"name":"stderr","text":"Detecting & Combining Features: 100%|██████████| 200/200 [01:13<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Multi-variation detection and feature combination complete.\nFeatures merge in 73.5505 sec\nLoaded LightGlue model\nPerforming LightGlue matching on combined features and building graph...\n","output_type":"stream"},{"name":"stderr","text":"LightGlue Matching & Graph Building:   0%|          | 0/9954 [00:00<?, ?it/s]<ipython-input-14-46078de9caab>:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  kp1_combined_orig = torch.load(kp1_pt_path).to(device)\n<ipython-input-14-46078de9caab>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  kp2_combined_orig = torch.load(kp2_pt_path).to(device)\nLightGlue Matching & Graph Building: 100%|██████████| 9954/9954 [17:31<00:00,  9.47it/s]\n<ipython-input-14-46078de9caab>:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  kp_combined_np = torch.load(kp_pt_path).cpu().numpy() # Load and convert to numpy\n","output_type":"stream"},{"name":"stdout","text":"Graph building complete. Performing clustering...\nClustering complete. Found 15 clusters.\nSaving per-cluster files...\nSaved global cluster filenames summary to /kaggle/working/result/featureout/amy_gardens/matches_global/clusters.json\nSaved global cluster summary to /kaggle/working/result/featureout/amy_gardens/matches_global/clusters.txt\nProcessing complete. Combined features saved to /kaggle/working/result/featureout/amy_gardens/features_combined Global matches saved to /kaggle/working/result/featureout/amy_gardens/matches_global\ngenerate cluster :  [[0, 1, 2, 3, 4, 130, 131, 135, 132, 137, 139, 142, 144, 147, 149, 152, 156, 161, 162, 163, 164, 165, 35, 168, 171, 174, 175, 176, 177, 50, 179, 52, 180, 54, 184, 57, 186, 187, 188, 189, 190, 62, 192, 193, 194, 60, 69, 70, 82, 83, 84, 86, 89, 95, 96, 97, 98, 109, 111, 112, 114, 115, 117, 118, 121, 122, 123, 125], [5, 6, 7, 8, 9, 10, 11, 140, 143, 36, 37, 61, 63, 67, 68, 72, 81, 93, 105, 126], [75, 13, 14, 79, 15, 16], [18, 134, 191], [128, 133, 138, 146, 19, 20, 21, 22, 23, 148, 158, 38, 39, 40, 41, 42, 43, 167, 45, 46, 47, 48, 49, 178, 51, 53, 55, 185, 58, 59, 66, 199, 78, 80, 87, 91, 92, 99, 100, 102, 107, 110, 113, 116, 119, 120], [64, 195, 197, 71, 141, 150, 151, 24, 154, 155], [65, 25], [129, 26, 27], [28, 29], [73, 106, 85, 103], [74, 183], [88, 101], [169, 90], [104, 145], [160, 153, 157, 198]]\ncluster num: 15\nFeatures matched in 1051.8084 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 68/68 [00:01<00:00, 36.58it/s]\n 24%|██▍       | 490/2016 [00:00<00:00, 4664.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 0] Ran RANSAC in 12.4521 sec\n[Cluster 0] Reconstruction done in 78.2344 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:00<00:00, 35.62it/s]\n 50%|█████     | 68/136 [00:00<00:00, 4602.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 1] Ran RANSAC in 1.7971 sec\n[Cluster 1] Reconstruction done in 7.4426 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 40.59it/s]\n1it [00:00, 935.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 2] Ran RANSAC in 0.0134 sec\n[Cluster 2] Reconstruction done in 0.0624 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 41.93it/s]\n1it [00:00, 901.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 3] Ran RANSAC in 0.1030 sec\n[Cluster 3] Reconstruction done in 0.2355 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 42.12it/s]\n1it [00:00, 918.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 4] Ran RANSAC in 0.0162 sec\n[Cluster 4] Reconstruction done in 0.0747 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 33.26it/s]\n1it [00:00, 1024.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 5] Ran RANSAC in 0.0230 sec\n[Cluster 5] Reconstruction done in 0.1964 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:00<00:00, 41.95it/s]\n6it [00:00, 1973.48it/s]             \n","output_type":"stream"},{"name":"stdout","text":"[Cluster 6] Ran RANSAC in 0.0599 sec\n[Cluster 6] Reconstruction done in 0.8085 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6/6 [00:00<00:00, 24.30it/s]\n8it [00:00, 2677.93it/s]             \n","output_type":"stream"},{"name":"stdout","text":"[Cluster 7] Ran RANSAC in 0.1912 sec\n[Cluster 7] Reconstruction done in 0.8649 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 24.69it/s]\n2it [00:00, 1285.61it/s]             \n","output_type":"stream"},{"name":"stdout","text":"[Cluster 8] Ran RANSAC in 0.0323 sec\n[Cluster 8] Reconstruction done in 0.2404 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 46/46 [00:01<00:00, 31.76it/s]\n 29%|██▊       | 223/780 [00:00<00:00, 4210.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 9] Ran RANSAC in 5.8340 sec\n[Cluster 9] Reconstruction done in 26.8732 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 36.66it/s]\n 86%|████████▌ | 18/21 [00:00<00:00, 3373.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 10] Ran RANSAC in 0.4238 sec\n[Cluster 10] Reconstruction done in 5.3907 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 31.31it/s]\n1it [00:00, 897.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 11] Ran RANSAC in 0.0371 sec\n[Cluster 11] Reconstruction done in 0.2036 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 31.16it/s]\n3it [00:00, 1745.69it/s]             \n","output_type":"stream"},{"name":"stdout","text":"[Cluster 12] Ran RANSAC in 0.1184 sec\n[Cluster 12] Reconstruction done in 0.3553 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 27.09it/s]\n1it [00:00, 999.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Cluster 13] Ran RANSAC in 0.0189 sec\n[Cluster 13] Reconstruction done in 0.1623 sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:00<00:00, 42.44it/s]\n6it [00:00, 2496.11it/s]             \n","output_type":"stream"},{"name":"stdout","text":"[Cluster 14] Ran RANSAC in 0.0934 sec\n[Cluster 14] Reconstruction done in 0.8438 sec\nReconstruction in 1221.6037 sec\nDataset \"amy_gardens\" -> Registered 162 / 200 images across 13 clusters\nSkipping \"fbk_vineyard\"\nSkipping \"ETs\"\nSkipping \"stairs\"\n\nResults\nDataset \"amy_gardens\" -> Registered 162 / 200 images across 13 clusters\n\nTimings\nshortlisting -> total=38.36 sec.\nfeature_detection -> total=1041.61 sec.\nfeature_augmentation -> total=1.89 sec.\nfeature_merge -> total=73.55 sec.\nfeature_matching -> total=1051.81 sec.\nRANSAC -> total=21.21 sec.\nReconstruction -> total=1343.59 sec.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Must Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:59:23.669443Z","iopub.execute_input":"2025-05-13T19:59:23.669699Z","iopub.status.idle":"2025-05-13T19:59:23.893631Z","shell.execute_reply.started":"2025-05-13T19:59:23.669676Z","shell.execute_reply":"2025-05-13T19:59:23.892638Z"}},"outputs":[{"name":"stdout","text":"dataset,scene,image,rotation_matrix,translation_vector\nimc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\nimc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Definitely Compute results if running on the training set.\n# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n\nif is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:59:23.894808Z","iopub.execute_input":"2025-05-13T19:59:23.895147Z","iopub.status.idle":"2025-05-13T20:00:02.286679Z","shell.execute_reply.started":"2025-05-13T19:59:23.895107Z","shell.execute_reply":"2025-05-13T20:00:02.285849Z"}},"outputs":[{"name":"stdout","text":"imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2023_heritage: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\nimc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\npt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\namy_gardens: score=26.43% (mAA=15.23%, clusterness=100.00%)\nfbk_vineyard: score=0.00% (mAA=0.00%, clusterness=0.00%)\nETs: score=0.00% (mAA=0.00%, clusterness=0.00%)\nstairs: score=0.00% (mAA=0.00%, clusterness=0.00%)\nAverage over all datasets: score=2.03% (mAA=1.17%, clusterness=7.69%)\nComputed metric in: 38.39 sec.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}