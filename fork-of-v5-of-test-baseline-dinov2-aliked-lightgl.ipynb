{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11217117,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":4535,"sourceType":"modelInstanceVersion","modelInstanceId":3327,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Example submission\n\nImage Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n\nThis notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n\nRemember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# IMPORTANT \n#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:12.735419Z","iopub.execute_input":"2025-05-09T22:29:12.735723Z","iopub.status.idle":"2025-05-09T22:29:18.012646Z","shell.execute_reply.started":"2025-05-09T22:29:12.735694Z","shell.execute_reply":"2025-05-09T22:29:18.011542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\n\n# IMPORTANT Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:18.014025Z","iopub.execute_input":"2025-05-09T22:29:18.014258Z","iopub.status.idle":"2025-05-09T22:29:38.945543Z","shell.execute_reply.started":"2025-05-09T22:29:18.014239Z","shell.execute_reply":"2025-05-09T22:29:38.94435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"PyTorch version:\", torch.__version__)\nimport sys\nprint(\"Python version:\", sys.version)\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"Device count:\", torch.cuda.device_count())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:38.94767Z","iopub.execute_input":"2025-05-09T22:29:38.948222Z","iopub.status.idle":"2025-05-09T22:29:39.069866Z","shell.execute_reply.started":"2025-05-09T22:29:38.948197Z","shell.execute_reply":"2025-05-09T22:29:39.068992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.071414Z","iopub.execute_input":"2025-05-09T22:29:39.071735Z","iopub.status.idle":"2025-05-09T22:29:39.693664Z","shell.execute_reply.started":"2025-05-09T22:29:39.071706Z","shell.execute_reply":"2025-05-09T22:29:39.692903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\n\n# Must Use efficientnet global descriptor to get matching shortlists.\ndef get_global_desc(fnames, device = torch.device('cpu')):\n    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/large/1')\n    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/large/1')\n    model = model.eval()\n    model = model.to(device)\n    global_descs_dinov2 = []\n    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n        timg = load_torch_image(img_fname_full)\n        with torch.inference_mode():\n            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs)\n            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n        global_descs_dinov2.append(dino_mac.detach().cpu())\n    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n    return global_descs_dinov2\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 30,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc(fnames, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n\n    # 只分析上三角（去掉对角线），避免重复\n    triu_indices = np.triu_indices_from(dm, k=1)\n    dm_flat = dm[triu_indices]\n    \n    # 打印统计信息\n    print(\"Distance Matrix Statistics:\")\n    print(f\"Min:  {dm_flat.min():.4f}\")\n    print(f\"Max:  {dm_flat.max():.4f}\")\n    print(f\"Mean: {dm_flat.mean():.4f}\")\n    print(f\"Std:  {dm_flat.std():.4f}\")\n    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n    print(f\"50%:  {np.percentile(dm_flat, 50):.4f}\")\n    print(f\"USED 75%:  {np.percentile(dm_flat, 75):.4f}\")\n    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n    # removing half\n    thr = min(np.percentile(dm_flat, 75), sim_th)\n    mask = dm <= thr\n    print(\"thr :\", thr)\n    # mask = dm<=threshold\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < threshold:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\ndef detect_aliked(img_fnames,\n                  feature_dir = '.featureout',\n                  num_features = 4096,\n                  resize_to = 1024,\n                  device=torch.device('cpu')):\n    dtype = torch.float32 # ALIKED has issues with float16\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.005).eval().to(device, dtype)\n    extractor.preprocess_conf[\"resize\"] = resize_to\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return\n\ndef match_with_lightglue(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout',\n                   device=torch.device('cpu'),\n                   min_matches=20,verbose=True):\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                \"depth_confidence\": -1,\n                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1,\n                                         desc2,\n                                         KF.laf_from_center_scale_ori(kp1[None]),\n                                         KF.laf_from_center_scale_ori(kp2[None]))\n            if len(idxs)  == 0:\n                continue\n            n_matches = len(idxs)\n            if verbose:\n                print (f'{key1}-{key2}: {n_matches} matches')\n            group  = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n    return\n\n\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.694687Z","iopub.execute_input":"2025-05-09T22:29:39.695012Z","iopub.status.idle":"2025-05-09T22:29:39.714639Z","shell.execute_reply.started":"2025-05-09T22:29:39.694976Z","shell.execute_reply":"2025-05-09T22:29:39.713912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image_names_from_json(cluster_path):\n    with open(os.path.join(cluster_path, 'images.json'), 'r') as f:\n        full_paths = json.load(f)  # 可能是 ['/path/to/images/img001.jpg', ...]\n        image_names = [os.path.basename(p) for p in full_paths]  # 提取 'img001.jpg'\n    return image_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.715426Z","iopub.execute_input":"2025-05-09T22:29:39.715703Z","iopub.status.idle":"2025-05-09T22:29:39.728961Z","shell.execute_reply.started":"2025-05-09T22:29:39.71567Z","shell.execute_reply":"2025-05-09T22:29:39.728257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def import_into_colmap_cluster(\n    img_dir,\n    cluster_path='.featureout/cluster_0',\n    database_path = '.featureout/cluster_0/colmap.db',\n    image_names = None\n):\n    \"\"\"\n    Import keypoints and matches into COLMAP database using helper functions.\n\n    Args:\n        img_dir (str): Directory containing image files\n        cluster_path (str): Path with matches.h5\n        database_path (str): Output database location\n        image_names (list[str]): Optional subset of image names to include\n    \"\"\"\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    # Add keypoints and images\n    fname_to_id = add_keypoints(\n        db=db,\n        h5_path=cluster_path,\n        image_path=img_dir,\n        img_ext='',\n        camera_model='simple-pinhole',\n        single_camera=single_camera\n    )\n    # Filter fname_to_id to only use the selected subset (if provided)\n    if image_names is not None:\n        fname_to_id = {k: v for k, v in fname_to_id.items() if k in image_names}\n\n    # Add matches between selected image pairs\n    add_matches(\n        db=db,\n        h5_path=cluster_path,\n        fname_to_id=fname_to_id\n    )\n    db.commit()\n    db.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.729721Z","iopub.execute_input":"2025-05-09T22:29:39.730015Z","iopub.status.idle":"2025-05-09T22:29:39.740614Z","shell.execute_reply.started":"2025-05-09T22:29:39.729982Z","shell.execute_reply":"2025-05-09T22:29:39.739975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n\ndef run_colmap_clusters(\n    feature_dir,\n    images_dir,\n    timings\n):\n    \"\"\"\n    Run COLMAP reconstruction for each cluster folder in feature_dir.\n\n    Parameters:\n    - feature_dir: Directory containing cluster_* folders with images.json, keypoints.h5, matches.h5\n    - images_dir: Path to raw image files\n    - timings: dict to record durations\n    - min_model_size: minimum model size for COLMAP mapping\n    - max_num_models: maximum number of maps to attempt\n    \"\"\"\n    cluster_folders = sorted([f for f in os.listdir(feature_dir) if f.startswith('cluster_')])\n    all_maps = []\n    for i, folder in enumerate(cluster_folders):\n        cluster_path = os.path.join(feature_dir, folder)\n        database_path = os.path.join(cluster_path, 'colmap.db')\n        image_list_path = os.path.join(cluster_path, 'images.json')\n\n        if not os.path.isfile(image_list_path):\n            print(f\"[Cluster {i}] Missing images.json, skipping.\")\n            continue\n\n        with open(image_list_path, 'r') as f:\n            image_names = [os.path.basename(x) for x in json.load(f)]\n\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n\n        gc.collect()\n        sleep(1)\n\n        # Step 1: import keypoints, matches into COLMAP db\n        import_into_colmap_cluster(\n            img_dir=images_dir,\n            cluster_path=cluster_path,\n            database_path=database_path,\n            image_names=image_names\n        )\n\n        # Step 2: RANSAC\n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        t_ransac = time() - t\n        timings['RANSAC'].append(t_ransac)\n        print(f'[Cluster {i}] Ran RANSAC in {t_ransac:.4f} sec')\n\n        # Step 3: Incremental mapping\n        output_path = os.path.join(cluster_path, 'colmap_rec_aliked')\n        os.makedirs(output_path, exist_ok=True)\n\n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 6\n        mapper_options.max_num_models = 4\n        mapper_options.mapper.filter_max_reproj_error\t = 2.0\n        # mapper_options.init_max_error = 6.0\n\n        t = time()\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path,\n            image_path=images_dir,\n            output_path=output_path,\n            options=mapper_options\n        )\n        t_rec = time() - t\n        timings['Reconstruction'].append(t_rec)\n        print(f'[Cluster {i}] Reconstruction done in {t_rec:.4f} sec')\n        all_maps.append(maps)\n    \n    return timings, all_maps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.742474Z","iopub.execute_input":"2025-05-09T22:29:39.742686Z","iopub.status.idle":"2025-05-09T22:29:39.755966Z","shell.execute_reply.started":"2025-05-09T22:29:39.742668Z","shell.execute_reply":"2025-05-09T22:29:39.75517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import networkx as nx\n\n# import networkx as nx\n# from tqdm import tqdm\n\n# def match_with_lightglue_and_cluster(\n#     img_fnames,\n#     index_pairs,\n#     feature_dir='.featureout',\n#     device=torch.device('cpu'),\n#     min_matches=20,\n#     aliked_dis_min=100,\n#     verbose=True\n# ):\n#     # 初始化 LightGlue 匹配器\n#     lg_matcher = KF.LightGlueMatcher(\n#         \"aliked\", {\n#             \"width_confidence\": -1,\n#             \"depth_confidence\": -1,\n#             \"mp\": 'cuda' in str(device)\n#         }\n#     ).eval().to(device)\n\n#     num_imgs = len(img_fnames)\n#     match_graph = nx.Graph()\n#     match_graph.add_nodes_from(range(num_imgs))\n\n#     # 加载关键点和描述子，准备写入匹配结果\n#     with h5py.File(os.path.join(feature_dir, 'keypoints.h5'), 'r') as f_kp, \\\n#          h5py.File(os.path.join(feature_dir, 'descriptors.h5'), 'r') as f_desc, \\\n#          h5py.File(os.path.join(feature_dir, 'matches.h5'), 'w') as f_match:\n\n#         # Step 1: 匹配每对图像\n#         for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n#             img1 = os.path.basename(img_fnames[idx1])\n#             img2 = os.path.basename(img_fnames[idx2])\n\n#             kp1 = torch.from_numpy(f_kp[img1][...]).to(device)\n#             kp2 = torch.from_numpy(f_kp[img2][...]).to(device)\n#             desc1 = torch.from_numpy(f_desc[img1][...]).to(device)\n#             desc2 = torch.from_numpy(f_desc[img2][...]).to(device)\n\n#             with torch.inference_mode():\n#                 _, idxs = lg_matcher(\n#                     desc1, desc2,\n#                     KF.laf_from_center_scale_ori(kp1[None]),\n#                     KF.laf_from_center_scale_ori(kp2[None])\n#                 )\n\n#             if len(idxs) == 0:\n#                 continue\n\n#             n_matches = len(idxs)\n#             if verbose:\n#                 print(f'{img1} - {img2}: {n_matches} matches')\n\n#             if n_matches >= min_matches:\n#                 match_graph.add_edge(idx1, idx2, weight=n_matches)\n#                 group = f_match.require_group(img1)\n#                 group.create_dataset(img2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n\n#         # Step 2: 提取聚类（connected components）\n#         raw_clusters = list(nx.connected_components(match_graph))\n#         final_clusters = []\n#         outliers = set()\n\n#         for cluster in raw_clusters:\n#             cluster = list(cluster)\n#             subgraph = match_graph.subgraph(cluster)\n#             valid_nodes = []\n\n#             for node in cluster:\n#                 strong_neighbors = [\n#                     neighbor for neighbor in subgraph.neighbors(node)\n#                     if subgraph[node][neighbor]['weight'] >= aliked_dis_min\n#                 ]\n#                 if strong_neighbors:\n#                     valid_nodes.append(node)\n#                 else:\n#                     outliers.add(node)\n\n#             if len(valid_nodes) >= 2:\n#                 final_clusters.append(valid_nodes)\n#             else:\n#                 outliers.update(valid_nodes)\n\n#         # Step 3: 清理掉涉及 outlier 的匹配项\n#         outlier_names = {os.path.basename(img_fnames[i]) for i in outliers}\n\n#         groups_to_delete = []\n\n#         for group_name in list(f_match.keys()):\n#             group = f_match[group_name]\n#             datasets_to_delete = []\n\n#             for dataset_name in list(group.keys()):\n#                 # 删除与 outlier 有关的 match\n#                 if group_name in outlier_names or dataset_name in outlier_names:\n#                     datasets_to_delete.append(dataset_name)\n#                     if verbose:\n#                         print(f\"Deleted match: {group_name} - {dataset_name} (outlier involved)\")\n\n#             # 先删除标记的 dataset\n#             for ds in datasets_to_delete:\n#                 del group[ds]\n\n#             # 新增逻辑：如果剩下不到 3 个 match，就删掉整个 group\n#             if len(group.keys()) < 3:\n#                 groups_to_delete.append(group_name)\n\n#         # 删除 group 本身\n#         for gname in groups_to_delete:\n#             del f_match[gname]\n#             if verbose:\n#                 print(f\"Deleted group: {gname} (too few matches)\")\n\n#     if True:\n#         print(list(outliers))\n#         for i, cluster in enumerate(final_clusters):\n#             print(f\"Cluster {i} ({len(cluster)} images):\")\n#     return final_clusters, sorted(list(outliers))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.75695Z","iopub.execute_input":"2025-05-09T22:29:39.757198Z","iopub.status.idle":"2025-05-09T22:29:39.769743Z","shell.execute_reply.started":"2025-05-09T22:29:39.75718Z","shell.execute_reply":"2025-05-09T22:29:39.769078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport h5py\nimport torch\nimport json\nimport networkx as nx\nfrom tqdm import tqdm\nimport kornia.feature as KF\n\n\ndef match_with_lightglue_and_cluster(\n    img_fnames,\n    index_pairs,\n    feature_dir='.featureout',\n    device=torch.device('cpu'),\n    min_matches=20,\n    aliked_dis_min=100,\n    verbose=True\n):\n    lg_matcher = KF.LightGlueMatcher(\n        \"aliked\", {\n            \"width_confidence\": -1,\n            \"depth_confidence\": -1,\n            \"mp\": 'cuda' in str(device)\n        }\n    ).eval().to(device)\n\n    match_graph = nx.Graph()\n    match_graph.add_nodes_from(range(len(img_fnames)))\n    all_matches = {}\n\n    with h5py.File(os.path.join(feature_dir, 'keypoints.h5'), 'r') as f_kp, \\\n         h5py.File(os.path.join(feature_dir, 'descriptors.h5'), 'r') as f_desc:\n\n        for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n            key1 = os.path.basename(img_fnames[idx1])\n            key2 = os.path.basename(img_fnames[idx2])\n\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n\n            with torch.inference_mode():\n                _, idxs = lg_matcher(\n                    desc1, desc2,\n                    KF.laf_from_center_scale_ori(kp1[None]),\n                    KF.laf_from_center_scale_ori(kp2[None])\n                )\n\n            if len(idxs) == 0:\n                continue\n\n            if len(idxs) >= min_matches:\n                match_graph.add_edge(idx1, idx2, weight=len(idxs))\n                all_matches.setdefault(key1, {})[key2] = idxs.cpu().detach().numpy().astype('int16')\n\n    raw_clusters = list(nx.connected_components(match_graph))\n    final_clusters = []\n    outliers = set()\n\n    for cluster in raw_clusters:\n        subgraph = match_graph.subgraph(cluster)\n        filtered_subgraph = nx.Graph()\n        for u, v, d in subgraph.edges(data=True):\n            if d['weight'] >= aliked_dis_min:\n                filtered_subgraph.add_edge(u, v)\n\n        for sub_cluster in nx.connected_components(filtered_subgraph):\n            if len(sub_cluster) >= 2:\n                final_clusters.append(list(sub_cluster))\n            else:\n                outliers.update(sub_cluster)\n    \n    with h5py.File(os.path.join(feature_dir, 'keypoints.h5'), 'r') as f_kp, \\\n         h5py.File(os.path.join(feature_dir, 'descriptors.h5'), 'r') as f_desc:\n\n        for i, cluster in enumerate(final_clusters):\n            cluster_fnames = [img_fnames[j] for j in cluster]\n            cluster_keys = [os.path.basename(f) for f in cluster_fnames]\n            cluster_dir = os.path.join(feature_dir, f'cluster_{i}')\n            os.makedirs(cluster_dir, exist_ok=True)\n\n            # save matches.h5\n            with h5py.File(os.path.join(cluster_dir, 'matches.h5'), 'w') as f_match:\n                for key1 in cluster_keys:\n                    if key1 not in all_matches:\n                        continue\n                    group = f_match.require_group(key1)\n                    for key2, match in all_matches[key1].items():\n                        if key2 in cluster_keys:\n                            group.create_dataset(key2, data=match)\n\n            # save images.json\n            with open(os.path.join(cluster_dir, 'images.json'), 'w') as f_json:\n                json.dump(cluster_fnames, f_json, indent=2)\n\n            # save cluster keypoints.h5 and descriptors.h5\n            # with h5py.File(os.path.join(cluster_dir, 'keypoints.h5'), 'w') as f_out_kp, \\\n            #      h5py.File(os.path.join(cluster_dir, 'descriptors.h5'), 'w') as f_out_desc:\n            #     for key in cluster_keys:\n            #         f_out_kp.create_dataset(key, data=f_kp[key][()])\n            #         f_out_desc.create_dataset(key, data=f_desc[key][()])\n    \n            with h5py.File(os.path.join(cluster_dir, 'keypoints.h5'), 'w') as f_out_kp:\n                for key in cluster_keys:\n                    f_out_kp.create_dataset(key, data=f_kp[key][()])\n\n    if True:\n        with open(os.path.join(feature_dir, 'clusters.txt'), 'w') as f:\n            for i, cluster in enumerate(final_clusters):\n                cluster_fnames = [img_fnames[j] for j in cluster]\n                cluster_keys = sorted([os.path.basename(f) for f in cluster_fnames])\n                f.write(f'Cluster {i} (size={len(cluster_keys)}):\\n')\n                for name in cluster_keys:\n                    f.write(f'  {name}\\n')\n                f.write('\\n')\n\n    return final_clusters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.770703Z","iopub.execute_input":"2025-05-09T22:29:39.771011Z","iopub.status.idle":"2025-05-09T22:29:39.947305Z","shell.execute_reply.started":"2025-05-09T22:29:39.770965Z","shell.execute_reply":"2025-05-09T22:29:39.946691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Collect vital info from the dataset\n\n@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None\n\n# Set is_train=True to run the notebook on the training data.\n# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\nis_train = False\ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:39.948063Z","iopub.execute_input":"2025-05-09T22:29:39.94869Z","iopub.status.idle":"2025-05-09T22:29:40.099409Z","shell.execute_reply.started":"2025-05-09T22:29:39.94866Z","shell.execute_reply":"2025-05-09T22:29:40.098772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\ndef delete_cluster_folders(base_dir):\n    for name in os.listdir(base_dir):\n        path = os.path.join(base_dir, name)\n        if os.path.isdir(path) and name.startswith(\"cluster\"):\n            print(f\"Deleting: {path}\")\n            shutil.rmtree(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:40.100168Z","iopub.execute_input":"2025-05-09T22:29:40.100368Z","iopub.status.idle":"2025-05-09T22:29:40.104658Z","shell.execute_reply.started":"2025-05-09T22:29:40.100349Z","shell.execute_reply":"2025-05-09T22:29:40.103851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\n\nmax_images = None  # Used For debugging only. Set to None to disable.\ndatasets_to_process = None  # Not the best convention, but None means all datasets.\n\nif is_train:\n    # max_images = 5\n\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n    \t# New data.\n    \t# 'amy_gardens',\n    \t# 'ETs',\n    \t# 'fbk_vineyard',\n    \t'stairs',\n    \t# Data from IMC 2023 and 2024.\n    \t# 'imc2024_dioscuri_baalshamin',\n    \t# 'imc2023_theather_imc2024_church',\n    \t# 'imc2023_heritage',\n    \t# 'imc2023_haiper',\n    \t# 'imc2024_lizard_pond',\n    \t# Crowdsourced PhotoTourism data.\n    \t# 'pt_stpeters_stpauls',\n    \t# 'pt_brandenburg_british_buckingham',\n    \t# 'pt_piazzasanmarco_grandplace',\n    \t# 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n\nprint (f\"Extracting on device {device}\")\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n    try:\n        t = time()\n        index_pairs = get_image_pairs_shortlist(\n            images,\n            sim_th = 0.27, # should be strict\n            min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n            exhaustive_if_less = 20,\n            device=device\n        )\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n    \n        t = time()\n\n        detect_aliked(images, feature_dir, 4096, device=device)\n        gc.collect()\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n        \n        delete_cluster_folders(feature_dir)\n        t = time()\n        # match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        clusternum = match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, aliked_dis_min=30, device=device, verbose=False)\n        print(\"generate cluster number: \", clusternum)\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n        \n\n        timings, all_maps = run_colmap_clusters(feature_dir, images_dir, timings)\n        # clear_output(wait=False)\n\n        registered = 0\n        cluster_global_index = 0  # 全局 cluster 索引\n        for maps in all_maps:  # 每个 maps 是 Dict[int, Reconstruction]\n            for map_index, cur_map in maps.items():\n                for _, image in cur_map.images.items():\n                    prediction_index = filename_to_index[image.name]\n                    predictions[prediction_index].cluster_index = cluster_global_index\n                    predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                    predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                    registered += 1\n                cluster_global_index += 1\n        \n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images across {cluster_global_index} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\n        # database_path = os.path.join(feature_dir, 'colmap.db')\n        # if os.path.isfile(database_path):\n        #     os.remove(database_path)\n        # gc.collect()\n        # sleep(1)\n        # import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n        # output_path = f'{feature_dir}/colmap_rec_aliked'\n        \n        # t = time()\n        # pycolmap.match_exhaustive(database_path)\n        # timings['RANSAC'].append(time() - t)\n        # print(f'Ran RANSAC in {time() - t:.4f} sec')\n        \n        # # By default colmap does not generate a reconstruction if less than 10 images are registered.\n        # # Lower it to 3.\n        # mapper_options = pycolmap.IncrementalPipelineOptions()\n        # mapper_options.min_model_size = 8\n        # mapper_options.max_num_models = 25\n        # # mapper_options.mapper.filter_max_reproj_error\t = 10.0\n\n        # os.makedirs(output_path, exist_ok=True)\n        # t = time()\n        # maps = pycolmap.incremental_mapping(\n        #     database_path=database_path, \n        #     image_path=images_dir,\n        #     output_path=output_path,\n        #     options=mapper_options)\n        # sleep(1)\n        # timings['Reconstruction'].append(time() - t)\n        # print(f'Reconstruction done in  {time() - t:.4f} sec')\n        # print(maps)\n\n        # clear_output(wait=False)\n    \n        # registered = 0\n        # for map_index, cur_map in maps.items():\n        #     for index, image in cur_map.images.items():\n        #         prediction_index = filename_to_index[image.name]\n        #         predictions[prediction_index].cluster_index = map_index\n        #         predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n        #         predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n        #         registered += 1\n        # mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        # mapping_result_strs.append(mapping_result_str)\n        # print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:29:40.105566Z","iopub.execute_input":"2025-05-09T22:29:40.10584Z","iopub.status.idle":"2025-05-09T22:33:28.574872Z","shell.execute_reply.started":"2025-05-09T22:29:40.105786Z","shell.execute_reply":"2025-05-09T22:33:28.574182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Must Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:33:28.575688Z","iopub.execute_input":"2025-05-09T22:33:28.57602Z","iopub.status.idle":"2025-05-09T22:33:28.742368Z","shell.execute_reply.started":"2025-05-09T22:33:28.575989Z","shell.execute_reply":"2025-05-09T22:33:28.741545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Definitely Compute results if running on the training set.\n# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n\nif is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T22:33:28.743263Z","iopub.execute_input":"2025-05-09T22:33:28.743556Z","iopub.status.idle":"2025-05-09T22:33:29.743558Z","shell.execute_reply.started":"2025-05-09T22:33:28.743522Z","shell.execute_reply":"2025-05-09T22:33:29.742882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}