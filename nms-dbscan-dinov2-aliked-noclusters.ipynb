{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dab73c9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00706,
     "end_time": "2025-05-14T16:37:05.395596",
     "exception": false,
     "start_time": "2025-05-14T16:37:05.388536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104daa38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:05.408802Z",
     "iopub.status.busy": "2025-05-14T16:37:05.408526Z",
     "iopub.status.idle": "2025-05-14T16:37:10.779611Z",
     "shell.execute_reply": "2025-05-14T16:37:10.778572Z"
    },
    "papermill": {
     "duration": 5.379535,
     "end_time": "2025-05-14T16:37:10.781352",
     "exception": false,
     "start_time": "2025-05-14T16:37:05.401817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aeffda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:10.797675Z",
     "iopub.status.busy": "2025-05-14T16:37:10.797420Z",
     "iopub.status.idle": "2025-05-14T16:37:34.643935Z",
     "shell.execute_reply": "2025-05-14T16:37:34.643237Z"
    },
    "papermill": {
     "duration": 23.855714,
     "end_time": "2025-05-14T16:37:34.645490",
     "exception": false,
     "start_time": "2025-05-14T16:37:10.789776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "from sklearn.cluster import DBSCAN\n",
    "# ... other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e505a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:34.660575Z",
     "iopub.status.busy": "2025-05-14T16:37:34.660016Z",
     "iopub.status.idle": "2025-05-14T16:37:34.749489Z",
     "shell.execute_reply": "2025-05-14T16:37:34.748610Z"
    },
    "papermill": {
     "duration": 0.098216,
     "end_time": "2025-05-14T16:37:34.750865",
     "exception": false,
     "start_time": "2025-05-14T16:37:34.652649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4f45a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:34.765702Z",
     "iopub.status.busy": "2025-05-14T16:37:34.765444Z",
     "iopub.status.idle": "2025-05-14T16:37:34.912295Z",
     "shell.execute_reply": "2025-05-14T16:37:34.911023Z"
    },
    "papermill": {
     "duration": 0.155915,
     "end_time": "2025-05-14T16:37:34.914069",
     "exception": false,
     "start_time": "2025-05-14T16:37:34.758154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6011b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:34.929354Z",
     "iopub.status.busy": "2025-05-14T16:37:34.929063Z",
     "iopub.status.idle": "2025-05-14T16:37:34.933350Z",
     "shell.execute_reply": "2025-05-14T16:37:34.932469Z"
    },
    "papermill": {
     "duration": 0.013124,
     "end_time": "2025-05-14T16:37:34.934612",
     "exception": false,
     "start_time": "2025-05-14T16:37:34.921488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3be5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:34.949397Z",
     "iopub.status.busy": "2025-05-14T16:37:34.949173Z",
     "iopub.status.idle": "2025-05-14T16:37:34.954522Z",
     "shell.execute_reply": "2025-05-14T16:37:34.953890Z"
    },
    "papermill": {
     "duration": 0.014101,
     "end_time": "2025-05-14T16:37:34.955780",
     "exception": false,
     "start_time": "2025-05-14T16:37:34.941679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pil_image(fname):\n",
    "    \"\"\"Loads an image using PIL.\"\"\"\n",
    "    return Image.open(fname).convert('RGB')\n",
    "\n",
    "def get_image_size(fname):\n",
    "    \"\"\"Gets image size (width, height) using PIL.\"\"\"\n",
    "    with Image.open(fname) as img:\n",
    "        return img.size # (width, height)\n",
    "\n",
    "def get_original_coords(kp_coords, img_orig_size, variation_info):\n",
    "    \"\"\"\n",
    "    Transforms keypoint coordinates from variation space back to original image space.\n",
    "\n",
    "    Args:\n",
    "        kp_coords (np.ndarray): Keypoint coordinates [N, 2] in the variation space.\n",
    "        img_orig_size (tuple): Original image size (width, height).\n",
    "        variation_info (dict): Dictionary containing 'type' ('orig' or 'crop'),\n",
    "                               'scale_factor' (scale used for resize),\n",
    "                               'crop_box' ([x, y, w, h] in original coords, None if type is 'orig').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n",
    "    \"\"\"\n",
    "    if len(kp_coords) == 0:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    coords = kp_coords.copy() # Work on a copy\n",
    "\n",
    "    # 1. Reverse scaling\n",
    "    scale_factor = variation_info['scale_factor']\n",
    "    coords /= scale_factor # Now coords are in the space of the original/cropped image (before resize)\n",
    "\n",
    "    # 2. Reverse cropping offset\n",
    "    if variation_info['type'] == 'crop' and variation_info['crop_box'] is not None:\n",
    "        x_crop, y_crop, _, _ = variation_info['crop_box']\n",
    "        coords[:, 0] += x_crop\n",
    "        coords[:, 1] += y_crop\n",
    "\n",
    "    # Ensure points are within original image bounds (optional, but good practice)\n",
    "    # coords[:, 0] = np.clip(coords[:, 0], 0, img_orig_size[0] - 1)\n",
    "    # coords[:, 1] = np.clip(coords[:, 1], 0, img_orig_size[1] - 1)\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ab46fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:34.969841Z",
     "iopub.status.busy": "2025-05-14T16:37:34.969633Z",
     "iopub.status.idle": "2025-05-14T16:37:34.974764Z",
     "shell.execute_reply": "2025-05-14T16:37:34.973955Z"
    },
    "papermill": {
     "duration": 0.013581,
     "end_time": "2025-05-14T16:37:34.976029",
     "exception": false,
     "start_time": "2025-05-14T16:37:34.962448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "GLOBAL_DESC_MODEL = '/kaggle/input/dinov2/pytorch/base/1' # Path to your DINOv2 model\n",
    "DATA_DIR = '.' # Base directory for outputs\n",
    "# FEATURE_DIR = os.path.join(DATA_DIR, 'features_combined')\n",
    "# MATCH_DIR = os.path.join(DATA_DIR, 'matches_global')\n",
    "\n",
    "# Initial detection parameters for cropping data collection\n",
    "INITIAL_DETECTION_RESIZE = 1280\n",
    "INITIAL_DETECTION_NUM_FEATURES = 2048\n",
    "\n",
    "# Parameters for TTA detection and combination\n",
    "TTA_SCALES = [1280, 2048]\n",
    "TTA_NUM_FEATURES = 2048\n",
    "USE_CROPPED_IMAGES = True\n",
    "\n",
    "# Parameters for New Cropping Method\n",
    "MIN_PAIRS_FOR_CROPPING = 3\n",
    "CROP_PADDING = 50\n",
    "DBSCAN_EPS = 20 # Fixed EPS fallback or base value\n",
    "DBSCAN_MIN_SAMPLES = 5 # DBSCAN min_samples parameter\n",
    "# New config for adaptive EPS based on resolution\n",
    "DBSCAN_EPS_RESOLUTION_RATIO = 0.02 # Ratio of max image dimension for EPS (e.g., 0.01 -> 1% of longer side)\n",
    "\n",
    "\n",
    "# Coordinate precision for deduplication (rounding float coordinates)\n",
    "COORD_PRECISION = 1 # Number of decimal places to round coordinates for uniqueness check\n",
    "\n",
    "# Matching parameters\n",
    "MIN_MATCHES_PER_VARIATION = 5 # Lowered this threshold slightly, as combining might filter some\n",
    "MIN_TOTAL_MATCHES_PER_PAIR = 20 # Minimum unique matches for a pair to be saved in global list\n",
    "\n",
    "# Output file names (within FEATURE_DIR and MATCH_DIR)\n",
    "KEYPOINTS_SUBDIR = 'keypoints'\n",
    "DESCRIPTORS_H5 = 'descriptors.h5'\n",
    "MATCHES_PT = 'matches.pt'\n",
    "CROP_DATA = 'crop_data.h5'\n",
    "CROP_INFO = 'crop_info.h5'\n",
    "\n",
    "# Parameters for graph building and clustering thresholds\n",
    "# These are the internal names used in the function; map external arguments to these if needed\n",
    "MIN_MATCHES_FOR_GRAPH_EDGE = 20  # Min matches for adding edge to graph\n",
    "MIN_MATCHES_FOR_FILTERED_GRAPH = 75 # Min matches for filtering graph (your aliked_dis_min)\n",
    "MIN_IMAGES_PER_CLUSTER = 5 # Min images in a final cluster\n",
    "\n",
    "\n",
    "# Config for NMS\n",
    "NMS_SIZE_PIXELS = 3 # Radius in pixels for NMS suppression\n",
    "NMS_SIZE_PIXELS_ratio = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af940ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:34.991054Z",
     "iopub.status.busy": "2025-05-14T16:37:34.990810Z",
     "iopub.status.idle": "2025-05-14T16:37:35.005743Z",
     "shell.execute_reply": "2025-05-14T16:37:35.005097Z"
    },
    "papermill": {
     "duration": 0.024291,
     "end_time": "2025-05-14T16:37:35.006995",
     "exception": false,
     "start_time": "2025-05-14T16:37:34.982704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "\n",
    "# # Must Use efficientnet global descriptor to get matching shortlists.\n",
    "# def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "#     processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "#     model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "#     model = model.eval()\n",
    "#     model = model.to(device)\n",
    "#     global_descs_dinov2 = []\n",
    "#     for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "#         key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "#         timg = load_torch_image(img_fname_full)\n",
    "#         with torch.inference_mode():\n",
    "#             inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "#         global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "#     global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "#     return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_global_desc(fnames, device=torch.device('cpu')):\n",
    "    \"\"\"Computes global descriptors for images.\"\"\"\n",
    "    print(f\"Computing global descriptors with DINOv2 on {device}...\")\n",
    "    processor = AutoImageProcessor.from_pretrained(GLOBAL_DESC_MODEL)\n",
    "    model = AutoModel.from_pretrained(GLOBAL_DESC_MODEL)\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    global_descs_dinov2 = []\n",
    "    for img_fname_full in tqdm(fnames, desc=\"DINOv2 Descriptors\"):\n",
    "        # Need error handling here for potentially problematic images\n",
    "        try:\n",
    "            timg = load_torch_image(img_fname_full, device=device)\n",
    "            with torch.inference_mode():\n",
    "                inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "                outputs = model(**inputs)\n",
    "                # Using CLS token or pooling as descriptor\n",
    "                # descriptor = outputs.last_hidden_state[:, 0].mean(dim=1) # CLS token\n",
    "                descriptor = F.normalize(outputs.last_hidden_state[:, 1:].max(dim=1)[0], dim=1, p=2) # Pool spatial tokens\n",
    "                global_descs_dinov2.append(descriptor.detach().cpu())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_fname_full} for global descriptor: {e}\")\n",
    "            # Append a zero vector or handle missing descriptor later\n",
    "            global_descs_dinov2.append(torch.zeros(1, model.config.hidden_size))\n",
    "\n",
    "\n",
    "    # Pad or handle missing descriptors if errors occurred\n",
    "    max_dim = max(d.shape[1] for d in global_descs_dinov2)\n",
    "    global_descs_dinov2 = [\n",
    "        F.pad(d, (0, max_dim - d.shape[1])) for d in global_descs_dinov2\n",
    "    ]\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 30,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"25%:  {np.percentile(dm_flat, 25):.4f}\")\n",
    "    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    threshold = dm_flat.mean() + np.sqrt(3) * dm_flat.std()\n",
    "    # removing half\n",
    "    # thr = min(np.percentile(dm_flat, 50), sim_th)\n",
    "    mask = dm <= np.percentile(dm_flat, 50)\n",
    "    # print(\"thr :\", thr)\n",
    "    # mask = dm<=threshold\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list\n",
    "\n",
    "# def detect_aliked(img_fnames,\n",
    "#                   feature_dir = '.featureout',\n",
    "#                   num_features = 4096,\n",
    "#                   resize_to = 1024,\n",
    "#                   device=torch.device('cpu')):\n",
    "#     dtype = torch.float32 # ALIKED has issues with float16\n",
    "#     extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "#     extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "#     if not os.path.isdir(feature_dir):\n",
    "#         os.makedirs(feature_dir)\n",
    "#     with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "#          h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "#         for img_path in tqdm(img_fnames):\n",
    "#             img_fname = img_path.split('/')[-1]\n",
    "#             key = img_fname\n",
    "#             with torch.inference_mode():\n",
    "#                 image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "#                 feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "#                 kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "#                 descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "#                 f_kp[key] = kpts\n",
    "#                 f_desc[key] = descs\n",
    "#     return\n",
    "\n",
    "# def match_with_lightglue(img_fnames,\n",
    "#                    index_pairs,\n",
    "#                    feature_dir = '.featureout',\n",
    "#                    device=torch.device('cpu'),\n",
    "#                    min_matches=20,verbose=True):\n",
    "#     lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "#                                                 \"depth_confidence\": -1,\n",
    "#                                                  \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "#     with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "#         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "#         h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "#         for pair_idx in tqdm(index_pairs):\n",
    "#             idx1, idx2 = pair_idx\n",
    "#             fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "#             key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "#             kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "#             kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "#             desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "#             desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "#             with torch.inference_mode():\n",
    "#                 dists, idxs = lg_matcher(desc1,\n",
    "#                                          desc2,\n",
    "#                                          KF.laf_from_center_scale_ori(kp1[None]),\n",
    "#                                          KF.laf_from_center_scale_ori(kp2[None]))\n",
    "#             if len(idxs)  == 0:\n",
    "#                 continue\n",
    "#             n_matches = len(idxs)\n",
    "#             if verbose:\n",
    "#                 print (f'{key1}-{key2}: {n_matches} matches')\n",
    "#             group  = f_match.require_group(key1)\n",
    "#             if n_matches >= min_matches:\n",
    "#                  group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "#     return\n",
    "\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f02eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.021347Z",
     "iopub.status.busy": "2025-05-14T16:37:35.021125Z",
     "iopub.status.idle": "2025-05-14T16:37:35.026479Z",
     "shell.execute_reply": "2025-05-14T16:37:35.025787Z"
    },
    "papermill": {
     "duration": 0.01407,
     "end_time": "2025-05-14T16:37:35.027901",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.013831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_kornia_resize_scale(original_size_hw, target_resize):\n",
    "    \"\"\"\n",
    "    Calculates the scale factor Kornia's default resize applies.\n",
    "    Assumes aspect ratio is maintained and longer side is scaled to target_resize,\n",
    "    only if longer side > target_resize.\n",
    "\n",
    "    Args:\n",
    "        original_size_hw (tuple): Original image size (H, W).\n",
    "        target_resize (int): The target size for the longer side.\n",
    "\n",
    "    Returns:\n",
    "        float: The scale factor applied (processed_size / original_size).\n",
    "    \"\"\"\n",
    "    h_orig, w_orig = original_size_hw\n",
    "    max_orig_dim = max(h_orig, w_orig)\n",
    "\n",
    "    if target_resize is None or target_resize <= 0 or max_orig_dim <= target_resize:\n",
    "        # No resizing or scaling up is needed based on default logic\n",
    "        return 1.0\n",
    "    else:\n",
    "        # Scale down the longer side to target_resize\n",
    "        return target_resize / max_orig_dim\n",
    "\n",
    "def transform_points_from_processed(kp_processed, processed_scale_factor, crop_box=None):\n",
    "    \"\"\"\n",
    "    Transforms keypoint coordinates from the 'processed' scale space\n",
    "    back to the original image space, accounting for scaling and cropping.\n",
    "\n",
    "    Args:\n",
    "        kp_processed (np.ndarray): Keypoint coordinates [N, 2] in the processed space (after scaling by ALIKED).\n",
    "        processed_scale_factor (float): The calculated scale factor applied by ALIKED (processed_size / original_or_cropped_size).\n",
    "        crop_box (list): [x, y, w, h] of the crop in original image coords, or None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Keypoint coordinates [N, 2] in the original image space.\n",
    "    \"\"\"\n",
    "    if len(kp_processed) == 0:\n",
    "        return np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "    coords = kp_processed.copy().astype(np.float32) # Ensure float32\n",
    "\n",
    "    # 1. Reverse scaling (from processed scale back to original or cropped scale)\n",
    "    if processed_scale_factor > 0:\n",
    "        coords /= processed_scale_factor\n",
    "    # else: scale_factor is 1.0, no change needed here\n",
    "\n",
    "    # 2. Add cropping offset (from cropped coordinates back to original coordinates)\n",
    "    if crop_box is not None and len(crop_box) == 4 and crop_box[2] > 0 and crop_box[3] > 0:\n",
    "        x_crop, y_crop, _, _ = crop_box\n",
    "        coords[:, 0] += x_crop\n",
    "        coords[:, 1] += y_crop\n",
    "\n",
    "    # Note: We don't clip to original bounds here, as that might discard valid points near edges.\n",
    "    # Downstream steps should handle points outside bounds if necessary.\n",
    "\n",
    "    return coords\n",
    "\n",
    "# Remove the old get_keypoint_original_coords function entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2570b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.041971Z",
     "iopub.status.busy": "2025-05-14T16:37:35.041750Z",
     "iopub.status.idle": "2025-05-14T16:37:35.046498Z",
     "shell.execute_reply": "2025-05-14T16:37:35.045885Z"
    },
    "papermill": {
     "duration": 0.013212,
     "end_time": "2025-05-14T16:37:35.047775",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.034563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 自定义 NMS 函数 ---\n",
    "def custom_nms_2d_keypoints(keypoints_np, scores_np, nms_radius):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression on 2D keypoints based on scores.\n",
    "    This is a custom implementation to replace kornia.feature.non_maximum_suppression2d\n",
    "    due to potential API differences across Kornia versions.\n",
    "\n",
    "    Args:\n",
    "        keypoints_np (np.ndarray): Keypoint coordinates (N, 2) in pixel space.\n",
    "        scores_np (np.ndarray): Scores for each keypoint (N,).\n",
    "        nms_radius (float): Radius for suppression in pixel units.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Indices of the keypoints that survive NMS (original indices).\n",
    "    \"\"\"\n",
    "    if len(keypoints_np) == 0:\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "    # Get indices sorted by score in descending order\n",
    "    order = scores_np.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    # suppressed array to track which keypoints have been suppressed\n",
    "    suppressed = np.zeros(len(keypoints_np), dtype=bool)\n",
    "\n",
    "    for i_sorted in order: # Iterate through keypoints from highest score to lowest\n",
    "        if suppressed[i_sorted]:\n",
    "            continue # This keypoint has already been suppressed by a higher-scoring one\n",
    "\n",
    "        keep.append(i_sorted) # Keep this keypoint\n",
    "\n",
    "        current_kp = keypoints_np[i_sorted]\n",
    "\n",
    "        # Calculate squared distances from the current keypoint to all other keypoints\n",
    "        # Using squared distance avoids sqrt for efficiency if only comparing to radius^2\n",
    "        distances_sq = np.sum((keypoints_np - current_kp)**2, axis=1)\n",
    "\n",
    "        # Identify keypoints within the suppression radius\n",
    "        points_within_radius_mask = distances_sq < nms_radius**2\n",
    "\n",
    "        # Mark these keypoints as suppressed\n",
    "        suppressed[points_within_radius_mask] = True\n",
    "\n",
    "    return np.array(keep, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdeca7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.062116Z",
     "iopub.status.busy": "2025-05-14T16:37:35.061879Z",
     "iopub.status.idle": "2025-05-14T16:37:35.087560Z",
     "shell.execute_reply": "2025-05-14T16:37:35.086876Z"
    },
    "papermill": {
     "duration": 0.034518,
     "end_time": "2025-05-14T16:37:35.088923",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.054405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... (imports, configs, utility functions like load_torch_image, load_pil_image, get_image_size)\n",
    "# Add the new calculate_kornia_resize_scale and transform_points_from_processed functions here\n",
    "\n",
    "def perform_initial_detection_and_matching(img_fnames, index_pairs, data_dir, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Performs detection (ALIKED) and matching (LightGlue) on original images\n",
    "    at a base resolution (e.g., 1024) to collect data for the cropping step.\n",
    "    Stores keypoints (at detection scale) and matches in temporary HDF5.\n",
    "    Analyzes matches to create crop data file.\n",
    "    \"\"\"\n",
    "    temp_feature_dir = os.path.join(data_dir, '.temp_crop_features')\n",
    "    os.makedirs(temp_feature_dir, exist_ok=True)\n",
    "\n",
    "    initial_feature_file = os.path.join(temp_feature_dir, f'initial_features_{INITIAL_DETECTION_RESIZE}.h5')\n",
    "    initial_match_file = os.path.join(temp_feature_dir, f'initial_matches_{INITIAL_DETECTION_RESIZE}.h5')\n",
    "    crop_data_file = os.path.join(data_dir, CROP_DATA)\n",
    "\n",
    "    if os.path.exists(crop_data_file):\n",
    "         print(f\"Initial detection and matching data for cropping exists: {crop_data_file}. deleting.\")\n",
    "         os.remove\n",
    "         # return crop_data_file\n",
    "\n",
    "    print(f\"Performing initial ALIKED detection ({INITIAL_DETECTION_RESIZE}) and LightGlue matching for cropping data...\")\n",
    "\n",
    "    # 1. Initial Detection\n",
    "    print(\"Running initial ALIKED detection...\")\n",
    "    try:\n",
    "        extractor = ALIKED(max_num_keypoints=INITIAL_DETECTION_NUM_FEATURES, detection_threshold=0.1).eval().to(device, dtype=torch.float32)\n",
    "        # Set resize parameter here\n",
    "        extractor.preprocess_conf[\"resize\"] = INITIAL_DETECTION_RESIZE\n",
    "        # Ensure ALIKED is on the correct device/dtype\n",
    "        # extractor.to(device, dtype=torch.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ALIKED extractor: {e}\")\n",
    "        return None\n",
    "\n",
    "    with h5py.File(initial_feature_file, mode='w') as f_kp_desc:\n",
    "        for img_path in tqdm(img_fnames, desc=\"Initial ALIKED Detection\"):\n",
    "            img_key = os.path.basename(img_path)\n",
    "            kp = None # Initialize features to None\n",
    "            desc = None\n",
    "            calculated_scale = None # Will store the calculated scale factor\n",
    "            original_pil_size = None\n",
    "\n",
    "            try:\n",
    "                # Load original PIL image to get its size\n",
    "                img_orig_pil = load_pil_image(img_path)\n",
    "                if img_orig_pil is None:\n",
    "                     print(f\"Skipping ALIKED for {img_path}: PIL image loading failed.\")\n",
    "                     continue\n",
    "                original_pil_size = img_orig_pil.size # (W, H)\n",
    "\n",
    "                # Calculate the expected scale factor ALIKED will apply\n",
    "                # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n",
    "                # Input image tensor size will be (H, W) after Kornia loading/conversion\n",
    "                input_tensor_size_hw = (original_pil_size[1], original_pil_size[0]) # Convert (W, H) to (H, W)\n",
    "                calculated_scale = calculate_kornia_resize_scale(input_tensor_size_hw, INITIAL_DETECTION_RESIZE)\n",
    "\n",
    "\n",
    "                # Load image as Kornia tensor for ALIKED\n",
    "                timg = K.image_to_tensor(np.array(img_orig_pil), keepdim=True).to(device, torch.float32) / 255.0 # Normalize\n",
    "                if timg.ndim == 3: timg = timg[None, ...] # Ensure BxCxHxW\n",
    "\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    # Pass resize to extractor, but we don't expect processed_size back\n",
    "                    # The scale is calculated based on preprocess_conf[\"resize\"]\n",
    "                    feats = extractor.extract(timg)\n",
    "\n",
    "\n",
    "                    # --- Add Error Handling for accessing feats dictionary ---\n",
    "                    try:\n",
    "                        # Check for expected keys ('keypoints', 'descriptors')\n",
    "                        if 'keypoints' in feats and 'descriptors' in feats and \\\n",
    "                           len(feats.get('keypoints', [])) > 0 and len(feats.get('descriptors', [])) > 0: # Use .get with default for safety\n",
    "\n",
    "                            kp = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                            desc = feats['descriptors'].reshape(len(kp), -1).detach().cpu().numpy()\n",
    "\n",
    "                            # Check length after reshaping just in case\n",
    "                            if len(kp) == 0 or len(desc) == 0:\n",
    "                                 print(f\"Warning: Extracted features are empty for {img_path} after reshape.\")\n",
    "                                 kp = None; desc = None # Invalidate data\n",
    "\n",
    "\n",
    "                        else:\n",
    "                             missing_keys = [k for k in ['keypoints', 'descriptors'] if k not in feats]\n",
    "                             empty_data_keys = [k for k in ['keypoints', 'descriptors'] if k in feats and len(feats[k]) == 0]\n",
    "\n",
    "                             if missing_keys or empty_data_keys:\n",
    "                                 print(f\"Warning: Missing or empty required data in ALIKED output for {img_path}. (Missing Keys: {missing_keys}, Empty Data Keys: {empty_data_keys})\")\n",
    "\n",
    "                             kp = None; desc = None # Ensure invalid data is cleared\n",
    "\n",
    "\n",
    "                    except KeyError as ke:\n",
    "                         # This catches if the keys 'keypoints' or 'descriptors' are unexpectedly missing entirely\n",
    "                         print(f\"Error: Expected key '{ke}' not found in ALIKED features for {img_path}.\")\n",
    "                         kp = None; desc = None\n",
    "                    except Exception as e:\n",
    "                         print(f\"Unexpected error processing ALIKED features result for {img_path}: {e}\")\n",
    "                         kp = None; desc = None\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during initial ALIKED feature extraction for {img_path}: {e}\")\n",
    "\n",
    "\n",
    "            # --- Check if features were successfully obtained and save ---\n",
    "            if kp is not None and desc is not None and calculated_scale is not None and original_pil_size is not None:\n",
    "                try:\n",
    "                    img_group = f_kp_desc.create_group(img_key)\n",
    "                    # Save keypoints AS IS (in the processed scale space)\n",
    "                    img_group.create_dataset('keypoints', data=kp.astype(np.float32))\n",
    "                    img_group.create_dataset('descriptors', data=desc.astype(np.float32))\n",
    "                    # Store the calculated scale factor and original size for later transformation\n",
    "                    img_group.attrs['calculated_scale_factor'] = float(calculated_scale)\n",
    "                    img_group.attrs['original_pil_size'] = original_pil_size # (W, H) tuple\n",
    "                    img_group.attrs['original_path'] = img_path # Store original path\n",
    "                except Exception as e:\n",
    "                     print(f\"Error saving initial features for {img_path} to HDF5: {e}\")\n",
    "            else:\n",
    "                print(f\"Skipping saving initial features for {img_path} due to extraction failure or empty results.\")\n",
    "\n",
    "\n",
    "    # ... (rest of the perform_initial_detection_and_matching function: Initial Matching, Analyze Matches, Save crop_data.h5)\n",
    "    # Ensure the rest of the function correctly handles cases where some images might not have initial features saved in f_kp_desc\n",
    "\n",
    "    # 2. Initial Matching (Keep this part as it's needed for the current cropping method)\n",
    "    print(\"Running initial LightGlue matching...\")\n",
    "    try:\n",
    "        lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                    \"depth_confidence\": -1,\n",
    "                                                    \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "        if device == torch.device('cpu'):\n",
    "             lg_matcher.to('cpu')\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error loading LightGlue matcher: {e}\")\n",
    "         pass # Allow to continue to analysis if possible\n",
    "\n",
    "\n",
    "    # Ensure initial_match_file is created even if empty, if matching failed entirely\n",
    "    # This prevents subsequent H5 read errors\n",
    "    if not os.path.exists(initial_match_file): # Corrected typo here\n",
    "        try:\n",
    "            with h5py.File(initial_match_file, mode='w') as f:\n",
    "                pass # Create an empty file\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating empty initial match file {initial_match_file}: {e}\")\n",
    "\n",
    "\n",
    "    # Match only images that had features successfully extracted\n",
    "    # Read keys from the initial_feature_file HDF5\n",
    "    extracted_image_keys = []\n",
    "    if os.path.exists(initial_feature_file): # Corrected typo here\n",
    "        try:\n",
    "            with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read:\n",
    "                 extracted_image_keys = list(f_kp_desc_read.keys())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading keys from initial feature file {initial_feature_file}: {e}\")\n",
    "\n",
    "\n",
    "    # Create a mapping from image key to its original index\n",
    "    # key_to_idx = {os.path.basename(fname): i for i, fname in enumerate(img_fnames)} # Not used in this block directly\n",
    "\n",
    "    # Filter index_pairs to include only pairs where both images had features extracted\n",
    "    filtered_index_pairs = []\n",
    "    for idx1, idx2 in index_pairs:\n",
    "         key1, key2 = os.path.basename(img_fnames[idx1]), os.path.basename(img_fnames[idx2])\n",
    "         if key1 in extracted_image_keys and key2 in extracted_image_keys:\n",
    "              filtered_index_pairs.append((idx1, idx2))\n",
    "         # else: print(f\"Skipping initial match for {key1}-{key2}: features not extracted for one or both.\")\n",
    "\n",
    "\n",
    "    if not filtered_index_pairs:\n",
    "         print(\"No image pairs with extracted features to perform initial matching.\")\n",
    "         # Proceed to analysis, crop_data.h5 might be empty\n",
    "\n",
    "    else:\n",
    "        with h5py.File(initial_feature_file, mode='r') as f_kp_desc_read, \\\n",
    "             h5py.File(initial_match_file, mode='a') as f_match: # Use append mode if file might exist but be incomplete\n",
    "\n",
    "            for idx1, idx2 in tqdm(filtered_index_pairs, desc=\"Initial LightGlue Matching\"):\n",
    "                fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "                key1, key2 = os.path.basename(fname1), os.path.basename(fname2)\n",
    "\n",
    "                # Check if pair already matched (useful if appending)\n",
    "                if key1 in f_match and key2 in f_match[key1]:\n",
    "                     continue # Skip if already matched\n",
    "\n",
    "                try:\n",
    "                    # Load keypoints and descriptors from the initial detection file\n",
    "                    # These KPs are at the processed scale\n",
    "                    # Check for dataset existence within the group\n",
    "                    if 'keypoints' not in f_kp_desc_read[key1] or 'descriptors' not in f_kp_desc_read[key1] or \\\n",
    "                       'keypoints' not in f_kp_desc_read[key2] or 'descriptors' not in f_kp_desc_read[key2]:\n",
    "                         print(f\"Warning: Missing keypoint/descriptor datasets for {key1}-{key2} in initial feature file. Skipping match.\")\n",
    "                         continue\n",
    "\n",
    "                    kp1 = torch.from_numpy(f_kp_desc_read[key1]['keypoints'][...]).to(device)\n",
    "                    kp2 = torch.from_numpy(f_kp_desc_read[key2]['keypoints'][...]).to(device)\n",
    "                    desc1 = torch.from_numpy(f_kp_desc_read[key1]['descriptors'][...]).to(device)\n",
    "                    desc2 = torch.from_numpy(f_kp_desc_read[key2]['descriptors'][...]).to(device)\n",
    "\n",
    "                    if len(kp1) == 0 or len(kp2) == 0:\n",
    "                         continue\n",
    "\n",
    "                    # Create LAFs based on processed scale keypoints\n",
    "                    # Assuming scale 1.0 relative to processed size is appropriate here\n",
    "                    laf1 = KF.laf_from_center_scale_ori(kp1[None])\n",
    "                    laf2 = KF.laf_from_center_scale_ori(kp2[None])\n",
    "\n",
    "                    with torch.inference_mode():\n",
    "                        dists, idxs = lg_matcher(desc1, desc2, laf1, laf2)\n",
    "                    if len(idxs) > 0:\n",
    "                        group = f_match.require_group(key1)\n",
    "                        group.create_dataset(key2, data=idxs.detach().cpu().numpy().astype(np.int32))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during initial LightGlue matching for {key1}-{key2}: {e}\")\n",
    "\n",
    "\n",
    "    # --- (rest of the Analyze Initial Matches part) ---\n",
    "    print(\"Analyzing initial matches for cropping data...\")\n",
    "\n",
    "    kp_match_pairs = {}\n",
    "\n",
    "    if os.path.exists(initial_match_file):\n",
    "        try:\n",
    "            with h5py.File(initial_match_file, mode='r') as f_match:\n",
    "                for img_key1 in f_match.keys():\n",
    "                    for img_key2 in f_match[img_key1].keys():\n",
    "                        try:\n",
    "                            matches = f_match[img_key1][img_key2][...] # Indices (kp1_idx, kp2_idx)\n",
    "\n",
    "                            for kp1_idx, kp2_idx in matches:\n",
    "                                if img_key1 not in kp_match_pairs: kp_match_pairs[img_key1] = {}\n",
    "                                if kp1_idx not in kp_match_pairs[img_key1]: kp_match_pairs[img_key1][kp1_idx] = set()\n",
    "                                kp_match_pairs[img_key1][kp1_idx].add(img_key2)\n",
    "\n",
    "                                if img_key2 not in kp_match_pairs: kp_match_pairs[img_key2] = {}\n",
    "                                if kp2_idx not in kp_match_pairs[img_key2]: kp_match_pairs[img_key2][kp2_idx] = set()\n",
    "                                kp_match_pairs[img_key2][kp2_idx].add(img_key1)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing initial match data for {img_key1}-{img_key2} in analysis: {e}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error reading initial match file {initial_match_file} for analysis: {e}\")\n",
    "\n",
    "\n",
    "    frequent_kp_data = {}\n",
    "\n",
    "    if os.path.exists(initial_feature_file):\n",
    "        try:\n",
    "            with h5py.File(initial_feature_file, mode='r') as f_kp_desc:\n",
    "                 for img_key in kp_match_pairs.keys():\n",
    "                     if img_key not in f_kp_desc:\n",
    "                          print(f\"Warning: Initial features not found for {img_key} (present in matches but not in detection file). Skipping crop data processing for this image.\")\n",
    "                          continue\n",
    "\n",
    "                     try:\n",
    "                         # Check for needed data/attrs\n",
    "                         if 'keypoints' not in f_kp_desc[img_key] or 'calculated_scale_factor' not in f_kp_desc[img_key].attrs or 'original_pil_size' not in f_kp_desc[img_key].attrs:\n",
    "                              print(f\"Warning: Incomplete initial feature data for {img_key}. Skipping crop data processing.\")\n",
    "                              continue\n",
    "\n",
    "                         all_kp_coords_processed = f_kp_desc[img_key]['keypoints'][...] # KPs at processed scale\n",
    "                         calculated_scale_factor = f_kp_desc[img_key].attrs['calculated_scale_factor']\n",
    "                         original_pil_size = f_kp_desc[img_key].attrs['original_pil_size'] # (W, H)\n",
    "\n",
    "                         kp_data_dict = kp_match_pairs[img_key]\n",
    "\n",
    "                         frequent_indices = [kp_idx for kp_idx, matched_pairs in kp_data_dict.items()\n",
    "                                             if len(matched_pairs) >= MIN_PAIRS_FOR_CROPPING and kp_idx < len(all_kp_coords_processed)] # Bounds check\n",
    "\n",
    "\n",
    "                         if frequent_indices:\n",
    "                             # Get coordinates at the processed scale\n",
    "                             frequent_coords_processed = all_kp_coords_processed[frequent_indices]\n",
    "\n",
    "                             # Transform coordinates back to original image space using the calculated scale\n",
    "                             # We pass None for crop_box as these points are from the original (non-cropped) image\n",
    "                             frequent_coords_orig_scale = transform_points_from_processed(\n",
    "                                 frequent_coords_processed, calculated_scale_factor, crop_box=None\n",
    "                             )\n",
    "\n",
    "                             # Check for valid transformed coordinates (non-negative)\n",
    "                             valid_frequent_coords_orig = frequent_coords_orig_scale[~np.any(frequent_coords_orig_scale < 0, axis=1)]\n",
    "\n",
    "\n",
    "                             if len(valid_frequent_coords_orig) > 0:\n",
    "                                 frequent_kp_data[img_key] = {\n",
    "                                     'kp_coords_original_scale': valid_frequent_coords_orig\n",
    "                                 }\n",
    "                             else:\n",
    "                                  print(f\"No valid frequent keypoints in original scale for {img_key}.\")\n",
    "\n",
    "                         else:\n",
    "                              print(f\"No frequent keypoints found for {img_key} (threshold={MIN_PAIRS_FOR_CROPPING} pairs).\")\n",
    "\n",
    "                     except Exception as e:\n",
    "                         print(f\"Error processing frequent keypoints for {img_key}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error reading initial feature file {initial_feature_file} for analysis: {e}\")\n",
    "\n",
    "\n",
    "    # ... (rest of the saving frequent_kp_data to crop_data_file) ...\n",
    "    with h5py.File(crop_data_file, mode='w') as f_crop_data:\n",
    "        if frequent_kp_data:\n",
    "            for img_key, data in frequent_kp_data.items():\n",
    "                 group = f_crop_data.create_group(img_key)\n",
    "                 group.create_dataset('kp_coords_original_scale', data=data['kp_coords_original_scale'])\n",
    "\n",
    "\n",
    "    print(f\"Initial detection and matching complete. Cropping data saved to {crop_data_file}\")\n",
    "\n",
    "    return crop_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cc367",
   "metadata": {
    "papermill": {
     "duration": 0.00652,
     "end_time": "2025-05-14T16:37:35.102503",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.095983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77aa7487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.117163Z",
     "iopub.status.busy": "2025-05-14T16:37:35.116857Z",
     "iopub.status.idle": "2025-05-14T16:37:35.128216Z",
     "shell.execute_reply": "2025-05-14T16:37:35.127482Z"
    },
    "papermill": {
     "duration": 0.020001,
     "end_time": "2025-05-14T16:37:35.129433",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.109432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_crop_boxes(img_fnames, crop_data_file, feature_dir, data_dir):\n",
    "    \"\"\"\n",
    "    Calculates crop bounding boxes for each image based on clustered frequent keypoints.\n",
    "    Stores crop boxes in a dedicated HDF5 file.\n",
    "    Uses a resolution-based ratio to determine DBSCAN EPS.\n",
    "    \"\"\"\n",
    "    print(\"Calculating crop boxes using DBSCAN with resolution-based adaptive EPS...\")\n",
    "    # Store crop info in a separate file\n",
    "    crop_info_file = os.path.join(data_dir, 'crop_info.h5')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    with h5py.File(crop_data_file, mode='r') as f_crop_data, \\\n",
    "         h5py.File(crop_info_file, mode='w') as f_crop_info:\n",
    "\n",
    "        for img_path in tqdm(img_fnames, desc=\"Calculating Crops\"):\n",
    "            img_key = os.path.basename(img_path)\n",
    "\n",
    "            if img_key not in f_crop_data:\n",
    "                 # print(f\"No cropping data for {img_key}. No crop will be used.\") # Optional verbose\n",
    "                 img_group = f_crop_info.create_group(img_key)\n",
    "                 img_group.attrs['has_crop'] = False\n",
    "                 img_group.attrs['crop_box'] = [0, 0, 0, 0]\n",
    "                 continue\n",
    "\n",
    "            try:\n",
    "                frequent_coords_orig_scale = f_crop_data[img_key]['kp_coords_original_scale'][...]\n",
    "\n",
    "                img_orig_wh = get_image_size(img_path)\n",
    "                if img_orig_wh is None:\n",
    "                    print(f\"Could not get original size for {img_key}. Skipping crop calculation.\")\n",
    "                    img_group = f_crop_info.create_group(img_key)\n",
    "                    img_group.attrs['has_crop'] = False\n",
    "                    img_group.attrs['crop_box'] = [0, 0, 0, 0]\n",
    "                    continue\n",
    "                img_orig_w, img_orig_h = img_orig_wh\n",
    "\n",
    "\n",
    "                if len(frequent_coords_orig_scale) > 0:\n",
    "                    n_samples = len(frequent_coords_orig_scale)\n",
    "\n",
    "                    # --- Adaptive EPS Calculation: Resolution * Ratio ---\n",
    "                    max_orig_dim = max(img_orig_w, img_orig_h)\n",
    "                    adaptive_eps = max_orig_dim * DBSCAN_EPS_RESOLUTION_RATIO\n",
    "\n",
    "                    # Add a safeguard: adaptive_eps should be > 0.\n",
    "                    # Fallback to fixed DBSCAN_EPS if calculation yields non-positive or ratio is zero/negative\n",
    "                    if adaptive_eps <= 0:\n",
    "                         # print(f\"Warning: Calculated resolution-based EPS is {adaptive_eps} for {img_key} (Max Dim={max_orig_dim}, Ratio={DBSCAN_EPS_RESOLUTION_RATIO}). Using fixed DBSCAN_EPS as fallback.\")\n",
    "                         # Fallback to the original fixed EPS config\n",
    "                         adaptive_eps = DBSCAN_EPS\n",
    "\n",
    "                    # DBSCAN requires at least min_samples + 1 points to form any cluster\n",
    "                    if n_samples >= DBSCAN_MIN_SAMPLES + 1:\n",
    "                        try:\n",
    "                             # --- Apply DBSCAN with Resolution-based Adaptive EPS ---\n",
    "                             # print(f\"Image {img_key}: Max Dim = {max_orig_dim}, Ratio = {DBSCAN_EPS_RESOLUTION_RATIO:.4f}, Adaptive EPS = {adaptive_eps:.2f}\") # Improved print format\n",
    "                             db = DBSCAN(eps=adaptive_eps, min_samples=DBSCAN_MIN_SAMPLES).fit(frequent_coords_orig_scale)\n",
    "                             labels = db.labels_\n",
    "\n",
    "                        except Exception as e:\n",
    "                             print(f\"Error during DBSCAN for {img_key}: {e}. Skipping crop calculation.\")\n",
    "                             labels = np.array([-1] * n_samples) # Treat all points as noise on error\n",
    "\n",
    "\n",
    "                    else:\n",
    "                         # print(f\"Warning: Not enough frequent points ({n_samples}) for DBSCAN with min_samples={DBSCAN_MIN_SAMPLES} for {img_key}. Skipping DBSCAN.\")\n",
    "                         labels = np.array([-1] * n_samples) # Treat all points as noise if not enough samples\n",
    "\n",
    "                    # Find the bounding box of all non-noise clustered points\n",
    "                    clustered_points = frequent_coords_orig_scale[labels != -1] # Exclude noise points (-1)\n",
    "\n",
    "                    if len(clustered_points) > 0:\n",
    "                        min_x, min_y = np.min(clustered_points, axis=0)\n",
    "                        max_x, max_y = np.max(clustered_points, axis=0)\n",
    "\n",
    "                        # Add padding and ensure bounds are within the original image\n",
    "                        min_x = max(0.0, min_x - CROP_PADDING)\n",
    "                        min_y = max(0.0, min_y - CROP_PADDING)\n",
    "                        max_x = min(float(img_orig_w - 1), max_x + CROP_PADDING)\n",
    "                        max_y = min(float(img_orig_h - 1), max_y + CROP_PADDING)\n",
    "\n",
    "                        # Ensure crop has positive dimensions\n",
    "                        crop_w = max_x - min_x + 1\n",
    "                        crop_h = max_y - min_y + 1\n",
    "\n",
    "                        if crop_w > 0 and crop_h > 0:\n",
    "                            # Crop box format: [x, y, w, h] (integers for simplicity)\n",
    "                            crop_box = [int(min_x), int(min_y), int(crop_w), int(crop_h)]\n",
    "                            # print(f\"Calculated crop box for {img_key}: {crop_box}\")\n",
    "\n",
    "                            # Store crop box\n",
    "                            img_group = f_crop_info.create_group(img_key)\n",
    "                            img_group.attrs['has_crop'] = True\n",
    "                            img_group.attrs['crop_box'] = crop_box\n",
    "                        else:\n",
    "                            # print(f\"Calculated crop box for {img_key} has zero dimensions. No crop will be used.\")\n",
    "                            img_group = f_crop_info.create_group(img_key)\n",
    "                            img_group.attrs['has_crop'] = False\n",
    "                            img_group.attrs['crop_box'] = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        # print(f\"DBSCAN found no significant clusters for {img_key}. No crop will be used.\")\n",
    "                        img_group = f_crop_info.create_group(img_key)\n",
    "                        img_group.attrs['has_crop'] = False\n",
    "                        img_group.attrs['crop_box'] = [0, 0, 0, 0]\n",
    "\n",
    "                else:\n",
    "                     print(f\"No frequent keypoints found for {img_key}. No crop will be used.\")\n",
    "                     img_group = f_crop_info.create_group(img_key)\n",
    "                     img_group.attrs['has_crop'] = False\n",
    "                     img_group.attrs['crop_box'] = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating crop box for {img_key}: {e}\")\n",
    "                # Ensure entry is made in crop_info.h5 even on error\n",
    "                if img_key not in f_crop_info:\n",
    "                     img_group = f_crop_info.create_group(img_key)\n",
    "                img_group.attrs['has_crop'] = False\n",
    "                img_group.attrs['crop_box'] = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "    print(\"Crop box calculation complete.\")\n",
    "    return crop_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24a00b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.144057Z",
     "iopub.status.busy": "2025-05-14T16:37:35.143709Z",
     "iopub.status.idle": "2025-05-14T16:37:35.172661Z",
     "shell.execute_reply": "2025-05-14T16:37:35.172049Z"
    },
    "papermill": {
     "duration": 0.037594,
     "end_time": "2025-05-14T16:37:35.173799",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.136205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... (imports)\n",
    "import kornia.feature as KF # Ensure KF is imported\n",
    "\n",
    "# --- Multi-Variation Detection and Combination ---\n",
    "\n",
    "def detect_and_combine_features(img_fnames, crop_info_file, feature_dir, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Detects ALIKED features for multiple scales and original/cropped images,\n",
    "    combines unique features per image (deduplicating based on original coords),\n",
    "    applies NMS, and saves combined features to .pt and .h5 files per image.\n",
    "    \"\"\"\n",
    "    print(\"Running multi-variation ALIKED detection and combining features (with NMS)...\")\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    keypoints_subdir_path = os.path.join(feature_dir, KEYPOINTS_SUBDIR)\n",
    "    os.makedirs(keypoints_subdir_path, exist_ok=True)\n",
    "    descriptors_h5_path = os.path.join(feature_dir, DESCRIPTORS_H5)\n",
    "\n",
    "    with h5py.File(descriptors_h5_path, mode='w') as f_descriptors, \\\n",
    "         h5py.File(crop_info_file, mode='r') as f_crop_info:\n",
    "\n",
    "        extractor = None # Initialize extractor outside the loop\n",
    "\n",
    "        for img_path in tqdm(img_fnames, desc=\"Detecting & Combining Features\"):\n",
    "            img_key = os.path.basename(img_path)\n",
    "            kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt')\n",
    "\n",
    "            # Skip if combined features already exist for this image\n",
    "            if os.path.exists(kp_pt_path) and img_key in f_descriptors:\n",
    "                 # print(f\"Combined features for {img_key} already exist. Skipping detection.\") # Optional verbose\n",
    "                 continue\n",
    "\n",
    "            img_orig_pil = load_pil_image(img_path)\n",
    "            if img_orig_pil is None:\n",
    "                print(f\"Could not load original image {img_path}. Skipping.\")\n",
    "                # Create empty files/datasets to indicate processing happened (and failed)\n",
    "                try:\n",
    "                    torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n",
    "                    if img_key not in f_descriptors:\n",
    "                         f_descriptors.create_group(img_key)\n",
    "                    print(f\"Created empty combined feature files for {img_key}.\")\n",
    "                except Exception as e:\n",
    "                     print(f\"Error creating empty combined feature files for {img_key}: {e}\")\n",
    "                continue\n",
    "\n",
    "            img_orig_w, img_orig_h = img_orig_pil.size\n",
    "\n",
    "            # Get crop info\n",
    "            has_crop = False\n",
    "            crop_box = [0, 0, 0, 0]\n",
    "            if img_key in f_crop_info:\n",
    "                 img_crop_group = f_crop_info[img_key]\n",
    "                 has_crop = img_crop_group.attrs.get('has_crop', False)\n",
    "                 crop_box = img_crop_group.attrs.get('crop_box', [0, 0, 0, 0]).tolist() \n",
    "            if has_crop and (not isinstance(crop_box, (tuple, list)) or len(crop_box) != 4 or crop_box[2] <= 0 or crop_box[3] <= 0):\n",
    "                 print(f\"Warning: Invalid crop box {crop_box} for {img_key} despite has_crop=True. Ignoring crop.\")\n",
    "                 has_crop = False\n",
    "                 crop_box = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "            variations_to_process = []\n",
    "            for scale in TTA_SCALES:\n",
    "                variations_to_process.append({'type': 'orig', 'scale_target': scale, 'crop_box': None, 'pil_img': img_orig_pil})\n",
    "                if USE_CROPPED_IMAGES and has_crop:\n",
    "                    x, y, w, h = crop_box\n",
    "                    try:\n",
    "                         img_cropped_pil = img_orig_pil.crop((x, y, x + w, y + h))\n",
    "                         variations_to_process.append({'type': 'crop', 'scale_target': scale, 'crop_box': crop_box, 'pil_img': img_cropped_pil})\n",
    "                    except Exception as e:\n",
    "                         print(f\"Error cropping image {img_key} with box {crop_box}: {e}. Skipping cropped variation.\")\n",
    "\n",
    "\n",
    "            all_kps_orig_coords = []\n",
    "            all_descriptors = []\n",
    "            all_scores = [] # Collect scores too\n",
    "\n",
    "            if extractor is None:\n",
    "                 try:\n",
    "                    extractor = ALIKED(max_num_keypoints=TTA_NUM_FEATURES, detection_threshold=0.1).eval().to(DEVICE, dtype=torch.float32)\n",
    "                    if DEVICE == torch.device('cpu'):\n",
    "                         extractor.to('cpu', torch.float32)\n",
    "                 except Exception as e:\n",
    "                    print(f\"Error loading ALIKED extractor: {e}\")\n",
    "                    try:\n",
    "                        torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n",
    "                        if img_key not in f_descriptors:\n",
    "                             f_descriptors.create_group(img_key)\n",
    "                        print(f\"Created empty combined feature files for {img_key} due to extractor failure.\")\n",
    "                    except Exception as e_save:\n",
    "                         print(f\"Error creating empty combined feature files for {img_key} on extractor failure: {e_save}\")\n",
    "                    continue\n",
    "\n",
    "            for var_info in variations_to_process:\n",
    "                 var_type = var_info['type']\n",
    "                 var_scale_target = var_info['scale_target']\n",
    "                 var_pil_img = var_info['pil_img']\n",
    "                 var_crop_box = var_info['crop_box']\n",
    "\n",
    "                 try:\n",
    "                     var_pil_size_wh = var_pil_img.size # (W, H)\n",
    "                     var_pil_size_hw = (var_pil_size_wh[1], var_pil_size_wh[0]) # (H, W)\n",
    "\n",
    "                     calculated_scale_variation = calculate_kornia_resize_scale(var_pil_size_hw, var_scale_target)\n",
    "\n",
    "                     if calculated_scale_variation is None:\n",
    "                         print(f\"Skipping ALIKED for {img_key} ({var_type}, {var_scale_target}): Could not calculate resize scale.\")\n",
    "                         # Append None for scores to indicate missing data\n",
    "                         all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                         all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32)) # Handle first variation empty\n",
    "                         all_scores.append(None)\n",
    "                         continue\n",
    "\n",
    "\n",
    "                     timg = K.image_to_tensor(np.array(var_pil_img), keepdim=True).to(device, torch.float32) / 255.0\n",
    "                     if timg.ndim == 3: timg = timg[None, ...]\n",
    " \n",
    "                     if timg.shape[2] == 0 or timg.shape[3] == 0:\n",
    "                          print(f\"Skipping ALIKED for {img_key} ({var_type}, {var_scale_target}): Image tensor is empty.\")\n",
    "                          # Append empty arrays/None for consistency\n",
    "                          all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                          all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32))\n",
    "                          all_scores.append(None)\n",
    "                          continue\n",
    "\n",
    "\n",
    "                     with torch.inference_mode():\n",
    "                         # Request scores too\n",
    "                         feats = extractor.extract(timg, resize=var_scale_target, return_processed_size=False, return_scores=True)\n",
    "\n",
    "\n",
    "                     try:\n",
    "                         # Ensure scores are present if keypoints/descriptors are\n",
    "                         if 'keypoints' in feats and 'descriptors' in feats and 'keypoint_scores' in feats and \\\n",
    "                            len(feats.get('keypoints', [])) > 0 and len(feats.get('descriptors', [])) > 0 and len(feats.get('keypoint_scores', [])) > 0:\n",
    "\n",
    "                             kp_variation = feats['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                             desc_variation = feats['descriptors'].reshape(len(kp_variation), -1).detach().cpu().numpy()\n",
    "                             score_variation = feats['keypoint_scores'].reshape(-1).detach().cpu().numpy()\n",
    "\n",
    "                             if len(kp_variation) > 0 and len(kp_variation) == len(desc_variation) and len(kp_variation) == len(score_variation) and calculated_scale_variation is not None:\n",
    "                                 kp_orig_coords = transform_points_from_processed(\n",
    "                                     kp_variation, calculated_scale_variation, var_crop_box\n",
    "                                 )\n",
    "\n",
    "                                 all_kps_orig_coords.append(kp_orig_coords)\n",
    "                                 all_descriptors.append(desc_variation)\n",
    "                                 all_scores.append(score_variation) # Store scores\n",
    "\n",
    "\n",
    "                             else:\n",
    "                                 print(f\"Warning: Extracted features/scores empty or count mismatch for {img_key} ({var_type}, {var_scale_target}).\")\n",
    "                                 # Append empty arrays/None for consistency\n",
    "                                 all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                                 all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32))\n",
    "                                 all_scores.append(None)\n",
    "\n",
    "\n",
    "                         else:\n",
    "                             missing_keys = [k for k in ['keypoints', 'descriptors', 'keypoint_scores'] if k not in feats]\n",
    "                             empty_data_keys = [k for k in ['keypoints', 'descriptors', 'keypoint_scores'] if k in feats and len(feats[k]) == 0]\n",
    "\n",
    "                             if missing_keys or empty_data_keys:\n",
    "                                  print(f\"Warning: Missing or empty required data in ALIKED output for {img_key} ({var_type}, {var_scale_target}). (Missing Keys: {missing_keys}, Empty Data Keys: {empty_data_keys})\")\n",
    "\n",
    "                             # Append empty arrays/None for consistency\n",
    "                             all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                             all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32))\n",
    "                             all_scores.append(None)\n",
    "\n",
    "\n",
    "                     except KeyError as ke:\n",
    "                          print(f\"Error: Expected key '{ke}' not found in ALIKED features for {img_key} ({var_type}, {var_scale_target}).\")\n",
    "                          all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                          all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32))\n",
    "                          all_scores.append(None) # Append None if scores key missing\n",
    "                     except Exception as e:\n",
    "                          print(f\"Unexpected error processing ALIKED features result for {img_key} ({var_type}, {var_scale_target}): {e}\")\n",
    "                          all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                          all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32))\n",
    "                          all_scores.append(None)\n",
    "\n",
    "\n",
    "                 except Exception as e:\n",
    "                      print(f\"Error during ALIKED feature extraction for {img_key} ({var_type}, {var_scale_target}): {e}\")\n",
    "                      all_kps_orig_coords.append(np.empty((0, 2), dtype=np.float32))\n",
    "                      all_descriptors.append(np.empty((0, desc_variation.shape[1] if all_descriptors else 0), dtype=np.float32))\n",
    "                      all_scores.append(None) # Ensure scores list matches others\n",
    "\n",
    "\n",
    "            # Combine all detected points (now all in original coords) and descriptors/scores\n",
    "            if not any(len(k) > 0 for k in all_kps_orig_coords): # Check if ANY variation yielded points\n",
    "                 print(f\"No valid keypoints detected for any variation of {img_key}.\")\n",
    "                 # Create empty files/datasets to indicate processing happened (and failed)\n",
    "                 try:\n",
    "                    torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n",
    "                    if img_key not in f_descriptors:\n",
    "                         f_descriptors.create_group(img_key)\n",
    "                    print(f\"Created empty combined feature files for {img_key}.\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Error creating empty combined feature files for {img_key}: {e}\")\n",
    "                 continue # Move to the next image\n",
    "\n",
    "            # Concatenate valid numpy arrays\n",
    "            # Need to handle potential variations with 0 points gracefully during concat\n",
    "            combined_kps_orig = np.concatenate([k for k in all_kps_orig_coords if len(k) > 0], axis=0)\n",
    "            combined_descriptors = np.concatenate([d for d in all_descriptors if len(d) > 0], axis=0)\n",
    "\n",
    "            # Only concatenate scores if scores were available for ALL variations\n",
    "            all_scores_available = all(s is not None and len(s) > 0 for s in all_scores) # Check if score arrays are non-empty\n",
    "            if all_scores_available:\n",
    "                 combined_scores_orig = np.concatenate([s for s in all_scores if s is not None and len(s) > 0], axis=0)\n",
    "                 # Ensure lengths match after filtering/concat\n",
    "                 if len(combined_kps_orig) != len(combined_scores_orig):\n",
    "                      print(f\"Warning: Length mismatch between concatenated KPs ({len(combined_kps_orig)}) and Scores ({len(combined_scores_orig)}) for {img_key}. Skipping NMS.\")\n",
    "                      all_scores_available = False # Disable NMS if lengths don't match\n",
    "\n",
    "            combined_kps_for_dedup = combined_kps_orig\n",
    "            combined_descs_for_dedup = combined_descriptors\n",
    "\n",
    "            # max_orig_dim = max(img_orig_w, img_orig_h)\n",
    "            # adaptive_eps = max_orig_dim * DBSCAN_EPS_RESOLUTION_RATIO\n",
    "\n",
    "            # adaptive_nms_size = min(max_orig_dim * NMS_SIZE_PIXELS_ratio, NMS_SIZE_PIXELS)\n",
    "            \n",
    "            # # --- Apply NMS if scores are available ---\n",
    "            # if all_scores_available and len(combined_kps_orig) > 0:\n",
    "            #      try:\n",
    "            #          # Convert to tensor on device for NMS\n",
    "            #          kps_tensor = torch.from_numpy(combined_kps_orig).to(device) # Shape (N, 2)\n",
    "            #          scores_tensor = torch.from_numpy(combined_scores_orig).to(device) # Shape (N,)\n",
    "\n",
    "            #          # Add batch dimension (B=1)\n",
    "            #          kps_tensor = kps_tensor[None, ...] # Shape (1, N, 2)\n",
    "            #          scores_tensor = scores_tensor[None, ...] # Shape (1, N)\n",
    "\n",
    "            #          indices_after_nms_np = custom_nms_2d_keypoints(\n",
    "            #              combined_kps_orig,  \n",
    "            #              combined_scores_orig,  \n",
    "            #              adaptive_nms_size \n",
    "            #          )\n",
    "\n",
    "            #          # Filter the combined arrays using NMS results\n",
    "            #          combined_kps_for_dedup = combined_kps_orig[indices_after_nms_np]\n",
    "            #          combined_descs_for_dedup = combined_descriptors[indices_after_nms_np]\n",
    "            #          combined_scores_orig_filtered = combined_scores_orig[indices_after_nms_np] # 分数不再保存\n",
    "\n",
    "\n",
    "            #          # print(f\"Image {img_key}: Features before NMS = {len(combined_kps_orig)}, Features after NMS = {len(combined_kps_for_dedup)}\")\n",
    "\n",
    "            #      except Exception as e:\n",
    "            #          print(f\"Error during NMS for {img_key}: {e}. Skipping NMS.\")\n",
    "            #          # Fallback: proceed to deduplication without NMS filtering\n",
    "            #          combined_kps_for_dedup = combined_kps_orig\n",
    "            #          combined_descs_for_dedup = combined_descriptors\n",
    "            # elif len(combined_kps_orig) > 0:\n",
    "            #      # print(f\"Skipping NMS for {img_key}: Scores not available for all variations.\") # Optional verbose\n",
    "            #      pass # Proceed directly to deduplication\n",
    "\n",
    "            # --- Perform Coordinate-based Deduplication ---\n",
    "            seen_coords = {}\n",
    "            unique_kps_orig = []\n",
    "            unique_descriptors = []\n",
    "\n",
    "            try:\n",
    "                for i, (kp_coord, descriptor) in enumerate(zip(combined_kps_for_dedup, combined_descs_for_dedup)):\n",
    "                    # Ensure coordinate is a tuple of floats for dictionary key\n",
    "                    # Add small offset before rounding to avoid issues near .5\n",
    "                    rounded_coord = tuple(np.round(kp_coord + 1e-6, COORD_PRECISION).astype(float))\n",
    "\n",
    "                    if rounded_coord not in seen_coords:\n",
    "                        seen_coords[rounded_coord] = len(unique_kps_orig) # Store index in unique list\n",
    "                        unique_kps_orig.append(kp_coord)\n",
    "                        unique_descriptors.append(descriptor) # Keep the descriptor from the first occurrence\n",
    "\n",
    "                # print(f\"Image {img_key}: Features after Deduplication = {len(unique_kps_orig)}\") # Optional verbose\n",
    "\n",
    "                # Convert lists to numpy arrays\n",
    "                unique_kps_orig_np = np.array(unique_kps_orig, dtype=np.float32)\n",
    "                unique_descriptors_np = np.array(unique_descriptors, dtype=np.float32)\n",
    "\n",
    "\n",
    "                # Save unique keypoints to .pt\n",
    "                try:\n",
    "                    torch.save(torch.from_numpy(unique_kps_orig_np), kp_pt_path)\n",
    "                    # print(f\"Saved unique keypoints for {img_key} to {kp_pt_path}\") # Optional verbose\n",
    "                except Exception as e:\n",
    "                     print(f\"Error saving keypoints .pt for {img_key}: {e}\")\n",
    "\n",
    "                # Save unique descriptors to descriptors.h5\n",
    "                try:\n",
    "                    # Use require_group just in case, though mode='w' should prevent existence at start\n",
    "                    img_desc_group = f_descriptors.require_group(img_key)\n",
    "                    img_desc_group.create_dataset('data', data=unique_descriptors_np, compression=\"gzip\") # Use compression\n",
    "                    # print(f\"Saved unique descriptors for {img_key} to {descriptors_h5_path}/{img_key}\") # Optional verbose\n",
    "                except Exception as e:\n",
    "                     print(f\"Error saving descriptors .h5 for {img_key}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during feature combination and deduplication for {img_key}: {e}\")\n",
    "                 # Create empty files/datasets if combination/saving fails\n",
    "                 try:\n",
    "                    torch.save(torch.empty(0, 2, dtype=torch.float32), kp_pt_path)\n",
    "                    if img_key not in f_descriptors:\n",
    "                         f_descriptors.create_group(img_key)\n",
    "                    print(f\"Created empty combined feature files for {img_key} after combination error.\")\n",
    "                 except Exception as e_save:\n",
    "                     print(f\"Error creating empty combined feature files for {img_key} after combination error: {e_save}\")\n",
    "\n",
    "\n",
    "    print(\"Multi-variation detection, combination, NMS, and deduplication complete.\")\n",
    "\n",
    "# ... (rest of the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "473f40a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.188537Z",
     "iopub.status.busy": "2025-05-14T16:37:35.188287Z",
     "iopub.status.idle": "2025-05-14T16:37:35.192221Z",
     "shell.execute_reply": "2025-05-14T16:37:35.191416Z"
    },
    "papermill": {
     "duration": 0.012784,
     "end_time": "2025-05-14T16:37:35.193473",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.180689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image_names_from_json(cluster_path):\n",
    "    with open(os.path.join(cluster_path, 'images.json'), 'r') as f:\n",
    "        full_paths = json.load(f)  # 可能是 ['/path/to/images/img001.jpg', ...]\n",
    "        image_names = [os.path.basename(p) for p in full_paths]  # 提取 'img001.jpg'\n",
    "    return image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c260c7a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.207649Z",
     "iopub.status.busy": "2025-05-14T16:37:35.207420Z",
     "iopub.status.idle": "2025-05-14T16:37:35.212049Z",
     "shell.execute_reply": "2025-05-14T16:37:35.211422Z"
    },
    "papermill": {
     "duration": 0.012845,
     "end_time": "2025-05-14T16:37:35.213227",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.200382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_into_colmap_cluster(\n",
    "    img_dir,\n",
    "    cluster_path='.featureout/cluster_0',\n",
    "    database_path = '.featureout/cluster_0/colmap.db',\n",
    "    image_names = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Import keypoints and matches into COLMAP database using helper functions.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Directory containing image files\n",
    "        cluster_path (str): Path with matches.h5\n",
    "        database_path (str): Output database location\n",
    "        image_names (list[str]): Optional subset of image names to include\n",
    "    \"\"\"\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    # Add keypoints and images\n",
    "    fname_to_id = add_keypoints(\n",
    "        db=db,\n",
    "        h5_path=cluster_path,\n",
    "        image_path=img_dir,\n",
    "        img_ext='',\n",
    "        camera_model='simple-pinhole',\n",
    "        single_camera=single_camera\n",
    "    )\n",
    "    # Filter fname_to_id to only use the selected subset (if provided)\n",
    "    if image_names is not None:\n",
    "        fname_to_id = {k: v for k, v in fname_to_id.items() if k in image_names}\n",
    "\n",
    "    # Add matches between selected image pairs\n",
    "    add_matches(\n",
    "        db=db,\n",
    "        h5_path=cluster_path,\n",
    "        fname_to_id=fname_to_id\n",
    "    )\n",
    "    db.commit()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b33921ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.227973Z",
     "iopub.status.busy": "2025-05-14T16:37:35.227733Z",
     "iopub.status.idle": "2025-05-14T16:37:35.242716Z",
     "shell.execute_reply": "2025-05-14T16:37:35.242098Z"
    },
    "papermill": {
     "duration": 0.0235,
     "end_time": "2025-05-14T16:37:35.243883",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.220383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Placeholder values for constants if they are not defined globally\n",
    "try:\n",
    "    MIN_MATCHES_FOR_GRAPH_EDGE = MIN_MATCHES_FOR_GRAPH_EDGE\n",
    "    KEYPOINTS_SUBDIR = KEYPOINTS_SUBDIR\n",
    "    DESCRIPTORS_H5 = DESCRIPTORS_H5\n",
    "except NameError:\n",
    "    MIN_MATCHES_FOR_GRAPH_EDGE = 100  # Default value if not defined\n",
    "    KEYPOINTS_SUBDIR = 'keypoints' # Default subdir name\n",
    "    DESCRIPTORS_H5 = 'descriptors.h5' # Default descriptor file name\n",
    "\n",
    "\n",
    "def match_images_global(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    data_dir='.',\n",
    "    device=torch.device('cpu'),\n",
    "    min_matches=MIN_MATCHES_FOR_GRAPH_EDGE,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs LightGlue matching on combined features for all image pairs\n",
    "    in index_pairs, builds a match graph (implicitly), and saves global files\n",
    "    (images.json, keypoints.h5, matches.h5) for the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        img_fnames (list): List of full paths to image files.\n",
    "        index_pairs (list): List of (idx1, idx2) tuples for image pairs to match.\n",
    "        data_dir (str): Base directory where 'features_combined' is located and\n",
    "                        where the global output will be created.\n",
    "        device (torch.device): Device to use for matching.\n",
    "        min_matches (int): Min matches for considering a pair and saving its matches.\n",
    "        verbose (bool): Whether to print detailed match info.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing a single list with the global indices of all images.\n",
    "              This is to maintain a similar structure to the clustering output,\n",
    "              indicating a single group.\n",
    "    \"\"\"\n",
    "    # Define paths based on data_dir and configuration\n",
    "    feature_dir_combined = os.path.join(data_dir, 'features_combined')\n",
    "    keypoints_subdir_path = os.path.join(feature_dir_combined, KEYPOINTS_SUBDIR) # Directory holding combined per-image .pt files\n",
    "    descriptors_h5_path = os.path.join(feature_dir_combined, DESCRIPTORS_H5) # HDF5 holding combined per-image descriptors\n",
    "\n",
    "    # Directory where the global output files will be created\n",
    "    global_output_dir = os.path.join(feature_dir_combined, 'global')\n",
    "    os.makedirs(global_output_dir, exist_ok=True) # Ensure global output dir exists\n",
    "\n",
    "    try:\n",
    "        lg_matcher = KF.LightGlueMatcher(\n",
    "            \"aliked\", {\n",
    "                \"width_confidence\": -1,\n",
    "                \"depth_confidence\": -1,\n",
    "                \"mp\": 'cuda' in str(device)\n",
    "            }\n",
    "        ).eval().to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LightGlue matcher: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Store match indices (relative to combined per-image features)\n",
    "    # This is needed to save matches.h5 later\n",
    "    all_matches = {}\n",
    "\n",
    "    # Open combined descriptors file once\n",
    "    try:\n",
    "        f_descriptors = h5py.File(descriptors_h5_path, mode='r')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening combined descriptors file {descriptors_h5_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    print(\"Performing LightGlue matching on combined features...\")\n",
    "\n",
    "    # Iterate through shortlisted pairs\n",
    "    for idx1, idx2 in tqdm(index_pairs, desc=\"LightGlue Matching\"):\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        key1 = os.path.basename(fname1)\n",
    "        key2 = os.path.basename(fname2)\n",
    "\n",
    "        kp1_pt_path = os.path.join(keypoints_subdir_path, f'{key1}.pt')\n",
    "        kp2_pt_path = os.path.join(keypoints_subdir_path, f'{key2}.pt')\n",
    "\n",
    "        # Check if combined features exist for both images\n",
    "        if not os.path.exists(kp1_pt_path) or key1 not in f_descriptors or \\\n",
    "           not os.path.exists(kp2_pt_path) or key2 not in f_descriptors:\n",
    "            if verbose:\n",
    "                tqdm.write(f\"Skipping {key1}-{key2}: Features not found.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load combined keypoints (original coordinates)\n",
    "            kp1_combined_orig = torch.load(kp1_pt_path, weights_only=False).to(device)\n",
    "            kp2_combined_orig = torch.load(kp2_pt_path, weights_only=False).to(device)\n",
    "\n",
    "            # Load combined descriptors\n",
    "            desc1_combined = torch.from_numpy(f_descriptors[key1]['data'][...]).to(device)\n",
    "            desc2_combined = torch.from_numpy(f_descriptors[key2]['data'][...]).to(device)\n",
    "\n",
    "            if len(kp1_combined_orig) == 0 or len(kp2_combined_orig) == 0 or \\\n",
    "               len(desc1_combined) == 0 or len(desc2_combined) == 0:\n",
    "                if verbose:\n",
    "                    tqdm.write(f\"Skipping {key1}-{key2}: Zero features found.\")\n",
    "                continue\n",
    "\n",
    "            # Create dummy LAFs centered at keypoints (using original coordinates)\n",
    "            # Ensure KPs are float tensors for LAF creation\n",
    "            kp1_tensor = kp1_combined_orig.float()[None] # Add batch dim\n",
    "            kp2_tensor = kp2_combined_orig.float()[None] # Add batch dim\n",
    "            laf1 = KF.laf_from_center_scale_ori(kp1_tensor) # Use batch size 1, scale 1.0\n",
    "            laf2 = KF.laf_from_center_scale_ori(kp2_tensor)\n",
    "\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                # Corrected: Expecting (scores, matches) output\n",
    "                # 'matches' should be a tensor of shape (N, 2) where N is the number of matches\n",
    "                scores, matches = lg_matcher(desc1_combined, desc2_combined, laf1, laf2)\n",
    "\n",
    "            # 'matches' tensor has shape (N, 2), where N is the number of matches\n",
    "            # Each row is [index_in_img1, index_in_img2]\n",
    "            n_matches = len(matches) # Number of rows in the matches tensor is the number of matches\n",
    "\n",
    "            if verbose:\n",
    "                tqdm.write(f'{key1}-{key2}: {n_matches} matches')\n",
    "\n",
    "            # Store matches if enough are found\n",
    "            if n_matches >= min_matches:\n",
    "                 # matches is already the (N, 2) numpy array needed for saving\n",
    "                 # Convert to numpy and int16\n",
    "                 matches_indices = matches.cpu().detach().numpy().astype('int16')\n",
    "\n",
    "                 # Use keys (basenames) for the all_matches dictionary\n",
    "                 all_matches.setdefault(key1, {})[key2] = matches_indices\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during matching combined features for {key1}-{key2}: {e}\")\n",
    "            # Optionally, you can log the traceback for more detailed debugging\n",
    "            # import traceback\n",
    "            # print(traceback.format_exc())\n",
    "\n",
    "\n",
    "    # Close the descriptors file\n",
    "    f_descriptors.close()\n",
    "\n",
    "    print(\"Matching complete. Saving global files...\")\n",
    "\n",
    "    # --- Save Global Files ---\n",
    "\n",
    "    # 1. Save images.json (list of full filenames for all images)\n",
    "    images_json_path = os.path.join(global_output_dir, 'images.json')\n",
    "    try:\n",
    "        with open(images_json_path, 'w') as f_json:\n",
    "            json.dump(img_fnames, f_json, indent=2)\n",
    "        print(f\"Saved global images list to {images_json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {images_json_path}: {e}\")\n",
    "\n",
    "    # 2. Save keypoints.h5 for all images (using combined keypoints per image)\n",
    "    keypoints_h5_path = os.path.join(global_output_dir, 'keypoints.h5')\n",
    "    try:\n",
    "        with h5py.File(keypoints_h5_path, 'w') as f_out_kp:\n",
    "            for img_idx, img_fname in enumerate(img_fnames):\n",
    "                img_key = os.path.basename(img_fname)\n",
    "                kp_pt_path = os.path.join(keypoints_subdir_path, f'{img_key}.pt') # Path to combined KPs for this image\n",
    "                try:\n",
    "                    if os.path.exists(kp_pt_path):\n",
    "                        # Load combined KPs for this image (original coords)\n",
    "                        kp_combined_np = torch.load(kp_pt_path, weights_only=False).cpu().numpy() # Load and convert to numpy\n",
    "                        f_out_kp.create_dataset(img_key, data=kp_combined_np.astype(np.float32))\n",
    "                        if verbose:\n",
    "                            print(f\"Saved combined KPs for {img_key} to global H5\")\n",
    "                    else:\n",
    "                         print(f\"Warning: Combined keypoints not found for {img_key} at {kp_pt_path}. Skipping saving to global H5.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                     print(f\"Error loading/saving combined keypoints for {img_key} to global H5: {e}\")\n",
    "\n",
    "        print(f\"Saved global keypoints to {keypoints_h5_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error creating or saving to {keypoints_h5_path}: {e}\")\n",
    "\n",
    "\n",
    "    # 3. Save matches.h5 for all valid pairs\n",
    "    matches_h5_path = os.path.join(global_output_dir, 'matches.h5')\n",
    "    if all_matches:\n",
    "        try:\n",
    "            with h5py.File(matches_h5_path, 'w') as f_match:\n",
    "                for key1, matches_dict in all_matches.items():\n",
    "                    if matches_dict:\n",
    "                        group = f_match.create_group(key1)\n",
    "                        for key2, match_data in matches_dict.items():\n",
    "                             group.create_dataset(key2, data=match_data, dtype='int16') # match_data is already numpy array\n",
    "            print(f\"Saved global matches to {matches_h5_path}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving {matches_h5_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"No matches found above threshold {min_matches} to save to {matches_h5_path}\")\n",
    "\n",
    "\n",
    "    # Return a list containing a single list of all image indices\n",
    "    all_image_indices = list(range(len(img_fnames)))\n",
    "    return [all_image_indices] # Return as a list of lists to match the original function's output structure\n",
    "\n",
    "# Note: This function assumes that detect_and_combine_features\n",
    "# has already been run and created the combined features in\n",
    "# data_dir/features_combined/keypoints/ and data_dir/features_combined/descriptors.h5.\n",
    "# It saves the global feature and match files into data_dir/features_combined/global/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885942d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.258384Z",
     "iopub.status.busy": "2025-05-14T16:37:35.258166Z",
     "iopub.status.idle": "2025-05-14T16:37:35.266470Z",
     "shell.execute_reply": "2025-05-14T16:37:35.265841Z"
    },
    "papermill": {
     "duration": 0.017068,
     "end_time": "2025-05-14T16:37:35.267686",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.250618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from time import time, sleep\n",
    "import pycolmap\n",
    "import h5py\n",
    "import numpy as np # Ensure numpy is imported if needed by import_into_colmap_cluster\n",
    "\n",
    "# Assume import_into_colmap_cluster is defined elsewhere and can handle\n",
    "# importing keypoints and matches from the .h5 files into a COLMAP database.\n",
    "# It will need access to keypoints.h5 and matches.h5 paths.\n",
    "# from your_colmap_utils import import_into_colmap_cluster\n",
    "\n",
    "\n",
    "def run_colmap_global(\n",
    "    feature_dir,\n",
    "    images_dir,\n",
    "    timings\n",
    "):\n",
    "    \"\"\"\n",
    "    Run COLMAP reconstruction for the entire dataset using global feature and match files.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_dir: Base directory containing 'features_combined/global' folder.\n",
    "    - images_dir: Path to raw image files.\n",
    "    - timings: dict to record durations.\n",
    "    \"\"\"\n",
    "    # Define the path to the global files\n",
    "    global_path = os.path.join(feature_dir, 'features_combined', 'global')\n",
    "\n",
    "    database_path = os.path.join(global_path, 'colmap.db')\n",
    "    image_list_path = os.path.join(global_path, 'images.json')\n",
    "    keypoints_h5_path = os.path.join(global_path, 'keypoints.h5') # Assuming you'll use this\n",
    "    matches_h5_path = os.path.join(global_path, 'matches.h5') # Assuming you'll use this\n",
    "\n",
    "\n",
    "    if not os.path.isfile(image_list_path):\n",
    "        print(\"[Global Reconstruction] Missing images.json in global folder, skipping.\")\n",
    "        return timings, None # Return None for maps if reconstruction is skipped\n",
    "\n",
    "    with open(image_list_path, 'r') as f:\n",
    "        # images.json in the global folder contains full paths, extract basenames\n",
    "        image_names = [os.path.basename(x) for x in json.load(f)]\n",
    "\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "        print(f\"[Global Reconstruction] Removed existing database at {database_path}\")\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    sleep(1)\n",
    "\n",
    "    # Step 1: import keypoints, matches into COLMAP db\n",
    "    # Your import_into_colmap_cluster function needs to be able to read from\n",
    "    # the global keypoints_h5 and matches_h5 paths.\n",
    "    import_into_colmap_cluster(\n",
    "        img_dir=images_dir,\n",
    "        cluster_path=global_path, # Pass the global path where H5 files are\n",
    "        database_path=database_path,\n",
    "        image_names=image_names # Pass the list of image basenames\n",
    "    )\n",
    "\n",
    "    # Step 2: RANSAC (Geometric verification)\n",
    "    # This step is typically run via pycolmap.match_exhaustive or similar after\n",
    "    # keypoints and matches are imported into the database.\n",
    "    # Ensure your import_into_colmap_cluster populates the database such that\n",
    "    # match_exhaustive can be run.\n",
    "    t = time()\n",
    "    # If using pycolmap.match_exhaustive, keypoints and matches should be in the DB\n",
    "    try:\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        t_ransac = time() - t\n",
    "        timings['RANSAC_Global'] = t_ransac # Use a specific key for global timings\n",
    "        print(f'[Global Reconstruction] Ran RANSAC in {t_ransac:.4f} sec')\n",
    "    except Exception as e:\n",
    "        print(f\"[Global Reconstruction] Error running RANSAC: {e}\")\n",
    "        timings['RANSAC_Global'] = -1 # Indicate failure\n",
    "        return timings, None # Skip reconstruction if RANSAC fails\n",
    "\n",
    "\n",
    "    # Step 3: Incremental mapping for the global dataset\n",
    "    output_path = os.path.join(global_path, 'colmap_rec_aliked')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "    # Adjust mapper options as needed for a large global reconstruction\n",
    "    mapper_options.min_model_size = 4 # Minimum number of registered images\n",
    "    mapper_options.max_num_models = 25 # We expect only one main model\n",
    "    mapper_options.mapper.filter_max_reproj_error = 10.0 # Example, adjust as needed\n",
    "\n",
    "    t = time()\n",
    "    try:\n",
    "        # pycolmap.incremental_mapping expects image_path to be the directory\n",
    "        # containing the actual image files.\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path,\n",
    "            image_path=images_dir, # Path to the actual image files\n",
    "            output_path=output_path,\n",
    "            options=mapper_options\n",
    "        )\n",
    "        t_rec = time() - t\n",
    "        timings['Reconstruction_Global'] = t_rec # Use a specific key for global timings\n",
    "        print(f'[Global Reconstruction] Reconstruction done in {t_rec:.4f} sec')\n",
    "        all_maps = [maps] # Return as a list containing the single global map\n",
    "    except Exception as e:\n",
    "        print(f\"[Global Reconstruction] Error during incremental mapping: {e}\")\n",
    "        timings['Reconstruction_Global'] = -1 # Indicate failure\n",
    "        all_maps = [] # Return empty list or None if reconstruction fails\n",
    "\n",
    "    return timings, all_maps\n",
    "\n",
    "# Note: This function now expects the output of match_images_global to be in\n",
    "# feature_dir/features_combined/global/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84845e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.282195Z",
     "iopub.status.busy": "2025-05-14T16:37:35.281957Z",
     "iopub.status.idle": "2025-05-14T16:37:35.446201Z",
     "shell.execute_reply": "2025-05-14T16:37:35.445085Z"
    },
    "papermill": {
     "duration": 0.173809,
     "end_time": "2025-05-14T16:37:35.448273",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.274464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = False\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d8e861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.470919Z",
     "iopub.status.busy": "2025-05-14T16:37:35.470633Z",
     "iopub.status.idle": "2025-05-14T16:37:35.474640Z",
     "shell.execute_reply": "2025-05-14T16:37:35.473938Z"
    },
    "papermill": {
     "duration": 0.01337,
     "end_time": "2025-05-14T16:37:35.475968",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.462598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_cluster_folders(base_dir):\n",
    "    for name in os.listdir(base_dir):\n",
    "        path = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(path) and name.startswith(\"cluster\"):\n",
    "            print(f\"Deleting: {path}\")\n",
    "            shutil.rmtree(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a282b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:37:35.490265Z",
     "iopub.status.busy": "2025-05-14T16:37:35.490015Z",
     "iopub.status.idle": "2025-05-14T16:46:08.912345Z",
     "shell.execute_reply": "2025-05-14T16:46:08.911204Z"
    },
    "papermill": {
     "duration": 513.431625,
     "end_time": "2025-05-14T16:46:08.914210",
     "exception": false,
     "start_time": "2025-05-14T16:37:35.482585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"stairs\" -> Registered 28 / 51 images across 2 clusters\n",
      "\n",
      "Results\n",
      "Dataset \"ETs\" -> Registered 20 / 22 images across 1 clusters\n",
      "Dataset \"amy_gardens\" -> Failed!\n",
      "Dataset \"fbk_vineyard\" -> Failed!\n",
      "Dataset \"imc2023_haiper\" -> Failed!\n",
      "Dataset \"imc2023_heritage\" -> Failed!\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> Failed!\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> Failed!\n",
      "Dataset \"imc2024_lizard_pond\" -> Failed!\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> Failed!\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> Failed!\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\n",
      "Dataset \"pt_stpeters_stpauls\" -> Failed!\n",
      "Dataset \"stairs\" -> Registered 28 / 51 images across 2 clusters\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t# 'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_augmentation\":[],\n",
    "    \"feature_merge\":[],\n",
    "    \"feature_matching\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "print (f\"Extracting on device {device}\")\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "    try:\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.5, # should be strict\n",
    "            min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "    \n",
    "        t = time()\n",
    "        \n",
    "        # 2. Perform initial detection and matching for cropping data\n",
    "        # This step will skip if the crop data file already exists\n",
    "        DATA_DIR = images_dir\n",
    "        FEATURE_DIR = os.path.join(feature_dir, 'features_combined')\n",
    "\n",
    "        crop_data_file = perform_initial_detection_and_matching(images, index_pairs, data_dir = feature_dir, device=DEVICE)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "    \n",
    "        # 3. Calculate crop boxes based on initial match analysis\n",
    "        # This step will skip if the crop info file already exists from a previous run\n",
    "        # However, the logic for skipping is currently inside calculate_crop_boxes itself (mode='w')\n",
    "        # Let's ensure it writes fresh crop_info based on potentially existing crop_data\n",
    "        t = time()\n",
    "        crop_info_file = calculate_crop_boxes(images, crop_data_file, FEATURE_DIR, data_dir = feature_dir)\n",
    "        gc.collect()\n",
    "        timings['feature_augmentation'].append(time() - t)\n",
    "        print(f'Features augmentation in {time() - t:.4f} sec')    \n",
    "    \n",
    "        # 4. Perform multi-variation ALIKED detection, combine features per image, save to .pt/.h5\n",
    "        # This step skips images whose combined features already exist\n",
    "        t = time()\n",
    "        detect_and_combine_features(images, crop_info_file, FEATURE_DIR, device=DEVICE)\n",
    "        gc.collect()\n",
    "        timings['feature_merge'].append(time() - t)\n",
    "        print(f'Features merge in {time() - t:.4f} sec')  \n",
    "\n",
    "        # 5. Load combined features and perform LightGlue matching, save global matches to .pt\n",
    "        # This step runs matching using the combined features generated in step 4\n",
    "        t = time()\n",
    "        match_images_global(images, index_pairs, data_dir = feature_dir, device=DEVICE)\n",
    "        gc.collect()\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "        \n",
    "        \n",
    "        timings, all_maps = run_colmap_global(feature_dir, images_dir, timings)\n",
    "        gc.collect()\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction in {time() - t:.4f} sec') \n",
    "        clear_output(wait=False)\n",
    "\n",
    "        registered = 0\n",
    "        cluster_global_index = 0  # 全局 cluster 索引\n",
    "        for maps in all_maps:  # 每个 maps 是 Dict[int, Reconstruction]\n",
    "            for map_index, cur_map in maps.items():\n",
    "                for _, image in cur_map.images.items():\n",
    "                    prediction_index = filename_to_index[image.name]\n",
    "                    predictions[prediction_index].cluster_index = cluster_global_index\n",
    "                    predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                    predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                    registered += 1\n",
    "                cluster_global_index += 1\n",
    "        \n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images across {cluster_global_index} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "# print('\\nTimings')\n",
    "# for k, v in timings.items():\n",
    "#     print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0790f5",
   "metadata": {
    "papermill": {
     "duration": 0.008128,
     "end_time": "2025-05-14T16:46:08.930725",
     "exception": false,
     "start_time": "2025-05-14T16:46:08.922597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd04104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:46:08.952156Z",
     "iopub.status.busy": "2025-05-14T16:46:08.951785Z",
     "iopub.status.idle": "2025-05-14T16:46:09.180679Z",
     "shell.execute_reply": "2025-05-14T16:46:09.179548Z"
    },
    "papermill": {
     "duration": 0.239805,
     "end_time": "2025-05-14T16:46:09.182228",
     "exception": false,
     "start_time": "2025-05-14T16:46:08.942423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "ETs_another_et_another_et001.png_public,ETs,cluster0,another_et_another_et001.png,0.999690111;-0.001021631;0.024872434;0.000472735;0.999756441;0.022064325;-0.024888918;-0.022045730;0.999447111,-1.765014772;-1.041247651;0.768083085\r\n",
      "ETs_another_et_another_et002.png_public,ETs,cluster0,another_et_another_et002.png,0.999948451;0.001230780;0.010078702;-0.001223758;0.999999004;-0.000702818;-0.010079557;0.000690448;0.999948962,-1.618437641;-0.382149707;-0.428701589\r\n",
      "ETs_another_et_another_et003.png_public,ETs,cluster0,another_et_another_et003.png,0.997413958;-0.038858877;0.060459779;0.042291757;0.997502160;-0.056576038;-0.058110279;0.058986680;0.996565987,-1.874731466;0.932567593;-1.820053624\r\n",
      "ETs_another_et_another_et004.png_public,ETs,cluster0,another_et_another_et004.png,0.998871181;-0.008099628;0.046805555;0.001822864;0.991165529;0.132618144;-0.047466210;-0.132383122;0.990061447,-1.764704373;-0.784491904;-1.550837750\r\n",
      "ETs_another_et_another_et005.png_public,ETs,cluster0,another_et_another_et005.png,0.994342749;0.005679535;0.106067149;-0.012618145;0.997814518;0.064861159;-0.105466960;-0.065832594;0.992241296,-2.265620160;-1.252641582;-0.407050653\r\n",
      "ETs_another_et_another_et006.png_public,ETs,cluster0,another_et_another_et006.png,0.924363274;0.197663862;-0.326315085;-0.222475470;0.974111204;-0.040150063;0.309930964;0.109710345;0.944407983,0.579344674;-0.112277584;-0.217991590\r\n",
      "ETs_another_et_another_et007.png_public,ETs,cluster0,another_et_another_et007.png,0.787947189;0.269088577;-0.553832615;-0.315117031;0.948967190;0.012748711;0.528999512;0.164476778;0.832530423,2.232848632;-0.007607914;-0.286584811\r\n",
      "ETs_another_et_another_et008.png_public,ETs,cluster0,another_et_another_et008.png,0.578325460;0.317189485;-0.751618582;-0.399546565;0.913385969;0.078029570;0.711268026;0.255180135;0.654966330,3.781557229;-0.424131022;0.723342075\r\n",
      "ETs_another_et_another_et009.png_public,ETs,cluster0,another_et_another_et009.png,0.332910458;0.352647726;-0.874534281;-0.497154333;0.853705876;0.154996279;0.801254139;0.383178625;0.459527960,5.163862937;-0.874899735;1.621458694\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2ea7939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:46:09.197581Z",
     "iopub.status.busy": "2025-05-14T16:46:09.197285Z",
     "iopub.status.idle": "2025-05-14T16:46:09.202123Z",
     "shell.execute_reply": "2025-05-14T16:46:09.201108Z"
    },
    "papermill": {
     "duration": 0.014016,
     "end_time": "2025-05-14T16:46:09.203577",
     "exception": false,
     "start_time": "2025-05-14T16:46:09.189561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9a142",
   "metadata": {
    "papermill": {
     "duration": 0.006766,
     "end_time": "2025-05-14T16:46:09.217372",
     "exception": false,
     "start_time": "2025-05-14T16:46:09.210606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11217117,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 550.052196,
   "end_time": "2025-05-14T16:46:12.772480",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-14T16:37:02.720284",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
