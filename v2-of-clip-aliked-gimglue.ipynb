{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":2998831,"sourceType":"datasetVersion","datasetId":1837209},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11924468,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":12051889,"sourceType":"datasetVersion","datasetId":7584231},{"sourceId":12054266,"sourceType":"datasetVersion","datasetId":7347439},{"sourceId":12054703,"sourceType":"datasetVersion","datasetId":7583742},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n#### This notebook is a Kaggle-run adaptation of the **8th place solution** from the Image Matching Challenge 2025.\n\n#### The original method and code were developed by **yangyefd**.\n\nOriginal link：https://www.kaggle.com/competitions/image-matching-challenge-2025/discussion/582844\n\nI ported the implementation from GitHub to Kaggle for easier reproducibility and experimentation.Minor adjustments were made for compatibility with the Kaggle environment.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# IMPORTANT \n#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:20.358940Z","iopub.execute_input":"2025-06-04T06:59:20.359312Z","iopub.status.idle":"2025-06-04T06:59:22.259117Z","shell.execute_reply.started":"2025-06-04T06:59:20.359283Z","shell.execute_reply":"2025-06-04T06:59:22.258110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\nfrom transformers import CLIPProcessor, CLIPModel\n\n# IMPORTANT Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:22.260482Z","iopub.execute_input":"2025-06-04T06:59:22.260763Z","iopub.status.idle":"2025-06-04T06:59:29.187233Z","shell.execute_reply.started":"2025-06-04T06:59:22.260738Z","shell.execute_reply":"2025-06-04T06:59:29.186553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q /kaggle/input/loguru-lib-ds/loguru-0.5.3-py3-none-any.whl\n!pip install -q /kaggle/input/imc2025/yacs-0.1.8-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:29.188569Z","iopub.execute_input":"2025-06-04T06:59:29.189126Z","iopub.status.idle":"2025-06-04T06:59:36.173236Z","shell.execute_reply.started":"2025-06-04T06:59:29.189102Z","shell.execute_reply":"2025-06-04T06:59:36.172281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q /kaggle/input/imc2025/ftfy-6.3.1-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:36.175200Z","iopub.execute_input":"2025-06-04T06:59:36.175524Z","iopub.status.idle":"2025-06-04T06:59:39.594515Z","shell.execute_reply.started":"2025-06-04T06:59:36.175494Z","shell.execute_reply":"2025-06-04T06:59:39.593259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sys.path.append('/kaggle/input/gim-lightglue')\nsys.path.append('/kaggle/input/imc2025-dependences')\nfrom data_process.db import *\nfrom GIMlightglue_match import Lightglue_Matcher","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:39.595762Z","iopub.execute_input":"2025-06-04T06:59:39.596127Z","iopub.status.idle":"2025-06-04T06:59:40.370260Z","shell.execute_reply.started":"2025-06-04T06:59:39.596086Z","shell.execute_reply":"2025-06-04T06:59:40.369589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom CLIP.clip import clip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.371185Z","iopub.execute_input":"2025-06-04T06:59:40.371833Z","iopub.status.idle":"2025-06-04T06:59:40.567333Z","shell.execute_reply.started":"2025-06-04T06:59:40.371801Z","shell.execute_reply":"2025-06-04T06:59:40.566474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom typing import Optional, Union\nfrom kornia.feature.laf import get_laf_center, get_laf_orientation, get_laf_scale\nimport pickle\n# from CLIP.clip import clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.568106Z","iopub.execute_input":"2025-06-04T06:59:40.568322Z","iopub.status.idle":"2025-06-04T06:59:40.633118Z","shell.execute_reply.started":"2025-06-04T06:59:40.568304Z","shell.execute_reply":"2025-06-04T06:59:40.632234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 假设 imc2025-dependences 是您添加的数据集名称\nsource_model_path = os.path.join(\"/kaggle/input/imc2025-dependences\", \"models\", \"gim_lightglue_100h.ckpt\")\ndestination_model_path = os.path.join(\"models\", \"gim_lightglue_100h.ckpt\") # 相对路径，会复制到 /kaggle/working/models/\n\n# 创建目标文件夹如果不存在\nos.makedirs(\"models\", exist_ok=True)\n\n# 复制文件\n!cp {source_model_path} {destination_model_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.634082Z","iopub.execute_input":"2025-06-04T06:59:40.634389Z","iopub.status.idle":"2025-06-04T06:59:40.893927Z","shell.execute_reply.started":"2025-06-04T06:59:40.634359Z","shell.execute_reply":"2025-06-04T06:59:40.892864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    import random\n    import numpy as np\n    import torch\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# 在主程序开始时调用\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.897118Z","iopub.execute_input":"2025-06-04T06:59:40.897368Z","iopub.status.idle":"2025-06-04T06:59:40.906285Z","shell.execute_reply.started":"2025-06-04T06:59:40.897346Z","shell.execute_reply":"2025-06-04T06:59:40.905504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.907890Z","iopub.execute_input":"2025-06-04T06:59:40.908163Z","iopub.status.idle":"2025-06-04T06:59:40.922400Z","shell.execute_reply.started":"2025-06-04T06:59:40.908130Z","shell.execute_reply":"2025-06-04T06:59:40.921592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\ndef get_global_desc(fnames, device=torch.device('cpu')):\n    processor = AutoImageProcessor.from_pretrained('models/dinov2-pytorch-base-v1')\n    model = AutoModel.from_pretrained('models/dinov2-pytorch-base-v1')\n    model = model.eval().to(device)\n    global_descs_dinov2 = []\n    for i, img_fname_full in tqdm(enumerate(fnames), total=len(fnames)):\n        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n        timg = load_torch_image(img_fname_full)\n        with torch.inference_mode():\n            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs)\n            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n        global_descs_dinov2.append(dino_mac.detach().cpu())\n    return torch.cat(global_descs_dinov2, dim=0)\n\ndef get_global_desc_clip(fnames, device=torch.device('cpu')):\n    try:\n        model, preprocess = clip.load(\"/kaggle/input/imc2025-dependences/models/ViT-B-32.pt\", device=device)\n        \n        print(\"分簇模型加载成功\")\n    except Exception as e:\n        print(f\"模型加载失败: {e}\")\n        # 返回一个空的特征向量或默认特征向量\n        return torch.zeros((len(fnames), 512)).float()\n\n    model = model.eval().to(device)\n    global_descs_dinov2 = []\n    \n    # 记录处理失败的图像索引\n    failed_indices = []\n    \n    for i, img_fname_full in tqdm(enumerate(fnames), total=len(fnames)):\n        try:\n            key = os.path.splitext(os.path.basename(img_fname_full))[0]\n            # 加载并预处理图像\n            timg = preprocess(Image.open(img_fname_full)).unsqueeze(0).to(device)\n\n            # 提取特征\n            with torch.no_grad():\n                features = model.encode_image(timg)\n            # 归一化特征\n            features = features / features.norm(dim=-1, keepdim=True)\n\n            global_descs_dinov2.append(features.detach().cpu())\n            # print(f\"image {i} done\")\n        except Exception as e:\n            print(f\"处理图像 {img_fname_full} 失败: {e}\")\n            # 记录失败的索引，并使用零向量替代\n            failed_indices.append(i)\n            # 创建一个零向量作为占位符\n            zero_feat = torch.zeros((1, 512), device='cpu').float()\n            global_descs_dinov2.append(zero_feat)\n    \n    # 输出失败图像的数量\n    if failed_indices:\n        print(f\"警告: {len(failed_indices)}张图像处理失败\")\n\n    return torch.cat(global_descs_dinov2, dim=0).float()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.923201Z","iopub.execute_input":"2025-06-04T06:59:40.923423Z","iopub.status.idle":"2025-06-04T06:59:40.934409Z","shell.execute_reply.started":"2025-06-04T06:59:40.923405Z","shell.execute_reply":"2025-06-04T06:59:40.933625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\ndef get_image_pairs_shortlist(fnames, sim_th=0.6, min_pairs=20, exhaustive_if_less=20, \n                            device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc(fnames, device=device)\n\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n    mask = dm <= sim_th\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < 1000:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n    return sorted(list(set(matching_list)))\n\ndef get_image_pairs_shortlist_clip(fnames, sim_th=0.6, min_pairs=20, exhaustive_if_less=20, \n                            device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc_clip(fnames, device=device)   \n    # print(\"descs done!!!!\")\n    # 计算余弦相似度矩阵 (N x N)\n    similarity = torch.mm(descs, descs.t()).detach().cpu().numpy()\n    \n    # 相似度大于阈值的保留\n    mask = similarity >= sim_th\n    \n    matching_list = []\n    ar = np.arange(num_imgs)\n    \n    for st_idx in range(num_imgs-1):\n        # 找出与当前图像相似度大于阈值的所有图像\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        # 如果符合条件的图像太少，选择相似度最高的前min_pairs个\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(similarity[st_idx])[::-1][:min_pairs+1]  # 降序排列并取前min_pairs+1个\n        for idx in to_match:\n            if st_idx == idx:  # 跳过自己与自己的匹配\n                continue\n            # 添加匹配对\n            matching_list.append(tuple(sorted((st_idx, idx.item() if hasattr(idx, 'item') else idx))))\n    \n    # 去重并排序\n    return sorted(list(set(matching_list)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.935233Z","iopub.execute_input":"2025-06-04T06:59:40.935437Z","iopub.status.idle":"2025-06-04T06:59:40.950949Z","shell.execute_reply.started":"2025-06-04T06:59:40.935419Z","shell.execute_reply":"2025-06-04T06:59:40.950359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_aliked(img_fnames, feature_dir='.featureout', num_features=4096, \n                 resize_to=1024, device=torch.device('cpu')):\n    dtype = torch.float32\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.3, \n                     resize=resize_to).eval().to(device, dtype)\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            key = key.split('\\\\')[-1]\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = extractor.extract(image0)\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.951785Z","iopub.execute_input":"2025-06-04T06:59:40.952084Z","iopub.status.idle":"2025-06-04T06:59:40.971378Z","shell.execute_reply.started":"2025-06-04T06:59:40.952055Z","shell.execute_reply":"2025-06-04T06:59:40.970687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_matches(img1_path, img2_path, kpts1, kpts2, matches, save_path=None, show=False):\n    \"\"\"可视化两张图片的匹配结果\n    \n    Args:\n        img1_path: 第一张图片路径\n        img2_path: 第二张图片路径  \n        kpts1: 第一张图片的特征点 (Nx2)\n        kpts2: 第二张图片的特征点 (Nx2)\n        matches: 匹配索引 (Mx2)\n        save_path: 保存路径,如果为None则显示\n        show: 是否显示结果\n    \"\"\"\n    # 读取图片\n    img1 = cv2.imread(img1_path)\n    img2 = cv2.imread(img2_path)\n    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n    \n    # 获取原始尺寸\n    h1, w1 = img1.shape[:2]\n    h2, w2 = img2.shape[:2]\n    \n    # 调整图片大小使其具有相同高度\n    height = min(h1, h2, 800)  # 限制最大高度为800像素\n    \n    # 计算缩放比例\n    scale1 = height / h1\n    scale2 = height / h2\n    \n    # 调整图像大小\n    img1 = cv2.resize(img1, (int(w1 * scale1), height))\n    img2 = cv2.resize(img2, (int(w2 * scale2), height))\n    \n    # 创建拼接图\n    vis = np.hstack([img1, img2])\n    \n    # 复制关键点并按比例缩放\n    kpts1_scaled = kpts1.copy()\n    kpts2_scaled = kpts2.copy()\n    \n    # 应用缩放比例\n    kpts1_scaled[:, 0] *= scale1\n    kpts1_scaled[:, 1] *= scale1\n    kpts2_scaled[:, 0] *= scale2\n    kpts2_scaled[:, 1] *= scale2\n    \n    # 绘制匹配线\n    offset = img1.shape[1]\n    for idx1, idx2 in matches:\n        pt1 = tuple(map(int, kpts1_scaled[idx1]))\n        pt2 = tuple(map(int, kpts2_scaled[idx2]))\n        pt2 = (pt2[0] + offset, pt2[1])\n        cv2.circle(vis, pt1, 2, (0, 255, 0), -1)\n        cv2.circle(vis, pt2, 2, (0, 255, 0), -1)\n        cv2.line(vis, pt1, pt2, (255, 0, 0), 1)\n    \n    # 添加匹配数量文本\n    cv2.putText(vis, f\"Matches: {len(matches)}\", (10, 30), \n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n    cv2.imwrite(save_path, cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.972075Z","iopub.execute_input":"2025-06-04T06:59:40.972278Z","iopub.status.idle":"2025-06-04T06:59:40.987891Z","shell.execute_reply.started":"2025-06-04T06:59:40.972249Z","shell.execute_reply":"2025-06-04T06:59:40.987157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_person(lightglue_matcher, img_fnames, feature_dir='.featureout', device=torch.device('cpu')):\n    #集成方法 ALIke sp各提一半点 2048个\n    dtype = torch.float32\n\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n\n    mask_lst = []\n    mask_dict = {}\n    for img_path in tqdm(img_fnames):\n        img_fname = img_path.split('/')[-1]\n        img_fname = img_fname.split('\\\\')[-1]\n        key = img_fname\n        with torch.inference_mode():\n            mask, mask_ratio, mask_num = lightglue_matcher.get_person_mask(img_path)\n            \n            if mask is not None:\n                mask_lst.append([key, mask, mask_ratio, mask_num])\n                mask_dict[key] = mask\n            else:\n                mask_dict[key] = np.zeros((0, 0), dtype=np.bool_)\n    if len(mask_lst) > 0:\n        mask_ratio_sum = 0\n        mask_num_sum = 0\n        for _, _, mask_ratio, mask_num in mask_lst:\n            mask_ratio_sum += mask_ratio\n            mask_num_sum += mask_num\n        mask_ratio_mean = mask_ratio_sum / len(mask_lst)\n        mask_num_mean = mask_num_sum / len(mask_lst)\n\n        if (mask_ratio_mean > 0.15 and abs(mask_num_mean - 1) < 0.5) or len(mask_lst) < 3:\n            mask_lst = []\n    with h5py.File(f'{feature_dir}/p_mask.h5', mode='w') as f_pmask:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            img_fname = img_fname.split('\\\\')[-1]\n            key = img_fname\n\n            f_pmask[key] = mask_dict[key] if len(mask_lst) > 0 else np.zeros((0, 0), dtype=np.bool_)\n            # print(f_pmask[key])\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:40.988641Z","iopub.execute_input":"2025-06-04T06:59:40.988855Z","iopub.status.idle":"2025-06-04T06:59:41.005797Z","shell.execute_reply.started":"2025-06-04T06:59:40.988836Z","shell.execute_reply":"2025-06-04T06:59:41.005058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_sp_ensemble(lightglue_matcher, img_fnames, feature_dir='.featureout', num_features=4096, \n                 resize_to=1024, device=torch.device('cpu')):\n    #集成方法 ALIke sp各提一半点 2048个\n    dtype = torch.float32\n\n    extractor_alike = ALIKED(max_num_keypoints=num_features, detection_threshold=0.01, \n                    resize=resize_to).eval().to(device, dtype)\n    \n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc, \\\n         h5py.File(f'{feature_dir}/size.h5', mode='w') as f_size,\\\n         h5py.File(f'{feature_dir}/scale.h5', mode='w') as f_scale,\\\n         h5py.File(f'{feature_dir}/mask.h5', mode='w') as f_mask:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            img_fname = img_fname.split('\\\\')[-1]\n            key = img_fname\n            # print(f\"img_path:{img_path}, key:{key}\")\n            with torch.inference_mode():\n                try:\n                # if 1:\n                    kpts = np.zeros((num_features*2,2)).astype(np.float32)\n                    # kpts_refine = np.zeros((num_features*2,2)).astype(np.float32)\n                    descs = np.zeros((num_features*2,256)).astype(np.float32)\n                    feats0, data = lightglue_matcher.extract(img_path,nms_radius=2,force=True)\n                    feats0_kpts = feats0['keypoints0'].reshape(-1, 2).detach().cpu().numpy()\n                    kpts[:len(feats0_kpts)] = feats0['keypoints0'].reshape(-1, 2).detach().cpu().numpy()\n                    # kpts_refine[:len(feats0_kpts)] = feats0['keypoints_refine0'].reshape(-1, 2).detach().cpu().numpy()\n                    descs[:len(feats0_kpts)] = feats0['descriptors0'].reshape(len(feats0_kpts), -1).detach().cpu().numpy()\n\n                    image0 = load_torch_image(img_path, device=device).to(dtype)\n                    feats0_alike = extractor_alike.extract(image0)\n                    feats0_alike_pkts = feats0_alike['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                    feats0_alike_descs = feats0_alike['descriptors'].reshape(len(feats0_alike_pkts), -1).detach().cpu().numpy()\n                    #使用分数对点和描述的位置进行排序，分高的放前\n                    feats0_alike_pkts_score = feats0_alike['keypoint_scores'].reshape(-1).detach().cpu().numpy()\n                    sort_idx = np.argsort(feats0_alike_pkts_score)[::-1]\n                    kpts[num_features:num_features+len(feats0_alike_pkts)] = feats0_alike_pkts[sort_idx]\n                    descs[num_features:num_features+len(feats0_alike_pkts),:128] = feats0_alike_descs[sort_idx]\n                    descs[num_features:num_features+len(feats0_alike_pkts),128:] = feats0_alike_descs[sort_idx]\n                    \n                    f_kp[key] = kpts\n                    f_desc[key] = descs\n                    f_size[key] = data['size0'].cpu()\n                    f_scale[key] = data['scale0'].cpu()\n                    f_mask[key] = np.array([len(feats0_kpts), len(feats0_alike_pkts)])\n                except Exception as e:\n                    print(f\"Error processing image {img_fname}: {e}\") \n                    # 处理异常情况，例如记录错误或跳过该图像\n                    # 可以选择继续处理下一个图像，或者根据需要进行其他操作\n                    continue\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.006597Z","iopub.execute_input":"2025-06-04T06:59:41.006813Z","iopub.status.idle":"2025-06-04T06:59:41.024486Z","shell.execute_reply.started":"2025-06-04T06:59:41.006793Z","shell.execute_reply":"2025-06-04T06:59:41.023803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def match_with_gimlightglue(lightglue_matcher, img_fnames, index_pairs, feature_dir='.featureout', \n                        device=torch.device('cpu'), min_matches=15, verbose=True, visualize=True):\n    match_matrix = np.zeros((len(img_fnames), len(img_fnames)), dtype=np.int32)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/size.h5', mode='r') as f_size, \\\n        h5py.File(f'{feature_dir}/scale.h5', mode='r') as f_scale, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            key1 = key1.split('\\\\')[-1]\n            key2 = key2.split('\\\\')[-1]\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n            pred = {}\n            pred['keypoints0'] = kp1[None]\n            pred['keypoints1'] = kp2[None]\n            pred['descriptors0'] = desc1[None]\n            pred['descriptors1'] = desc2[None]\n            pred['size0'] = torch.from_numpy(f_size[key1][...]).to(device)\n            pred['size1'] = torch.from_numpy(f_size[key2][...]).to(device)\n            pred['scale0'] = torch.from_numpy(f_scale[key1][...]).to(device)\n            pred['scale1'] = torch.from_numpy(f_scale[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, idxs = lightglue_matcher.match(pred)\n            if len(idxs) == 0:\n                continue\n                \n            #  # 应用区域筛选方法\n            # filtered_idxs = adaptive_match_filtering(\n            #     lightglue_matcher, kp1, kp2, idxs.cpu().numpy(), fname1, fname2, device\n            # )\n            # # 转回tensor\n            # if isinstance(filtered_idxs, np.ndarray):\n            #     idxs = torch.from_numpy(filtered_idxs).to(idxs.device)\n\n            n_matches = len(idxs)\n            if verbose:\n                print(f'{key1}-{key2}: {n_matches} matches')\n            group = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n                match_matrix[idx1,idx2] = len(idxs.detach().cpu().numpy().reshape(-1, 2))\n                                # 添加可视化\n                # if visualize:\n                #     vis_dir = os.path.join(feature_dir, 'visualizations')\n                #     os.makedirs(vis_dir, exist_ok=True)\n                #     save_path = os.path.join(vis_dir, f'{key1}_{key2}_matches.png')\n                #     visualize_matches(fname1, fname2, \n                #                    kp1.cpu().numpy(), \n                #                    kp2.cpu().numpy(),\n                #                    idxs.cpu().numpy(),\n                #                    save_path)\n    return match_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.025318Z","iopub.execute_input":"2025-06-04T06:59:41.025535Z","iopub.status.idle":"2025-06-04T06:59:41.045845Z","shell.execute_reply.started":"2025-06-04T06:59:41.025517Z","shell.execute_reply":"2025-06-04T06:59:41.045222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_duplicate_matches(idxs, idxs_alike_adjusted, kp1, kp2, duplicate_threshold=3.0):\n    \"\"\"\n    Filter out duplicate ALIKE matches that are too close to SuperPoint matches.\n    \n    Args:\n        idxs: Tensor of shape (N, 2) containing SuperPoint match indices.\n        idxs_alike_adjusted: Tensor of shape (M, 2) containing ALIKE match indices.\n        kp1: Tensor of shape (K, 2) containing keypoints for image 1.\n        kp2: Tensor of shape (K, 2) containing keypoints for image 2.\n        duplicate_threshold: Float, pixel distance threshold for duplicates.\n    \n    Returns:\n        Tensor of combined non-duplicate matches.\n    \"\"\"\n    # Get coordinates for SuperPoint matches\n    sp_coords1 = kp1[idxs[:, 0]]  # Shape: (N, 2)\n    sp_coords2 = kp2[idxs[:, 1]]  # Shape: (N, 2)\n    \n    # Get coordinates for ALIKE matches\n    alike_coords1 = kp1[idxs_alike_adjusted[:, 0]]  # Shape: (M, 2)\n    alike_coords2 = kp2[idxs_alike_adjusted[:, 1]]  # Shape: (M, 2)\n    \n    # Compute pairwise distances using broadcasting\n    # dist1: Distance between ALIKE points in image 1 and SuperPoint points in image 1\n    dist1 = torch.cdist(alike_coords1, sp_coords1, p=2)  # Shape: (M, N)\n    dist2 = torch.cdist(alike_coords2, sp_coords2, p=2)  # Shape: (M, N)\n    \n    # Check for duplicates: both distances must be below threshold\n    duplicate_mask = (dist1 < duplicate_threshold) & (dist2 < duplicate_threshold)\n    valid_mask = ~torch.any(duplicate_mask, dim=1)  # Shape: (M,)\n    \n    # Filter non-duplicate ALIKE matches\n    filtered_alike_matches = idxs_alike_adjusted[valid_mask]\n    \n    # Combine matches\n    combined_matches = torch.cat([idxs, filtered_alike_matches], dim=0)\n    \n    return combined_matches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.046669Z","iopub.execute_input":"2025-06-04T06:59:41.046949Z","iopub.status.idle":"2025-06-04T06:59:41.063890Z","shell.execute_reply.started":"2025-06-04T06:59:41.046922Z","shell.execute_reply":"2025-06-04T06:59:41.063064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_clusters(img1_path, img2_path, mkpts1, mkpts2, labels1, labels2, \n                      cluster_centers1, cluster_centers2, cluster_radii1, cluster_radii2,\n                      save_path=None, all_kp1=None, all_kp2=None):\n    \"\"\"\n    可视化两张图片的聚类结果，包括聚类中心、聚类区域的圆圈和所有特征点\n    \n    Args:\n        img1_path: 第一张图片路径\n        img2_path: 第二张图片路径\n        mkpts1: 第一张图片的特征点 (Nx2)\n        mkpts2: 第二张图片的特征点 (Nx2)\n        labels1: 第一张图片特征点的聚类标签 (N)\n        labels2: 第二张图片特征点的聚类标签 (N)\n        cluster_centers1: 第一张图片的聚类中心 [(x1,y1), (x2,y2), ...]\n        cluster_centers2: 第二张图片的聚类中心 [(x1,y1), (x2,y2), ...]\n        cluster_radii1: 第一张图片的聚类半径 [r1, r2, ...]\n        cluster_radii2: 第二张图片的聚类半径 [r1, r2, ...]\n        save_path: 保存路径，如果为None则显示\n        all_kp1: 第一张图片的所有特征点 (Mx2)，可以为None\n        all_kp2: 第二张图片的所有特征点 (Mx2)，可以为None\n    \"\"\"\n    # 读取图片\n    img1 = cv2.imread(img1_path)\n    img2 = cv2.imread(img2_path)\n    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n    \n    # 获取原始尺寸\n    h1, w1 = img1.shape[:2]\n    h2, w2 = img2.shape[:2]\n    \n    # 调整图片大小使其具有相同高度\n    height = min(h1, h2, 800)  # 限制最大高度为800像素\n    \n    # 计算缩放比例\n    scale1 = height / h1\n    scale2 = height / h2\n    \n    # 调整图像大小\n    img1 = cv2.resize(img1, (int(w1 * scale1), height))\n    img2 = cv2.resize(img2, (int(w2 * scale2), height))\n    \n    # 创建拼接图\n    vis = np.hstack([img1, img2])\n    \n    # 偏移量\n    offset = img1.shape[1]\n    \n    # 如果提供了所有特征点，则先绘制它们（作为背景）\n    if all_kp1 is not None:\n        # 确保是numpy数组\n        if isinstance(all_kp1, torch.Tensor):\n            all_kp1 = all_kp1.cpu().numpy()\n        \n        # 缩放所有特征点\n        all_kp1_scaled = all_kp1.copy()\n        all_kp1_scaled[:, 0] *= scale1\n        all_kp1_scaled[:, 1] *= scale1\n        \n        # 绘制所有特征点（淡灰色小点）\n        for pt in all_kp1_scaled:\n            pt = tuple(map(int, pt))\n            cv2.circle(vis, pt, 1, (80, 80, 80), -1)\n    \n    if all_kp2 is not None:\n        # 确保是numpy数组\n        if isinstance(all_kp2, torch.Tensor):\n            all_kp2 = all_kp2.cpu().numpy()\n        \n        # 缩放所有特征点\n        all_kp2_scaled = all_kp2.copy()\n        all_kp2_scaled[:, 0] *= scale2\n        all_kp2_scaled[:, 1] *= scale2\n        \n        # 绘制所有特征点（淡灰色小点）\n        for pt in all_kp2_scaled:\n            pt = tuple(map(int, (pt[0] + offset / scale2 * scale1, pt[1])))\n            cv2.circle(vis, pt, 1, (80, 80, 80), -1)\n    \n    # 复制关键点并按比例缩放\n    mkpts1_scaled = mkpts1.copy()\n    mkpts2_scaled = mkpts2.copy()\n    \n    # 应用缩放比例\n    mkpts1_scaled[:, 0] *= scale1\n    mkpts1_scaled[:, 1] *= scale1\n    mkpts2_scaled[:, 0] *= scale2\n    mkpts2_scaled[:, 1] *= scale2\n    \n    # 所有聚类的颜色映射\n    unique_labels1 = np.unique(labels1[labels1 >= 0])\n    unique_labels2 = np.unique(labels2[labels2 >= 0])\n    num_clusters = max(len(unique_labels1), len(unique_labels2), 1)  # 至少有一种颜色\n    \n    # 生成随机颜色，但确保对比度足够\n    colors = []\n    for i in range(num_clusters):\n        # 生成HSV颜色以确保彩色和亮度多样性\n        h = int(i * 180 / num_clusters) % 180  # 色调均匀分布\n        s = 200 + np.random.randint(55)  # 高饱和度\n        v = 200 + np.random.randint(55)  # 适中亮度\n        bgr_color = cv2.cvtColor(np.uint8([[[h, s, v]]]), cv2.COLOR_HSV2RGB)[0][0]\n        colors.append((int(bgr_color[0]), int(bgr_color[1]), int(bgr_color[2])))\n    \n    # 绘制聚类区域（半透明填充区域）\n    for i, (center, radius) in enumerate(zip(cluster_centers1, cluster_radii1)):\n        center = (int(center[0] * scale1), int(center[1] * scale1))\n        radius = int(radius * scale1)\n        color_idx = i % len(colors)\n        \n        # 创建一个透明图层\n        overlay = vis.copy()\n        cv2.circle(overlay, center, radius, colors[color_idx], -1)  # 填充圆\n        # 添加透明效果\n        alpha = 0.2  # 透明度\n        cv2.addWeighted(overlay, alpha, vis, 1 - alpha, 0, vis)\n    \n    # 第二张图片的聚类区域\n    for i, (center, radius) in enumerate(zip(cluster_centers2, cluster_radii2)):\n        center = (int(center[0] * scale2) + offset, int(center[1] * scale2))\n        radius = int(radius * scale2)\n        color_idx = i % len(colors)\n        \n        # 创建一个透明图层\n        overlay = vis.copy()\n        cv2.circle(overlay, center, radius, colors[color_idx], -1)  # 填充圆\n        # 添加透明效果\n        alpha = 0.2  # 透明度\n        cv2.addWeighted(overlay, alpha, vis, 1 - alpha, 0, vis)\n    \n    # 绘制特征点和聚类关系\n    for i, (pt1, pt2, l1, l2) in enumerate(zip(mkpts1_scaled, mkpts2_scaled, labels1, labels2)):\n        pt1 = tuple(map(int, pt1))\n        pt2 = tuple(map(int, pt2))\n        pt2 = (pt2[0] + offset, pt2[1])\n        \n        # 对聚类中的点使用聚类颜色\n        if l1 >= 0:\n            color_idx = np.where(unique_labels1 == l1)[0][0] % len(colors)\n            color = colors[color_idx]\n            cv2.circle(vis, pt1, 3, color, -1)\n        else:\n            # 噪声点为灰色\n            cv2.circle(vis, pt1, 2, (128, 128, 128), -1)\n            \n        if l2 >= 0:\n            color_idx = np.where(unique_labels2 == l2)[0][0] % len(colors)\n            color = colors[color_idx]\n            cv2.circle(vis, pt2, 3, color, -1)\n        else:\n            # 噪声点为灰色\n            cv2.circle(vis, pt2, 2, (128, 128, 128), -1)\n        \n        # 如果两点都属于聚类，绘制连线\n        if l1 >= 0 and l2 >= 0:\n            # 找出l1和l2对应的索引\n            color_idx1 = np.where(unique_labels1 == l1)[0][0] % len(colors)\n            color_idx2 = np.where(unique_labels2 == l2)[0][0] % len(colors)\n            \n            # 使用混合颜色\n            if color_idx1 == color_idx2:\n                line_color = colors[color_idx1]\n            else:\n                # 使用灰色作为不匹配聚类的连线\n                line_color = (200, 200, 200)\n                \n            cv2.line(vis, pt1, pt2, line_color, 1)\n    \n    # 绘制聚类中心和圆圈边界\n    for i, (center, radius) in enumerate(zip(cluster_centers1, cluster_radii1)):\n        center = (int(center[0] * scale1), int(center[1] * scale1))\n        radius = int(radius * scale1)\n        color_idx = i % len(colors)\n        cv2.circle(vis, center, 6, colors[color_idx], -1)  # 聚类中心\n        cv2.circle(vis, center, radius, colors[color_idx], 2)  # 聚类区域边界\n        \n        # 添加聚类编号\n        cv2.putText(vis, f\"{i}\", (center[0] + 10, center[1]), \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, colors[color_idx], 2)\n    \n    # 第二张图片的聚类中心和圆圈边界\n    for i, (center, radius) in enumerate(zip(cluster_centers2, cluster_radii2)):\n        center = (int(center[0] * scale2) + offset, int(center[1] * scale2))\n        radius = int(radius * scale2)\n        color_idx = i % len(colors)\n        cv2.circle(vis, center, 6, colors[color_idx], -1)  # 聚类中心\n        cv2.circle(vis, center, radius, colors[color_idx], 2)  # 聚类区域边界\n        \n        # 添加聚类编号\n        cv2.putText(vis, f\"{i}\", (center[0] + 10, center[1]), \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, colors[color_idx], 2)\n    \n    # 添加说明文本\n    cv2.putText(vis, f\"Clusters img1: {len(unique_labels1)}\", (10, 30), \n                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n    cv2.putText(vis, f\"Clusters img2: {len(unique_labels2)}\", (offset + 10, 30), \n                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n    \n    # 添加灰色噪声点说明\n    cv2.putText(vis, \"Gray: Noise points\", (10, 70), \n                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)\n    \n    # 添加深灰色背景点说明\n    if all_kp1 is not None or all_kp2 is not None:\n        cv2.putText(vis, \"Dark gray: All features\", (10, 100), \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (80, 80, 80), 2)\n\n    # 保存图像\n    cv2.imwrite(save_path, cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n    \n    return vis\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.065073Z","iopub.execute_input":"2025-06-04T06:59:41.065271Z","iopub.status.idle":"2025-06-04T06:59:41.088114Z","shell.execute_reply.started":"2025-06-04T06:59:41.065254Z","shell.execute_reply":"2025-06-04T06:59:41.087390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_clusters_by_match_count(idxs, features_data, key1, key2, cluster_centers1, cluster_centers2, \n                                    cluster_radius1, cluster_radius2, min_matches_per_cluster=5):\n    \"\"\"\n    根据聚类中心和半径，计算每个匹配对所属簇，并过滤掉匹配对数量不足阈值的簇\n    \"\"\"\n    if isinstance(idxs, torch.Tensor):\n        idxs_np = idxs.cpu().numpy()\n    else:\n        idxs_np = idxs\n    \n    # 没有聚类时直接返回原始匹配\n    if len(cluster_centers1) == 0 or len(cluster_centers2) == 0:\n        return idxs\n    \n    # 获取匹配对坐标\n    kp1 = features_data[key1]['kp']\n    kp2 = features_data[key2]['kp']\n    \n    if isinstance(kp1, torch.Tensor):\n        kp1 = kp1.cpu().numpy()\n    if isinstance(kp2, torch.Tensor):\n        kp2 = kp2.cpu().numpy()\n    \n    # 获取匹配对的坐标\n    match_coords1 = kp1[idxs_np[:, 0]]\n    match_coords2 = kp2[idxs_np[:, 1]]\n    \n    # 初始化每个匹配对所属的簇\n    match_cluster_ids = -np.ones(len(idxs_np), dtype=int)\n    \n    # 每个簇的匹配对计数\n    cluster_match_counts = {}\n    \n    # 为每个匹配对分配簇\n    for i, (coord1, coord2) in enumerate(zip(match_coords1, match_coords2)):\n        # 检查第一张图像中点所属的簇\n        cluster1_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers1, cluster_radius1)):\n            dist = np.sqrt(np.sum((coord1 - center) ** 2))\n            if dist <= radius:\n                cluster1_id = c_id\n                break\n        \n        # 检查第二张图像中点所属的簇\n        cluster2_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers2, cluster_radius2)):\n            dist = np.sqrt(np.sum((coord2 - center) ** 2))\n            if dist <= radius:\n                cluster2_id = c_id\n                break\n        \n        # 只有当两个点都属于某个簇时，才认为这个匹配对属于一个有效簇\n        if cluster1_id >= 0 and cluster2_id >= 0:\n            cluster_pair = (cluster1_id, cluster2_id)\n            match_cluster_ids[i] = hash(cluster_pair) % 10000000  # 使用哈希值作为簇对的唯一标识\n            \n            if cluster_pair not in cluster_match_counts:\n                cluster_match_counts[cluster_pair] = 0\n            cluster_match_counts[cluster_pair] += 1\n    \n    # 找出满足最小匹配对数量的簇\n    valid_cluster_pairs = {pair for pair, count in cluster_match_counts.items() \n                           if count >= min_matches_per_cluster}\n    \n    # 生成过滤掩码，只保留属于有效簇的匹配对\n    valid_mask = np.zeros(len(idxs_np), dtype=bool)\n    \n    for i, (coord1, coord2) in enumerate(zip(match_coords1, match_coords2)):\n        # 再次检查第一张图像中点所属的簇\n        cluster1_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers1, cluster_radius1)):\n            dist = np.sqrt(np.sum((coord1 - center) ** 2))\n            if dist <= radius:\n                cluster1_id = c_id\n                break\n        \n        # 再次检查第二张图像中点所属的簇\n        cluster2_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers2, cluster_radius2)):\n            dist = np.sqrt(np.sum((coord2 - center) ** 2))\n            if dist <= radius:\n                cluster2_id = c_id\n                break\n        \n        # 如果匹配对属于有效簇，则保留\n        if cluster1_id >= 0 and cluster2_id >= 0:\n            cluster_pair = (cluster1_id, cluster2_id)\n            if cluster_pair in valid_cluster_pairs:\n                valid_mask[i] = True\n        else:\n            # 不属于任何簇的匹配对也保留（可选，视需求而定）\n            valid_mask[i] = True\n    \n    # 应用过滤\n    idxs_filter_np = idxs_np[valid_mask]\n    \n    # 转回原始类型\n    if isinstance(idxs, torch.Tensor):\n        return torch.tensor(idxs_filter_np, device=idxs.device, dtype=idxs.dtype)\n    else:\n        return idxs_filter_np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.088785Z","iopub.execute_input":"2025-06-04T06:59:41.089016Z","iopub.status.idle":"2025-06-04T06:59:41.101438Z","shell.execute_reply.started":"2025-06-04T06:59:41.088970Z","shell.execute_reply":"2025-06-04T06:59:41.100641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_clusters_by_match_count_with_scores(idxs, scores, features_data, key1, key2, cluster_centers1, cluster_centers2, \n                                    cluster_radius1, cluster_radius2, min_matches_per_cluster=5):\n    \"\"\"\n    根据聚类中心和半径，计算每个匹配对所属簇，并过滤掉匹配对数量不足阈值的簇\n    同时保留每个匹配对应的分数\n    \"\"\"\n    if isinstance(idxs, torch.Tensor):\n        idxs_np = idxs.cpu().numpy()\n    else:\n        idxs_np = idxs\n        \n    if isinstance(scores, torch.Tensor):\n        scores_np = scores.cpu().numpy()\n    else:\n        scores_np = scores\n    \n    # 没有聚类时直接返回原始匹配\n    if len(cluster_centers1) == 0 or len(cluster_centers2) == 0:\n        return idxs, scores\n    \n    # 获取匹配对坐标\n    kp1 = features_data[key1]['kp']\n    kp2 = features_data[key2]['kp']\n    \n    if isinstance(kp1, torch.Tensor):\n        kp1 = kp1.cpu().numpy()\n    if isinstance(kp2, torch.Tensor):\n        kp2 = kp2.cpu().numpy()\n    \n    # 获取匹配对的坐标\n    match_coords1 = kp1[idxs_np[:, 0]]\n    match_coords2 = kp2[idxs_np[:, 1]]\n    \n    # 初始化每个匹配对所属的簇\n    match_cluster_ids = -np.ones(len(idxs_np), dtype=int)\n    \n    # 每个簇的匹配对计数\n    cluster_match_counts = {}\n    \n    # 为每个匹配对分配簇\n    for i, (coord1, coord2) in enumerate(zip(match_coords1, match_coords2)):\n        # 检查第一张图像中点所属的簇\n        cluster1_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers1, cluster_radius1)):\n            dist = np.sqrt(np.sum((coord1 - center) ** 2))\n            if dist <= radius:\n                cluster1_id = c_id\n                break\n        \n        # 检查第二张图像中点所属的簇\n        cluster2_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers2, cluster_radius2)):\n            dist = np.sqrt(np.sum((coord2 - center) ** 2))\n            if dist <= radius:\n                cluster2_id = c_id\n                break\n        \n        # 只有当两个点都属于某个簇时，才认为这个匹配对属于一个有效簇\n        if cluster1_id >= 0 and cluster2_id >= 0:\n            cluster_pair = (cluster1_id, cluster2_id)\n            match_cluster_ids[i] = hash(cluster_pair) % 10000000  # 使用哈希值作为簇对的唯一标识\n            \n            if cluster_pair not in cluster_match_counts:\n                cluster_match_counts[cluster_pair] = 0\n            cluster_match_counts[cluster_pair] += 1\n    \n    # 找出满足最小匹配对数量的簇\n    valid_cluster_pairs = {pair for pair, count in cluster_match_counts.items() \n                           if count >= min_matches_per_cluster}\n    \n    # 生成过滤掩码，只保留属于有效簇的匹配对\n    valid_mask = np.zeros(len(idxs_np), dtype=bool)\n    \n    for i, (coord1, coord2) in enumerate(zip(match_coords1, match_coords2)):\n        # 再次检查第一张图像中点所属的簇\n        cluster1_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers1, cluster_radius1)):\n            dist = np.sqrt(np.sum((coord1 - center) ** 2))\n            if dist <= radius:\n                cluster1_id = c_id\n                break\n        \n        # 再次检查第二张图像中点所属的簇\n        cluster2_id = -1\n        for c_id, (center, radius) in enumerate(zip(cluster_centers2, cluster_radius2)):\n            dist = np.sqrt(np.sum((coord2 - center) ** 2))\n            if dist <= radius:\n                cluster2_id = c_id\n                break\n        \n        # 如果匹配对属于有效簇，则保留\n        if cluster1_id >= 0 and cluster2_id >= 0:\n            cluster_pair = (cluster1_id, cluster2_id)\n            if cluster_pair in valid_cluster_pairs:\n                valid_mask[i] = True\n        else:\n            # 不属于任何簇的匹配对也保留（可选，视需求而定）\n            valid_mask[i] = True\n    \n    # 应用过滤\n    idxs_filter_np = idxs_np[valid_mask]\n    scores_filter_np = scores_np[valid_mask]\n    \n    # 转回原始类型\n    if isinstance(idxs, torch.Tensor):\n        filtered_idxs = torch.tensor(idxs_filter_np, device=idxs.device, dtype=idxs.dtype)\n        filtered_scores = torch.tensor(scores_filter_np, device=scores.device, dtype=scores.dtype)\n        return filtered_idxs, filtered_scores\n    else:\n        return idxs_filter_np, scores_filter_np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.102362Z","iopub.execute_input":"2025-06-04T06:59:41.102655Z","iopub.status.idle":"2025-06-04T06:59:41.119836Z","shell.execute_reply.started":"2025-06-04T06:59:41.102627Z","shell.execute_reply":"2025-06-04T06:59:41.119054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def second_match_ensemble(mkpts1, mkpts2, idxs, match_scores, features_data, key1, key2, lg_matcher, device, startidx=4096):\n    \"\"\"二次匹配函数，增加索引映射功能确保结果与原始特征点对应\n    \n    Args:\n        mkpts1, mkpts2: 初次匹配的特征点对\n        idxs: 初次匹配的索引对\n        match_scores: 初次匹配的分数\n        features_data: 特征数据字典\n        key1, key2: 图像标识符\n        lg_matcher: 匹配器实例\n        startidx: 特征起始索引\n        \n    Returns:\n        mapped_idxs: 映射回原始索引的匹配结果\n        merged_scores: 合并后的匹配分数\n    \"\"\"\n\n    # 将原始匹配对转换为集合形式，便于快速查找\n    orig_idxs = idxs.clone().cpu().numpy()\n    orig_matches_set = {(int(idx[0])) for idx in orig_idxs}\n    \n    # 保存原始匹配分数，用于后续合并\n    orig_scores_dict = {}\n    for i, (idx1, idx2) in enumerate(orig_idxs):\n        orig_scores_dict[(int(idx1), int(idx2))] = match_scores[i].item()\n\n    # 根据图像大小调整eps参数\n    img_width = max(features_data[key1]['size'][0][0].item(), features_data[key2]['size'][0][0].item())\n    eps = max(18, img_width * 0.03)  # 自适应聚类距离\n    min_radius = img_width * 0.15  # 最小半径\n    \n    db1 = DBSCAN(eps=eps, min_samples=3).fit(mkpts1)\n    db2 = DBSCAN(eps=eps, min_samples=3).fit(mkpts2)\n    \n    labels1 = db1.labels_.copy()\n    labels2 = db2.labels_.copy()\n\n    n = len(mkpts1)\n    adj = defaultdict(set)\n\n    # 只记录成功聚类的索引\n    for i in range(n):\n        if labels1[i] != -1:\n            adj[f'1_{labels1[i]}'].add(i)\n        if labels2[i] != -1:\n            adj[f'2_{labels2[i]}'].add(i)\n\n    # DFS 合并\n    clusters = []\n    visited = set()\n\n    def dfs_rec(i, cluster):\n        if i in visited:\n            return\n        visited.add(i)\n        cluster.add(i)\n\n        l1 = labels1[i]\n        l2 = labels2[i]\n\n        if l1 != -1:\n            for j in adj[f'1_{l1}']:\n                dfs(j, cluster)\n        if l2 != -1:\n            for j in adj[f'2_{l2}']:\n                dfs(j, cluster)\n\n    def dfs(i, cluster):\n        stack = [i]\n        while stack:\n            curr = stack.pop()\n            if curr in visited:\n                continue\n            visited.add(curr)\n            cluster.add(curr)\n            l1 = labels1[curr]\n            l2 = labels2[curr]\n            if l1 != -1:\n                for j in adj[f'1_{l1}']:\n                    if j not in visited:\n                        stack.append(j)\n            if l2 != -1:\n                for j in adj[f'2_{l2}']:\n                    if j not in visited:\n                        stack.append(j)\n\n    # 初始化最终标签为 -1\n    merged_labels = -1 * np.ones(n, dtype=int)\n\n    # 仅合并至少有一边聚类成功的点\n    for i in range(n):\n        if i not in visited and (labels1[i] != -1 or labels2[i] != -1):\n            cluster = set()\n            dfs(i, cluster)\n            if len(cluster) > 0:\n                clusters.append(cluster)\n\n    # 分配新标签\n    for new_label, cluster in enumerate(clusters):\n        for i in cluster:\n            merged_labels[i] = new_label\n    # 可以将 merged_labels 应用于 mkpts1 和 mkpts2（它们是一一对应的）\n    labels1 = merged_labels.copy()\n    labels2 = merged_labels.copy()\n\n    # 提取有效聚类\n    valid_clusters1 = np.unique(labels1[labels1 >= 0])\n    valid_clusters2 = np.unique(labels2[labels2 >= 0])\n    \n    # 加载所有特征点\n    all_kp1 = features_data[key1]['kp'][startidx:].clone()\n    all_kp2 = features_data[key2]['kp'][startidx:].clone()\n    all_desc1 = features_data[key1]['desc'][startidx:,:128].clone()\n    all_desc2 = features_data[key2]['desc'][startidx:,:128].clone()\n    fp_maks1 = features_data[key1]['mask'].clone()[-1]\n    fp_maks2 = features_data[key2]['mask'].clone()[-1]\n    all_kp1 = all_kp1[:fp_maks1]\n    all_kp2 = all_kp2[:fp_maks2]\n    all_desc1 = all_desc1[:fp_maks1]\n    all_desc2 = all_desc2[:fp_maks2]\n\n    # 为每个聚类创建掩码，判断哪些点在聚类区域内\n    all_kp1_np = all_kp1.cpu().numpy()\n    all_kp2_np = all_kp2.cpu().numpy()\n    \n    # 默认所有点都不在区域内\n    in_region_mask1 = np.zeros(len(all_kp1), dtype=bool)\n    in_region_mask2 = np.zeros(len(all_kp2), dtype=bool)\n    \n    # 区域扩展系数 - 将聚类区域扩大\n    region_expansion = 1.1\n    \n    cluster_centers1 = []\n    cluster_centers2 = []\n    cluster_radius1 = []\n    cluster_radius2 = []\n    # 对每个聚类，找出其中心和半径\n    for cluster_id in valid_clusters1:\n        cluster_points = mkpts1[labels1 == cluster_id]\n        centers = np.mean(cluster_points, axis=0)\n        # 计算聚类半径 (最大距离 * 扩展系数)\n        distances = np.sqrt(np.sum((cluster_points - centers)**2, axis=1))\n        radius = np.max(distances) * region_expansion\n        radius = max(radius, min_radius)  # 确保半径至少为最小半径\n        \n        # 计算所有点到聚类中心的距离，并标记在扩展区域内的点\n        all_distances = np.sqrt(np.sum((all_kp1_np - centers)**2, axis=1))\n        in_region_mask1 |= (all_distances < radius)\n        cluster_centers1.append(centers)\n        cluster_radius1.append(radius)\n    \n    # 对第二张图像重复相同的操作\n    for cluster_id in valid_clusters2:\n        cluster_points = mkpts2[labels2 == cluster_id]\n        centers = np.mean(cluster_points, axis=0)\n        distances = np.sqrt(np.sum((cluster_points - centers)**2, axis=1))\n        radius = np.max(distances) * region_expansion\n        radius = max(radius, min_radius)  # 确保半径至少为最小半径\n\n        all_distances = np.sqrt(np.sum((all_kp2_np - centers)**2, axis=1))\n        in_region_mask2 |= (all_distances < radius)\n        cluster_centers2.append(centers)\n        cluster_radius2.append(radius)\n    \n    #     # 可视化聚类结果\n    # if (len(valid_clusters1) > 0 or len(valid_clusters2) > 0):\n    #     # 提取图像路径\n    #     images_dir = os.path.dirname(os.path.dirname(features_data[key1]['size'].device.type))\n    #     images_dir = '../image-matching-challenge-2025/train/stairs'\n    #     img1_path = os.path.join(images_dir, key1)\n    #     img2_path = os.path.join(images_dir, key2)\n        \n    #     # 确保可视化输出目录存在\n    #     # vis_dir = os.path.join(os.path.dirname(images_dir), 'visualizations', 'clusters')\n    #     vis_dir = './results/featureout/cluster'\n    #     os.makedirs(vis_dir, exist_ok=True)\n    #     save_path = os.path.join(vis_dir, f'{key1}_{key2}_clusters.png')\n    #     if \"stairs_split_1_1710453626698.png_stairs_split_1_1710453620694.png_clusters\" in save_path:\n    #         print(\"hh\")\n    #     # 可视化聚类\n    #     visualize_clusters(\n    #         img1_path, img2_path, \n    #         mkpts1, mkpts2, \n    #         labels1, labels2, \n    #         cluster_centers1, cluster_centers2, \n    #         cluster_radius1, cluster_radius2,\n    #         save_path,\n    #         all_kp1, all_kp2\n    #     )\n\n    # 如果没有有效聚类，返回空结果\n    if len(valid_clusters1) == 0 or len(valid_clusters2) == 0:\n        return torch.zeros((0,2)), torch.zeros(0)\n    else:\n        # 使用区域内的特征点进行第二阶段匹配\n        region_kp1 = all_kp1[in_region_mask1]\n        region_kp2 = all_kp2[in_region_mask2]\n        region_desc1 = all_desc1[in_region_mask1]\n        region_desc2 = all_desc2[in_region_mask2]\n        \n        # 关键：记录区域内点与原始点的索引映射关系\n        region1_to_original = np.where(in_region_mask1)[0]\n        region2_to_original = np.where(in_region_mask2)[0]\n        \n        # 确保包含原始匹配点\n        mkpts1_indices = []\n        for pt in mkpts1:\n            # 找到与pt最接近的点在all_kp1_np中的索引\n            distances = np.sum((all_kp1_np - pt)**2, axis=1)\n            closest_idx = np.argmin(distances)\n            mkpts1_indices.append(closest_idx)\n            \n        mkpts2_indices = []\n        for pt in mkpts2:\n            distances = np.sum((all_kp2_np - pt)**2, axis=1)\n            closest_idx = np.argmin(distances)\n            mkpts2_indices.append(closest_idx)\n        \n        # 确保这些索引在掩码中标记为True\n        in_region_mask1[mkpts1_indices] = True\n        in_region_mask2[mkpts2_indices] = True\n        \n        # 更新区域内点与原始点的索引映射关系\n        region1_to_original = np.where(in_region_mask1)[0]\n        region2_to_original = np.where(in_region_mask2)[0]\n        \n        # 重新获取区域内的特征点\n        region_kp1 = all_kp1[in_region_mask1]\n        region_kp2 = all_kp2[in_region_mask2]\n        region_desc1 = all_desc1[in_region_mask1]\n        region_desc2 = all_desc2[in_region_mask2]\n        \n        # 执行第二阶段匹配\n        region_pred = {\n            'keypoints0': region_kp1[:3072][None].to(device),\n            'keypoints1': region_kp2[:3072][None].to(device),\n            'descriptors0': region_desc1[:3072,:128].to(device),\n            'descriptors1': region_desc2[:3072,:128].to(device),\n            'size0': features_data[key1]['size'].to(device),\n            'size1': features_data[key2]['size'].to(device),\n            # 'scale0': features_data[key1]['scale'],\n            # 'scale1': features_data[key2]['scale'],\n        }\n\n        with torch.inference_mode():\n            region_dist, region_idxs = lg_matcher(region_pred['descriptors0'].float(), region_pred['descriptors1'].float(),\n                KF.laf_from_center_scale_ori(region_pred['keypoints0'].float()),\n                KF.laf_from_center_scale_ori(region_pred['keypoints1'].float()))\n            region_valid_mask = (region_dist > 0.25)\n            region_dist = region_dist[region_valid_mask[:,0]]\n            region_idxs = region_idxs[region_valid_mask[:,0]]\n            \n        # 关键：将区域内的匹配索引映射回原始索引\n        if len(region_idxs) > 0:\n            # 限制区域匹配的索引范围\n            valid_mask = (region_idxs[:, 0] < len(region1_to_original)) & (region_idxs[:, 1] < len(region2_to_original))\n            region_idxs = region_idxs[valid_mask]\n            region_scores = region_dist[valid_mask]\n            \n            if len(region_idxs) > 0:\n                # 将区域内索引映射回原始索引\n                mapped_idxs = torch.zeros_like(region_idxs)\n                mapped_idxs[:, 0] = torch.tensor(region1_to_original[region_idxs[:, 0].cpu().numpy()])\n                mapped_idxs[:, 1] = torch.tensor(region2_to_original[region_idxs[:, 1].cpu().numpy()])\n                mapped_idxs += startidx\n\n                # 转换为numpy进行后续处理\n                mapped_idxs_np = mapped_idxs.cpu().numpy()\n                \n                # 区分重叠匹配和新增匹配\n                refined_matches = []\n                refined_scores = []\n                new_matches = []\n                new_scores = []\n                \n                for i, (idx1, idx2) in enumerate(mapped_idxs_np):\n                    idx1, idx2 = int(idx1), int(idx2)\n                    if (idx1) in orig_matches_set:\n                        refined_matches.append([idx1, idx2])\n                        refined_scores.append(region_scores[i].item())\n                    else:\n                        new_matches.append([idx1, idx2])\n                        new_scores.append(region_scores[i].item())\n                \n                # 保存原始匹配中未被区域匹配覆盖的部分\n                preserved_matches = []\n                preserved_scores = []\n                \n                for i, (idx1, idx2) in enumerate(orig_idxs):\n                    idx1, idx2 = int(idx1), int(idx2)\n                    match_key = (idx1, idx2)\n                    if match_key not in {(r[0], r[1]) for r in refined_matches}:\n                        preserved_matches.append([idx1, idx2])\n                        if match_key in orig_scores_dict:\n                            preserved_scores.append(orig_scores_dict[match_key])\n                        else:\n                            preserved_scores.append(0.5)  # 默认值\n                \n                # 合并结果：保留的原始匹配 + 精细化匹配 + 新增匹配\n                all_matches = np.array(preserved_matches + refined_matches + new_matches)\n                all_scores = torch.tensor(preserved_scores + refined_scores + new_scores, device=region_dist.device)\n                \n                # 按照分数从大到小排序\n                if len(all_scores) > 0:\n                    sorted_indices = torch.argsort(all_scores, descending=True)\n                    merged_idxs = torch.tensor(all_matches, device=sorted_indices.device, dtype=idxs.dtype)[sorted_indices]\n                    merged_scores = all_scores[sorted_indices]\n                    \n                    # 应用聚类过滤并保留对应的分数\n                    filtered_idxs, filtered_scores = filter_clusters_by_match_count_with_scores(\n                        merged_idxs, merged_scores, features_data, key1, key2, \n                        cluster_centers1, cluster_centers2, cluster_radius1, cluster_radius2\n                    )\n                    \n                    return filtered_idxs, filtered_scores\n                \n    # 如果找不到合适的区域匹配或区域匹配后没有结果，返回空结果\n    return torch.zeros((0,2)), torch.zeros(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.120575Z","iopub.execute_input":"2025-06-04T06:59:41.120773Z","iopub.status.idle":"2025-06-04T06:59:41.148574Z","shell.execute_reply.started":"2025-06-04T06:59:41.120756Z","shell.execute_reply":"2025-06-04T06:59:41.147876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def match_with_gimlightglue_ensemble(lightglue_matcher, img_fnames, index_pairs, feature_dir='.featureout', \n                                           device=torch.device('cpu'), min_matches=20, batch_size=2, \n                                           tok_limit=1200, match_limit=4096, verbose=True, visualize=True):\n    \"\"\"\n    使用批处理方式进行特征匹配，点数不会超过 max_points，但可能小于。\n    对于点数相同的匹配对进行批处理，点数不同的匹配对单独处理。\n\n    Args:\n        lightglue_matcher: LightGlue 匹配器实例\n        img_fnames: 图像文件名列表\n        index_pairs: 图像对索引列表\n        feature_dir: 特征存储目录\n        device: 设备 (CPU/GPU)\n        min_matches: 最小匹配数\n        batch_size: 批处理大小\n        batch_points: 每张图像的最大点数\n        verbose: 是否打印详细信息\n        visualize: 是否可视化匹配结果\n    \"\"\"\n    def lg_forward(\n        lg_matcher,\n        desc1,\n        desc2,\n        lafs1,\n        lafs2,\n    ):\n        \"\"\"Run forward.\n\n        Args:\n            desc1: Batch of descriptors of a shape :math:`(B1, D)`.\n            desc2: Batch of descriptors of a shape :math:`(B2, D)`.\n            lafs1: LAFs of a shape :math:`(1, B1, 2, 3)`.\n            lafs2: LAFs of a shape :math:`(1, B2, 2, 3)`.\n            hw1: Height/width of image.\n            hw2: Height/width of image.\n\n        Return:\n            - Descriptor distance of matching descriptors, shape of :math:`(B3, 1)`.\n            - Long tensor indexes of matching descriptors in desc1 and desc2,\n                shape of :math:`(B3, 2)` where :math:`0 <= B3 <= B1`.\n\n        \"\"\"\n        keypoints1 = get_laf_center(lafs1)\n        keypoints2 = get_laf_center(lafs2)\n        dev = lafs1.device\n\n        hw1_ = keypoints1.max(dim=1)[0].squeeze().flip(0)\n        hw2_ = keypoints2.max(dim=1)[0].squeeze().flip(0)\n \n        ori0 = torch.deg2rad(get_laf_orientation(lafs1).reshape(1, -1))\n        ori0[ori0 < 0] += 2.0 * torch.pi\n        ori1 = torch.deg2rad(get_laf_orientation(lafs2).reshape(1, -1))\n        ori1[ori1 < 0] += 2.0 * torch.pi\n        input_dict = {\n            \"image0\": {\n                \"keypoints\": keypoints1,\n                \"scales\": get_laf_scale(lafs1).reshape(1, -1),\n                \"oris\": ori0,\n                \"lafs\": lafs1,\n                \"descriptors\": desc1,\n                \"image_size\": hw1_.flip(0).reshape(-1, 2).to(dev),\n            },\n            \"image1\": {\n                \"keypoints\": keypoints2,\n                \"lafs\": lafs2,\n                \"scales\": get_laf_scale(lafs2).reshape(1, -1),\n                \"oris\": ori1,\n                \"descriptors\": desc2,\n                \"image_size\": hw2_.flip(0).reshape(-1, 2).to(dev),\n            },\n        }\n        pred = lg_matcher.matcher(input_dict)\n        matches0_batch, mscores0_batch = pred[\"matches0\"], pred[\"matching_scores0\"]\n        matches0_batch_lst = []\n        mscores0_batch_lst = []\n        for idx, matches0 in enumerate(matches0_batch):\n            valid = matches0 > -1\n            matches = torch.stack([torch.where(valid)[0], matches0[valid]], -1)\n            matches0_batch_lst.append(matches)\n            mscores0_batch_lst.append(mscores0_batch[idx][valid])\n        \n        return mscores0_batch_lst, matches0_batch_lst\n    \n    \n    # 另外保存到一个字典中，格式为{key1-key2:[idxs,scores]}\n    match_dict = {}\n    match_matrix = np.zeros((len(img_fnames), len(img_fnames)), dtype=np.int32)\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                        \"depth_confidence\": -1,\n                                        \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n\n    # 加载特征数据\n    print(\"加载特征数据...\")\n    features_data = {}\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n         h5py.File(f'{feature_dir}/size.h5', mode='r') as f_size, \\\n         h5py.File(f'{feature_dir}/scale.h5', mode='r') as f_scale, \\\n         h5py.File(f'{feature_dir}/mask.h5', mode='r') as f_mask:\n        for img_path in tqdm(img_fnames):\n            try:\n                key = img_path.split('/')[-1].split('\\\\')[-1]\n                features_data[key] = {\n                    'kp': torch.from_numpy(f_kp[key][...]).to(device),\n                    'desc': torch.from_numpy(f_desc[key][...]).to(device),\n                    'size': torch.from_numpy(f_size[key][...]).to(device),\n                    'scale': torch.from_numpy(f_scale[key][...]).to(device),\n                    'mask': torch.from_numpy(f_mask[key][...]).to(device)\n                }\n            except Exception as e:\n                print(f\"Error loading features for {key}: {e}\")\n                continue\n\n    # 将图像对按点数分组\n    batch_pairs_lst = []\n    single_pairs_lst = []\n    for pair_idx in index_pairs:\n        try:\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1 = fname1.split('/')[-1].split('\\\\')[-1]\n            key2 = fname2.split('/')[-1].split('\\\\')[-1]\n            batch_points = 4096\n            num_points10, _  = features_data[key1]['mask']\n            num_points20, _  = features_data[key2]['mask']\n            if num_points10 == batch_points and num_points20 == batch_points:\n                batch_pairs_lst.append(pair_idx)\n            else:\n                single_pairs_lst.append(pair_idx)\n        except Exception as e:\n            print(f\"Error processing pair {pair_idx}: {e}\")\n            continue\n\n    run_pairs = 0\n    success_pairs = 0\n    lg_finetuned = False\n    # 批量处理点数相同的图像对\n    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        # 将图像对分成批次\n        num_batches = (len(batch_pairs_lst) + batch_size - 1) // batch_size\n        for batch_idx in tqdm(range(num_batches)):\n            start_idx = batch_idx * batch_size\n            end_idx = min((batch_idx + 1) * batch_size, len(batch_pairs_lst))\n            batch_pairs = batch_pairs_lst[start_idx:end_idx]\n            \n            batch_data = []\n            batch_data_alike = []\n            batch_info = []\n            \n            # 准备批次数据\n            for pair_idx in batch_pairs:\n                idx1, idx2 = pair_idx\n                fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n                key1 = fname1.split('/')[-1].split('\\\\')[-1]\n                key2 = fname2.split('/')[-1].split('\\\\')[-1]\n                try:\n                    # 获取图像特征\n                    kp1 = features_data[key1]['kp']\n                    kp2 = features_data[key2]['kp']\n                    desc1 = features_data[key1]['desc']\n                    desc2 = features_data[key2]['desc']\n                    num_pts_h = len(kp1)\n                except Exception as e:\n                    print(f\"Error loading features for {key1} or {key2}: {e}\")\n                    continue\n\n                pred = {\n                    'keypoints0': kp1[:match_limit][None],\n                    'keypoints1': kp2[:match_limit][None],\n                    'descriptors0': desc1[:match_limit][None],\n                    'descriptors1': desc2[:match_limit][None],\n                    'size0': features_data[key1]['size'],\n                    'size1': features_data[key2]['size'],\n                    'scale0': features_data[key1]['scale'],\n                    'scale1': features_data[key2]['scale'],\n                }\n\n                \n                # pred_alike = {\n                #     'keypoints0': kp1[4096:][None],\n                #     'keypoints1': kp2[4096:][None],\n                #     'descriptors0': desc1[4096:,:128][None],\n                #     'descriptors1': desc2[4096:,:128][None],\n                #     'size0': features_data[key1]['size'],\n                #     'size1': features_data[key2]['size'],\n                #     'scale0': features_data[key1]['scale'],\n                #     'scale1': features_data[key2]['scale'],\n                # }\n\n                batch_data.append(pred)\n                # batch_data_alike.append(pred_alike)\n                batch_info.append((idx1, idx2, key1, key2, fname1, fname2))\n            \n            # 批量匹配\n            # print(f\"处理批次 {batch_idx+1}/{num_batches} ({len(batch_pairs)} 对图像)...\")\n            \n            # 合并批次预测数据\n            batch_preds = {\n                'keypoints0': torch.cat([data['keypoints0'] for data in batch_data], dim=0).to(device),\n                'keypoints1': torch.cat([data['keypoints1'] for data in batch_data], dim=0).to(device),\n                'descriptors0': torch.cat([data['descriptors0'] for data in batch_data], dim=0).to(device),\n                'descriptors1': torch.cat([data['descriptors1'] for data in batch_data], dim=0).to(device),\n                'size0': torch.stack([data['size0'] for data in batch_data], dim=0).to(device),\n                'size1': torch.stack([data['size1'] for data in batch_data], dim=0).to(device),\n                'scale0': torch.stack([data['scale0'] for data in batch_data], dim=0).to(device),\n                'scale1': torch.stack([data['scale1'] for data in batch_data], dim=0).to(device),\n            }\n            # batch_preds_alike = {\n            #     'keypoints0': torch.cat([data['keypoints0'] for data in batch_data_alike], dim=0).to(device),\n            #     'keypoints1': torch.cat([data['keypoints1'] for data in batch_data_alike], dim=0).to(device),\n            #     'descriptors0': torch.cat([data['descriptors0'] for data in batch_data_alike], dim=0).to(device),\n            #     'descriptors1': torch.cat([data['descriptors1'] for data in batch_data_alike], dim=0).to(device),\n            #     'size0': torch.stack([data['size0'] for data in batch_data_alike], dim=0).to(device),\n            #     'size1': torch.stack([data['size1'] for data in batch_data_alike], dim=0).to(device),\n            #     'scale0': torch.stack([data['scale0'] for data in batch_data_alike], dim=0).to(device),\n            #     'scale1': torch.stack([data['scale1'] for data in batch_data_alike], dim=0).to(device),\n            # }\n            # # 合并批次预测数据\n            # batch_preds_finetune = {\n            #     'keypoints0': torch.cat([data['keypoints0'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'keypoints1': torch.cat([data['keypoints1'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'descriptors0': torch.cat([data['descriptors0'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'descriptors1': torch.cat([data['descriptors1'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'size0': torch.stack([data['size0'] for data in batch_data], dim=0).to(device),\n            #     'size1': torch.stack([data['size1'] for data in batch_data], dim=0).to(device),\n            #     'scale0': torch.stack([data['scale0'] for data in batch_data], dim=0).to(device),\n            #     'scale1': torch.stack([data['scale1'] for data in batch_data], dim=0).to(device),\n            # }\n\n            # 批量推理\n            with torch.inference_mode():\n                batch_dists, batch_idxs = lightglue_matcher.match_batch(batch_preds)\n                # batch_dists_fine, batch_idxs_fine = lightglue_matcher.match_batch_finetune(batch_preds_finetune)\n                # batch_dists_fine, batch_idxs_fine = lg_forward(lg_matcher, batch_preds_alike['descriptors0'].float(), batch_preds_alike['descriptors1'].float(),\n                #         KF.laf_from_center_scale_ori(batch_preds_alike['keypoints0'].float()),\n                #         KF.laf_from_center_scale_ori(batch_preds_alike['keypoints1'].float()))\n            \n            batch_dists_fine = []\n            batch_idxs_fine = []\n            for i, (idx1, idx2, key1, key2, fname1, fname2) in enumerate(batch_info):\n                    kp1 = features_data[key1]['kp']\n                    kp2 = features_data[key2]['kp']\n                    desc1 = features_data[key1]['desc']\n                    desc2 = features_data[key2]['desc']\n                    mask1_alike = features_data[key1]['mask'][-1]\n                    mask2_alike = features_data[key2]['mask'][-1]\n                    pred_alike = {\n                        'keypoints0': kp1[4096:][:mask1_alike][None],\n                        'keypoints1': kp2[4096:][:mask2_alike][None],\n                        'descriptors0': desc1[4096:,:128][:mask1_alike],\n                        'descriptors1': desc2[4096:,:128][:mask2_alike],\n                    }\n\n                    # 批量推理\n                    with torch.inference_mode():\n                        dists_finetune, idxs_finetune = lg_matcher(pred_alike['descriptors0'].float(), pred_alike['descriptors1'].float(),\n                            KF.laf_from_center_scale_ori(pred_alike['keypoints0'].float()),\n                            KF.laf_from_center_scale_ori(pred_alike['keypoints1'].float()))\n                        dists_finetune = dists_finetune[:,0]\n                        idxs_finetune += 4096\n                        dists_finetune_mask = dists_finetune > 0.2\n                        idxs_finetune = idxs_finetune[dists_finetune_mask]\n                        dists_finetune = dists_finetune[dists_finetune_mask]\n                        batch_dists_fine.append(dists_finetune)\n                        batch_idxs_fine.append(idxs_finetune)\n                    \n            # 对 batch_idxs 按照 batch_dists 分数排序并保留最大的 1500 个匹配\n            sorted_idxs = []\n            sorted_dists = []\n            for i in range(len(batch_dists)):\n                if len(batch_dists[i]) > 0 or len(batch_dists_fine[i]) > 0:\n                    dists = torch.cat([batch_dists[i],batch_dists_fine[i]])\n                    idxs = torch.cat([batch_idxs[i],batch_idxs_fine[i]])\n\n                    # dists = batch_dists[i]\n                    # idxs = batch_idxs[i]\n                    dists, idxs = match_nms(dists, idxs, batch_info[i], features_data, 1)\n                    sorted_indices = torch.argsort(dists, descending=True)\n                    sorted_dists_one = dists[sorted_indices]\n                    sorted_idxs_one = idxs[sorted_indices]\n                    top_k = min(tok_limit, len(sorted_dists_one))\n                    sorted_idxs.append(sorted_idxs_one[:top_k])\n                    sorted_dists.append(sorted_dists_one[:top_k])\n                else:\n                    sorted_idxs.append([])\n                    sorted_dists.append([])\n\n            batch_idxs = sorted_idxs   \n            batch_dists = sorted_dists\n            # 处理结果\n            for i, (idx1, idx2, key1, key2, fname1, fname2) in enumerate(batch_info):\n                run_pairs += 1\n                if i >= len(batch_idxs) or batch_idxs[i] is None or len(batch_idxs[i]) == 0:\n                    continue\n                \n                idxs = batch_idxs[i]\n                match_scores = batch_dists[i]\n                \n                if verbose:\n                    print(f'{key1}-{key2}: {n_matches} matches')\n                \n                try:\n                    if len(idxs) < 800:\n                        # # 进行第二阶段匹配\n                        mkpts1 = features_data[key1]['kp'][idxs[:,0]]\n                        mkpts2 = features_data[key2]['kp'][idxs[:,1]]\n                        # 进行第二阶段匹配\n                        region_idxs, region_dists = second_match_ensemble(mkpts1.cpu().numpy(), mkpts2.cpu().numpy(), idxs, match_scores, features_data, key1, key2, lg_matcher)\n                        print(f'{key1}-{key2}')\n                        print(\"region_dists:\", len(idxs), len(region_idxs))\n                        idxs = region_idxs[:1500]\n                        match_scores = region_dists[:1500]\n                except Exception as e:\n                    print(f\"Error in second match: {e}\")\n                    continue\n                n_matches = len(idxs)\n                # 保存匹配结果\n                if n_matches >= min_matches:\n                    # kpts0 = features_data[key1]['kp'][idxs[:,0]]\n                    # kpts1 = features_data[key2]['kp'][idxs[:,1]]\n                    # # robust fitting\n                    # _, mask = cv2.findFundamentalMat(kpts0.cpu().detach().numpy(),\n                    #                                 kpts1.cpu().detach().numpy(),\n                    #                                 cv2.USAC_MAGSAC, ransacReprojThreshold=1.0,\n                    #                                 confidence=0.999999, maxIters=10000)\n                    # mask = mask.ravel() > 0\n                    # idxs = idxs[mask]\n                    if len(idxs) >= min_matches:\n                        group = f_match.require_group(key1)\n                        group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n                        match_matrix[idx1, idx2] = n_matches\n                        \n                        match_key = f\"{key1}-{key2}\"\n                        match_dict[match_key] = [idxs.detach().cpu().numpy(), match_scores.detach().cpu().numpy()]\n                        \n                        success_pairs += 1\n                            \n                        # 可视化匹配\n                        if visualize:\n                            vis_dir = os.path.join(feature_dir, 'visualizations')\n                            os.makedirs(vis_dir, exist_ok=True)\n                            save_path = os.path.join(vis_dir, f'{key1}_{key2}_matches.png')\n                            visualize_matches(\n                                fname1, fname2,\n                                features_data[key1]['kp'].cpu().numpy(),\n                                features_data[key2]['kp'].cpu().numpy(),\n                                idxs.cpu().numpy(),\n                                save_path\n                            )\n\n    with open(os.path.join(feature_dir, 'match_dict.pkl'), 'wb') as f:\n        pickle.dump(match_dict, f)\n\n    return match_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.149351Z","iopub.execute_input":"2025-06-04T06:59:41.149563Z","iopub.status.idle":"2025-06-04T06:59:41.180798Z","shell.execute_reply.started":"2025-06-04T06:59:41.149545Z","shell.execute_reply":"2025-06-04T06:59:41.180141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def match_with_gimlightglue_ensemble_withfine(lightglue_matcher, img_fnames, index_pairs, feature_dir='.featureout', \n                                           device=torch.device('cpu'), min_matches=20, batch_size=4, \n                                           tok_limit=1200, match_limit=4096, verbose=True, visualize=True):\n    \"\"\"\n    使用批处理方式进行特征匹配，点数不会超过 max_points，但可能小于。\n    对于点数相同的匹配对进行批处理，点数不同的匹配对单独处理。\n\n    Args:\n        lightglue_matcher: LightGlue 匹配器实例\n        img_fnames: 图像文件名列表\n        index_pairs: 图像对索引列表\n        feature_dir: 特征存储目录\n        device: 设备 (CPU/GPU)\n        min_matches: 最小匹配数\n        batch_size: 批处理大小\n        batch_points: 每张图像的最大点数\n        verbose: 是否打印详细信息\n        visualize: 是否可视化匹配结果\n    \"\"\"\n    def lg_forward(\n        lg_matcher,\n        desc1,\n        desc2,\n        lafs1,\n        lafs2,\n    ):\n        \"\"\"Run forward.\n\n        Args:\n            desc1: Batch of descriptors of a shape :math:`(B1, D)`.\n            desc2: Batch of descriptors of a shape :math:`(B2, D)`.\n            lafs1: LAFs of a shape :math:`(1, B1, 2, 3)`.\n            lafs2: LAFs of a shape :math:`(1, B2, 2, 3)`.\n            hw1: Height/width of image.\n            hw2: Height/width of image.\n\n        Return:\n            - Descriptor distance of matching descriptors, shape of :math:`(B3, 1)`.\n            - Long tensor indexes of matching descriptors in desc1 and desc2,\n                shape of :math:`(B3, 2)` where :math:`0 <= B3 <= B1`.\n\n        \"\"\"\n        keypoints1 = get_laf_center(lafs1)\n        keypoints2 = get_laf_center(lafs2)\n        dev = lafs1.device\n\n        hw1_ = keypoints1.max(dim=1)[0].squeeze().flip(0)\n        hw2_ = keypoints2.max(dim=1)[0].squeeze().flip(0)\n \n        ori0 = torch.deg2rad(get_laf_orientation(lafs1).reshape(1, -1))\n        ori0[ori0 < 0] += 2.0 * torch.pi\n        ori1 = torch.deg2rad(get_laf_orientation(lafs2).reshape(1, -1))\n        ori1[ori1 < 0] += 2.0 * torch.pi\n        input_dict = {\n            \"image0\": {\n                \"keypoints\": keypoints1,\n                \"scales\": get_laf_scale(lafs1).reshape(1, -1),\n                \"oris\": ori0,\n                \"lafs\": lafs1,\n                \"descriptors\": desc1,\n                \"image_size\": hw1_.flip(0).reshape(-1, 2).to(dev),\n            },\n            \"image1\": {\n                \"keypoints\": keypoints2,\n                \"lafs\": lafs2,\n                \"scales\": get_laf_scale(lafs2).reshape(1, -1),\n                \"oris\": ori1,\n                \"descriptors\": desc2,\n                \"image_size\": hw2_.flip(0).reshape(-1, 2).to(dev),\n            },\n        }\n        pred = lg_matcher.matcher(input_dict)\n        matches0_batch, mscores0_batch = pred[\"matches0\"], pred[\"matching_scores0\"]\n        matches0_batch_lst = []\n        mscores0_batch_lst = []\n        for idx, matches0 in enumerate(matches0_batch):\n            valid = matches0 > -1\n            matches = torch.stack([torch.where(valid)[0], matches0[valid]], -1)\n            matches0_batch_lst.append(matches)\n            mscores0_batch_lst.append(mscores0_batch[idx][valid])\n        \n        return mscores0_batch_lst, matches0_batch_lst\n    \n    \n    # 另外保存到一个字典中，格式为{key1-key2:[idxs,scores]}\n    match_dict = {}\n    match_matrix = np.zeros((len(img_fnames), len(img_fnames)), dtype=np.int32)\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                        \"depth_confidence\": -1,\n                                        \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n\n    # 加载特征数据\n    print(\"加载特征数据...\")\n    features_data = {}\n    load_gpu_first = len(img_fnames) < 500\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n         h5py.File(f'{feature_dir}/size.h5', mode='r') as f_size, \\\n         h5py.File(f'{feature_dir}/scale.h5', mode='r') as f_scale, \\\n         h5py.File(f'{feature_dir}/mask.h5', mode='r') as f_mask:\n        for img_path in tqdm(img_fnames):\n            try:\n                key = img_path.split('/')[-1].split('\\\\')[-1]\n                if load_gpu_first:\n                    features_data[key] = {\n                        'kp': torch.from_numpy(f_kp[key][...]).to(device),\n                        'desc': torch.from_numpy(f_desc[key][...]).to(device),\n                        'size': torch.from_numpy(f_size[key][...]).to(device),\n                        'scale': torch.from_numpy(f_scale[key][...]).to(device),\n                        'mask': torch.from_numpy(f_mask[key][...]).to(device)\n                    }\n                else:\n                    features_data[key] = {\n                        'kp': torch.from_numpy(f_kp[key][...]),\n                        'desc': torch.from_numpy(f_desc[key][...]),\n                        'size': torch.from_numpy(f_size[key][...]),\n                        'scale': torch.from_numpy(f_scale[key][...]),\n                        'mask': torch.from_numpy(f_mask[key][...])\n                    }\n            except Exception as e:\n                print(f\"Error loading features for {key}: {e}\")\n                continue\n\n    # 将图像对按点数分组\n    batch_pairs_lst = []\n    single_pairs_lst = []\n    for pair_idx in index_pairs:\n        try:\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1 = fname1.split('/')[-1].split('\\\\')[-1]\n            key2 = fname2.split('/')[-1].split('\\\\')[-1]\n            batch_points = 4096\n            num_points10, _  = features_data[key1]['mask']\n            num_points20, _  = features_data[key2]['mask']\n            if num_points10 == batch_points and num_points20 == batch_points:\n                batch_pairs_lst.append(pair_idx)\n            else:\n                single_pairs_lst.append(pair_idx)\n        except Exception as e:\n            print(f\"Error processing pair {pair_idx}: {e}\")\n            continue\n\n    run_pairs = 0\n    success_pairs = 0\n    lg_finetuned = False\n    # 批量处理点数相同的图像对\n    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        # 将图像对分成批次\n        num_batches = (len(batch_pairs_lst) + batch_size - 1) // batch_size\n        for batch_idx in tqdm(range(num_batches)):\n            start_idx = batch_idx * batch_size\n            end_idx = min((batch_idx + 1) * batch_size, len(batch_pairs_lst))\n            batch_pairs = batch_pairs_lst[start_idx:end_idx]\n            \n            batch_data = []\n            batch_data_alike = []\n            batch_info = []\n            \n            # 准备批次数据\n            for pair_idx in batch_pairs:\n                idx1, idx2 = pair_idx\n                fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n                key1 = fname1.split('/')[-1].split('\\\\')[-1]\n                key2 = fname2.split('/')[-1].split('\\\\')[-1]\n                try:\n                    # 获取图像特征\n                    kp1 = features_data[key1]['kp']\n                    kp2 = features_data[key2]['kp']\n                    desc1 = features_data[key1]['desc']\n                    desc2 = features_data[key2]['desc']\n                    num_pts_h = len(kp1)\n                except Exception as e:\n                    print(f\"Error loading features for {key1} or {key2}: {e}\")\n                    continue\n\n                pred = {\n                    'keypoints0': kp1[:match_limit][None],\n                    'keypoints1': kp2[:match_limit][None],\n                    'descriptors0': desc1[:match_limit][None],\n                    'descriptors1': desc2[:match_limit][None],\n                    'size0': features_data[key1]['size'],\n                    'size1': features_data[key2]['size'],\n                    'scale0': features_data[key1]['scale'],\n                    'scale1': features_data[key2]['scale'],\n                }\n\n                \n                # pred_alike = {\n                #     'keypoints0': kp1[4096:][None],\n                #     'keypoints1': kp2[4096:][None],\n                #     'descriptors0': desc1[4096:,:128][None],\n                #     'descriptors1': desc2[4096:,:128][None],\n                #     'size0': features_data[key1]['size'],\n                #     'size1': features_data[key2]['size'],\n                #     'scale0': features_data[key1]['scale'],\n                #     'scale1': features_data[key2]['scale'],\n                # }\n\n                batch_data.append(pred)\n                # batch_data_alike.append(pred_alike)\n                batch_info.append((idx1, idx2, key1, key2, fname1, fname2))\n            \n            # 批量匹配\n            # print(f\"处理批次 {batch_idx+1}/{num_batches} ({len(batch_pairs)} 对图像)...\")\n            \n            # 合并批次预测数据\n            batch_preds = {\n                'keypoints0': torch.cat([data['keypoints0'] for data in batch_data], dim=0).to(device),\n                'keypoints1': torch.cat([data['keypoints1'] for data in batch_data], dim=0).to(device),\n                'descriptors0': torch.cat([data['descriptors0'] for data in batch_data], dim=0).to(device),\n                'descriptors1': torch.cat([data['descriptors1'] for data in batch_data], dim=0).to(device),\n                'size0': torch.stack([data['size0'] for data in batch_data], dim=0).to(device),\n                'size1': torch.stack([data['size1'] for data in batch_data], dim=0).to(device),\n                'scale0': torch.stack([data['scale0'] for data in batch_data], dim=0).to(device),\n                'scale1': torch.stack([data['scale1'] for data in batch_data], dim=0).to(device),\n            }\n            # batch_preds_alike = {\n            #     'keypoints0': torch.cat([data['keypoints0'] for data in batch_data_alike], dim=0).to(device),\n            #     'keypoints1': torch.cat([data['keypoints1'] for data in batch_data_alike], dim=0).to(device),\n            #     'descriptors0': torch.cat([data['descriptors0'] for data in batch_data_alike], dim=0).to(device),\n            #     'descriptors1': torch.cat([data['descriptors1'] for data in batch_data_alike], dim=0).to(device),\n            #     'size0': torch.stack([data['size0'] for data in batch_data_alike], dim=0).to(device),\n            #     'size1': torch.stack([data['size1'] for data in batch_data_alike], dim=0).to(device),\n            #     'scale0': torch.stack([data['scale0'] for data in batch_data_alike], dim=0).to(device),\n            #     'scale1': torch.stack([data['scale1'] for data in batch_data_alike], dim=0).to(device),\n            # }\n            # # 合并批次预测数据\n            # batch_preds_finetune = {\n            #     'keypoints0': torch.cat([data['keypoints0'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'keypoints1': torch.cat([data['keypoints1'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'descriptors0': torch.cat([data['descriptors0'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'descriptors1': torch.cat([data['descriptors1'][:,:1024] for data in batch_data], dim=0).to(device),\n            #     'size0': torch.stack([data['size0'] for data in batch_data], dim=0).to(device),\n            #     'size1': torch.stack([data['size1'] for data in batch_data], dim=0).to(device),\n            #     'scale0': torch.stack([data['scale0'] for data in batch_data], dim=0).to(device),\n            #     'scale1': torch.stack([data['scale1'] for data in batch_data], dim=0).to(device),\n            # }\n\n            # 批量推理\n            with torch.inference_mode():\n                batch_dists, batch_idxs = lightglue_matcher.match_batch(batch_preds)\n                # batch_dists_fine, batch_idxs_fine = lightglue_matcher.match_batch_finetune(batch_preds_finetune)\n                # batch_dists_fine, batch_idxs_fine = lg_forward(lg_matcher, batch_preds_alike['descriptors0'].float(), batch_preds_alike['descriptors1'].float(),\n                #         KF.laf_from_center_scale_ori(batch_preds_alike['keypoints0'].float()),\n                #         KF.laf_from_center_scale_ori(batch_preds_alike['keypoints1'].float()))\n            \n            batch_dists_fine = []\n            batch_idxs_fine = []\n            for i, (idx1, idx2, key1, key2, fname1, fname2) in enumerate(batch_info):\n                    kp1 = features_data[key1]['kp']\n                    kp2 = features_data[key2]['kp']\n                    desc1 = features_data[key1]['desc']\n                    desc2 = features_data[key2]['desc']\n                    mask1_alike = features_data[key1]['mask'][-1]\n                    mask2_alike = features_data[key2]['mask'][-1]\n                    pred_alike = {\n                        'keypoints0': kp1[4096:][:mask1_alike][None].to(device),\n                        'keypoints1': kp2[4096:][:mask2_alike][None].to(device),\n                        'descriptors0': desc1[4096:,:128][:mask1_alike].to(device),\n                        'descriptors1': desc2[4096:,:128][:mask2_alike].to(device),\n                    }\n\n                    # 批量推理\n                    # with torch.inference_mode(), torch.amp.autocast('cuda'):\n                    with torch.inference_mode():\n                        dists_finetune, idxs_finetune = lg_matcher(pred_alike['descriptors0'].float(), pred_alike['descriptors1'].float(),\n                            KF.laf_from_center_scale_ori(pred_alike['keypoints0'].float()),\n                            KF.laf_from_center_scale_ori(pred_alike['keypoints1'].float()))\n                        dists_finetune = dists_finetune[:,0]\n                        idxs_finetune += 4096\n                        dists_finetune_mask = dists_finetune > 0.2\n                        idxs_finetune = idxs_finetune[dists_finetune_mask]\n                        dists_finetune = dists_finetune[dists_finetune_mask]\n                        batch_dists_fine.append(dists_finetune)\n                        batch_idxs_fine.append(idxs_finetune)\n                    \n            # 对 batch_idxs 按照 batch_dists 分数排序并保留最大的 1500 个匹配\n            sorted_idxs = []\n            sorted_dists = []\n            for i in range(len(batch_dists)):\n                if len(batch_dists[i]) > 0 or len(batch_dists_fine[i]) > 0:\n                    dists = torch.cat([batch_dists[i],batch_dists_fine[i]])\n                    idxs = torch.cat([batch_idxs[i],batch_idxs_fine[i]])\n\n                    # dists = batch_dists[i]\n                    # idxs = batch_idxs[i]\n                    dists, idxs = match_nms(dists, idxs, batch_info[i], features_data, 1)\n                    sorted_indices = torch.argsort(dists, descending=True)\n                    sorted_dists_one = dists[sorted_indices]\n                    sorted_idxs_one = idxs[sorted_indices]\n                    top_k = min(tok_limit, len(sorted_dists_one))\n                    sorted_idxs.append(sorted_idxs_one[:top_k])\n                    sorted_dists.append(sorted_dists_one[:top_k])\n                else:\n                    sorted_idxs.append([])\n                    sorted_dists.append([])\n\n            batch_idxs = sorted_idxs   \n            batch_dists = sorted_dists\n            # 处理结果\n            for i, (idx1, idx2, key1, key2, fname1, fname2) in enumerate(batch_info):\n                run_pairs += 1\n                if i >= len(batch_idxs) or batch_idxs[i] is None or len(batch_idxs[i]) == 0:\n                    continue\n                \n                idxs = batch_idxs[i]\n                match_scores = batch_dists[i]\n                \n                if verbose:\n                    print(f'{key1}-{key2}: {n_matches} matches')\n                \n                try:\n                    if len(idxs) < 800:\n                        # # 进行第二阶段匹配\n                        mkpts1 = features_data[key1]['kp'][idxs[:,0]]\n                        mkpts2 = features_data[key2]['kp'][idxs[:,1]]\n                        # 进行第二阶段匹配\n                        region_idxs, region_dists = second_match_ensemble(mkpts1.cpu().numpy(), mkpts2.cpu().numpy(), idxs, match_scores, features_data, key1, key2, lg_matcher, device)\n                        # print(f'{key1}-{key2}')\n                        # print(\"region_dists:\", len(idxs), len(region_idxs))\n                        idxs = region_idxs[:1500]\n                        match_scores = region_dists[:1500]\n                except Exception as e:\n                    print(f\"Error in second match: {e}\")\n                    continue\n                n_matches = len(idxs)\n                # 保存匹配结果\n                if n_matches >= min_matches:\n                    # kpts0 = features_data[key1]['kp'][idxs[:,0]]\n                    # kpts1 = features_data[key2]['kp'][idxs[:,1]]\n                    # # robust fitting\n                    # _, mask = cv2.findFundamentalMat(kpts0.cpu().detach().numpy(),\n                    #                                 kpts1.cpu().detach().numpy(),\n                    #                                 cv2.USAC_MAGSAC, ransacReprojThreshold=1.0,\n                    #                                 confidence=0.999999, maxIters=10000)\n                    # mask = mask.ravel() > 0\n                    # idxs = idxs[mask]\n                    if len(idxs) >= min_matches:\n                        group = f_match.require_group(key1)\n                        group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n                        match_matrix[idx1, idx2] = n_matches\n                        \n                        match_key = f\"{key1}-{key2}\"\n                        match_dict[match_key] = [idxs.detach().cpu().numpy(), match_scores.detach().cpu().numpy()]\n                        \n                        success_pairs += 1\n\n    with open(os.path.join(feature_dir, 'match_dict.pkl'), 'wb') as f:\n        pickle.dump(match_dict, f)\n\n    return match_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.181595Z","iopub.execute_input":"2025-06-04T06:59:41.181833Z","iopub.status.idle":"2025-06-04T06:59:41.212443Z","shell.execute_reply.started":"2025-06-04T06:59:41.181805Z","shell.execute_reply":"2025-06-04T06:59:41.211864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def match_nms(dists, idxs, batch_info, features_data, radius=3):\n    \"\"\"\n    对匹配点对进行非极大值抑制，删除距离过近的冗余匹配\n    \n    Args:\n        dists: 匹配点对的置信度分数\n        idxs: 匹配点对索引 (Nx2)\n        batch_info: 图像信息元组 (idx1, idx2, key1, key2, fname1, fname2)\n        features_data: 特征数据字典\n        radius: NMS半径，用于确定匹配点是否过近\n        \n    Returns:\n        filtered_dists: 过滤后的置信度分数\n        filtered_idxs: 过滤后的匹配点对索引\n    \"\"\"\n    if len(idxs) == 0:\n        return dists, idxs\n    \n    # 获取图像标识符\n    _, _, key1, key2, _, _ = batch_info\n    \n    # 获取匹配点坐标\n    kp1 = features_data[key1]['kp']\n    kp2 = features_data[key2]['kp']\n    \n    # 确保数据在CPU上\n    if isinstance(dists, torch.Tensor):\n        dists = dists.cpu()\n    if isinstance(idxs, torch.Tensor):\n        idxs = idxs.cpu()\n    if isinstance(kp1, torch.Tensor):\n        kp1 = kp1.cpu()\n    if isinstance(kp2, torch.Tensor):\n        kp2 = kp2.cpu()\n    \n    # 获取匹配点的坐标\n    pts1 = kp1[idxs[:, 0]]\n    pts2 = kp2[idxs[:, 1]]\n    \n    # 按置信度排序\n    sorted_indices = torch.argsort(dists, descending=True)\n    sorted_dists = dists[sorted_indices]\n    sorted_idxs = idxs[sorted_indices]\n    sorted_pts1 = pts1[sorted_indices]\n    sorted_pts2 = pts2[sorted_indices]\n    \n    # 初始化保留标志\n    keep = torch.ones(len(sorted_dists), dtype=torch.bool)\n    \n    # 执行NMS\n    for i in range(len(sorted_dists)):\n        if not keep[i]:\n            continue\n            \n        # 计算当前点与其他点的欧氏距离\n        dist1 = torch.sqrt(torch.sum((sorted_pts1[i+1:] - sorted_pts1[i].unsqueeze(0))**2, dim=1))\n        dist2 = torch.sqrt(torch.sum((sorted_pts2[i+1:] - sorted_pts2[i].unsqueeze(0))**2, dim=1))\n        \n        # 如果两张图像中的点都在半径内，则标记为抑制\n        suppress = (dist1 < radius) & (dist2 < radius)\n        keep[i+1:][suppress] = False\n    \n    # 应用过滤\n    filtered_dists = sorted_dists[keep]\n    filtered_idxs = sorted_idxs[keep]\n    \n    # 确保返回与输入相同的设备\n    if isinstance(dists, torch.Tensor):\n        filtered_dists = filtered_dists.to(dists.device)\n        filtered_idxs = filtered_idxs.to(idxs.device)\n    \n    return filtered_dists, filtered_idxs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.213260Z","iopub.execute_input":"2025-06-04T06:59:41.213448Z","iopub.status.idle":"2025-06-04T06:59:41.231818Z","shell.execute_reply.started":"2025-06-04T06:59:41.213431Z","shell.execute_reply":"2025-06-04T06:59:41.231258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def match_with_lightglue(img_fnames, index_pairs, feature_dir='.featureout', \n                        device=torch.device('cpu'), min_matches=20, verbose=True, visualize=True):\n    match_dict = {}\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                              \"depth_confidence\": -1,\n                                              \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            key1, key2 = key1.split('\\\\')[-1], key2.split('\\\\')[-1]\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1, desc2,\n                                       KF.laf_from_center_scale_ori(kp1[None]),\n                                       KF.laf_from_center_scale_ori(kp2[None]))\n            if len(idxs) == 0:\n                continue\n            n_matches = len(idxs)\n            if verbose:\n                print(f'{key1}-{key2}: {n_matches} matches')\n            group = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n                                # 添加可视化\n                \n                match_key = f\"{key1}-{key2}\"\n                match_dict[match_key] = [idxs.detach().cpu().numpy(), dists.detach().cpu().numpy()]\n                        \n                if visualize:\n                    vis_dir = os.path.join(feature_dir, 'visualizations')\n                    os.makedirs(vis_dir, exist_ok=True)\n                    save_path = os.path.join(vis_dir, f'{key1}_{key2}_matches.png')\n                    visualize_matches(fname1, fname2, \n                                   kp1.cpu().numpy(), \n                                   kp2.cpu().numpy(),\n                                   idxs.cpu().numpy(),\n                                   save_path)\n    with open(os.path.join(feature_dir, 'match_dict.pkl'), 'wb') as f:\n        pickle.dump(match_dict, f)\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.232624Z","iopub.execute_input":"2025-06-04T06:59:41.232910Z","iopub.status.idle":"2025-06-04T06:59:41.250291Z","shell.execute_reply.started":"2025-06-04T06:59:41.232881Z","shell.execute_reply":"2025-06-04T06:59:41.249623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PointIndexer:\n    def __init__(self, coord_tolerance=1.5):\n        \"\"\"\n        coord_tolerance: 匹配坐标时的精度容忍度（单位：像素），例如 0.5 表示将坐标四舍五入到 0.5 像素内。\n        \"\"\"\n        self.coord_tolerance = coord_tolerance\n\n        self.image_dict = {}          # image_key -> point3D_id\n        self.image_point_index = {}     # image_key -> list of (x, y)，参与匹配的原始点\n        self.coord_hash = {}            # (image_key, rounded_x, rounded_y) -> point3D_id\n        self.point3D_id_map = {}        # (image_key, x, y) -> point3D_id（原始坐标）\n\n    def _round_coord(self, coord):\n        \"\"\"将浮点坐标按容忍精度归一化（用于索引）\"\"\"\n        return tuple(np.round(np.array(coord) / self.coord_tolerance).astype(int))\n\n    def process_match(self, key1, key2, mkpts0, mkpts1):\n        \"\"\"\n        key1/key2: 图像名（str）\n        mkpts0/mkpts1: 匹配点数组 (N,2)，为 numpy array 或 torch tensor\n        return: (N,2) 的全局 point3D_id 对\n        \"\"\"\n        if isinstance(mkpts0, torch.Tensor):\n            mkpts0 = mkpts0.cpu().numpy()\n        if isinstance(mkpts1, torch.Tensor):\n            mkpts1 = mkpts1.cpu().numpy()\n\n        match_indices = []\n        for pt0, pt1 in zip(mkpts0, mkpts1):\n            id0 = self._get_or_assign_index(key1, pt0)\n            id1 = self._get_or_assign_index(key2, pt1)\n            match_indices.append([id0, id1])\n        return np.array(match_indices, dtype=np.int32)\n\n    def _get_or_assign_index(self, image_key, coord):\n        \"\"\"\n        对于一个图像的某个坐标，查找或分配 point3D_id。\n        coord: (x, y)\n        \"\"\"\n        rounded = self._round_coord(coord)\n        coord = (rounded[0] * self.coord_tolerance, rounded[1] * self.coord_tolerance)\n        hash_key = (image_key, rounded[0], rounded[1])\n\n        if image_key not in self.image_dict:\n            self.image_dict[image_key] = 0\n        if hash_key in self.coord_hash:\n            return self.coord_hash[hash_key]\n        else:\n            self.coord_hash[hash_key] = self.image_dict[image_key]\n            self.image_dict[image_key] += 1\n            self.image_point_index.setdefault(image_key, []).append((float(coord[0]), float(coord[1])))\n            return self.coord_hash[hash_key]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.253344Z","iopub.execute_input":"2025-06-04T06:59:41.253541Z","iopub.status.idle":"2025-06-04T06:59:41.268669Z","shell.execute_reply.started":"2025-06-04T06:59:41.253524Z","shell.execute_reply":"2025-06-04T06:59:41.268024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def match_with_gimloftr(lightglue_matcher, img_fnames, index_pairs, feature_dir='.featureout', \n                        device=torch.device('cpu'), min_matches=15, verbose=True, visualize=True):\n    match_matrix = np.zeros((len(img_fnames), len(img_fnames)), dtype=np.int32)\n    indexer = PointIndexer()\n    with h5py.File(f'{feature_dir}/feat_c.h5', mode='r') as f_c, \\\n        h5py.File(f'{feature_dir}/feat_f.h5', mode='r') as f_f, \\\n        h5py.File(f'{feature_dir}/size.h5', mode='r') as f_size, \\\n        h5py.File(f'{feature_dir}/scale.h5', mode='r') as f_scale, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            key1 = key1.split('\\\\')[-1]\n            key2 = key2.split('\\\\')[-1]\n            feat_c1 = torch.from_numpy(f_c[key1][...]).to(device)\n            feat_c2 = torch.from_numpy(f_c[key2][...]).to(device)\n            feat_f1 = torch.from_numpy(f_f[key1][...]).to(device)\n            feat_f2 = torch.from_numpy(f_f[key2][...]).to(device)\n            pred = {}\n            pred['feat_c0'] = feat_c1\n            pred['feat_c1'] = feat_c2\n            pred['feat_f0'] = feat_f1\n            pred['feat_f1'] = feat_f2\n            pred['hw0_i'] = torch.from_numpy(f_size[key1][...]).to(device)\n            pred['hw1_i'] = torch.from_numpy(f_size[key2][...]).to(device)\n            pred['scale0'] = torch.from_numpy(f_scale[key1][...]).to(device)\n            pred['scale1'] = torch.from_numpy(f_scale[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, kpts0, kpts1 = lightglue_matcher.loftr_match(pred)\n            \n            # if len(idxs) == 0:\n            #     continue\n            if kpts0 is None or len(kpts0) == 0:\n                continue\n            \n            idxs = indexer.process_match(key1, key2, kpts0.cpu().numpy(), kpts1.cpu().numpy())    \n            #  # 应用区域筛选方法\n            # filtered_idxs = adaptive_match_filtering(\n            #     lightglue_matcher, kp1, kp2, idxs.cpu().numpy(), fname1, fname2, device\n            # )\n            # # 转回tensor\n            # if isinstance(filtered_idxs, np.ndarray):\n            #     idxs = torch.from_numpy(filtered_idxs).to(idxs.device)\n\n            n_matches = len(idxs)\n            if verbose:\n                print(f'{key1}-{key2}: {n_matches} matches')\n            group = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                group.create_dataset(key2, data=idxs.reshape(-1, 2))\n                match_matrix[idx1,idx2] = len(idxs.reshape(-1, 2))\n                                # 添加可视化\n                # # break\n                if visualize:\n                    vis_dir = os.path.join(feature_dir, 'visualizations')\n                    os.makedirs(vis_dir, exist_ok=True)\n                    save_path = os.path.join(vis_dir, f'{key1}_{key2}_matches.png')\n                    visualize_matches(fname1, fname2, \n                                   kpts0.cpu().numpy(), \n                                   kpts1.cpu().numpy(),\n                                   np.stack((np.arange(0,len(kpts0)),np.arange(0,len(kpts0))),axis=1),\n                                   save_path)\n\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n        for image_key, coords in indexer.image_point_index.items():\n            pts = np.array(coords).astype(np.float32)\n            # print(f'Image {image_key}: {len(pts)} points')\n            f_kp[image_key] = pts\n            \n    return match_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.269715Z","iopub.execute_input":"2025-06-04T06:59:41.269966Z","iopub.status.idle":"2025-06-04T06:59:41.289127Z","shell.execute_reply.started":"2025-06-04T06:59:41.269937Z","shell.execute_reply":"2025-06-04T06:59:41.288334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_matches_from_h5(matches_file_path, pairs_to_remove):\n    \"\"\"\n    从匹配的 h5 文件中删除特定的匹配对\n    \n    参数:\n        matches_file_path: 匹配 h5 文件的路径\n        pairs_to_remove: 要删除的匹配对列表 [(key1, key2), ...]\n    \n    返回:\n        保存新的 h5 文件的路径\n    \"\"\"\n    # 检查文件是否存在\n    if not os.path.exists(matches_file_path):\n        raise FileNotFoundError(f\"文件 {matches_file_path} 不存在\")\n    \n    # 将pairs_to_remove转换为集合以加快查找\n    pairs_set = set((pair[0], pair[1]) for pair in pairs_to_remove)\n    \n    # 创建一个临时文件路径\n    temp_file_path = matches_file_path + '.temp'\n    \n    # 读取原始文件并创建新文件\n    with h5py.File(matches_file_path, 'r') as src_file, h5py.File(temp_file_path, 'w') as dst_file:\n        # 统计匹配对总数\n        total_pairs = sum(len(src_file[key1].keys()) for key1 in src_file.keys())\n        \n        removed_count = 0\n        print(f\"开始处理匹配文件，共有 {len(src_file.keys())} 个源图像\")\n        \n        # 遍历所有key1\n        for key1 in tqdm(src_file.keys()):\n            # 为每个key1创建组\n            group = dst_file.require_group(key1)\n            # 遍历所有key2\n            for key2 in src_file[key1].keys():\n                # 检查当前匹配对是否在要删除的列表中\n                if (key1, key2) in pairs_set:\n                    removed_count += 1\n                    continue  # 跳过此匹配对\n                \n                # 复制匹配数据到新文件\n                src_file.copy(f\"{key1}/{key2}\", group)\n        \n        print(f\"成功删除了 {removed_count} 个匹配对，占总数的 {removed_count/total_pairs*100:.2f}%\")\n    \n    # 备份原文件\n    backup_file_path = matches_file_path + '.bak'\n    os.rename(matches_file_path, backup_file_path)\n    \n    # 将临时文件重命名为原文件名\n    os.rename(temp_file_path, matches_file_path)\n    \n    print(f\"原文件已备份为 {backup_file_path}\")\n    print(f\"新匹配文件已保存为 {matches_file_path}\")\n    \n    return matches_file_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.289919Z","iopub.execute_input":"2025-06-04T06:59:41.290144Z","iopub.status.idle":"2025-06-04T06:59:41.301954Z","shell.execute_reply.started":"2025-06-04T06:59:41.290126Z","shell.execute_reply":"2025-06-04T06:59:41.301174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def import_into_colmap(img_dir, feature_dir='.featureout', database_path='colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(db, feature_dir, fname_to_id)\n    db.commit()\n    return\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.302749Z","iopub.execute_input":"2025-06-04T06:59:41.302974Z","iopub.status.idle":"2025-06-04T06:59:41.319270Z","shell.execute_reply.started":"2025-06-04T06:59:41.302954Z","shell.execute_reply":"2025-06-04T06:59:41.318532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_reconstruction_models(maps1, maps2, images):\n    \"\"\"比较两个重建模型的质量，返回更优的模型\"\"\"\n    def evaluate_model(maps):\n        if not maps:\n            return 0, 0, 0, 0, 0\n        \n        total_registered = 0\n        total_clusters = len(maps)\n        avg_track_length = 0\n        total_3d_points = 0\n        avg_reprojection_error = 0\n        \n        for map_index, cur_map in maps.items():\n            total_registered += len(cur_map.images)\n            total_3d_points += len(cur_map.points3D)\n            \n            # 计算平均track长度\n            if hasattr(cur_map, 'points3D') and cur_map.points3D:\n                track_lengths = [len(point.track.elements) for point in cur_map.points3D.values()]\n                if track_lengths:\n                    avg_track_length += sum(track_lengths) / len(track_lengths)\n                \n                # 计算平均重投影误差\n                errors = [point.error for point in cur_map.points3D.values()]\n                if errors:\n                    avg_reprojection_error += sum(errors) / len(errors)\n        \n        if total_clusters > 0:\n            avg_track_length /= total_clusters\n            avg_reprojection_error /= total_clusters\n            \n        return total_registered, total_clusters, avg_track_length, total_3d_points, avg_reprojection_error\n    \n    reg1, clusters1, track_len1, points1, error1 = evaluate_model(maps1)\n    reg2, clusters2, track_len2, points2, error2 = evaluate_model(maps2)\n    \n    # 改进的评分策略\n    # 1. 注册图像数量权重最高\n    # 2. track长度很重要，但要防止异常值\n    # 3. 聚类数量考虑合理性（1-3个为正常范围）\n    # 4. 3D点数量反映重建密度\n    # 5. 重投影误差反映精度（越小越好）\n    \n    # 聚类数量惩罚（1-3个聚类为正常，超过3个给予惩罚）\n    cluster_penalty1 = max(0, clusters1 - 3) * 5\n    cluster_penalty2 = max(0, clusters2 - 3) * 5\n    \n    # track长度标准化（防止异常值影响）\n    normalized_track1 = min(track_len1, 10)  # 限制最大值为10\n    normalized_track2 = min(track_len2, 10)\n    \n    # 重投影误差标准化（误差越小越好）\n    error_score1 = max(0, 10 - error1) if error1 > 0 else 0\n    error_score2 = max(0, 10 - error2) if error2 > 0 else 0\n    \n    score1 = (reg1 * 2 +                    # 注册图像数量（最高权重）\n              normalized_track1 * 5 +         # 平均track长度\n              points1 * 0.01 +               # 3D点数量\n              error_score1 * 5 -             # 重投影误差（转为正向分数）\n              cluster_penalty1)              # 聚类数量惩罚\n              \n    score2 = (reg2 * 2 + \n              normalized_track2 * 5 + \n              points2 * 0.01 + \n              error_score2 * 5 - \n              cluster_penalty2)\n    \n    print(f\"Model 1: {reg1} registered, {clusters1} clusters, track: {track_len1:.2f}, \"\n          f\"points: {points1}, error: {error1:.2f}, score: {score1:.2f}\")\n    print(f\"Model 2: {reg2} registered, {clusters2} clusters, track: {track_len2:.2f}, \"\n          f\"points: {points2}, error: {error2:.2f}, score: {score2:.2f}\")\n    \n    return maps1 if score1 >= score2 else maps2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.320054Z","iopub.execute_input":"2025-06-04T06:59:41.320253Z","iopub.status.idle":"2025-06-04T06:59:41.337733Z","shell.execute_reply.started":"2025-06-04T06:59:41.320236Z","shell.execute_reply":"2025-06-04T06:59:41.337061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def incremental_mapping_with_monitoring(database_path, image_path, output_path, mapper_options, images_num,\n                                       time_threshold=30.0, max_attempts=2):\n    \"\"\"\n    带监控的增量建图，如果运行时间小于阈值则重新建图并比较结果\n    \n    Args:\n        database_path: COLMAP数据库路径\n        images_dir: 图像目录\n        output_path: 输出路径\n        mapper_options: 建图选项\n        time_threshold: 时间阈值（秒），小于此值则重新建图\n        max_attempts: 最大尝试次数\n    \n    Returns:\n        best_maps: 最优的建图结果\n        total_time: 总建图时间\n        num_attempts: 实际尝试次数\n    \"\"\"\n    best_maps = None\n    total_time = 0\n    attempt = 1\n\n    images_dir = image_path\n    \n    print(f\"开始第 {attempt} 次建图...\")\n    \n    # 第一次建图\n    t_start = time()\n    os.makedirs(f\"{output_path}_attempt_{attempt}\", exist_ok=True)\n    \n    try:\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path,\n            image_path=images_dir,\n            output_path=f\"{output_path}_attempt_{attempt}\",\n            options=mapper_options\n        )\n        first_time = time() - t_start\n        total_time += first_time\n        best_maps = maps\n        \n        print(f\"第 {attempt} 次建图完成，耗时: {first_time:.2f}秒\")\n        print(f\"注册图像数: {sum(len(m.images) for m in maps.values()) if maps else 0}\")\n        print(f\"聚类数: {len(maps) if maps else 0}\")\n        \n        # 检查是否需要重新建图\n        if first_time < time_threshold and attempt < max_attempts and images_num < 60:\n            print(f\"建图时间 {first_time:.2f}秒 小于阈值 {time_threshold}秒，进行第二次建图...\")\n            \n            attempt += 1\n            print(f\"开始第 {attempt} 次建图...\")\n            \n            # 修改建图参数以获得不同结果\n            modified_options = deepcopy(mapper_options)\n            \n            # 调整参数策略\n            if hasattr(modified_options.mapper, 'init_min_tri_angle'):\n                modified_options.mapper.init_min_tri_angle *= 0.8\n            if hasattr(modified_options.mapper, 'abs_pose_min_inliers_ratio'):\n                modified_options.mapper.abs_pose_min_inliers_ratio *= 0.9\n            if hasattr(modified_options.mapper, 'filter_max_reproj_error'):\n                modified_options.mapper.filter_max_reproj_error *= 1.1\n            \n            # 第二次建图\n            t_start2 = time()\n            os.makedirs(f\"{output_path}_attempt_{attempt}\", exist_ok=True)\n            \n            try:\n                maps2 = pycolmap.incremental_mapping(\n                    database_path=database_path,\n                    image_path=images_dir,\n                    output_path=f\"{output_path}_attempt_{attempt}\",\n                    options=modified_options\n                )\n                second_time = time() - t_start2\n                total_time += second_time\n                \n                print(f\"第 {attempt} 次建图完成，耗时: {second_time:.2f}秒\")\n                print(f\"注册图像数: {sum(len(m.images) for m in maps2.values()) if maps2 else 0}\")\n                print(f\"聚类数: {len(maps2) if maps2 else 0}\")\n                \n                # 比较两次建图结果\n                print(\"比较两次建图结果...\")\n                best_maps = compare_reconstruction_models(maps, maps2, None)\n                \n                # 确定最优结果对应的attempt\n                if best_maps == maps:\n                    print(\"第一次建图结果更优\")\n                    final_output = f\"{output_path}_attempt_1\"\n                else:\n                    print(\"第二次建图结果更优\")\n                    final_output = f\"{output_path}_attempt_2\"\n                \n                # 复制最优结果到最终输出目录\n                if os.path.exists(output_path):\n                    shutil.rmtree(output_path)\n                shutil.copytree(final_output, output_path)\n                \n            except Exception as e:\n                print(f\"第二次建图失败: {e}\")\n                print(\"使用第一次建图结果\")\n        else:\n            if first_time >= time_threshold:\n                print(f\"建图时间 {first_time:.2f}秒 >= 阈值 {time_threshold}秒，不进行重新建图\")\n            else:\n                print(f\"已达到最大尝试次数 {max_attempts}，停止重新建图\")\n    \n    except Exception as e:\n        print(f\"第一次建图失败: {e}\")\n        best_maps = {}\n        \n    return best_maps, total_time, attempt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.338575Z","iopub.execute_input":"2025-06-04T06:59:41.338774Z","iopub.status.idle":"2025-06-04T06:59:41.354439Z","shell.execute_reply.started":"2025-06-04T06:59:41.338751Z","shell.execute_reply":"2025-06-04T06:59:41.353781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclasses.dataclass\nclass Prediction:\n    image_id: Optional[str]  # 或使用 Union[str, None]\n    dataset: str\n    filename: str \n    cluster_index: Optional[int] = None\n    rotation: Optional[np.ndarray] = None\n    translation: Optional[np.ndarray] = None\n\n# Main processing\nis_train = False\nis_OneTest = False \ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/results'\nos.makedirs(workdir, exist_ok=True)\n\nif is_OneTest:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels_one.csv' if is_train else 'sample_submission.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv' if is_train else 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\n\nfor _, row in competition_data.iterrows():\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.355129Z","iopub.execute_input":"2025-06-04T06:59:41.355326Z","iopub.status.idle":"2025-06-04T06:59:41.519967Z","shell.execute_reply.started":"2025-06-04T06:59:41.355309Z","shell.execute_reply":"2025-06-04T06:59:41.519345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matcher = Lightglue_Matcher(device=device, num_features=4096)\npred, data = matcher.extract(\"/kaggle/input/image-matching-challenge-2025/train/ETs/another_et_another_et001.png\", force=True)\nprint(pred.keys())\nprint(\"Keypoints:\", pred['keypoints0'].shape)\nprint(\"Scores:\", pred['scores0'].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:41.520639Z","iopub.execute_input":"2025-06-04T06:59:41.520835Z","iopub.status.idle":"2025-06-04T06:59:42.504342Z","shell.execute_reply.started":"2025-06-04T06:59:41.520818Z","shell.execute_reply":"2025-06-04T06:59:42.503604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\nmax_images = None\ndatasets_to_process = None\n\ntimings = {\n    \"shortlisting\": [],\n    \"feature_detection\": [],\n    \"feature_matching\": [],\n    \"RANSAC\": [],\n    \"Reconstruction\": []\n}\nmapping_result_strs = []\n\nprint(f\"Extracting on device {device}\")\n\nif is_OneTest:\n    dataset_train_test_lst = [\n        'ETs_one',\n        'stairs_one'\n    ]\nelse:\n    dataset_train_test_lst = [\n        'ETs',\n        # 'stairs'\n        # 'imc2023_heritage'\n    ]\n    \nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    if is_train is True:\n        if dataset not in dataset_train_test_lst:\n            continue\n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    # images = [\n    #     '../image-matching-challenge-2025/train/imc2023_heritage/dioscuri_img_0095.png',\n    #     '../image-matching-challenge-2025/train/imc2023_heritage/dioscuri_archive_0003.png'\n    # ]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # if 1:\n    try:\n        t = time()\n        # index_pairs = get_image_pairs_shortlist(images, sim_th=0.3, min_pairs=20, \n        #                                         exhaustive_if_less=20, device=device)\n        index_pairs = get_image_pairs_shortlist_clip(images, sim_th=0.76, min_pairs=1, \n                                            exhaustive_if_less=20, device=device)\n        timings['shortlisting'].append(time() - t)\n        print(f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n        # Clear CUDA cache if available\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()  # Wait for all operations to complete\n\n        # t = time()\n        # detect_aliked(images, feature_dir, 4096, device=device)\n        # timings['feature_detection'].append(time() - t)\n        # print(f'Features detected in {time() - t:.4f} sec')\n        \n        # t = time()\n        # match_with_GIMdkm(dkm_matcher, images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        # timings['feature_matching'].append(time() - t)\n        # print(f'Features matched in {time() - t:.4f} sec')\n\n        lightglue_matcher = Lightglue_Matcher(device=device,num_features=4096)\n        \n        # t = time()\n        # detect_aliked_second(images, feature_dir, 4096, device=device)\n        # # detect_person(lightglue_matcher, images, feature_dir, device=device)\n        # print(f'person_mask in {time() - t:.4f} sec')\n\n        # index_pairs.append((9,3))\n        # index_pairs.append((9,4))\n        t = time()\n        # detect_aliked(images, feature_dir, 4096, device=device)\n        detect_sp_ensemble(lightglue_matcher, images, feature_dir, 4096, device=device)\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n        gc.collect()\n        # Clear CUDA cache if available\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()  # Wait for all operations to complete\n\n\n        # t = time()\n        # # detect_aliked(images, feature_dir, 4096, device=device)\n        # loftr_feature(lightglue_matcher, images, feature_dir, device=device)\n        # timings['feature_detection'].append(time() - t)\n        # print(f'Features detected in {time() - t:.4f} sec')\n            \n        # t = time()\n        # match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        # timings['feature_matching'].append(time() - t)\n        # print(f'Features matched in {time() - t:.4f} sec')\n\n        # # 3. 微调LightGlue\n        # t = time()\n        # finetuned_model = fine_tune_lightglue(\n        #     lightglue_matcher,\n        #     images, \n        #     feature_dir, \n        #     device,\n        #     batch_size=4,\n        #     epochs=1,\n        #     up_limits=15\n        # )\n        # lightglue_matcher.update_model(finetuned_model)\n        # lightglue_matcher.model = finetuned_model\n        # print(f'模型微调完成，耗时 {time() - t:.4f} sec')\n        \n\n        # from safe_match import match_with_gimlightglue_ensemble_withfine_safe\n        t = time()\n        # match_matrix = match_with_gimloftr(lightglue_matcher, images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        match_matrix = match_with_gimlightglue_ensemble_withfine(lightglue_matcher, images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        # match_matrix = refine_matches(lightglue_matcher, images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n        print('match_matrix', match_matrix.sum())\n        gc.collect()\n\n        if 1:\n            from data_process.filter_match import filter_matches_graph, visualize_filtered_matches, visualize_connections\n            features_data = {}\n            with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n                h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc:\n                # h5py.File(f'{feature_dir}/size.h5', mode='r') as f_size, \\\n                # h5py.File(f'{feature_dir}/scale.h5', mode='r') as f_scale, \\\n                # h5py.File(f'{feature_dir}/mask.h5', mode='r') as f_mask:\n                for img_path in tqdm(images):\n                    key = img_path.split('/')[-1].split('\\\\')[-1]\n                    features_data[key] = {\n                        'kp': torch.from_numpy(f_kp[key][...]).to(device),\n                        'desc': torch.from_numpy(f_desc[key][...]).to(device),\n                        # 'size': torch.from_numpy(f_size[key][...]).to(device),\n                        # 'scale': torch.from_numpy(f_scale[key][...]).to(device),\n                        # 'mask': torch.from_numpy(f_mask[key][...]).to(device)\n                    }\n    \n            with open(os.path.join(feature_dir, 'match_dict.pkl'), 'rb') as f:\n                matches_dict = pickle.load(f)\n            cycle_csv_path = os.path.join(feature_dir, 'matches.csv')\n\n        # from train_LR.extract_features import extract_match_features\n        # from train_LR.predict import filter_match_with_lr\n        # from train_cnn.inference import filter_matches_with_cnn\n        # output_csv_path = os.path.join(feature_dir, 'matches_features.csv')\n        # 提取特征并保存到CSV\n        # df = extract_match_features(matches_dict, features_data, output_csv_path)\n        # # cycle_csv_path = None\n        # lr_model_path = './results/combined_model/'\n        # lr_model_path = './lr_model/LR_PB47'\n        # lr_out_csv_path = os.path.join(feature_dir, 'lr_pred.csv')\n        # filtered_matches_dict = filter_match_with_lr(matches_dict, features_data, model_dir=lr_model_path,threshold=0.3785,output_csv=lr_out_csv_path)\n        # cnn_model_path = './models/0528_best_model.pth'\n        # filtered_matches_dict = filter_matches_with_cnn(cnn_model_path, matches_dict, images, threshold=0.01, max_filter_ratio=0.3)\n        filtered_matches_dict, cycle_error_data = filter_matches_graph(images, matches_dict, features_data, output_csv=cycle_csv_path, verbose=False)\n        \n        # # 示例调用\n        # key = \"stairs_split_1_1710453930259.png\"  # 你想作为中心的图像关键字\n        # visualize_connections(key, filtered_matches_dict, features_data, images, \"connections_viz\")\n\n        # # 可视化过滤结果\n        # visualize_filtered_matches(images, matches_dict, filtered_matches_dict, features_data, os.path.join(feature_dir, 'graph_results'))\n        \n        import shutil\n        # 备份原始 matches.h5 文件（如果存在）\n        matches_h5_path = os.path.join(feature_dir, 'matches.h5')\n        if os.path.exists(matches_h5_path):\n            backup_path = matches_h5_path + '.bak'\n            shutil.copy2(matches_h5_path, backup_path)\n            print(f\"原始 matches.h5 已备份为 {backup_path}\")\n\n        # 将过滤后的匹配结果保存为 matches.h5\n        with h5py.File(matches_h5_path, 'w') as f_match:\n            for match_key, match_data in filtered_matches_dict.items():\n                key1, key2 = match_key.split('-')\n                match_indices = match_data  # 获取匹配索引\n                \n                # 创建key1的组并保存匹配结果\n                group = f_match.require_group(key1)\n                group.create_dataset(key2, data=match_indices)\n                \n        print(f\"已将过滤后的匹配结果保存至 {matches_h5_path}\")\n\n        # exit()\n        #删除无用文件\n        if os.path.exists(f'{feature_dir}/feat_f.h5'):\n            os.remove(f'{feature_dir}/feat_f.h5')\n        if os.path.exists(f'{feature_dir}/feat_c.h5'):\n            os.remove(f'{feature_dir}/feat_c.h5')\n    \n        database_path = os.path.join(feature_dir, 'colmap.db')\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n        gc.collect()\n    \n        # matches_file_path = \"./results/featureout/ETs_one/matches.h5\"\n        # # 要删除的匹配对列表\n        # pairs_to_remove = [\n        #     (\"another_et_another_et003.png\", \"another_et_another_et009.png\"),\n        #     (\"another_et_another_et004.png\", \"another_et_another_et009.png\"),\n        # ]\n        # # 执行删除操作\n        # remove_matches_from_h5(matches_file_path, pairs_to_remove)\n    \n        sleep(1)\n        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n        output_path = f'{feature_dir}/colmap_rec_aliked'\n        \n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        timings['RANSAC'].append(time() - t)\n        print(f'Ran RANSAC in {time() - t:.4f} sec')\n        \n        # best_pair = find_best_initial_pair(match_matrix, features_data)\n        # if best_pair:\n        #     mapper_options.init_image_id1 = best_pair[0]\n        #     mapper_options.init_image_id2 = best_pair[1]\n    \n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 5\n        mapper_options.max_num_models = 30\n        # mapper_options.mapper.abs_pose_min_num_inliers = 15\n        mapper_options.mapper.num_threads = 1\n        # if max_pair is not None:\n        #     image_id1, image_id2 = pair_id_to_image_pair(max_pair)\n        #     mapper_options.init_image_id1 = image_id1\n        #     mapper_options.init_image_id2 = image_id2\n    \n        os.makedirs(output_path, exist_ok=True)\n        t = time()\n        # maps = pycolmap.incremental_mapping(database_path=database_path, \n        #                                     image_path=images_dir,\n        #                                     output_path=output_path,\n        #                                     options=mapper_options)\n        \n        maps, mapping_time, num_attempts = incremental_mapping_with_monitoring(\n            database_path=database_path,\n            image_path=images_dir,\n            output_path=output_path,\n            mapper_options=mapper_options,\n            images_num=len(images),\n            time_threshold=300.0,  # 30秒阈值，可根据需要调整\n            max_attempts=2\n        )\n        if mapping_time == 0:\n            maps = pycolmap.incremental_mapping(database_path=database_path, \n                                        image_path=images_dir,\n                                        output_path=output_path,\n                                        options=mapper_options)\n        \n    \n        sleep(1)\n        timings['Reconstruction'].append(time() - t)\n        print(f'Reconstruction done in  {time() - t:.4f} sec')\n        print(maps)\n    \n        registered = 0\n        for map_index, cur_map in maps.items():\n            for index, image in cur_map.images.items():\n                prediction_index = filename_to_index[image.name]\n                predictions[prediction_index].cluster_index = map_index\n                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                registered += 1\n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:59:42.505441Z","iopub.execute_input":"2025-06-04T06:59:42.505804Z","execution_failed":"2025-06-04T07:03:50.019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/imc2025-dependences/models\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-04T07:03:50.019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\n\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-04T07:03:50.020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute results for training set\nif is_train:\n    t = time()\n    if is_OneTest:\n        final_score, dataset_scores = metric.score(\n            gt_csv=os.path.join(data_dir, 'train_labels_one.csv'),\n            user_csv=submission_file,\n            thresholds_csv=os.path.join(data_dir, 'train_thresholds_one.csv'),\n            mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n            inl_cf=0,\n            strict_cf=-1,\n            verbose=True,\n        )\n    else:\n        final_score, dataset_scores = metric.score(\n            gt_csv=os.path.join(data_dir, 'train_labels.csv'),\n            user_csv=submission_file,\n            thresholds_csv=os.path.join(data_dir, 'train_thresholds.csv'),\n            mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n            inl_cf=0,\n            strict_cf=-1,\n            verbose=True,\n        )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-04T07:03:50.020Z"}},"outputs":[],"execution_count":null}]}