{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c5ced9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00666,
     "end_time": "2025-05-19T19:16:10.273287",
     "exception": false,
     "start_time": "2025-05-19T19:16:10.266627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example submission\n",
    "\n",
    "Image Matching Challenge 2025: https://www.kaggle.com/competitions/image-matching-challenge-2025\n",
    "\n",
    "This notebook creates a simple submission using ALIKED and LightGlue, plus DINO for shortlisting, on GPU. Adapted from [last year](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n",
    "\n",
    "Remember to select an accelerator on the sidebar to the right, and to disable internet access when submitting a notebook to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f00b8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:10.286104Z",
     "iopub.status.busy": "2025-05-19T19:16:10.285767Z",
     "iopub.status.idle": "2025-05-19T19:16:17.652152Z",
     "shell.execute_reply": "2025-05-19T19:16:17.650829Z"
    },
    "papermill": {
     "duration": 7.375212,
     "end_time": "2025-05-19T19:16:17.654219",
     "exception": false,
     "start_time": "2025-05-19T19:16:10.279007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\r\n",
      "Installing collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\r\n",
      "  Attempting uninstall: kornia-rs\r\n",
      "    Found existing installation: kornia_rs 0.1.8\r\n",
      "    Uninstalling kornia_rs-0.1.8:\r\n",
      "      Successfully uninstalled kornia_rs-0.1.8\r\n",
      "  Attempting uninstall: kornia\r\n",
      "    Found existing installation: kornia 0.8.0\r\n",
      "    Uninstalling kornia-0.8.0:\r\n",
      "      Successfully uninstalled kornia-0.8.0\r\n",
      "Successfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\r\n",
      "Processing /kaggle/input/imc2025/kneed-0.8.5-py3-none-any.whl\r\n",
      "Processing /kaggle/input/imc2025/yacs-0.1.8-py3-none-any.whl\r\n",
      "Installing collected packages: yacs, kneed\r\n",
      "Successfully installed kneed-0.8.5 yacs-0.1.8\r\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT \n",
    "#Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n",
    "\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!pip install --no-index /kaggle/input/imc2025/* --no-deps\n",
    "\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76eba77a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:17.667807Z",
     "iopub.status.busy": "2025-05-19T19:16:17.667444Z",
     "iopub.status.idle": "2025-05-19T19:16:48.681570Z",
     "shell.execute_reply": "2025-05-19T19:16:48.680381Z"
    },
    "papermill": {
     "duration": 31.022915,
     "end_time": "2025-05-19T19:16:48.683373",
     "exception": false,
     "start_time": "2025-05-19T19:16:17.660458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "import torch\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# IMPORTANT Utilities: importing data into colmap and competition metric\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric\n",
    "from torch.backends import cudnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9289e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.698262Z",
     "iopub.status.busy": "2025-05-19T19:16:48.697544Z",
     "iopub.status.idle": "2025-05-19T19:16:48.702115Z",
     "shell.execute_reply": "2025-05-19T19:16:48.700956Z"
    },
    "papermill": {
     "duration": 0.014287,
     "end_time": "2025-05-19T19:16:48.704696",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.690409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(\"PyTorch version:\", torch.__version__)\n",
    "# import sys\n",
    "# print(\"Python version:\", sys.version)\n",
    "\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA version:\", torch.version.cuda)\n",
    "# print(\"Device count:\", torch.cuda.device_count())\n",
    "# print(\"Current device:\", torch.cuda.current_device())\n",
    "# print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c09908e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.719415Z",
     "iopub.status.busy": "2025-05-19T19:16:48.719019Z",
     "iopub.status.idle": "2025-05-19T19:16:48.723342Z",
     "shell.execute_reply": "2025-05-19T19:16:48.721850Z"
    },
    "papermill": {
     "duration": 0.014666,
     "end_time": "2025-05-19T19:16:48.725704",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.711038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d120d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.739085Z",
     "iopub.status.busy": "2025-05-19T19:16:48.738722Z",
     "iopub.status.idle": "2025-05-19T19:16:48.748030Z",
     "shell.execute_reply": "2025-05-19T19:16:48.746317Z"
    },
    "papermill": {
     "duration": 0.017953,
     "end_time": "2025-05-19T19:16:48.749798",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.731845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to select an accelerator on the sidebar to the right.\n",
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076858eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.763896Z",
     "iopub.status.busy": "2025-05-19T19:16:48.763543Z",
     "iopub.status.idle": "2025-05-19T19:16:48.767352Z",
     "shell.execute_reply": "2025-05-19T19:16:48.766406Z"
    },
    "papermill": {
     "duration": 0.012969,
     "end_time": "2025-05-19T19:16:48.769102",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.756133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r /kaggle/working/result/featureout/ETs/featurept.zip /kaggle/working/result/featureout/ETs/featurept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162ed50f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.783553Z",
     "iopub.status.busy": "2025-05-19T19:16:48.783145Z",
     "iopub.status.idle": "2025-05-19T19:16:48.788087Z",
     "shell.execute_reply": "2025-05-19T19:16:48.786968Z"
    },
    "papermill": {
     "duration": 0.014411,
     "end_time": "2025-05-19T19:16:48.789831",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.775420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8074857e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.804407Z",
     "iopub.status.busy": "2025-05-19T19:16:48.804016Z",
     "iopub.status.idle": "2025-05-19T19:16:48.811075Z",
     "shell.execute_reply": "2025-05-19T19:16:48.810010Z"
    },
    "papermill": {
     "duration": 0.016805,
     "end_time": "2025-05-19T19:16:48.812867",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.796062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def draw_and_save_feature_points(image_path, keypoints, result_folder):\n",
    "    \"\"\"\n",
    "    Draw feature points on the image and save to result folder.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the input image.\n",
    "        keypoints (np.ndarray): (N, 2) array of (x, y) coordinates.\n",
    "        result_folder (str or Path): Folder to save the output image.\n",
    "    \"\"\"\n",
    "    # Load image in BGR\n",
    "    return\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Cannot read image from {image_path}\")\n",
    "\n",
    "    # Draw keypoints\n",
    "    for (x, y) in keypoints.astype(int):\n",
    "        cv2.circle(image, (x, y), radius=2, color=(0, 255, 0), thickness=-1)  # Green dots\n",
    "\n",
    "    img_fname = image_path.split('/')[-1]\n",
    "\n",
    "    result_folder = Path(result_folder)\n",
    "    img_fname = Path(image_path).stem  # no extension\n",
    "    output_path = result_folder / f\"{img_fname}_fe.png\"\n",
    "\n",
    "    cv2.imwrite(str(output_path), image)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433d9a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.828045Z",
     "iopub.status.busy": "2025-05-19T19:16:48.827661Z",
     "iopub.status.idle": "2025-05-19T19:16:48.852958Z",
     "shell.execute_reply": "2025-05-19T19:16:48.851840Z"
    },
    "papermill": {
     "duration": 0.034635,
     "end_time": "2025-05-19T19:16:48.854958",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.820323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.normalize(torch.mean(x.clamp(min=eps).pow(p), dim=1).pow(1/p), p=2, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu'), is_max = True):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            if is_max:\n",
    "                dino_fea = gem(outputs.last_hidden_state[:,1:], p=3)\n",
    "            else:\n",
    "                dino_fea = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_fea.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 10,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu'),\n",
    "                              max_pairs = 30):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device, is_max = False)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # print(dm)\n",
    "    # 只分析上三角（去掉对角线），避免重复\n",
    "    triu_indices = np.triu_indices_from(dm, k=1)\n",
    "    dm_flat = dm[triu_indices]\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(\"Distance Matrix Statistics:\")\n",
    "    print(f\"Min:  {dm_flat.min():.4f}\")\n",
    "    print(f\"Max:  {dm_flat.max():.4f}\")\n",
    "    print(f\"Mean: {dm_flat.mean():.4f}\")\n",
    "    print(f\"Std:  {dm_flat.std():.4f}\")\n",
    "    print(f\"20%:  {np.percentile(dm_flat, 20):.4f}\")\n",
    "    print(f\"30%:  {np.percentile(dm_flat, 30):.4f}\")\n",
    "    print(f\"USED 50%:  {np.percentile(dm_flat, 50):.4f}\")\n",
    "    print(f\"75%:  {np.percentile(dm_flat, 75):.4f}\")\n",
    "    \n",
    "    thr = np.percentile(dm_flat, 50)\n",
    "    threshold = max(dm_flat.mean() + np.sqrt(3) * dm_flat.std(), thr)\n",
    "    # removing half\n",
    "    mask = dm <= thr\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs :\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        if len(to_match) >= max_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:max_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < threshold:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list\n",
    "\n",
    "def detect_aliked(img_fnames,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 2048,\n",
    "                  device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.1).eval().to(device, dtype)\n",
    "    extractor.preprocess_conf[\"resize\"] = resize_to\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    # Calculate the expected scale factor ALIKED will apply\n",
    "    # ALIKED uses preprocess_conf[\"resize\"] on the *input image tensor*\n",
    "    # Input image tensor size will be (H, W) after Kornia loading/conversion\n",
    "    \n",
    "    draw_feature_dir = os.path.join(feature_dir, 'featurept')\n",
    "    os.makedirs(draw_feature_dir, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "                draw_and_save_feature_points(img_path, kpts, draw_feature_dir)\n",
    "    return\n",
    "\n",
    "def match_with_lightglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=30,\n",
    "                   verbose=False,\n",
    "                   match_score_thresh = 0.15):\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                         desc2,\n",
    "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            # if verbose:\n",
    "            #     print (f'{key1}-{key2}: {n_matches} matches')\n",
    "            # group  = f_match.require_group(key1)\n",
    "            # if n_matches >= min_matches:\n",
    "            #      group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "            # Filter by match score (distance)\n",
    "            \n",
    "            mask = dists > match_score_thresh\n",
    "            idxs_filtered = idxs[mask.squeeze(1)]\n",
    "    \n",
    "            n_matches = len(idxs_filtered)\n",
    "            if n_matches == 0:\n",
    "                continue\n",
    "    \n",
    "            if verbose:\n",
    "                print(f'{key1}-{key2}: {n_matches} matches (filtered from {len(idxs)})')\n",
    "    \n",
    "            group = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=idxs_filtered.detach().cpu().numpy().reshape(-1, 2))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3796a98a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.869356Z",
     "iopub.status.busy": "2025-05-19T19:16:48.868940Z",
     "iopub.status.idle": "2025-05-19T19:16:48.875483Z",
     "shell.execute_reply": "2025-05-19T19:16:48.874257Z"
    },
    "papermill": {
     "duration": 0.015688,
     "end_time": "2025-05-19T19:16:48.877347",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.861659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    base_path = Path(\"/kaggle/input/image-matching-challenge-2025\")\n",
    "    feature_dir = Path(workdir) / \".feature_outputs\"\n",
    "\n",
    "    device = K.utils.get_cuda_device_if_available(0)\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    num_trials = 2\n",
    "\n",
    "    # keypoint_detection_args = {\n",
    "    #     \"num_features\": 4096,\n",
    "    #     \"detection_threshold\": 0.01,\n",
    "    #     \"resize_to\": 1024,\n",
    "    # }\n",
    "\n",
    "    # keypoint_distances_args = {\n",
    "    #     \"early_stopping_thr\": 1000,\n",
    "    #     \"min_matches\": [100, 125],\n",
    "    #     \"verbose\": False,\n",
    "    # }\n",
    "\n",
    "    doppelgangers_classifier_args = {\n",
    "        \"loftr_weight_path\": Path(\"/kaggle/input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt\"),\n",
    "        \"doppelgangers_weight_path\": Path(\"/kaggle/input/doppelgangers-repo/doppelgangers/weights/doppelgangers_classifier_loftr.pt\"),\n",
    "    }\n",
    "\n",
    "    # remove_ambiguous_area_args = {\n",
    "    #     \"pcd_used_num_images_ratio\": 0.5,\n",
    "    #     \"erode_mask_ratio\": 0.0,\n",
    "    # }\n",
    "\n",
    "    verify_matches_args = {\n",
    "        \"doppelgangers_min_thr\": 0.3,\n",
    "        \"doppelgangers_max_thr\": 0.5,\n",
    "        \"filter_iterations\": 1,\n",
    "        \"filter_threshold\": 1,\n",
    "    }\n",
    "\n",
    "    # colmap_mapper_options = {\n",
    "    #     \"min_model_size\": 3,\n",
    "    #     \"max_num_models\": 2,\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc7fc44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:48.892322Z",
     "iopub.status.busy": "2025-05-19T19:16:48.891883Z",
     "iopub.status.idle": "2025-05-19T19:16:49.067752Z",
     "shell.execute_reply": "2025-05-19T19:16:49.066682Z"
    },
    "papermill": {
     "duration": 0.185871,
     "end_time": "2025-05-19T19:16:49.069809",
     "exception": false,
     "start_time": "2025-05-19T19:16:48.883938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "sys.path.append(\"/kaggle/input/doppelgangers-repo/doppelgangers\")\n",
    "from doppelgangers.third_party.loftr import LoFTR, default_cfg\n",
    "from doppelgangers.utils.loftr_matches import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd097b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:49.083630Z",
     "iopub.status.busy": "2025-05-19T19:16:49.083240Z",
     "iopub.status.idle": "2025-05-19T19:16:49.089102Z",
     "shell.execute_reply": "2025-05-19T19:16:49.088015Z"
    },
    "papermill": {
     "duration": 0.014818,
     "end_time": "2025-05-19T19:16:49.090913",
     "exception": false,
     "start_time": "2025-05-19T19:16:49.076095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_matching_list_from_h5(pair_probability_file_path, fname_to_index):\n",
    "    \"\"\"\n",
    "    Convert h5 pair-probability file into list of index pairs.\n",
    "    \"\"\"\n",
    "    # map: filename -> index    \n",
    "    \n",
    "    matching_list = []\n",
    "\n",
    "    with h5py.File(pair_probability_file_path, 'r') as f:\n",
    "        for key1 in f.keys():\n",
    "            if key1 not in fname_to_index:\n",
    "                continue\n",
    "            for key2 in f[key1].keys():\n",
    "                if key2 not in fname_to_index:\n",
    "                    continue\n",
    "                idx1, idx2 = fname_to_index[key1], fname_to_index[key2]\n",
    "                if idx1 != idx2:\n",
    "                    matching_list.append(tuple(sorted((idx1, idx2))))\n",
    "\n",
    "    matching_list = sorted(list(set(matching_list)))  # remove duplicates\n",
    "    return matching_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab9686a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:49.105388Z",
     "iopub.status.busy": "2025-05-19T19:16:49.104985Z",
     "iopub.status.idle": "2025-05-19T19:16:49.127435Z",
     "shell.execute_reply": "2025-05-19T19:16:49.126387Z"
    },
    "papermill": {
     "duration": 0.031972,
     "end_time": "2025-05-19T19:16:49.129110",
     "exception": false,
     "start_time": "2025-05-19T19:16:49.097138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def save_loftr_matches(data_path, pairs_info, pairs_info_index, output_path, device_id, model_weight_path=\"weights/outdoor_ds.ckpt\"):\n",
    "    # The default config uses dual-softmax.\n",
    "    # The outdoor and indoor models share the same config.\n",
    "    # You can change the default values like thr and coarse_match_type.\n",
    "\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "\n",
    "    matcher = LoFTR(config=default_cfg)\n",
    "    matcher.load_state_dict(torch.load(model_weight_path, weights_only=False)['state_dict'])\n",
    "    matcher = matcher.eval().to(device)\n",
    "\n",
    "    img_size = 1024\n",
    "    df = 8\n",
    "    padding = True\n",
    "    print(\"generating LOFTR\")\n",
    "    for _pairs_info, idx in tqdm(zip(pairs_info, pairs_info_index), total=len(pairs_info_index), desc=f\"Running LOFTR [GPU{device_id}]\"):\n",
    "\n",
    "        output_file_path = output_path / \"loftr_match\" / f\"{idx}.npy\"\n",
    "        if output_file_path.exists():\n",
    "            continue\n",
    "\n",
    "        name0, name1, _, _, _ = _pairs_info\n",
    "\n",
    "        img0_pth = data_path / name0\n",
    "        img1_pth = data_path / name1\n",
    "        img0_raw, mask0 = read_image(str(img0_pth), img_size, df, padding)\n",
    "        img1_raw, mask1 = read_image(str(img1_pth), img_size, df, padding)\n",
    "        img0 = torch.from_numpy(img0_raw).to(device)\n",
    "        img1 = torch.from_numpy(img1_raw).to(device)\n",
    "        mask0 = torch.from_numpy(mask0).to(device)\n",
    "        mask1 = torch.from_numpy(mask1).to(device)\n",
    "        batch = {'image0': img0, 'image1': img1, 'mask0': mask0, 'mask1':mask1}\n",
    "\n",
    "        # Inference with LoFTR and get prediction\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            matcher(batch)\n",
    "            mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "            mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "            mconf = batch['mconf'].cpu().numpy()\n",
    "            np.save(output_file_path, {\"kpt0\": mkpts0, \"kpt1\": mkpts1, \"conf\": mconf})\n",
    "\n",
    "\n",
    "def doppelgangers_classifier(cfg, pretrained_model_path, pair_path, drop_score = 0.2):\n",
    "    # basic setup\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # initial dataset\n",
    "    data_lib = importlib.import_module(cfg.data.type)\n",
    "    loaders = data_lib.get_data_loaders(cfg.data)\n",
    "    test_loader = loaders[\"test_loader\"]\n",
    "\n",
    "    # initial model\n",
    "    decoder_lib = importlib.import_module(cfg.models.decoder.type)\n",
    "    decoder = decoder_lib.decoder(cfg.models.decoder)\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "    # load pretrained model\n",
    "    ckpt = torch.load(pretrained_model_path, weights_only=False)\n",
    "    new_ckpt = deepcopy(ckpt[\"dec\"])\n",
    "    for key, _ in ckpt[\"dec\"].items():\n",
    "        if \"module.\" in key:\n",
    "            new_ckpt[key[len(\"module.\"):]] = new_ckpt.pop(key)\n",
    "    decoder.load_state_dict(new_ckpt, strict=True)\n",
    "\n",
    "    # evaluate on test set\n",
    "    decoder.eval()\n",
    "    prob_list = list()\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Running doppelgangers\"):\n",
    "\n",
    "            # TTA\n",
    "            img = data[\"image\"].cuda()\n",
    "\n",
    "            img_list = []\n",
    "            img_list.append(img)\n",
    "            # lrflip\n",
    "            # img = torch.flip(img, [3])\n",
    "            # img_list.append(img)\n",
    "            # stack\n",
    "            img = torch.cat(img_list)\n",
    "\n",
    "            logits = decoder(img)\n",
    "            prob = torch.nn.functional.softmax(logits, dim=1)\n",
    "            prob = torch.mean(prob, dim=0)\n",
    "            prob_list.append(prob[1].detach().cpu().numpy())\n",
    "\n",
    "    y_scores = np.array(prob_list)\n",
    "\n",
    "    pairs_info = np.load(pair_path, allow_pickle=True)\n",
    "    pair_probability_file_path = cfg.data.output_path / \"pair_probability_list.h5\"\n",
    "    with h5py.File(pair_probability_file_path, mode=\"w\") as f_matches:\n",
    "\n",
    "        for idx in range(pairs_info.shape[0]):\n",
    "\n",
    "            key1, key2, _, _, _ = pairs_info[idx]\n",
    "            score = y_scores[idx]\n",
    "            if(score > drop_score):\n",
    "                group  = f_matches.require_group(key1)\n",
    "                group.create_dataset(key2, data=score)\n",
    "\n",
    "    return pair_probability_file_path\n",
    "\n",
    "def create_image_pair_list(matches_path, output_path):\n",
    "\n",
    "    dummy = 0\n",
    "    pairs_list = []\n",
    "    with h5py.File(matches_path, mode=\"r\") as f_matches:\n",
    "\n",
    "        for key1 in f_matches.keys():\n",
    "            group = f_matches[key1]\n",
    "            for key2 in group.keys():\n",
    "                pairs_list.append([key1, key2, dummy, dummy, dummy])\n",
    "\n",
    "    pairs_list = np.concatenate(pairs_list, axis=0).reshape(-1, 5)\n",
    "\n",
    "    pairs_list_path = output_path / \"pairs_list.npy\"\n",
    "    np.save(pairs_list_path, pairs_list)\n",
    "\n",
    "    return pairs_list_path\n",
    "\n",
    "def create_image_pair_list_frompairs(images, index_pairs, output_path):\n",
    "\n",
    "    dummy = 0\n",
    "    pairs_list = []\n",
    "    \n",
    "    for i, pair in enumerate(index_pairs):\n",
    "        key1 = Path(images[pair[0]]).name\n",
    "        key2 = Path(images[pair[1]]).name\n",
    "        pairs_list.append([key1, key2, dummy, dummy, dummy])\n",
    "    pairs_list = np.concatenate(pairs_list, axis=0).reshape(-1, 5)\n",
    "    # print(pairs_list)\n",
    "    pairs_list_path = output_path / \"pairs_list.npy\"\n",
    "    np.save(pairs_list_path, pairs_list)\n",
    "\n",
    "    return pairs_list_path\n",
    "\n",
    "def exec_doppelgangers_classifier(\n",
    "    images_dir,\n",
    "    images,\n",
    "    feature_dir,\n",
    "    index_pairs,\n",
    "    num_gpus,\n",
    "    loftr_weight_path,\n",
    "    doppelgangers_weight_path,\n",
    "):\n",
    "    print(feature_dir)\n",
    "    loftr_matches_path = feature_dir / \"loftr_match\"\n",
    "    loftr_matches_path.mkdir(parents=True, exist_ok=True)\n",
    "    pair_path = create_image_pair_list_frompairs(images, index_pairs, feature_dir)\n",
    "    pairs_info = np.load(pair_path, allow_pickle=True)\n",
    "    pairs_info_index = np.arange(pairs_info.shape[0])\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(\n",
    "            save_loftr_matches,\n",
    "            itertools.repeat(images_dir),\n",
    "            np.array_split(pairs_info, num_gpus),\n",
    "            np.array_split(pairs_info_index, num_gpus),\n",
    "            itertools.repeat(feature_dir),\n",
    "            range(num_gpus),\n",
    "            itertools.repeat(loftr_weight_path),\n",
    "        )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    config = {\n",
    "        \"data\": {\n",
    "            \"image_dir\": images_dir,\n",
    "            \"loftr_match_dir\": loftr_matches_path,\n",
    "            \"output_path\": feature_dir,\n",
    "            \"type\": \"doppelgangers.datasets.sfm_disambiguation_dataset\",\n",
    "            \"num_workers\": 1,\n",
    "            \"test\": {\n",
    "                \"batch_size\": 1,\n",
    "                \"img_size\": 1024,\n",
    "                \"pair_path\": pair_path,\n",
    "            },\n",
    "        },\n",
    "        \"models\": {\n",
    "            \"decoder\": {\n",
    "                \"type\": \"doppelgangers.models.cnn_classifier\",\n",
    "                \"input_dim\": 10,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def dict2namespace(config):\n",
    "        namespace = argparse.Namespace()\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, dict):\n",
    "                new_value = dict2namespace(value)\n",
    "            else:\n",
    "                new_value = value\n",
    "            setattr(namespace, key, new_value)\n",
    "        return namespace\n",
    "    \n",
    "    config = dict2namespace(config)\n",
    "\n",
    "    # Running Doppelgangers classifier model on image pairs\n",
    "    print(\"Running Doppelgangers classifier model on image pairs\")\n",
    "    pair_probability_file_path = doppelgangers_classifier(config, doppelgangers_weight_path, pair_path)\n",
    "    \n",
    "    shutil.rmtree(loftr_matches_path)\n",
    "\n",
    "    return pair_probability_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5914a566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:49.142978Z",
     "iopub.status.busy": "2025-05-19T19:16:49.142644Z",
     "iopub.status.idle": "2025-05-19T19:16:49.317121Z",
     "shell.execute_reply": "2025-05-19T19:16:49.316046Z"
    },
    "papermill": {
     "duration": 0.183322,
     "end_time": "2025-05-19T19:16:49.318892",
     "exception": false,
     "start_time": "2025-05-19T19:16:49.135570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Collect vital info from the dataset\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n",
    "\n",
    "# Set is_train=True to run the notebook on the training data.\n",
    "# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\n",
    "is_train = True\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "samples = {}\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "for _, row in competition_data.iterrows():\n",
    "    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01301eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:16:49.332634Z",
     "iopub.status.busy": "2025-05-19T19:16:49.332296Z",
     "iopub.status.idle": "2025-05-19T19:29:46.880435Z",
     "shell.execute_reply": "2025-05-19T19:29:46.878866Z"
    },
    "papermill": {
     "duration": 777.557765,
     "end_time": "2025-05-19T19:29:46.882866",
     "exception": false,
     "start_time": "2025-05-19T19:16:49.325101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting on device cpu\n",
      "Skipping \"imc2023_haiper\"\n",
      "Skipping \"imc2023_heritage\"\n",
      "Skipping \"imc2023_theather_imc2024_church\"\n",
      "Skipping \"imc2024_dioscuri_baalshamin\"\n",
      "Skipping \"imc2024_lizard_pond\"\n",
      "Skipping \"pt_brandenburg_british_buckingham\"\n",
      "Skipping \"pt_piazzasanmarco_grandplace\"\n",
      "Skipping \"pt_sacrecoeur_trevi_tajmahal\"\n",
      "Skipping \"pt_stpeters_stpauls\"\n",
      "Skipping \"amy_gardens\"\n",
      "Skipping \"fbk_vineyard\"\n",
      "Skipping \"ETs\"\n",
      "\n",
      "Processing dataset \"stairs\": 51 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:34<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix Statistics:\n",
      "Min:  0.1598\n",
      "Max:  0.4240\n",
      "Mean: 0.2807\n",
      "Std:  0.0452\n",
      "20%:  0.2433\n",
      "30%:  0.2557\n",
      "USED 50%:  0.2767\n",
      "75%:  0.3089\n",
      "Shortlisting. Number of pairs to match: 647. Done in 36.5036 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [12:19<00:00, 14.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features detected in 740.3003 sec\n",
      "/kaggle/working/result/featureout/stairs\n",
      "number sections must be larger than 0.\n",
      "Dataset \"stairs\" -> Failed!\n",
      "\n",
      "Results\n",
      "Dataset \"stairs\" -> Failed!\n",
      "\n",
      "Timings\n",
      "shortlisting -> total=36.50 sec.\n",
      "feature_detection -> total=740.30 sec.\n",
      "feature_matching -> total=0.00 sec.\n",
      "image doppelgangers processing -> total=0.00 sec.\n",
      "RANSAC -> total=0.00 sec.\n",
      "Reconstruction -> total=0.00 sec.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t# 'amy_gardens',\n",
    "    \t# 'ETs',\n",
    "    \t# 'fbk_vineyard',\n",
    "    \t'stairs', \n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\":[],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\":[],\n",
    "    \"image doppelgangers processing\":[],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "\n",
    "print (f\"Extracting on device {device}\")\n",
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "    try:\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.5, # should be strict\n",
    "            min_pairs = 10, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "    \n",
    "        t = time()\n",
    "\n",
    "        detect_aliked(images, feature_dir, 8192, device=device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "\n",
    "        t = time()\n",
    "        pair_probability_file_path = exec_doppelgangers_classifier(\n",
    "            Path(images_dir),\n",
    "            images,\n",
    "            Path(feature_dir),\n",
    "            index_pairs,\n",
    "            Config.num_gpus,\n",
    "            **Config.doppelgangers_classifier_args,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        updated_index_pairs = extract_matching_list_from_h5(pair_probability_file_path, filename_to_index)\n",
    "        print(f\"updates matches with LOFTR from {len(index_pairs)} to {len(updated_index_pairs)}\")\n",
    "        timings['image doppelgangers processing'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        t = time()\n",
    "        match_with_lightglue(images, updated_index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        # match_with_lightglue_and_cluster(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'Ran RANSAC in {time() - t:.4f} sec')\n",
    "        \n",
    "        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "        # Lower it to 3.\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = 5\n",
    "        mapper_options.max_num_models = 25\n",
    "        mapper_options.mapper.filter_max_reproj_error\t = 6.0\n",
    "        # mapper_options.min_num_matches\t = 50\n",
    "        # mapper_options.ba_local_max_num_iterations = 100\n",
    "        # mapper_options.ba_local_num_images = 10\n",
    "        mapper_options.ba_global_images_freq = 5\n",
    "        \n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path, \n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options)\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction done in  {time() - t:.4f} sec')\n",
    "        print(maps)\n",
    "\n",
    "        # clear_output(wait=False)\n",
    "    \n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            img_list =[]\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                img_list.append(image.name)\n",
    "                registered += 1\n",
    "            img_list_str = ' '.join(img_list) \n",
    "            print(f\"map_index = {map_index}\", img_list_str)\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0ff150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:29:46.912758Z",
     "iopub.status.busy": "2025-05-19T19:29:46.912395Z",
     "iopub.status.idle": "2025-05-19T19:29:47.099880Z",
     "shell.execute_reply": "2025-05-19T19:29:47.098323Z"
    },
    "papermill": {
     "duration": 0.203987,
     "end_time": "2025-05-19T19:29:47.101951",
     "exception": false,
     "start_time": "2025-05-19T19:29:46.897964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset,scene,image,rotation_matrix,translation_vector\r\n",
      "imc2023_haiper,outliers,fountain_image_116.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_108.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_101.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_082.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_071.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_025.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_000.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_007.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n",
      "imc2023_haiper,outliers,fountain_image_012.png,nan;nan;nan;nan;nan;nan;nan;nan;nan,nan;nan;nan\r\n"
     ]
    }
   ],
   "source": [
    "# Must Create a submission file.\n",
    "\n",
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6b322e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T19:29:47.128075Z",
     "iopub.status.busy": "2025-05-19T19:29:47.127681Z",
     "iopub.status.idle": "2025-05-19T19:29:47.213784Z",
     "shell.execute_reply": "2025-05-19T19:29:47.212452Z"
    },
    "papermill": {
     "duration": 0.101183,
     "end_time": "2025-05-19T19:29:47.215647",
     "exception": false,
     "start_time": "2025-05-19T19:29:47.114464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imc2023_haiper: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2023_heritage: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2023_theather_imc2024_church: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_dioscuri_baalshamin: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "imc2024_lizard_pond: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_brandenburg_british_buckingham: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_piazzasanmarco_grandplace: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_sacrecoeur_trevi_tajmahal: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "pt_stpeters_stpauls: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "amy_gardens: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "fbk_vineyard: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "ETs: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "stairs: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "Average over all datasets: score=0.00% (mAA=0.00%, clusterness=0.00%)\n",
      "Computed metric in: 0.08 sec.\n"
     ]
    }
   ],
   "source": [
    "# Definitely Compute results if running on the training set.\n",
    "# Do not do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431f492",
   "metadata": {
    "papermill": {
     "duration": 0.012239,
     "end_time": "2025-05-19T19:29:47.241052",
     "exception": false,
     "start_time": "2025-05-19T19:29:47.228813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "isSourceIdPinned": false,
     "sourceId": 91498,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6988459,
     "sourceId": 11217117,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7347439,
     "sourceId": 11705628,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 238214639,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322,
     "modelInstanceId": 2742,
     "sourceId": 3840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 21716,
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 22086,
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 823.677992,
   "end_time": "2025-05-19T19:29:50.495073",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-19T19:16:06.817081",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
